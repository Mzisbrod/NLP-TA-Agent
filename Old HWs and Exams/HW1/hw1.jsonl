{"Title": "In this assignment you will build a trigram language model in Python.\nYou will complete the code provided in the file trigram_model.py Download trigram_model.py. The main component of the language model will be implemented in the class TrigramModel. Parts of this class have already been provided for you and are explained below.\nOne important idea behind implementing language models is that the probability distributions are not precomputed. Instead, the model only stores the raw counts of n-gram occurrences and then computes the probabilities on demand. This makes smoothing possible.\nThe data you will work with is available in a single zip file here: hw1_data.zip Download hw1_data.zip.Download .There are two data sets in this zip file, which are described below in more detail.\nWhat you need to submit: Submit only your trigram_model.py file. Do not submit the data files.",
  "Question": None,
  "Answer": None
}
{"Title": "Extracting n-grams from a sentence (20 pts)",
  "Question": "1. Complete the function get_ngrams, which takes a list of strings and an integer n as input, and returns padded n-grams over the list of strings. The result should be a list of Python tuples. For example:\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],1)\n[('START',), ('natural',), ('language',), ('processing',), ('STOP',)]\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],2)\n('START', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'STOP')]\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],3)\n[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]",
  "Answer": "def get_ngrams(sequence, n):\n    '''\n    COMPLETE THIS FUNCTION (PART 1)\n    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n    This should work for arbitrary values of 1 <= n < len(sequence).\n    '''\n\n    if n > 1:\n        sequence = ['START'] * (n - 1) + sequence\n    else:\n        sequence = ['START'] + sequence\n    sequence += ['STOP']\n    ngrams = []\n    for i in range(len(sequence) - n + 1):\n        ngrams.append(tuple(sequence[i:i + n]))\n    return ngrams"
}
{"Title": "Counting n-grams in a corpus (15 pts)",
  "Question": "2. We will work with two different data sets. The first data set is the Brown corpus, which is a sample of American written English collected in the 1950s. The format of the data is a plain text file brown_train.txt, containing one sentence per line. Each sentence has already been tokenized. For this assignment, no further preprocessing is necessary.\nDon't touch brown_test.txt yet. We will use this data to compute the perplexity of our language model.\nReading the Corpus and Dealing with Unseen Words\nThis part has been implemented for you and are explained in this section. Take a look at the function corpus_readerin trigram_model.py. This function takes the name of a text file as a parameter and returns a Python generator object. Generators allow you to iterate over a collection, one item at a time without ever having to represent the entire data set in a data structure (such as a list). This is a form of lazy evaluation. You could use this function as follows:\n>>> generator = corpus_reader(\"\")\n>>> for sentence in generator:\n             print(sentence)\n\n['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n['the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n['the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan', 'allen', 'jr', '&', '.']\n...\nNote that iterating over this generator object works only once. After you are done, you need to create a new generator to do it again.\nAs discussed in class, there are two sources of data sparseness when working with language models: Completely unseen words and unseen contexts. One way to deal with unseen words is to use a pre-defined lexicon before we extract ngrams. The function corpus_reader has an optional parameter lexicon, which should be a Python set containing a list of tokens in the lexicon. All tokens that are not in the lexicon will be replaced with a special \"UNK\" token.\nInstead of pre-defining a lexicon, we collect one from the training corpus. This is the purpose of the function get_lexicon(corpus). This function takes a corpus iterarator (as returned by corpus_reader) as a parameter and returns a set of all words that appear in the corpus more than once. The idea is that words that appear only once are so rare that they are a good stand-in for words that have not been seen at all in unseen text. You do not have to modify this function.\nNow take a look at the __init__ method of TrigramModel (the constructor):\n    def __init__(self, corpusfile):\n\n        # Iterate through the corpus once to build a lexicon\n        generator = corpus_reader(corpusfile)\n        self.lexicon = get_lexicon(generator)\n        self.lexicon.add('UNK')\n        self.lexicon.add('START')\n        self.lexicon.add('STOP')\n        self.num_sentences = 0  # keep track of the number of sentences\n\n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self.lexicon)\n        self.count_ngrams(generator)\n        self.total_num_words = sum(count for gram, count in self.unigramcounts.items()) - self.unigramcounts[('START',)] \nWhen a new TrigramModel is created, we pass in the filename of a corpus file. We then iterate through the corpus twice: once to collect the lexicon, and once to count n-grams. You will implement the method to count n-grams in the next step.\nNow it's your turn again. In this step, you will implement the method count_ngrams that should count the occurrence frequencies for ngrams in the corpus. The method already creates three instance variables of TrigramModel, which store the unigram, bigram, and trigram counts in the corpus. Each variable is a dictionary (a hash map) that maps the n-gram to its count in the corpus.\nFor example, after populating these dictionaries, we want to be able to query\n>>> model.trigramcounts[('START','START','the')]\n5478\n>>> model.bigramcounts[('START','the')]\n5478\n>>> model.unigramcounts[('the',)]\n61428\nWhere model is an instance of TrigramModel that has been trained on a corpus. Note that the unigrams are represented as one-element tuples (indicated by the , in the end). Note that the actual numbers might be slightly different depending on how you set things up.",
  "Answer": "    def count_ngrams(self, corpus):\n        '''\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts.\n        '''\n\n        self.unigramcounts = defaultdict(int)  # might want to use defaultdict or Counter instead\n        self.bigramcounts = defaultdict(int)\n        self.trigramcounts = defaultdict(int)\n\n        ##Your code here\n        for sent in corpus:\n            self.num_sentences += 1\n            unigrams = get_ngrams(sent, 1)\n            for gram in unigrams:\n                self.unigramcounts[gram] += 1\n            bigrams = get_ngrams(sent, 2)\n            for gram in bigrams:\n                self.bigramcounts[gram] += 1\n            trigrams = get_ngrams(sent, 3)\n            for gram in trigrams:\n                self.trigramcounts[gram] += 1"
}
{"Title": "Raw n-gram probabilities (15 pts)",
  "Question": "3. Write the methods raw_trigram_probability(trigram),  raw_bigram_probability(bigram), and raw_unigram_probability(unigram).\nEach of these methods should return an unsmoothed probability computed from the trigram, bigram, and unigram counts. This part is easy, except that you also need to keep track of the total number of words in order to compute the unigram probabilities.\nIt's expected that many of the n-gram probabilities are 0, that is the ngram sequence has not been observed during training. One issue you will encounter is the case if which you have a trigram u,w,v   where  count(u,w,v) = 0 but count(u,w) is also 0. In that case, it is not immediately clear what P(v | u,w) should be. My recommendation is to make P(v | u,w) = 1 / |V|  (where |V| is the size of the lexicon), if count(u,w) is 0. That is, if the context for a trigram is unseen, the distribution over all possible words in that context is uniform.  Another option would be to use the unigram probability for v, so P(v | u,w) = P(v).\nInterlude - Generating text (OPTIONAL)\nThis part is a little trickier. Write the method generate_sentence, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\",\"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each.\nDraw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated. Here are some examples for how this method should behave:\nmodel.generate_sentence()\n['the', 'last', 'tread', ',', 'mama', 'did', 'mention', 'to', 'the', 'opposing', 'sector', 'of', 'our', 'natural', 'resources', '.', 'STOP']\n>>> model.generate_sentence()\n['the', 'specific', 'group', 'which', 'caused', 'this', 'to', 'fundamentals', 'and', 'each', 'berated', 'the', 'other', 'resident', '.', 'STOP']\nThe optional t parameter of the method specifies the maximum sequence length so that no more tokens are generated if the \"STOP\" token is not reached before t words.",
  "Answer": "    def raw_trigram_probability(self, trigram):\n        '''\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        '''\n        context = (trigram[0], trigram[1])\n        if context == ('START', 'START'):\n            # P(START, START, w) = P(START | w)\n            return self.raw_bigram_probability((trigram[1], trigram[2]))\n\n        trigram_count = self.trigramcounts[trigram]\n\n        if trigram_count == 0:\n            # We don't know anything about the distribution of the trigram[2]\n            # in the context of trigram[0], trigram[1] -- assume\n            #  uniform distribution\n            return 1 / self.total_num_words\n\n        elif self.bigramcounts[context] == 0:\n            return self.raw_unigram_probability(trigram[2])\n\n        return trigram_count / float(self.bigramcounts[context])\n\n    def raw_bigram_probability(self, bigram):\n        '''\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        '''\n        if self.unigramcounts[(bigram[0],)] == 0:\n            return self.bigramcounts[bigram] / float(self.total_num_words)\n\n        return self.bigramcounts[bigram] / float(self.unigramcounts[(bigram[0],)])\n\n    def raw_unigram_probability(self, unigram):\n        '''\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        '''\n\n        # hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once,\n        # store in the TrigramModel instance, and then re-use it.\n        if unigram == ('START',):\n            return 0\n        return self.unigramcounts[unigram] / self.total_num_words"
}
{
  "Title": "Smoothed probabilities (10 pts)",
  "Question": "4. Write the method smoothed_trigram_probability(self, trigram) which uses linear interpolation between the raw trigram, unigram, and bigram probabilities (see lecture for how to compute this). Set the interpolation parameters to lambda1 = lambda2 = lambda3 = 1/3. Use the raw probability methods defined before.",
  "Answer": "    def smoothed_trigram_probability(self, trigram):\n        '''\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation).\n        '''\n        lambda1 = 1 / 3.0\n        lambda2 = 1 / 3.0\n        lambda3 = 1 / 3.0\n        smoothed_prob = lambda1 * self.raw_trigram_probability(trigram) + \\\n                        lambda2 * self.raw_bigram_probability((trigram[1], trigram[2])) + \\\n                        lambda3 * self.raw_unigram_probability((trigram[2],))\n        return smoothed_prob",
}
{
  "Title": "Computing Sentence Probability (10 pts)",
  "Question": "5. Write the method sentence_logprob(sentence), which returns the log probability of an entire sequence (see lecture how to compute this). Use the get_ngrams function to compute trigrams and the smoothed_trigram_probability method to obtain probabilities. Convert each probability into logspace using math.log2. For example:\n>>> math.log2(0.8)\n-0.3219280948873623\nThen, instead of multiplying probabilities, add the log probabilities. Regular probabilities would quickly become too small, leading to numeric issues, so we typically work with log probabilities instead.",
  "Answer": "    def sentence_logprob(self, sentence):\n        '''\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        '''\n        trigrams = get_ngrams(sentence, 3)\n        return sum(math.log2(self.smoothed_trigram_probability(trigram)) for trigram in trigrams)"
}
{
//  "Title": "Perplexity (15 pts)",
  "Question": "6. Write the method perplexity(corpus), which should compute the perplexity of the model on an entire corpus.\nCorpus is a corpus iterator (as returned by the corpus_reader method).\nRecall that the perplexity is defined as 2-l, where l is defined as:\nl=\\frac{1}{M}\\sum\\limits_{i=1}^m \\log P(s_i)\nHere M is the total number of word tokens, while m is the number of sentences in the test corpus. So to compute the perplexity, sum the log probability for each sentence, and then divide by the total number of words tokens in the corpus. For consistency, use the base 2 logarithm.\nRun the perplexity function on the test set for the Brown corpus brown_test.txt (see main section at the bottom of the Python file for how to do this). The perplexity should be less than 400. Also try computing the perplexity on the training data (which should be a lot lower, unsurprisingly).\nThis is a form of intrinsic evaluation.",
  "Answer": "    def perplexity(self, corpus):\n        '''\n        COMPLETE THIS METHOD (PART 6)\n        Returns the log probability of an entire sequence.\n        '''\n        corpus, corpus_copy = itertools.tee(corpus)\n        logprob_all_sents = sum(self.sentence_logprob(sent) for sent in corpus)\n        total_words_in_corpus = sum(len(sent) + 1 for sent in corpus_copy)\n        perp = 2 ** (-logprob_all_sents / float(total_words_in_corpus))\n        return perp"
}
{
  "Title": "Using the Model for Text Classification (15 pts)",
  "Question": "7. In this final part of the problem we will apply the trigram model to a text classification task. We will use a data set of essays written by non-native speakers of English for the ETS TOEFL test. These essays are scored according to skill level low, medium, or high. We will only consider essays that have been scored as \"high\" or \"low\". We will train a different language model on a training set of each category and then use these models to automatically score unseen essays. We compute the perplexity of each language model on each essay. The model with the lower perplexity determines the class of the essay.\nThe files ets_toefl_data/train_high.txt and ets_toefl_data/train_low.txt in the data zip file contain the training data for high and low skill essays, respectively. The directories ets_toefl_data/test_high and ets_toefl_data/test_low contain test essays (one per file) of each category.\nComplete the method essay_scoring_experiment. The method should be called by passing two training text files, and two testing directories (containing text files of individual essays). It returns the accuracy of the prediction.\nThe method already creates two trigram models, reads in the test essays from each directory, and computes the perplexity for each essay. All you have to do is compare the perplexities and the returns the accuracy (correct predictions / total predictions).\nOn the essay data set, you should easily get an accuracy of > 80%.\nData use policy: Note that the ETS data set is proprietary and licensed to Columbia University for research and educational use only (as part of the Linguistic Data Consortium. This data set is extracted from https://catalog.ldc.upenn.edu/LDC2014T06. You may not use or share this data set for any other purpose than for this class.",
  "Answer": "def essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n    # Part 7\n    model1 = TrigramModel(training_file1)\n    model2 = TrigramModel(training_file2)\n\n    total = 0\n    correct = 0\n\n    for f in os.listdir(testdir1):\n        pp1 = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))\n        pp2 = model2.perplexity(corpus_reader(os.path.join(testdir1, f), model2.lexicon))\n        total += 1\n        if pp1 <= pp2:\n            correct += 1\n\n    for f in os.listdir(testdir2):\n        pp1 = model1.perplexity(corpus_reader(os.path.join(testdir2, f), model1.lexicon))\n        pp2 = model2.perplexity(corpus_reader(os.path.join(testdir2, f), model2.lexicon))\n        total += 1\n        if pp2 <= pp1:\n            correct += 1\n    return float(correct) / total"
}