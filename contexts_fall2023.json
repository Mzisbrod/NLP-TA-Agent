[
  "Homework 1. Question 1: Extracting n-grams from a sentence. Complete the function get_ngrams, which takes a list of strings and an integer n as input, and returns padded n-grams over the list of strings. The result should be a list of Python tuples. For example:\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],1)\n[('START',), ('natural',), ('language',), ('processing',), ('STOP',)]\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],2)\n('START', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'STOP')]\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],3)\n[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]. Question 2: Counting n-grams in a corpus. We will work with two different data sets. The first data set is the Brown corpus, which is a sample of American written English collected in the 1950s. The format of the data is a plain text file brown_train.txt, containing one sentence per line. Each sentence has already been tokenized. For this assignment, no further preprocessing is necessary.\nDon't touch brown_test.txt yet. We will use this data to compute the perplexity of our language model.\nReading the Corpus and Dealing with Unseen Words\nThis part has been implemented for you and are explained in this section. Take a look at the function corpus_readerin trigram_model.py. This function takes the name of a text file as a parameter and returns a Python generator object. Generators allow you to iterate over a collection, one item at a time without ever having to represent the entire data set in a data structure (such as a list). This is a form of lazy evaluation. You could use this function as follows:\n>>> generator = corpus_reader(\"\")\n>>> for sentence in generator:\n             print(sentence)\n\n['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n['the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n['the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan', 'allen', 'jr', '&', '.']\n...\nNote that iterating over this generator object works only once. After you are done, you need to create a new generator to do it again.\nAs discussed in class, there are two sources of data sparseness when working with language models: Completely unseen words and unseen contexts. One way to deal with unseen words is to use a pre-defined lexicon before we extract ngrams. The function corpus_reader has an optional parameter lexicon, which should be a Python set containing a list of tokens in the lexicon. All tokens that are not in the lexicon will be replaced with a special \"UNK\" token.\nInstead of pre-defining a lexicon, we collect one from the training corpus. This is the purpose of the function get_lexicon(corpus). This function takes a corpus iterarator (as returned by corpus_reader) as a parameter and returns a set of all words that appear in the corpus more than once. The idea is that words that appear only once are so rare that they are a good stand-in for words that have not been seen at all in unseen text. You do not have to modify this function.\nNow take a look at the __init__ method of TrigramModel (the constructor):\n    def __init__(self, corpusfile):\n\n        # Iterate through the corpus once to build a lexicon\n        generator = corpus_reader(corpusfile)\n        self.lexicon = get_lexicon(generator)\n        self.lexicon.add('UNK')\n        self.lexicon.add('START')\n        self.lexicon.add('STOP')\n        self.num_sentences = 0  # keep track of the number of sentences\n\n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self.lexicon)\n        self.count_ngrams(generator)\n        self.total_num_words = sum(count for gram, count in self.unigramcounts.items()) - self.unigramcounts[('START',)] \nWhen a new TrigramModel is created, we pass in the filename of a corpus file. We then iterate through the corpus twice: once to collect the lexicon, and once to count n-grams. You will implement the method to count n-grams in the next step.\nNow it's your turn again. In this step, you will implement the method count_ngrams that should count the occurrence frequencies for ngrams in the corpus. The method already creates three instance variables of TrigramModel, which store the unigram, bigram, and trigram counts in the corpus. Each variable is a dictionary (a hash map) that maps the n-gram to its count in the corpus.\nFor example, after populating these dictionaries, we want to be able to query\n>>> model.trigramcounts[('START','START','the')]\n5478\n>>> model.bigramcounts[('START','the')]\n5478\n>>> model.unigramcounts[('the',)]\n61428\nWhere model is an instance of TrigramModel that has been trained on a corpus. Note that the unigrams are represented as one-element tuples (indicated by the , in the end). Note that the actual numbers might be slightly different depending on how you set things up. Question 3: Raw n-gram probabilities. Write the methods raw_trigram_probability(trigram),  raw_bigram_probability(bigram), and raw_unigram_probability(unigram).\nEach of these methods should return an unsmoothed probability computed from the trigram, bigram, and unigram counts. This part is easy, except that you also need to keep track of the total number of words in order to compute the unigram probabilities.\nIt's expected that many of the n-gram probabilities are 0, that is the ngram sequence has not been observed during training. One issue you will encounter is the case if which you have a trigram u,w,v   where  count(u,w,v) = 0 but count(u,w) is also 0. In that case, it is not immediately clear what P(v | u,w) should be. My recommendation is to make P(v | u,w) = 1 / |V|  (where |V| is the size of the lexicon), if count(u,w) is 0. That is, if the context for a trigram is unseen, the distribution over all possible words in that context is uniform.  Another option would be to use the unigram probability for v, so P(v | u,w) = P(v).\nInterlude - Generating text (OPTIONAL)\nThis part is a little trickier. Write the method generate_sentence, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\",\"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each.\nDraw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated. Here are some examples for how this method should behave:\nmodel.generate_sentence()\n['the', 'last', 'tread', ',', 'mama', 'did', 'mention', 'to', 'the', 'opposing', 'sector', 'of', 'our', 'natural', 'resources', '.', 'STOP']\n>>> model.generate_sentence()\n['the', 'specific', 'group', 'which', 'caused', 'this', 'to', 'fundamentals', 'and', 'each', 'berated', 'the', 'other', 'resident', '.', 'STOP']\nThe optional t parameter of the method specifies the maximum sequence length so that no more tokens are generated if the \"STOP\" token is not reached before t words. Question 4: Smoothed probabilities. Write the method smoothed_trigram_probability(self, trigram) which uses linear interpolation between the raw trigram, unigram, and bigram probabilities (see lecture for how to compute this). Set the interpolation parameters to lambda1 = lambda2 = lambda3 = 1/3. Use the raw probability methods defined before. Question 5: Computing sentence probability. Write the method sentence_logprob(sentence), which returns the log probability of an entire sequence (see lecture how to compute this). Use the get_ngrams function to compute trigrams and the smoothed_trigram_probability method to obtain probabilities. Convert each probability into logspace using math.log2. For example:\n>>> math.log2(0.8)\n-0.3219280948873623\nThen, instead of multiplying probabilities, add the log probabilities. Regular probabilities would quickly become too small, leading to numeric issues, so we typically work with log probabilities instead. Question 6: Perplexity. Write the method perplexity(corpus), which should compute the perplexity of the model on an entire corpus.\nCorpus is a corpus iterator (as returned by the corpus_reader method).\nRecall that the perplexity is defined as 2-l, where l is defined as:\nl=\\frac{1}{M}\\sum\\limits_{i=1}^m \\log P(s_i)\nHere M is the total number of word tokens, while m is the number of sentences in the test corpus. So to compute the perplexity, sum the log probability for each sentence, and then divide by the total number of words tokens in the corpus. For consistency, use the base 2 logarithm.\nRun the perplexity function on the test set for the Brown corpus brown_test.txt (see main section at the bottom of the Python file for how to do this). The perplexity should be less than 400. Also try computing the perplexity on the training data (which should be a lot lower, unsurprisingly).\nThis is a form of intrinsic evaluation. Question 7: Using the model for text classification. In this final part of the problem we will apply the trigram model to a text classification task. We will use a data set of essays written by non-native speakers of English for the ETS TOEFL test. These essays are scored according to skill level low, medium, or high. We will only consider essays that have been scored as \"high\" or \"low\". We will train a different language model on a training set of each category and then use these models to automatically score unseen essays. We compute the perplexity of each language model on each essay. The model with the lower perplexity determines the class of the essay.\nThe files ets_toefl_data/train_high.txt and ets_toefl_data/train_low.txt in the data zip file contain the training data for high and low skill essays, respectively. The directories ets_toefl_data/test_high and ets_toefl_data/test_low contain test essays (one per file) of each category.\nComplete the method essay_scoring_experiment. The method should be called by passing two training text files, and two testing directories (containing text files of individual essays). It returns the accuracy of the prediction.\nThe method already creates two trigram models, reads in the test essays from each directory, and computes the perplexity for each essay. All you have to do is compare the perplexities and the returns the accuracy (correct predictions / total predictions).\nOn the essay data set, you should easily get an accuracy of > 80%.\nData use policy: Note that the ETS data set is proprietary and licensed to Columbia University for research and educational use only (as part of the Linguistic Data Consortium. This data set is extracted from https://catalog.ldc.upenn.edu/LDC2014T06. You may not use or share this data set for any other purpose than for this class.",
  "Homework 2. Question 1: Reading the grammar and getting started. The class Pcfg represents a PCFG grammar in chomsky normal form. To instantiate a Pcfg object, you need to pass a file object to the constructor, which contains the data. You can then access the instance variables of the Pcfg instance to get information about the rules. The dictionary lhs_to_rules maps left-hand-side (lhs) symbols to lists of rules. Each rule in the list is represented as (lhs, rhs, probability) triple. The rhs_to_rules dictionary contains the same rules as values, but indexed by right-hand-side. Write the method verify_grammar, that checks that the grammar is a valid PCFG in Chomsky Normal form. Specifically you need to verify that each rule corresponds to one of the formats permitted in CNF. To do this, you can assume that the lhs symbols of the rules make up the inventory of nonterminals. Any other symbol should be interpreted as a terminal. \nYou also need to ensure all probabilities for the same lhs symbol sum to 1.0 (approximately). Then change the main section of grammar.py to read in the grammar, print out a confirmation if the grammar is a valid PCFG in CNF or print an error message if it is not. You should now be able to run grammar.py on grammars and verify that they are well formed for the CKY parser. Question 2: Membership checking with CKY. The file cky.py already contains a class CkyParser. When a CkyParser instance is created a grammar instance is passed to the constructor. The instance variable grammar can then be used to access this Pcfg object. Write the method is_in_language(self, tokens) by implementing the CKY algorithm. Your method should read in a list of tokens and return True if the grammar can parse this sentence and False otherwise. While parsing, you will need to access the dictionary self.grammar.rhs_to_rules. You can use any data structure you want to represent the parse table (or read ahead to question 3 of this assignment, where a specific data structure is prescribed). Question 3: Parsing with backpointers. The parsing method in part 2 can identify if a string is in the language of the grammar, but it does not produce a parse tree. It also does not take probabilities into account. You will now extend the parser so that it retrieves the most probable parse for the input sentence, given the PCFG probabilities in the grammar. The first object is parse table containing backpointers, represented as a dictionary (this is more convenient in Python than a 2D array). The keys of the dictionary are spans, for example table[(0,3)]  retrieves the entry for span 0 to 3 from the chart. The values of the dictionary should be dictionaries that map nonterminal symbols to backpointers. For example: table[(0,3)]['NP'] returns the backpointers to the table entries that were used to create the NP phrase over the span 0 and 3. For example, the value of table[(0,3)]['NP'] could be ((\"NP\",0,2),(\"FLIGHTS\",2,3)). This means that the parser has recognized an NP covering the span 0 to 3, consisting of another NP from 0 to 2 and FLIGHTS from 2 to 3. The split recorded in the table at table[(0,3)]['NP'] is the one that results in the most probable parse for the span [0,3] that is rooted in NP. \nTerminal symbols in the table could just be represented as strings. For example the table entry for table[(2,3)][\"FLIGHTS\"] should be \"flights\".\n\nThe second object is similar, but records log probabilities instead of backpointers. For example the value of probs[(0,3)]['NP'] might be -12.1324. This value represents the log probability of the best parse tree (according to the grammar) for the span 0,3 that results in an NP. During parsing, when you fill an entry on the backpointer parse table and iterate throught the possible splits for a (span/nonterminal) combination, that entry on the table will contain the back-pointers for the the current-best split you have found so far. For each new possible split, you need to check if that split would produce a higher log probability. If so, you update the entry in the backpointer table, as well as the entry in the probability table. \n\nAfter parsing has finished, the table entry table[0,len(toks)][grammar.startsymbol] will contain the best backpointers for the left and right subtree under the root node. probs[0,len(toks)][grammar.startsymbol] will contain the total log-probability for the best parse. \n\ncky.py contains two test functions check_table_format(table) and check_prob_format(probs) that you can use to make sure the two table data structures are formatted correctly. Both functions should return True. Note that passing this test does not guarantee that the content of the tables is correct, just that the data structures are probably formatted correctly. Write the method parse_with_backpointers(self, tokens). You should modify your CKY implementation from part 2, but use (and return) specific data structures. The method should take a list of tokens as input and returns a) the parse table b) a probability table. Both objects should be constructed during parsing. They replace whatever table data structure you used in part 2. Question 4: Retrieving a parse tree. You now have a working parser, but in order to evaluate its performance we still need to reconstruct a parse tree from the backpointer table returned by parse_with_backpointers. Note that the intended format is the same as the data in the treebank. Each tree is represented as tuple where the first element is the parent node and the remaining elements are children. Each child is either a tree or a terminal string. \n\nHint: Recursively traverse the parse chart to assemble this tree. Write the function get_tree(chart, i,j, nt) which should return the parse-tree rooted in non-terminal nt and covering span i,j. Question 5: Evaluating the parser. The program evaluate_parser.py evaluates your parser by comparing the output trees to the trees in the test data set. The program imports your CKY parser class. It then reads in the test file line-by-line. For each test tree, it extracts the yield of the tree (i.e. the list of leaf nodes). It then feeds this list as input to your parser, obtains a parse chart, and then retrieves the predicted tree by calling your get_tree method. It then compares the predicted tree against the target tree in the following way (this is a standard approach to evaluating constiuency parsers, called PARSEVAL): \nFirst it obtains a set of the spans in each tree (including the nonterminal label). It then computes precision, recall, and F-score (which is the harmonic mean between precision and recall) between these two sets. The script finally reports the coverage (percentage of test sentences that had any parse), the average F-score for parsed sentences, and the average F-score for all sentences (including 0 f-scores for unparsed sentences). \n\n What should the results on the atis3 test corpus be?",
  "Homework 3. Question 1: Obtaining the Vocabulary. In this assignment you will train a feed-forward neural network to predict the transitions of an arc-standard dependency parser. The input to this network will be a representation of the current state (including words on the stack and buffer). The output will be a transition (shift, left_arc, right_arc), together with a dependency relation label.\nMuch of the parser code  is provided, but you will need to implement the input representation for the neural net, decode the output of the network, and also specify the network architecture and train the model. The first 5 entries are special symbols. <CD> stands for any number (anything tagged with the POS tag CD), <NNP> stands for any proper name (anything tagged with the POS tag NNP). <UNK> stands for unknown words (in the training data, any word that appears only once). <ROOT> is a special root symbol (the word associated with the word 0, which is initially placed on the stack of the dependency parser). <NULL> is used to pad context windows. Run the following to generate an index of words and POS indices: $python get_vocab.py data/train.conll data/words.vocab data/pos.vocab\n This contains all words that appear more than once in the training data. What will the words look like? Question 2: Extracting Input/Output matrices for training. To train the neural network we first need to obtain a set of input/output training pairs. More specifically, each training example should be a pair (x,y), where x is a parser state and y is the transition the parser should make in that state.\n\nTake a look at the file extract_training_data.py \nStates: The input will be an instance of the class State, which represents a parser state. The attributes of this class consist of a stack, buffer, and partially built dependency structure deps. stack and buffer are lists of word ids (integers).\nThe top of the stack is the last word in the list stack[-1]. The next word on the buffer is also the last word in the list, buffer[-1].\nDeps is a list of (parent, child, relation) triples, where parent and child are integer ids and relation is a string (the dependency label). \n\nTransitions: The output is a pair (transition, label), where the transition can be one of \"shift\", \"left_arc\", or \"right_arc\" and the label is a dependency label. If the transition is \"shift\", the dependency label is None. Since there are 45 dependency relations (see list deps_relations), there are 45*2+1 possible outputs. \n\nObtaining oracle transitions and a sequence of input/output examples. \nAs discussed in class, we cannot observe the transitions directly from the treebank. We only see the resulting dependency structures. We therefore need to convert the trees into a sequence of (state, transition) pairs that we use for training. That part has already been implemented in the function get_training_instances(dep_structure). Given a DependencyStructure instance, this method returns a list of (State, Transition) pairs in the format described above. Your task will be to convert the input/output pairs into a representation suitable for the neural network. You will complete the method get_input_representation(self, words, pos, state) in the class FeatureExtractor. The constructor of the class FeatureExtractor takes the two vocabulary files as inputs (file objects). It then stores a word-to-index dictionary in the attribute word_vocab and POS-to-index dictionary in the attribute pos_vocab. Implement get_input_representation(self, words, pos, state), tt takes as parameters a list of words in the input sentence, a list of POS tags in the input sentence and an instance of class State. It should return an encoding of the input to the neural network, i.e. a single vector. Write also get_output_representation(self, output_pair), which takes a (transition, label) pair as parameter and return a one-hot representation of these actions. Question 3: Designing and Training the network. Network topology Now that we have training data, we can build the actual neural net. In the file train_model.py, write the function build_model(word_types, pos_types, outputs). word_types is the number of possible words, pos_types is the number of possible POS, and outputs is the size of the output vector. Start by building a network as follows:\n\nOne Embedding layer, the input_dimension should be the number possible words, the input_length is the number of words using this same embedding layer. This should be 6, because we use the 3 top-word on the stack and the 3 next words on the buffer. \nThe output_dim of the embedding layer should be 32.\nA Dense hidden layer of 100 units using relu activation. (note that you want to Flatten the output of the embedding layer first).  \nA Dense hidden layer of 10 units using relu activation. \nAn output layer using softmax activation.\nFinally, the method should prepare the model for training, in this case using categorical crossentropy  as the loss and the Adam optimizer with a learning rate of 0.01. Complete the build_model function that takes in word_types, pos_types, outputs and returns the model. Question 4: Greedy Parsing Algorithm - Building and Evaluating the Parser. We will now use the trained model to construct a parser. In the file decoder.py, take a look at the class Parser. The class constructor takes the name of a keras model file, loads the model and stores it in the attribute model. It also uses the feature extractor from question 2. The algorithm is the standard transition-based algorithm. As long as the buffer is not empty, we use the feature extractor to obtain a representation of the current state. We then call model.predict(features) and retrieve a softmax actived vector of possible actions. Write the method parse_sentence(self, words, pos), which takes as parameters a list of words and POS tags in the input sentence. The method will return an instance of DependencyStructure.",
  "Homework 4. Question 1: Candidate Synonyms from WordNet. In this assignment you will work on a lexical substitution task, using, WordNet, pre-trained Word2Vec embeddings, and BERT. This task was first proposed as a shared task at SemEval 2007 Task 10\n\nIn this task, the goal is to find lexical substitutes for individual target words in context. For example, given the following sentence:\n\n\"Anyway , my pants are getting tighter every day .\" \n\nthe goal is to propose an alternative word for tight, such that the meaning of the sentence is preserved. Such a substitute could be constricting, small or uncomfortable.\n\nIn the sentence\n\n\"If your money is tight don't cut corners .\" \n\nthe substitute small would not fit, and instead possible substitutes include scarce, sparse, limitited, constricted. You will implement a number of basic approaches to this problem and compare their performance. Write the function get_candidates(lemma, pos) that takes a lemma and part of speech ('a','n','v','r' for adjective, noun, verb, or adverb) as parameters and returns a set of possible substitutes. To do this, look up the lemma and part of speech in WordNet and retrieve all synsets that the lemma appears in. Then obtain all lemmas that appear in any of these synsets. Make sure that the output does not contain the input lemma itself. The output can contain multiword expressions such as \"turn around\". WordNet will represent such lemmas as \"turn_around\", so you need to remove the _. Question 2: WordNet Frequency Baseline. Write the function wn_frequency_predictor(context) that takes a context object as input and predicts the possible synonym with the highest total occurence frequency (according to WordNet). Note that you have to sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets. You can use the get_candidates method or just duplicate the code for finding candidate synonyms (this is possibly more convenient). Using this simple baseline should give you about 10% precision and recall. Take a look at the output to see what kinds of mistakes the system makes. Write the function wn_frequency_predictor(context) that takes a context object as input and predicts the possible synonym with the highest total occurence frequency (according to WordNet). Quesiton 3: Simple Lesk Algorithm. To perform WSD, implement the simple Lesk algorithm. Look at all possible synsets that the target word apperas in. Compute the overlap between the definition of the synset and the context of the target word. You may want to remove stopwords (function words that don't tell you anything about a word's semantics). Implement the function wn_simple_lesk_predictor(context). This function uses Word Sense Disambiguation (WSD) to select a synset for the target word. It should then return the most frequent synonym from that synset as a substitute. Question 4: Most Similar Synonym. You will now implement approaches based on Word2Vec embeddings. These will be implemented as methods in the class Word2VecSubst. The reason these are methods is that the Word2VecSubst instance can store the word2vec model as an instance variable. The constructor for the class Word2VecSubst already includes code to load the model. You may need to change the value of W2VMODEL_FILENAME to point to the correct file. Write the method predict_nearest(context) that should first obtain a set of possible synonyms from WordNet (either using the method from part 1 or you can rewrite this code as you see fit), and then return the synonym that is most similar to the target word, according to the Word2Vec embeddings. Question 5: Using BERT's masked language model. BERT is trained on a masked language model objective, that is, given some input sentence with some words replaced with a [MASK] symbol, the model tries to predict the best words to fit the masked positions. In this way, BERT can make use of both the left and right context to learn word representations (unlike ELMo and GPT).\n\nWe are going to use a pre-trained BERT model. Typically, BERT is used to compute contextualized word embeddings, or a dense sentence representation. In these applications, the masked LM layer, which uses the contextualized embeddings to predict masked words, is removed and only the embeddings are used.  However, we are going to use the masked language model as-is.\n\nThe basic idea is that we can feed the context sentence into BERT and it will tell us \"good\" replacements for the target word. The output will be a vector of length |V| for each word, containing a score for each possible output word. There are two possible approaches: We can either leave the target word in the input, or replace it with the [MASK] symbol. lexub_xml.Context works as follows: Obtain a set of candidate synonyms (for example, by calling get_candidates), Convert the information in context into a suitable masked input representation for the DistilBERT model. Make sure you store the index of the masked target word (the position of the [MASK] token), Run the DistilBERT model on the input representation, Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target position (i.e. the position of the masked word). Return this word. Write the method predict(context) in the class BertPredictor, where context is an instance of lexsub_xml. Question 6: Other ideas?. By now you should have realized that the lexical substitution task is far from trivial. Implement your own refinements to the approaches proposed above, or even a completely different approach. Any small improvement (or conceptually interesting approach) will do to get credit for part 6.",
  "Homework 5. Question 1: 1.1. We need to be able to maintain the correct labels (B-I-O tags) for each of the subwords. Complete the following function that takes a list of tokens and a list of B-I-O labels of the same length as parameters, and returns a new token / label pair, as illustrated in the following example. >>> tokenize_with_labels(\"the fancyful penguin devoured yummy fish .\".split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\".split(), tokenizer)\n(['the',\n  'fancy',\n  '##ful',\n  'penguin',\n  'dev',\n  '##oured',\n  'yu',\n  '##mmy',\n  'fish',\n  '.'],\n ['B-ARG0',\n  'I-ARG0',\n  'I-ARG0',\n  'I-ARG0',\n  'B-V',\n  'I-V',\n  'B-ARG1',\n  'I-ARG1',\n  'I-ARG1',\n  'O']). def tokenize_with_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n    Word piece tokenization makes it difficult to match word labels\n    back up with individual word pieces. This function tokenizes each\n    word one at a time so that it is easier to preserve the correct\n    label for each subword. It is, of course, a bit slower in processing\n    time, but it will help our model achieve higher accuracy.\n    \"\"\"\n\n    tokenized_sentence = []\n    labels = []\n    \n    for word, label in zip(sentence, text_labels):\n\n        # Tokenize the word and count # of subwords the word is broken into\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n\n        # Add the tokenized word to the final tokenized word list\n        tokenized_sentence.extend(tokenized_word)\n\n        # Add the same label to the new list of labels `n_subwords` times\n                  \n        if label.startswith(\"B\"):\n            labels.append(label)\n            labels.extend([\"I-\"+label.split(\"-\",1)[1]] * (n_subwords-1))\n        else: \n            labels.extend([label]* n_subwords)\n            \n\n    return tokenized_sentence, labels tokenize_with_labels(\"the fancyful penguin devoured yummy fish .\".split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\".split(), tokenizer)\n Question 1.2: Next, we are creating a PyTorch Dataset class. This class acts as a contained for the training, development, and testing data in memory.\n\n 1.2.1 TODO: Write the __init__(self, filename) method that reads in the data from a data file (specified by the filename).\n\nFor each annotation you start with the tokens in the sentence, and the BIO tags. Then you need to create the following\n\ncall the tokenize_with_labels function to tokenize the sentence.\nAdd the (token, label) pair to the self.items list. 1.2.2 TODO: Write the __len__(self) method that returns the total number of items.\n\n 1.2.3 TODO: Write the __getitem__(self, k) method that returns a single item in a format BERT will understand.\n\n We need to process the sentence by adding \"[CLS]\" as the first token and \"[SEP]\" as the last token. The need to pad the token sequence to 128 tokens using the \"[PAD]\" symbol. This needs to happen both for the inputs (sentence token sequence) and outputs (BIO tag sequence).\n\nWe need to create an attention mask, which is a sequence of 128 tokens indicating the actual input symbols (as a 1) and [PAD] symbols (as a 0).\n\nWe need to create a predicate indicator mask, which is a sequence of 128 tokens with at most one 1, in the position of the \"B-V\" tag. All other entries should be 0. The model will use this information to understand where the predicate is located.\n\nFinally, we need to convert the token and tag sequence into numeric indices. For the tokens, this can be done using the tokenizer.convert_tokens_to_ids method. For the tags, use the role_to_id dictionary. Each sequence must be a pytorch tensor of shape (1,128). You can convert a list of integer values like this torch.tensor(token_ids, dtype=torch.long).\n\nTo keep everything organized, we will return a dictionary in the following format\n\n{'ids': token_tensor,\n 'targets': tag_tensor,\n 'mask': attention_mask_tensor,\n 'pred': predicate_indicator_tensor}\n(Hint: To debug these, read in the first annotation only / the first few annotations). from torch.utils.data import Dataset, DataLoader \n\nclass SrlData(Dataset):\n    \n    def __init__(self, filename):\n\n        super(SrlData, self).__init__()\n        \n        self.max_len = 128 # the max number of tokens inputted to the transformer. \n        \n        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)        \n        \n        self.items = []\n        # complete this method \n                                                        \n    def __len__(self):\n        return 0 # replace\n    \n    def __getitem__(self, k):\n        \n        \n        #complete this method \n        return {'ids': #token_tensor,\n                'mask':  #attn_mask, \n                'targets': #label_tensor, \n                'pred': #pred_tensor\n               }\n        \n # Reading the training data takes a while for the entire data because we preprocess all data offline\ndata = SrlData(\"propbank_train.tsv\"). Question 2: from torch.nn import Module, Linear, CrossEntropyLoss\nfrom transformers import BertModel. We will define the pyTorch model as a subclass of the torch.nn.Module class. The code for the model is provided for you. It may help to take a look at the documentation to see how Module works for later parts. class SrlModel(Module):\n    \n    def __init__(self):\n        \n        super(SrlModel, self).__init__()\n        \n        self.encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n    \n        # The following two lines would freeze the BERT parameters and allow us to train the classifier by itself.\n        # We are fine-tuning the model, so you can leave this commented out!\n        # for param in self.encoder.parameters():\n        #    param.requires_grad = False\n        \n        # The linear classifier head, see model figure in the introduction. \n        self.classifier = Linear(768, len(role_to_id))\n                \n        \n    def forward(self, input_ids, attn_mask, pred_indicator):\n    \n        # This defines the flow of data through the model \n    \n        # Note the use of the \"token type ids\" which represents the segment encoding explained in the introduction. \n        # In our segment encoding, 1 indicates the predicate, and 0 indicates everything else. \n        bert_output =  self.encoder(input_ids=input_ids, attention_mask=attn_mask, token_type_ids=pred_indicator)\n\n        enc_tokens = bert_output[0] # the result of encoding the input with BERT\n        logits = self.classifier(enc_tokens) #feed into the classification layer to produce scores for each tag.\n        \n        # Note that we are only interested in the argmax for each token, so we do not have to normalize \n        # to a probability distribution using softmax. The CrossEntropyLoss loss function takes this into account.\n        # It essentially computes the softmax first and then computes the negative log-likelihood for the target classes. \n        return logits        model = SrlModel().to('cuda') # create new model and store weights in GPU memory\n Now we are ready to try running the model with just a single input example to check if it is working correctly. Clearly it has not been trained, so the output is not what we expect. But we can see what the loss looks like for an initial sanity check.\n\nTODO:\n\nTake a single data item from the dev set, as provided by your Dataset class defined above. Obtain the input token ids, attention mask, predicate indicator mask, and target labels.\nRun the model on the ids, attention mask, and predicate mask like this: # pick an item from the dataset. Then run \n\noutputs = model(ids, mask, pred). TODO: Compute the loss on this one item only. The initial loss should be close to -ln(1/num_labels)\n\nWithout training we would assume that all labels for each token (including the target label) are equally likely, so the negative log probability for the targets should be approximately -ln(1/num_labels) This is what the loss function should return on a single example.\n\n import math\n-math.log(1 / len(role_to_id), math.e) loss_function = CrossEntropyLoss(ignore_index = -100, reduction='mean')\n\n# complete this. Note that you still have to provide a (batch_size, input_pos) \n# tensor for each parameter, where batch_size =1\n\n# outputs = model(ids, mask, pred) \n# loss = loss_function(...)\n# loss.item()   #this should be approximately the score from the previous cell. TODO: At this point you should also obtain the actual predictions by taking the argmax over each position. The result should look something like this (values will differ).\n\ntensor([[ 1,  4,  4,  4,  4,  4,  5, 29, 29, 29,  4, 28,  6, 32, 32, 32, 32, 32,\n         32, 32, 30, 30, 32, 30, 32,  4, 32, 32, 30,  4, 49,  4, 49, 32, 30,  4,\n         32,  4, 32, 32,  4,  2,  4,  4, 32,  4, 32, 32, 32, 32, 30, 32, 32, 30,\n         32,  4,  4, 49,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  6,  6, 32, 32,\n         30, 32, 32, 32, 32, 32, 30, 30, 30, 32, 30, 49, 49, 32, 32, 30,  4,  4,\n          4,  4, 29,  4,  4,  4,  4,  4,  4, 32,  4,  4,  4, 32,  4, 30,  4, 32,\n         30,  4, 32,  4,  4,  4,  4,  4, 32,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n          4,  4]], device='cuda:0')\nThen use the id_to_role dictionary to decode to actual tokens.\n\n['[CLS]', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'O', 'B-V', 'B-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG1', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'O', 'I-ARGM-TMP', 'O', 'I-ARGM-TMP', 'I-ARG2', 'I-ARG1', 'O', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'O', '[SEP]', 'O', 'O', 'I-ARG2', 'O', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'O', 'O', 'I-ARGM-TMP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG2', 'I-ARG1', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARG2', 'I-ARG2', 'I-ARG1', 'O', 'O', 'O', 'O', 'I-ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ARG2', 'O', 'O', 'O', 'I-ARG2', 'O', 'I-ARG1', 'O', 'I-ARG2', 'I-ARG1', 'O', 'I-ARG2', 'O', 'O', 'O', 'O', 'O', 'I-ARG2', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\nFor now, just make sure you understand how to do this for a single example. Later, you will write a more formal function to do this once we have trained the model. Question 3: pytorch provides a DataLoader class that can be wrapped around a Dataset to easily use the dataset for training. The DataLoader allows us to easily adjust the batch size and shuffle the data.\n\nfrom torch.utils.data import DataLoader\nloader = DataLoader(data, batch_size = 32, shuffle = True)\nThe following cell contains the main training loop. The code should work as written and report the loss after each batch, cumulative average loss after each 100 batches, and print out the final average loss after the epoch.\n\nTODO: Modify the training loop belowso that it also computes the accuracy for each batch and reports the average accuracy after the epoch. The accuracy is the number of correctly predicted token labels out of the number of total predictions. Make sure you exclude [PAD] tokens, i.e. tokens for which the target label is -100. It's okay to include [CLS] and [SEP] in the accuracy calculation.\n\nloss_function = CrossEntropyLoss(ignore_index = -100, reduction='mean')\n\nLEARNING_RATE = 1e-05\noptimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n\ndevice = 'cuda'\n\ndef train():\n    \"\"\"\n    Train the model for one epoch.\n    \"\"\"\n    tr_loss = 0 \n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # put model in training mode\n    model.train()\n    \n    for idx, batch in enumerate(loader):\n        \n        # Get the encoded data for this batch and push it to the GPU\n        ids = batch['ids'].to(device, dtype = torch.long)\n        mask = batch['mask'].to(device, dtype = torch.long)\n        targets = batch['targets'].to(device, dtype = torch.long)\n        pred_mask = batch['pred'].to(device, dtype = torch.long)\n\n        # Run the forward pass of the model\n        logits = model(input_ids=ids, attn_mask=mask, pred_indicator=pred_mask)        \n        loss = loss_function(logits.transpose(2,1), targets) \n        tr_loss += loss.item()\n        print(\"Batch loss: \", loss.item()) # can comment out if too verbose.\n        \n        nb_tr_steps += 1\n        nb_tr_examples += targets.size(0)\n        \n        if idx % 100==0:\n            #torch.cuda.empty_cache() # can help if you run into memory issues\n            curr_avg_loss = tr_loss/nb_tr_steps\n            print(f\"Current average loss: {curr_avg_loss}\")\n                   \n        # Compute accuracy for this batch\n        matching = torch.sum(torch.argmax(logits,dim=2) == targets)       \n        predictions = torch.sum(torch.where(targets==-100,0,1))\n                \n        # Run the backward pass to update parameters\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\nNow let's train the model for one epoch. This will take a while (up to a few hours).\n\ntrain()\nIn my experiments, I found that two epochs are needed for good performance.\n\ntrain() \nI ended up with a training loss of about 0.19 and a training accuracy of 0.94. Specific values may differ.\n\nAt this point, it's a good idea to save the model (or rather the parameter dictionary) so you can continue evaluating the model without having to retrain.\n\ntorch.save(model.state_dict(), \"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"). Question 4: # Optional step: If you stopped working after part 3, first load the trained model \n\nmodel = SrlModel().to('cuda') \nmodel.load_state_dict(torch.load(\"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"))\nmodel = model.to('cuda')\nTODO (this is the fun part): Now that we have a trained model, let's try labeling an unseen example sentence. Complete the functions decode_output and label_sentence below. decode_output takes the logits returned by the model, extracts the argmax to obtain the label predictions for each token, and then translate the result into a list of string labels.\n\nlabel_sentence takes a list of input tokens and a predicate index, prepares the model input, call the model and then call decode_output to produce a final result.\n\nNote that you have already implemented all components necessary (preparing the input data from the token list and predicate index, decoding the model output). But now you are putting it together in one convenient function.\n\ntokens = \"A U. N. team spent an hour inside the hospital , where it found evident signs of shelling and gunfire .\".split()\ndef decode_output(logits): # it will be useful to have this in a separate function later on\n    \"\"\"\n    Given the model output, return a list of string labels for each token. \n    \"\"\"\n    pass\n    \ndef label_sentence(tokens, pred_idx):\n    \n    # complete this function to prepare token_ids, attention mask, predicate mask, then call the model. \n    # Decode the output to produce a list of labels. \n    pass\n        \n# Now you should be able to run\n\nlabel_test = label_sentence(tokens, 13) # Predicate is \"found\"\nzip(tokens, label_test)\nThe expected output is somethign like this:\n\n ('A', 'O'),\n ('U.', 'O'),\n ('N.', 'O'),\n ('team', 'O'),\n ('spent', 'O'),\n ('an', 'O'),\n ('hour', 'O'),\n ('inside', 'O'),\n ('the', 'B-ARGM-LOC'),\n ('hospital', 'I-ARGM-LOC'),\n (',', 'O'),\n ('where', 'B-ARGM-LOC'),\n ('it', 'B-ARG0'),\n ('found', 'B-V'),\n ('evident', 'B-ARG1'),\n ('signs', 'I-ARG1'),\n ('of', 'I-ARG1'),\n ('shelling', 'I-ARG1'),\n ('and', 'I-ARG1'),\n ('gunfire', 'I-ARG1'),\n ('.', 'O'), Question 5: Evaluation 1: Token-Based Accuracy\nWe want to evaluate the model on the dev or test set.\n\ndev_data = SrlData(\"propbank_dev.tsv\") # Takes a while because we preprocess all data offline\nfrom torch.utils.data import DataLoader\nloader = DataLoader(dev_data, batch_size = 1, shuffle = False)\n# Optional: Load the model again if you stopped working prior to this step. \n# model = SrlModel()\n# model.load_state_dict(torch.load(\"srl_model_fulltrain_2epoch_finetune_1e-05.pt\"))\n# model = mode.to('cuda')\nTODO: Complete the evaluate_token_accuracy function below. The function should iterate through the items in the data loader (see training loop in part 3). Run the model on each sentence/predicate pair and extract the predictions.\n\nFor each sentence, count the correct predictions and the total predictions. Finally, compute the accuracy as #correct_predictions / #total_predictions\n\nCareful: You need to filter out the padded positions ([PAD] target tokens), as well as [CLS] and [SEP]. It's okay to include [B-V] in the count though.\n\ndef evaluate_token_accuracy(model, loader):\n    \n    model.eval() # put model in evaluation mode\n    \n    # for the accuracy \n    total_correct = 0 # number of correct token label predictions. \n    total_predictions = 0 # number of total predictions = number of tokens in the data. \n    \n    # iterate over the data here. \n\n    acc = total_correct / total_predictions\n    print(f\"Accuracy: {acc}\")\n  Question 6: Span-Base devaluation\nWhile the accuracy score in part 5 is encouraging, an accuracy-based evaluation is problematic for two reasons. First, most of the target labels are actually O. Second, it only tells us that per-token prediction works, but does not directly evaluate the SRL performance.\n\nInstead, SRL systems are typically evaluated on micro-averaged precision, recall, and F1-score for predicting labeled spans.\n\nMore specifically, for each sentence/predicate input, we run the model, decode the output, and extract a set of labeled spans (from the output and the target labels). These spans are (i,j,label) tuples.\n\nWe then compute the true_positives, false_positives, and false_negatives based on these spans.\n\nIn the end, we can compute\n\nPrecision: true_positive / (true_positives + false_positives) , that is the number of correct spans out of all predicted spans.\n\nRecall: true_positives / (true_positives + false_negatives) , that is the number of correct spans out of all target spans.\n\nF1-score: (2 * precision * recall) / (precision + recall). Span-Base devaluation\nWhile the accuracy score in part 5 is encouraging, an accuracy-based evaluation is problematic for two reasons. First, most of the target labels are actually O. Second, it only tells us that per-token prediction works, but does not directly evaluate the SRL performance.\n\nInstead, SRL systems are typically evaluated on micro-averaged precision, recall, and F1-score for predicting labeled spans.\n\nMore specifically, for each sentence/predicate input, we run the model, decode the output, and extract a set of labeled spans (from the output and the target labels). These spans are (i,j,label) tuples.\n\nWe then compute the true_positives, false_positives, and false_negatives based on these spans.\n\nIn the end, we can compute\n\nPrecision: true_positive / (true_positives + false_positives) , that is the number of correct spans out of all predicted spans.\n\nRecall: true_positives / (true_positives + false_negatives) , that is the number of correct spans out of all target spans.\n\nF1-score: (2 * precision * recall) / (precision + recall). def extract_spans(labels):\n    spans = {} # map (start,end) ids to label\n    current_span_start = 0\n    current_span_type = \"\"\n    inside = False\n    for i, label in enumerate(labels):\n        if label.startswith(\"B\"):            \n            if inside: \n                if current_span_type != \"V\":\n                    spans[(current_span_start,i)] = current_span_type            \n            current_span_start = i\n            current_span_type = label[2:]\n            inside = True\n        elif inside and label.startswith(\"O\"):\n            if current_span_type != \"V\":\n                spans[(current_span_start,i)] = current_span_type\n            inside = False\n        elif inside and label.startswith(\"I\") and label[2:] != current_span_type:            \n            if current_span_type != \"V\":\n                spans[(current_span_start,i)] = current_span_type\n            inside = False\n    return spans def evaluate_spans(model, loader):\n    \n    \n    total_tp = 0\n    total_fp = 0\n    total_fn = 0\n        \n    for idx, batch in enumerate(loader):\n        \n        pass # compelte this\n    \n    \n    total_p = total_tp / (total_tp + total_fp)\n    total_r = total_tp / (total_tp + total_fn)\n    total_f = (2 * total_p *total_r) / (total_p + total_r)\n            \n    print(f\"Overall P: {total_p}  Overall R: {total_r}  Overall F1: {total_f}\")\n    \nevaluate(model, loader) In my evaluation, I got an F score of 0.82 (which slightly below the state-of-the art in 2018)\n\n",
  "Ungraded Exercise: Naive Bayes. Consider the following training corpus of emails with the class labels ham and spam. The content of each email has already been processed and is provided as a bag of words.\n\nEmail1 (spam): buy car Nigeria profit\nEmail2 (spam): money profit home bank\nEmail3 (spam): Nigeria bank check wire\nEmail4 (ham): money bank home car\nEmail5 (ham): home Nigeria fly\n\na) Based on this data, estimate the prior probability for a random email to be spam or ham if we don't know anything about its content, i.e. P(Class)?\n\nb) Based on this data, estimate the conditional probability distributions for each word given the class, i.e. P(Word | Class). You can write down these distributions in a table. \n\nc) Using the Naive Bayes' approach and your probability estimates, what is the predicted class label for each of the following emails?\n\nNigeria\nNigeria home\nhome bank money",
  "Ungraded Exercise: n-gram language models. Part 1\n\nAssume a bigram language model is trained on the following corpus of sentences.\n\nSTART dog bites human END\nSTART dog bites duck END\nSTART dog eats duck END\nSTART duck bites human END\nSTART human eats duck END\nSTART human eats salad END\n\nAssume maximum likelihood estimates (MLE) with add-one smoothing (also called Laplace smoothing) were used to compute bigram probabilities. What is the probability assigned to the following sentences by the bigram model?\n\na) dog eats salad\n\nb) human bites dog\n\nPart 2  (optional and more difficult)\n\nShow that, if you sum up the probabilities of all sentence of length n under a bigram language model, this sum is exactly 1 (i.e. the model defines a proper probability distribution). Assume a vocabulary size of V. Hint: Use induction over the the sentence length. \n\nComment: This property actually holds for any m-gram model, but you only have to show it for bigrams.\n\n",
  "Ungraded Exercise: HMM - Viterbi & Forward Algorithm. The following state transition diagram and the emission probabilities below specify a Hidden Markov Model (HMM): starts-> V: 1/2, start->N: 1/2, N->N: 1/4, N->V: 2/4, N->Prep: 1/4, V->N: 2/3, V->Prep: 1/3, Prep->N: 1, P(time|N) = 2/4\nP(flies|N) = 1/4\nP (leaves|N ) = 1/4\n\n\nP(time|V ) = 1/6 \nP(flies|V ) = 2/6\nP (leaves|V ) = 1/6\nP (like|V ) = 2/6\n\nP(like|Prep) = 1\n\n\na) Use the Viterbi algorithm to compute the most likely sequence of hidden states (POS tags) for the observed sequence \"time flies like leaves\" (surface words).\n\nb) Use the Forward algorithm to compute the probability of the surface sequence \"time flies like leaves\".\n\nc) Can you write out the parameters of a trigram model that assigns the same probability distribution to a set of surface sequences as the HMM above? Is this always possible?",
  "Ungraded Exercise: CKY parsing. Consider the following grammar in Chomsky Normal form:\n\nS -> NP VP\nS -> NP V\nNP -> D N\nVP -> V Adv\nVP -> V NP\n\nNP -> rain\nN -> rain\nN -> rains\nV -> rain\nV -> rains\nD -> the\nAdv -> down\n\na) Show how the CKY algorithm for membership checking would parse the sentence \"the rain rains down\" by filling a 5x5 table with rows and columns from 0-4. b) Now consider a version of CKY that also records backpointers. Write down all parse trees for the sentence according to the grammar.",
  "Ungraded Exercise: Transition-based dependency parsing. a)  Assuming an arc-standard transition system, write down the sequence of transitions that the oracle algorithm discussed in class would produce for the following gold dependency tree: root (pred)->sent, sent (nsubj)->he, sent(jobj)->her, sent(advmod)->today, sent(dobj)->meme, meme (det)->a, meme (amod)->funny. Start at the following initial state:\n\n([root ] σ , [he, sent, her, a, funny, meme, today ] β , {} A )\n\nAlso write down the state resulting from each transition. \n\nb) Show that arc-standard sequences are not unique, that is there are multiple transitions that result in the same dependency tree.",
  "Ungraded Exercise: Linear Models and Feature Functions. In this problem we will use a linear model for Part-of-Speach (POS) tagging. The model predicts the POS tag t_i for each word w_i given the word itself, the previous word w_i-1, and the POS of the previous word t_i-1. To predict the POS for the word at position i, the model evaluated t_i* = argmax_t_i(Φ(w_i-1, w_i, t_i-1, t_i)⋅v=argmax_t_i(sum_j(Φ_j(w_i-1, w_i, t_i-1, t_i))⋅v_j, where Φ is a feature function defined over the input words, previous tag, and a candidate output POS tag t_i, and v is a weight vector. Consider the following training data:\n\nSentence 1: time/N flies/V like/P flies/N\nSentence 2: fruit/N flies/N like/V fruit/N\nSentence 3: flies/N time/V fruit/N\nTODO: \n\na) Define Φ by specifying the conditions under which each dimension of the feature vector is 1. \n\nb)  Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus.  \n\nc) More advanced: define the parameters of a log-linear model to make the correct POS prediction for each token in the training data.",
  "Ungraded Exercise: AMR. Translate the following AMR into English. \n\nSome advise on these translations: If you find it difficult to read the AMR in the PENMAN format, draw it as a tree first. Then start with the root of the tree and make it the main verb. Then look at the semantic roles for this main event and recursively translate the arguments.\n\nThe harder examples are more difficult than what you can realistically expect on exam 2.\n\n1.\n \n(y / yield-03\n      :ARG0 (f / fund\n            :mod (t / top)\n            :mod (m / money))\n      :ARG1 (o / over\n            :op1 (p / percentage-entity :value 9)\n            :degree (w / well))\n      :time (c / current))\n\n \n\n2 (harder).\n\n(h / hold-04\n  :ARG1 (b / breakfast-01\n          :mod (b2 / buffet))\n  :location (m / museum\n              :location-of (b3 / ban-01\n                             :ARG1 (a / and\n                                     :op1 (f / food)\n                                     :op2 (d / drink))\n                             :ARG2 (p / person\n                                     :ARG0-of (v / visit-01)\n                                     :mod (e / everyday)))))\nWrite an AMR representing the meaning of the following English sentences:\n\n1. \"He admired Bob's love of math\"\n\n2 (harder). \"We are expected to meet him at 4:00pm in the courtyard.\"",
  "Ungraded Exercise: IBM Model 2. Consider the following parallel training corpus.\n\nExample #\tEnglish sentence E\tForeign sentence F\n1\ta frog ate\tqta brk e\n2\tthe bird ate\tqta rwk et\n3\tthe bird ate a fly\tskwk e qta skwk et\n4\ta frog ate a fly\tskwk e qta brk e\n5\ta bird ate the frog\tbrk et qta skwk e\n \n\n1) Looking at the examples, can you figure out which words correspond to each other and create an E<->F dictionary.  Note: there is a case of lexical divergence in this dataset.\n\n2) Specify t(f |e) and q(j|i, l, m) parameter values for IBM model 2 that maximize P(F,A|E) for all training pairs shown in the table above. Do not specify parameters with value 0. Assume that there is only one \"correct\" alignment -- that is the one you determined in part 1).\n\n3) Using your parameters, compute the probability P(F|E) for training example 3.",
  "Ungraded Exercise: Graph-Based Dependency Parsing. In class (and in homework 3), we discussed transition-based dependency parsing. An alternative approach is graph-based dependency parsing.\n\n\nIn graph-based dependency parsing, we are given an input sentence [root, w1, w2, . . . , wm]. For each wi, our goal is to identify the correct parent word in the dependency tree by computing a score score(i,j) for each possible parent wj , as well as a score score(i, root). We then select the parent with the maximal score (we are ignoring dependency labels in this task).\n\n\nAssume we first use BERT to convert the input sentence into a sequence of d-dimensional word representations [hroot, h1, h2, . . . , hm].\nWe then compute score(i, j)=(h_i^TW)h_j where W is a dxd weight matrix. a) Is this approach guaranteed to always produce a single tree? Can the model generate trees that the\ntransition-based model cannot generate? If so, which?\n\nb) How could you use the scores to compute the probability P(head = j|dependent =i)\n\nc) Assume we are given some training corpus of N sentences, each annotated with a dependency tree. Define a training objective for the scoring model described above.",
  "Syllabus. Course Description\nThis course provides an introduction to the field of Natural Language Processing (NLP). We will discuss properties of human language at different levels of representation (morphology, syntax , semantics, pragmatics), and will learn how to create systems that can analyze, understand, and generate natural language. We  will introduce core NLP techniques for language modeling, tagging, parsing, and word-sense disambiguation. We will  also discuss applications such as machine translation and image caption generation. We will study machine learning methods used in NLP, such as various forms of Neural Networks. We will discuss ethical aspects of NLP research and applications. Homework assignments  will consist of programming projects in Python.\n\nPrerequisites\nData Structures (COMS 3134 or COMS 3137), and Discrete Math (COMS 3203). Some experience programming Python and background in probability/statistics and Linear Algebra is helpful. Some previous or concurrent exposure to AI and machine learning is beneficial, but not required. \n\nMain Course topics\n\nLinguistics background: Levels of linguistic representation. Ambiguity. Diversity in human languages.\nMachine Learning techniques for NLP (supervised machine learning, bayesian models, hidden markov models, neural networks, RNNs, transformers).\nLanguage modeling. \nTagging models (part-of-speech tagging, named-entity recognition).\nLinguistic representations, language complexity, and grammar formalisms. \nSyntactic analysis (dependency and constituency parsing).\nLexical semantics, distributed word representations (embeddings), word-sense-disambiguation.\nComputational semantics. Semantic parsing.\nMachine translation. \nEthics, Bias, and Fairness in NLP.\nSupplemental Textbooks\nYou do not have to buy any textbooks for this class. The following books are available online. From time to time I will post additional research papers and reading material on Courseworks.\nDaniel Jurafsky, James H. Martin, Speech and language processing, 2nd edition. Prentice Hall. 2009.\nA draft of the third edition is available here: https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\n\nYoav Goldberg, Neural Network Methods for Natural Language Processing. Morgan & Claypool (Synthesis Lectures on Human Language Technologies). 2017\nAvailable as an e-book through the Columbia library (https://clio.columbia.edu/catalog/13676351)\n\nRequirements and Grading\nYour course grade will be based on 5 programming assignments, lowest score dropped (10% each = 40%), 2 exams (25% each, 50% total), and participation (10%, online participation on Ed counts) \n\nExams\nThere will be two paper based in-class exams (65 min each). The exams will be closed notes / closed book. For exam 2, one double-sided letter-sized \"cheat sheet\" will be allowed. Calculators recommended.\n\nHomework Policy\nYou will have approximately 2 weeks to complete each programming assignment. Homework assignments may be submitted up to 4 days late for a 20 point penalty. Other extensions will be granted for documented emergencies only. Regrade requests will only be accepted up to 72 hours after homework scores are released on Courseworks by emailing the TA who graded your assignment.\n\nTentative schedule of topics:\n\nDate\tTopics\nWed 9/6\tIntroduction and course overview. Levels of linguistic representation. Ambiguity.\n9/11 & 9/13\tProbability review and machine learning basics.\nText classification. Naive Bayes.\n9/18 & 9/20\t\nn-gram language models, smoothing.\n\nSequence labeling. Hidden Markov models (HMMs). Part-of-speech tagging. B-I-O tagging and named-entity recognition.\n\n 9/25 & 9/27\t\nSyntax and grammar. Context-free grammars (CFGs).\nParsing with CFG and PCFGs (probabilistic CFGs): CKY.\n\n10/2 & 10/4\t\nTransition-based Dependency Parsing.  Machine Learning for NLP.\n\n10/9 & 10/11\t\nMachine Learning for NLP.  Introduction to neural networks and backpropagation.\n\n10/16 & 10/18\t\nNeural language models. Distributed word representations. Static word embeddings. Pre-training.\n\nExam 1 Review.\n\n10/23 & 10/25\t\nExam 1, Monday 10/23 (65 minutes)\n\nWordNet and Word Sense Disambiguation.\n\n10/30 & 11/1\t\nRecurrent Neural Networks and LSTMs. Contextualized embeddings. ELMo.\n\n11/8\t\nUniversity holiday on Monday (Election Day).\n\nNeural MT. Attention.\n\n11/13 & 11/15\t\nSelf-attention and transformer-based models.  BERT, GPT and friends, and various applications.\n\n11/20\t\nPrompting. Reinforcement-Learning from Human Feedback.\n\nUniversity holiday on Wednesday (Thanksgiving).\n\n11/27 & 11/29\t\nSemantic Role Labeling (FrameNet, PropBank). Abstract Meaning Representation.\n\nAdditional applications: text generation and summarization.\n\n12/4  & 12/6\tMultimodal NLP (language and vision). Exam 2 Review on 12/6. \nMon 12/11\tExam 2 (65 minutes, 1 double sided letter-sized cheat sheet allowed)\nAttendance Policy, Classroom Interaction\nAttendance, in person or online with instructor approval, is expected for all sessions and is reflected in the participation grade. The instructor understands that there may be extenuating circumstances that prevent you from attending all sessions. If you have to miss a session (or part of it), you are responsible for catching up on the material. \n\nDuring class, you are expected to behave professionally, respectful and courteous to all course participants. You will use your time in class time most effectively if you fully participating in the class by asking questions and engaging in classroom activities.\n\nIf you are participating on zoom, keep your camera turned on and keep yourself muted. Ask questions in the chat. From time-to time, I may unmute individual participants. I will  also occasionally use polls and ask that you participate using the yes/no/hands up functionality.\n\nDisability-Related Accommodations\nTo receive disability-related academic accommodations for this course, students must first be registered with their school Disability Services (DS) office. Detailed information is available online for both the Columbia and Barnard registration processes. Refer to the appropriate website for information regarding deadlines, disability documentation requirements, and drop-in hours (Columbia)/intake session (Barnard).\n\nFor this course, students registered with the Columbia DS office can refer to the “Courses that do not require professor signature” section of the DS Testing Accommodations page for more information about accessing their accommodations.\n\nAcademic Honesty Policy\nIt is important that you read and understand this section. Any form of academic misconduct will result in a homework or exam grade of zero and can potentially be reported to the appropriate office.\n\nInteraction With Other Students: All homework assignments must be solved individually. You are encouraged to discuss problems with others, but when you sit down to code up your solution you must work on your own, without any further interaction. You are not allowed to share your solutions (literal code and theory solutions) with other students or other groups. \n\nOnline Material: Treat coding problems like essay assignments: You are not permitted to copy any part of other people’s work without attribution. This applies to code produced by other students and to material found on the internet. The problems in this course are designed to be solved with the course materials only. However, sometimes online sources (for instance Stackoverflow) can be useful as a reference. If you have to use code snippets found online you must attribute your source in a comment (complete link). You are not allowed to copy non-trivial code fragments from these sources.\n\nCode is non-trivial if either of the following applies: \n\nAny code that directly solves the the assignment.\nAny code you do not fully understand.\nCode longer than three lines.\nIf we find that you copied a source with attribution, but you used it to solve part of the problem, we will deduct points, but we will not treat this as an example of academic misconduct. \n\nNote that you may use any code provided in the course materials and in the textbook without attribution. \n\nIn addition to this policy, the CS department’s academic honesty policy, as well as the individual policies of your school applies to this course.\n\nAI-Tools/LLMs: The use of language model based AI to complete programming assignments is not permitted in this course, unless specified as part of an assignment.\n\nCampus Resources\nThe instructor is committed to promoting students' well being and advancing an inclusive and welcoming campus culture. He is aware that students may experience personal, social, or financial challenges, whether related or unrelated  to their coursework, that may affect their health and academic performance. In addition, the high levels of stress experienced by many Columbia students may affect their mental and physical health. These effects may be exacerbated during the Covid-19 pandemic.\nIf you are in need of support, you are encouraged to reach out to your school's adviser. If you feel comfortable notifying the instructor, he will make every effort to provide support and connect you to available Columbia resources.\n\nIf you or someone you know feels overwhelmed or suffers from depression or anxiety, please contact\n\nCounseling and Psychological Services (CPS, Columbia) - 212-853-2878\nFurman Counseling Center (Barnard) - 212-854-2092\nFor additional campus resources, see https://universitylife.columbia.edu/student-resources-directory."
]