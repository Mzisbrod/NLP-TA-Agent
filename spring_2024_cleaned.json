{
    "0": {
        "title": "Hw1 Part4",
        "category": "Assignments",
        "question": "Is the expectation for part 4 that the method returns a dictionary with the overall linear interpolation probability of all the words in the text file? ",
        "comments": [],
        "answers": [
            "No. You should not return an entire dictionary. You should just be returning the smoothed probability of the trigram provided as an argument to the function call."
        ]
    },
    "1": {
        "title": "Avighna's Office Hours",
        "category": "General",
        "question": "Hi, I am just wondering if Avighna's office hours are happening/if I will be able to meet with her. I've been in the \"Host has joined. We've let them know you're here\" for a bit now. Thanks!",
        "comments": [],
        "answers": [
            "Hi! I am taking people one by one out of the waiting room as fast as I can but it is packed right now and ends in 2 mins :(, so I don't think I will be able to get to everyone. There are a lot of other OH today though so please go to one of those if I haven't been able to get to you! !"
        ]
    },
    "2": {
        "title": "HW1 submission main method",
        "category": "Assignments",
        "question": "Hi, Regarding the submission of HW1, should we keep the main method's contents unchanged as provided, with most lines commented out, or does it not make a difference? Thank you!",
        "comments": [],
        "answers": [
            "It doesn't really make a difference -- we will test the individual methods, not the main code. "
        ]
    },
    "3": {
        "title": "Homework 1",
        "category": "Assignments",
        "question": "According to #8 , why should get_ngrams(['n', 'l', 'p'], 1) also add a START even though n doesn't exceed len(sequence)? Is it that there should be at least one START? If it is, what should the output be for get_ngrams(['natural', 'language'], 2)? &gt;&gt; get_ngrams(['natural', 'language'], 2)[('START', 'natural'), ('natural', 'language'), ('language', 'STOP')]Is it like this? That is we have at least one START and STOP and (n-1)*'START' should be added.",
        "comments": [],
        "answers": [
            "&gt;&gt; get_ngrams(['natural', 'language'], 2)[('START', 'natural'), ('natural', 'language'), ('language', 'STOP')]Is it like this? Yes, that's correct. You need to add n-1 START symbols and one STOP symbol. "
        ]
    },
    "4": {
        "title": "Waiting to be accepted into zoom link for todays OH",
        "category": "General",
        "question": "titlewondering if there was a limit or time schedule or something i missed out on! Or if I joined the wrong link like last time I attended OH on zoom? Thanks!",
        "comments": [],
        "answers": [
            "Hi! I am letting people in one by one from the waiting room as I get to them!"
        ]
    },
    "5": {
        "title": "HW1 - Part 5 Values of Sentence Probs",
        "category": "Assignments",
        "question": "I'm getting very strange values for perplexity in part 6, so I wanted to check to see if my log sentence probabilities were reasonable. For most sentences, I am getting somewhere between -10 to -60 when calling model. sentence_logprob(sentence) on some sentence from the training corpus. Is this reasonable? I think this is correct as sentences should have a very low probability, but I could be wrong about just how low the probabilities are supposed to be. A negative log prob of &lt; -10 means an extremely low probability. ",
        "comments": [],
        "answers": [
            "The range should be more like -10 to -100s (-200, -300, -400, etc. ). This may not be the only correct answer though. I was incorrectly calculating raw_unigram_probs to be higher than they should have been. With this fixed, I am getting good perplexity values and accuracy values. Updating incase someone else runs into this"
        ]
    },
    "6": {
        "title": "Perplexity score",
        "category": "General",
        "question": "I'm getting around ~8 for a perplexity score when running python trigram_model. py brown_test. txt brown_test. txt. I don't really understand why it would be so low, I don't see any big problems with my functions. Am I running this correctly (the above)? Or is it then something in a function of mine? In my perplexity function, I'm using M = number of word tokens (total word count), is that what is meant by tokens? EDIT: running python trigram_model. py brown_test. txt brown_train. txt: 6e^19running python trigram_model. py brown_test. txt brown_test. txt: 8. 8running python trigram_model. py brown_train. txt brown_train. txt: 11. 3running python trigram_model. py brown_train. txt brown_test. txt: 1. 6",
        "comments": [],
        "answers": [
            "I had a similar problem, according to the HW description we should \"compute the perplexity of the model on an entire corpus. \" In other words, M should be calculated based on the test corpus, not the one used to build the model I believe",
            "Possibly you are not using the correct value for M when computing the perplexity (see student answer above). M is the total number of tokens (word occurrences, including duplicates) in the corpus you are testing. But also, you would expect the perplexity of the model trained on \"brown_test\" to have a very low perplexity on \"brown_test\" itself. What's more concerning is your last result:running python trigram_model. py brown_train. txt brown_test. txt: 1. 6This should be much higher -- most people report something around 150 to 300. "
        ]
    },
    "7": {
        "title": "HW1 - Part5",
        "category": "Assignments",
        "question": "How should we handle the log probability calculation when the smoothed probability of a trigram is zero? Thank you!",
        "comments": [],
        "answers": [
            "It was mentioned in #39 that the interpolated probability would not be 0"
        ]
    },
    "8": {
        "title": "HW 1 part 6",
        "category": "Assignments",
        "question": "Hi! Firstly, when I run : dev_corpus = corpus_reader(sys. argv[2], model. lexicon)\npp = model. perplexity(dev_corpus)\r\nprint(pp)\nI get File \"C:\\Users\\HP\\Downloads\\trigram_model. py\", line 208, in  dev_corpus = corpus_reader(sys. argv[2], model. lexicon) IndexError: list index out of rangeAlso, it seems my code is going into an infinite loop after I added the code for perplexity function. Any help would be really really appreciated! Thanks! Below is my code (attaching everything to increase clarity):def get_ngrams(sequence, n):\r\n   \r\n    if n == 1:\r\n        padded_seq = ['START'] + sequence + ['STOP']  \r\n    else:\r\n        padded_seq = (['START'] * (n-1)) + sequence + ['STOP']\r\n\r\n    res = []\r\n    for i in range(0, len(padded_seq)-n+1):\r\n        res. append(tuple(padded_seq[i: i+n]))\r\n\r\n    return res\r\n\r\n\r\nclass TrigramModel(object):\r\n    \r\n    def __init__(self, corpusfile):\r\n    \r\n        # Iterate through the corpus once to build a lexicon \r\n        generator = corpus_reader(corpusfile)\r\n        self. lexicon = get_lexicon(generator)\r\n        self. lexicon. add(\"UNK\")\r\n        self. lexicon. add(\"START\")\r\n        self. lexicon. add(\"STOP\")\r\n    \r\n        # Now iterate through the corpus again and count ngrams\r\n        generator = corpus_reader(corpusfile, self. lexicon)\r\n        self. count_ngrams(generator)\r\n\r\n\r\n    def count_ngrams(self, corpus):\r\n        self. unigramcounts = Counter() # might want to use defaultdict or Counter instead\r\n        self. bigramcounts = Counter() \r\n        self. trigramcounts = Counter() \r\n\r\n        for s in corpus:\r\n            # print(s)\r\n            unigrams = get_ngrams(s, 1)\r\n            bigrams = get_ngrams(s, 2)\r\n            trigrams = get_ngrams(s, 3)\r\n\r\n            # ref on how to update counter objects https://docs. python. org/3/library/collections. html#counter-objects\r\n            # print(trigrams)\r\n            self. unigramcounts. update(unigrams)\r\n            self. bigramcounts. update(bigrams)\r\n            self. trigramcounts. update(trigrams)\r\n\r\n    def raw_trigram_probability(self, trigram):\r\n        u, v, w = trigram\r\n        tri_count = self. trigramcounts[trigram]\r\n        if u == 'START' and v == 'START':\r\n            bi_count = self. bigramcounts[(v, w)]\r\n        else:\r\n            bi_count = self. bigramcounts[(u, v)]\r\n        \r\n        lexi_size = len(self. unigramcounts) - 1\r\n\r\n        if lexi_size == 0:\r\n            return 0. 0\r\n\r\n        if bi_count == 0:\r\n            ans = 1 / lexi_size\r\n        else:\r\n            ans = tri_count / bi_count\r\n        return ans\r\n\r\n    def raw_bigram_probability(self, bigram):        \r\n        u, v = bigram\r\n        bi_count = self. bigramcounts[bigram]\r\n        uni_count = self. unigramcounts[(u, )]\r\n        lexi_size = len(self. unigramcounts) - 1\r\n\r\n        if lexi_size == 0:\r\n            return 0. 0\r\n        \r\n        if uni_count == 0:\r\n            ans = 1/lexi_size\r\n        else:\r\n            ans = bi_count/uni_count\r\n        return ans\r\n    \r\n    def raw_unigram_probability(self, unigram):\r\n        total_words = sum(self. unigramcounts. values()) - self. unigramcounts[('START', )]\r\n        uni_count = self. unigramcounts[unigram]\r\n        if total_words == 0:\r\n            return 0. 0\r\n        else:\r\n            return uni_count / total_words \r\n\r\n    def generate_sentence(self, t=20): \r\n        \"\"\"\r\n        COMPLETE THIS METHOD (OPTIONAL)\r\n        Generate a random sentence from the trigram model. t specifies the\r\n        max length, but the sentence may be shorter if STOP is reached.\r\n        \"\"\"\r\n        return result            \r\n\r\n    def smoothed_trigram_probability(self, trigram):\r\n        lambda1 = 1/3. 0\r\n        lambda2 = 1/3. 0\r\n        lambda3 = 1/3. 0\r\n        u, v, w = trigram\r\n        p_tri = self. raw_trigram_probability(trigram)\r\n        p_bi = self. raw_bigram_probability((v, w))\r\n        p_uni = self. raw_unigram_probability((w, ))\r\n        return (lambda1 * p_uni) + (lambda2 * p_bi) + (lambda3 * p_tri)\r\n        \r\n    def sentence_logprob(self, sentence):\r\n        \r\n        trigrams = get_ngrams(sentence, 3)\r\n        ans = 0. 0\r\n\r\n        for t in trigrams:\r\n            # print('hi1')\r\n            p = self. smoothed_trigram_probability(t)\r\n\r\n            if p == 0:\r\n                return float(\"-inf\")\r\n            ans += math. log2(p)\r\n\r\n        return ans\r\n\r\n    def perplexity(self, corpus):\r\n\r\n        sum = 0. 0\r\n        \r\n        for s in corpus:\r\n            log_prob = self. sentence_logprob(s)\r\n            \r\n            if log_prob == float(\"-inf\"):\r\n                return float(\"-inf\")\r\n            \r\n            sum += log_prob\r\n\r\n        l = -1 * (sum / (len(self. unigramcounts) - 1))\r\n        print(2**l)\r\n        return  2**l\n",
        "comments": [],
        "answers": [
            "Make sure to pass the train and test files to check perplexity: python trigram_model. py brown_train. txt brown_test. txt",
            "I don't think you implementing M for perplexity in the correct way. Follow #45 and feel free to visit OHs",
            "corpus_reader() deals with tokenization and you have to get the total number of tokens from the testing dataset. Follow an example below: ",
            "Attended OH and got it fixed, thank you!"
        ]
    },
    "9": {
        "title": "Class today?",
        "category": "Lectures",
        "question": "Is todays class still going to be remote? ",
        "comments": [],
        "answers": [
            "+1 ",
            "Sorry I just read this. In case you missed it, the recording of today's class should be up in the Video Library in a bit. "
        ]
    },
    "10": {
        "title": "HW 1 - Part 6",
        "category": "General",
        "question": "For the perplexity calc and the corpus, the instructions say the following:Write the method perplexity(corpus), which should compute the perplexity of the model on an entire corpus. \nCorpus is a corpus iterator (as returned by the corpus_reader method). \nRecall that the perplexity is defined as 2-l, where l is defined as: \nIt's not clear to me how exactly the corpus should be passed into this method as there's an existing method that reads in the corpus. Should it be read in again somehow, local to this function? Also, is it possible to get examples of what the log prob of sample sentences would be to help check work along the way?",
        "comments": [],
        "answers": [
            "You should first call corpus_reader to obtain a corpus iterator object. Then pass this object to the perplexity method. "
        ]
    },
    "11": {
        "title": "HW 1 - Part 3",
        "category": "General",
        "question": "When we calculate raw_unigram_probability(unigram), it's the count(unigram) / (total tokens). In this case should \"total tokens\" include counts of 'START' and 'STOP' or only \"real tokens\"?",
        "comments": [],
        "answers": [
            "#26"
        ]
    },
    "12": {
        "title": "HW1 - Part 2",
        "category": "General",
        "question": "For HW1 - Part 2, when we read in line by line, should each line have a number of 'START's inserted in the front, and one 'STOP' at the end? Or, should there only be one 'STOP' and one set of 'START's for the whole corpus? I'm assuming the former, but wanted to be sure.",
        "comments": [],
        "answers": [
            "Each entry you iterate over in the corpus corresponds to a sentence (see excerpt from the HW1 Description:)&gt;&gt;&gt; generator = corpus_reader(\"\")\n&gt;&gt;&gt; for sentence in generator:\n             print(sentence)START and STOP are applied on this sentence level rather than on the level of the whole corpus"
        ]
    },
    "13": {
        "title": "HW 1 Reasonable Perplexity + Accuracy",
        "category": "Problem Sets",
        "question": "Hi, I am getting 293 perplexity for the Brown test set and 84. 66% accuracy for the essay classification, do these numbers seem reasonable or do they seem indicative of an underlying error? I get 18 perplexity when I run the Brown train set on the Brown train set",
        "comments": [],
        "answers": [
            "These look good."
        ]
    },
    "14": {
        "title": "Ungraded exercise: Naive Bayes",
        "category": "General",
        "question": "Hi! Thanks for posting the solution for the Ungraded exercise: Naive Bayes. I am a little confused about the difference between that solution and the worked example on page 7 of chapter 4 in Jurafsky and Martin's book. To me, it seems that both b) in the ungraded exercise and the worked example (screenshot attached) present a very similar structure, but different approaches to solving, leading to different solutions. I would appreciate your help figuring out the what the differences are between the problems. Thanks!",
        "comments": [],
        "answers": [
            "Its the same approach, but in the textbook example they are additionally using add-one smoothing for the conditional probabilities. "
        ]
    },
    "15": {
        "title": "HW1 Part 7",
        "category": "Assignments",
        "question": "Hi, I got a trigram key error from my code and was wondering how to account for trigrams that occurred in the test file but not in the training file?   Thank you!",
        "comments": [],
        "answers": [
            "Because you are using smoothed_trigram_probability, there will never be a trigram with probability 0.  "
        ]
    },
    "16": {
        "title": "OH on zoom from 9.30 am",
        "category": "General",
        "question": "Hi! Is there still going to be an OH today on zoom as indicated by the calendar? Thanks!",
        "comments": [],
        "answers": [
            "It's on! Feel free to join with the Zoom link in the calendar."
        ]
    },
    "17": {
        "title": "How do I test my functions?",
        "category": "Problem Sets",
        "question": "Hi! This might be a silly question, but is there a way to test each part of my hw? Or is it only possible to test it once the entire hw is done? Thanks!",
        "comments": [],
        "answers": [
            "One way is to instantiate a new TrigramModel using your own txt file. For example, you could create a txt file based on the corpus from the ungraded n-gram exercise, instantiate a TrigramModel with that file, and then verify that the probability outputs match up with the solutions."
        ]
    },
    "18": {
        "title": "Problem Viewing OH Calendar",
        "category": "General",
        "question": "Is anyone else having problems seeing the calendar for office hours? Could this be because I have a Barnard email and not a Columbia one? If so, could those would a Barnard email be added to it? Thank you!",
        "comments": [],
        "answers": [
            "I added all the students that responded on #20, but I'll make sure now to add all of the Barnard students from this course manually."
        ]
    },
    "19": {
        "title": "Final Exam Timing",
        "category": "General",
        "question": "Hi, I know it's about 3 months away, but due to unplanned circumstances, I might not able to take the exam with my current group on 4/26. I was wondering if there's an option to take the final exam with the second group on 4/29 or any other alternative. Thank you in advance, Orr",
        "comments": [],
        "answers": [
            "Please directly email the professor."
        ]
    },
    "20": {
        "title": "Naive Bayes Warm Up Clarification",
        "category": "Problem Sets",
        "question": "Super quick question for calculating the class of a document given a word. I know the solutions state that we can disregard the denominator because \"It's sufficient to compute the joint probabilities, because the denominator for the conditional probability is constant between classes. \" Can someone explain a little bit more what is meant by \"constant between classes\"? Are we saying that bc there are only ten possible words in the \"universe\" for these documents we don't need to calculate the probability that one word occurs because they're all equally likely to occur (1/10 chance of occurring)? Or is it something else.",
        "comments": [],
        "answers": [
            "Yes, so the goal of the Naive Bayes classifier is to make a classification decision: based on the observed words in the given document, which class is the \"best\". The way you do that is by comparing the probabilities that the model assigns, under the assumption class = Spam vs. the assumption class = Ham. It doesn't matter if you compare the conditional probabilities P(spam | crown-prince) vs. P(ham | crown-prince) or the conditional probabilities P(spam, crown-prince) or P(ham, crown-prince). The reason is that P(crown-prince) is the same under both assumptions. Even though the conditional and joint probability are of course not identical, since your goal here is only to classify, you can use either. "
        ]
    },
    "21": {
        "title": "Where is cs ta room?",
        "category": "General",
        "question": "As title.",
        "comments": [],
        "answers": [
            "It's on the first floor of Mudd, room 122A. If you're coming for my OH: I'm sitting on the first floor, right next to the street level doors since it's fully occupied right now. Reach out to me at am6203@columbia. edu for any other questions as I figure out the OH location. "
        ]
    },
    "22": {
        "title": "Hw1 Part 6",
        "category": "Problem Sets",
        "question": "Hi, For the value of M in Part 6, should we count the number of 'START' and 'STOP' token too ? I believe we do but we get a value closer to 400 for the perplexity if we don't.",
        "comments": [],
        "answers": [
            "Include STOP, but exclude START."
        ]
    },
    "23": {
        "title": "Andrew OH happening right now?",
        "category": "General",
        "question": "the zoom is empty /:",
        "comments": [],
        "answers": []
    },
    "24": {
        "title": "HW1 part 7",
        "category": "Assignments",
        "question": "For part 7, I am a bit confused on what should I do. We need the accuracy of each model, but how do we determine if a perplexity is correct? The two for loops given seems to iterate the high model through the high data set, and iterate the low model through the low data set separately. Are we supposed to loop the high model through both high and low, and loop the low model through both high and low, and compare their results for each test data to see if their perplexity difference is correct?",
        "comments": [],
        "answers": [
            "Hi, I have a similar question as a sanity check- I am assuming we have to compare the perplexity of the same file calculated from one model (trained on high scores) compared to the other model (trained on low scores), i. e. for a given sample file, both models should compute it's perplexity? (and then we label it based on the lower perplexity score, and identify if that labelling is correct based on the directory it originated from? )",
            "Yes, I think your understanding is correct. The example code is misleading. The idea is that you iterate through all \"low\" essays and apply both the \"low\" and \"high\" models to each. If the \"low\" model has a lower perplexity than the \"high\" model, you would count that essay as classified correctly. Then you repeat with all the \"high\" quality essays. "
        ]
    },
    "25": {
        "title": "HW1 Part3 n-gram probabilities are 0",
        "category": "General",
        "question": "For the case the prob for n-gram is 0, do we need to consider similar case for bigram? And what if the count(u, w, v) is 0, while count(u, w) is not 0?",
        "comments": [],
        "answers": [
            "For the raw probabilities, just return 0 for those cases. For the interpolated probabilities, the result will never be 0 because the unigram will be non-0. "
        ]
    },
    "26": {
        "title": "Hw1 Part1",
        "category": "General",
        "question": "Is this the correct logic of how i should be using the START and STOP padding for the ngram? ",
        "comments": [],
        "answers": [
            "That looks good to me. (Side note: I recommend that you work on this in a regular IDE or a text editor, rather than Jupyter notebook. You may encounter some issues with being able to test the code as intended because not all methods may be defined correctly  its also difficult to pass command line parameters). "
        ]
    },
    "27": {
        "title": "Hw1 Part 2",
        "category": "Assignments",
        "question": "I just have a question about this part of the homework. What exactly does the content in the corpus looklike? Are we counting the instances of specific trigram, bigram and unigrams or are we counting trigram, bigrams and unigrams in general (hence looking at whats in the corpus and looking at the size and counting it occordingly)? Lastly what would be the best way to test this part of the code specifically? Thank you!",
        "comments": [],
        "answers": [
            "I had a similar question! What would sys. argv[1] be reading in to generate the corpus? ",
            "Hm Im not sure I fully understand your question. The corpus reader iterates through the corpus and produces one sentence at a time. You should count all unigrams, bigrams, and trigrams that appear in the corpus. To test, I would recommend running the code on a small sample corpus (one or two sentences)  just put them in a text file. Then you can directly compare the obtained counts to your manual counts. "
        ]
    },
    "28": {
        "title": "Hw1 part 3 |V|",
        "category": "Assignments",
        "question": "We need to do P(v | u, w) = 1 / |V| (where |V| is the size of the lexicon), if count(u, w) is 0. But I am confused what |V| represent. Does it mean number of types of bigrams or number of types of single words or frequency of word \"v\"?",
        "comments": [],
        "answers": [
            "|V| is the magnitude (size) of the set V, which contains unigram types. "
        ]
    },
    "29": {
        "title": "HW1 Part 4",
        "category": "General",
        "question": "Should we just use the given values (1/3 each) for lambda?",
        "comments": [],
        "answers": [
            "Yes."
        ]
    },
    "30": {
        "title": "HW1 - Interlude",
        "category": "General",
        "question": "Is the optional portion of the homework extra credit? Or is it strictly for fun?",
        "comments": [],
        "answers": [
            "There's no extra credit in this course, so it's just for fun! :)"
        ]
    },
    "31": {
        "title": "HW1 Part 5",
        "category": "General",
        "question": "Are we using bigrams or trigrams to calculate the log probability?",
        "comments": [],
        "answers": [
            "Use trigrams."
        ]
    },
    "32": {
        "title": "HW1 Part 3",
        "category": "Assignments",
        "question": "Can we compute the total words each time we run this function or will we be docked points for doing this?",
        "comments": [],
        "answers": [
            "Not necessarily; we'll only make deductions if your code takes too long to run when grading. If you want to err on the side of caution, we highly recommend creating a variable to store the total number of words, and reusing that for later computations."
        ]
    },
    "33": {
        "title": "Possible need for an extension for HW1",
        "category": "Assignments",
        "question": "Hello, I am a medic in the Army National Guard, and I will have to go away for training for a week starting this Friday. I will try very hard to finish the HW as soon as I can and on time, but how could I obtain an extension given the circumstances? I can submit paperwork that showcases my responsibilities.  Thank you",
        "comments": [],
        "answers": [
            "Please email me directly. "
        ]
    },
    "34": {
        "title": "HW1 Part 6",
        "category": "General",
        "question": "Hi, Is M total number of unique word tokens or number of word tokens which might be repeated in the corpus? When we sum log probability for each sentence to calculate perplexity, do we include START and END tokens to calculate log probablity of a sentence? Do we include START and STOP tokens in M? ThanksAbhinav",
        "comments": [],
        "answers": [
            "For the log probability, you should include the probability of STOP/END, but not START. Think about the model as predicting one token at a time, given the left context. The last predicted token is STOP. START is never predicted, it just provides the context for the first token. M is the number of word tokens (i. e. including duplicate counts), not types. It should include STOP but not START (since the STOP token is predicted, but the START token is not). "
        ]
    },
    "35": {
        "title": "HW1 part 6",
        "category": "General",
        "question": "For part 6, is the \"perplexity\" function supposed to return perplexity for a sentence or the entire corpus? The comments in the function is different from in the assignment on courseworks. Also for part 6, when we use the log2 probabilities for calculating perplexity, do we need to log the output of the function sentence_logprob again? or is the output already log2 prob?",
        "comments": [],
        "answers": [
            "The perplexity function is supposed to return the complexity of the model on the entire corpus. The sentence log probabilities should already use log2 (see part 5). "
        ]
    },
    "36": {
        "title": "Hw1 part 7 accuracy file path",
        "category": "Assignments",
        "question": "The starter code (acc = essay_scoring_experiment('train_high. txt', \"train_low. txt\", \"test_high\", \"test_low\") ) does not work for me and it keeps saying No such file or directory: 'train_high. txt' And my python file is in the same directory with them. I am not sure why it says no such file. So I used my exact file path which worked. I wonder if I should submit with my exact file path or change it back, or should I comment out the testing part?",
        "comments": [],
        "answers": [
            "Your working directory is probably different from the directory where the files are located. I recommend just switching it back after you are done testing for your final submission, but it's not a big deal to leave the absolute pathnames. "
        ]
    },
    "37": {
        "title": "HW1 part3 total # of words",
        "category": "Assignments",
        "question": "HiIf I understand correctly, total number of words is not including the (START, ) and (STOP, ) token? Why do we not count those tokens in unigrams? (My guess is the start and stop token are meaningless here. )Best,",
        "comments": [],
        "answers": [
            "This is for the unigram probabilities? START should definitely not be included, because the model would never predict it. I recommend including STOP. You will need the unigram probability of STOP when you compute linear interpolation in part 4. ",
            "What's the harm in including Start as a normal token?"
        ]
    },
    "38": {
        "title": "HW1 perplexity question",
        "category": "Assignments",
        "question": "Hi. I am perplexed by a perplexity score . Specifically, I ran perplexity on the brown_train which gave me 14. 6, a low value as expected. However, when running on brown_test I  get 9. 07 a low score, but importantly, it's lower than the training perplexity. This seemed odd since I expected the test perplexity to be higher than that of the training data. Can I assume there is a bug in my code, or is it possible for test perplexity be lower than train perplexity? ",
        "comments": [],
        "answers": [
            "The test perplexity should never be lower than the one on the training set. I would double check whether you're using the number of tokens in the training set when computing your test perplexity."
        ]
    },
    "39": {
        "title": "HW! part 3",
        "category": "Assignments",
        "question": "For HW1 part 3 where we need to compute the total words for unigram, can we edit __init__ directly to add object variables? Or can we only edit the functions specifically for each part?",
        "comments": [],
        "answers": [
            "That's fine. As long as you keep the function signatures and return types the same, you're free to make any other modifications."
        ]
    },
    "40": {
        "title": "Add-one smoothing",
        "category": "Lectures",
        "question": "Does V (the size of the vocab) in the denominator include all the START and END tokens? ",
        "comments": [],
        "answers": [
            "We never count START as part of the vocabulary because it never appears on the left side of any conditional probability calculation we do in training. We count END because we will always see something like (END|word)."
        ]
    },
    "41": {
        "title": "Exam format",
        "category": "General",
        "question": "For the exams will there be coding? How are they formatted? Sorry I know this is super in advance just curious! ",
        "comments": [],
        "answers": [
            "The exams will consist of short answer and free response questions that are similar to the ungraded exercises. You may have to adjust a couple lines of pseudocode for a specific algorithm, but we will not ask you to directly code anything from scratch."
        ]
    },
    "42": {
        "title": "hw1 part2 about Punctuations",
        "category": "Assignments",
        "question": "Do we need to regard the punctuations as a \"word\" or we need to remove all of them? Eg. For bigram, \"Hi, Bob. \", do we use [. .. ('Hi', ', '), (', ', 'Bob'). .. ] or [. .. ('Hi', 'Bob'), . .. ]",
        "comments": [],
        "answers": [
            "The corpus reader is performing any text normalization for you. You dont have to worry about tokenization or removing symbols. "
        ]
    },
    "43": {
        "title": "HW 1 Part 5",
        "category": "Assignments",
        "question": "What is the datatype of \"sentence\" in sentence_logprob(sentence)? Can we assume it will be given as an array? Or is it a string?",
        "comments": [],
        "answers": [
            "Its a sequence (either a list or tuple) of tokens. "
        ]
    },
    "44": {
        "title": "HW1 Part 1",
        "category": "Problem Sets",
        "question": "Should the case for n = 3 have only one start at the beginning, or is that correct? IE:[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]Should be[('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]",
        "comments": [],
        "answers": [
            "Not at TA but I think #8 answers this!"
        ]
    },
    "45": {
        "title": "HW 1 Part 3",
        "category": "General",
        "question": "What are the unigram, trigram, bigram arguments for the functions? Are they particular n-gram queries or are they the dictionaries populated in part 2?",
        "comments": [],
        "answers": [
            "The arguments will be individual ngrams (formatted as a tuple). "
        ]
    },
    "46": {
        "title": "Request for OH Google Calendar",
        "category": "General",
        "question": "Hi! Would it be possible to create a Google calendar of scheduled office hours so we can import them into our own calendar? No worries if not, but it would be really helpful! Thank you so much!",
        "comments": [],
        "answers": [
            "Here! I'll add it to Courseworks as well:https://calendar. google. com/calendar/u/0? cid=Y180ZWMwOWFjZDM0YWU4Yzc1ZWI2ZmY1NTgxYjg1MzY5ZGU3ZjRlNWEwNDg2YTViMzU5ZjI1ZWMzOGM3OTMyNmZhQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20"
        ]
    },
    "47": {
        "title": "HW 1 Part 4",
        "category": "Assignments",
        "question": "For this part, can we use this equation to implement it?",
        "comments": [],
        "answers": [
            "Yes, that's the correct formula. See below for the slide of the lectures that refers to linear interpolation:"
        ]
    },
    "48": {
        "title": "HW 1 Part 2",
        "category": "Problem Sets",
        "question": "Just to clarify, nothing is returned in count_ngrams, right?",
        "comments": [],
        "answers": [
            "I think that's correct. That function should just populate the associated dictionaries. "
        ]
    },
    "49": {
        "title": "Where to code",
        "category": "Assignments",
        "question": "Are we recommended to work in a specific python environment?  ",
        "comments": [],
        "answers": [
            "Ill leave that up to you. We wont be using any libraries for homework 1 and 2, but make sure you have Python 3. 7 or higher. A lot of people really like vscode, but any editor or IDE will do. "
        ]
    },
    "50": {
        "title": "HW1 submission",
        "category": "Problem Sets",
        "question": "Hi, just wonder how we should submit HW1 since canvas only accepts zip/tgz while we are asked to submit . py only",
        "comments": [],
        "answers": [
            "Thanks. Ill change it to . py. "
        ]
    },
    "51": {
        "title": "Bigram Probability, First Word Not in Lexicon",
        "category": "General",
        "question": "From the homework \"\"\"One issue you will encounter is the case if which you have a trigram u, w, v where count(u, w, v) = 0 but count(u, w) is also 0. In that case, it is not immediately clear what P(v | u, w) should be. My recommendation is to make P(v | u, w) = 1 / |V| (where |V| is the size of the lexicon), if count(u, w) is 0\"\"\"Doesn't this same behavior happen for bigrams if u is not in the lexicon? Would the approach be the same?",
        "comments": [],
        "answers": [
            "That wont happen because all unigrams that are not in the lexicon are replaced with the UNK token by the corpus reader. "
        ]
    },
    "52": {
        "title": "Question regarding get_ngrams function.",
        "category": "Assignments",
        "question": "Hi! I had a quick question about the get_ngrams() function for HW1. I'm assuming this function should pad out our inputs even if n is greater than the number of words in the input sequence? Ex: sequence = [\"Hi\", \"there\"]n = 4Output: [('START', 'START', 'START', 'Hi'),    ('START', 'START', 'Hi', 'there'),    ('START', 'Hi', 'there', 'STOP')]",
        "comments": [],
        "answers": [
            "That's correct!"
        ]
    },
    "53": {
        "title": "Ungraded Exercise Submission",
        "category": "General",
        "question": "Hello, Do we need to submit the ungraded exercise? Thank you very much.",
        "comments": [],
        "answers": [
            "No, they won't be submitted. If you'd like feedback on your solutions though, you're always welcome to stop by any of our office hours!"
        ]
    },
    "54": {
        "title": "What is the term \"Class\" referring to?",
        "category": "General",
        "question": "I am wondering what the term \"class\" refers to in relation to a document-- is class related to a word? ",
        "comments": [],
        "answers": [
            "The class/label is the type of the document, for example spam or not spam. Each document has exactly one class. So this is a property of the entire document, not a single word. "
        ]
    },
    "55": {
        "title": "Question about Participation Points",
        "category": "General",
        "question": "Hi Professor and TAs, I hope you are well! I was wondering about how to earn participation points (the 10% of our overall grade) as someone who works 9-5 and needs to watch the recordings asynchronously. If I do not participate during class time, can I still earn the full participation points by participating on Ed instead? If it is mandatory that I attend class in-person, I can try and work something out with my boss. For the next few weeks, however, I will need to watch the recordings afterward. And I'm just worried I will miss out on those participation points! :) I appreciate your help in advance! Best regards, Mia",
        "comments": [],
        "answers": [
            "Participation on Ed will count for that score! I highly recommend responding to the professor's ungraded exercises and discussion questions to show evidence of your participation. In addition, you could also try to answer other students' questions when we start releasing the graded homework assignments.",
            "If I recall correctly, you are a CVN student. Participating in Ed is sufficient in your situation, as Shivansh describes. CVN students are explicitly allowed to participate asynchronously. "
        ]
    },
    "56": {
        "title": "Game to Discriminate Between Human and AI Conversation",
        "category": "General",
        "question": "If you think you could be a good candidate to tell the difference between a human and an AI in a Turing Test, check out this website. After having a 2-minute live conversation with either a randomly assigned other human player or an AI, you submit your guess as to whether or not they were an AI. If you end up playing it and get a crazy conversation feel free to comment it!",
        "comments": [
            "I just played the game and won by identifying that the other participant was a human. Their responses were strange and unnatural, almost like a human trying to act non-human. So, the big takeaway is, \"If you're going to trick someone, don't be too deliberate. Be natural. \"",
            "This is quite a cool site, and I've seen clips of it going around on the internet. I think what's interesting is that this is slightly different from Turing's interpretation of the \"Imitation Game\", where humans try and convince the evaluator that they are human. In this game, human players may try and act like AI, perhaps making it even more difficult than Turing's game. It's also very likely they're reusing different games to fine-tune their models lol. Will be interesting to see how this evolves.",
            "I feel like when I played the game some things made me feel like I was talking to a human like the misspelling of certain words and the \"insta? \" at the end which is (sort of) new slang. It was interesting cause at some point I felt like I was being deceived but I just think that was me overthinking it. I have seen a lot of videos on TikTok where people try to guess whether images are AI-generated or real photographs and I feel like that is almost easier than this. I was wondering what everyone else thinks.",
            "Pretty sure I just turing'd someone. ",
            "this was interesting for sure ",
            "Is it just me or does the link not work anymore?",
            "Interesting, thanks for sharing Brian! Seems like site is down, will keep trying. "
        ],
        "answers": []
    }
}