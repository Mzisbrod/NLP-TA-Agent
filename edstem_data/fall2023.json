[
    {
        "source": "Solution to final exam&HW5. <document version=\"2.0\"><paragraph>Can we get solution to final exam as well as HW5?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>It\u2019ll take a while for me to get these up, but I can post them eventually.\u00a0</paragraph></document>"
    },
    {
        "source": "Canvas Average. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.  Is the average currently reflected in Canvas, the accurate final grade?  Or do I need to any other calculation on my own?</paragraph><paragraph>It looks like everything is updated, but I just want to make sure if that is okay.</paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Courseworks does not support \u201cdropping\u201d and assignment, so you need to calculate the final score yourself using the formula on the syllabus.\u00a0</paragraph></document>"
    },
    {
        "source": "Lowest Programming Assignment Dropped?. <document version=\"2.0\"><paragraph>Hi NLP Teaching Staff, </paragraph><paragraph>Hope the new year is off to a good start for all of you. I just wanted to make sure that the lowest programming assignment was dropped in my final grade. I calculated my final grade with my lowest assignment dropped and it seems to be different from the one appearing on courseworks. I did not submit hw3 because I took it as my dropped assignment.</paragraph><paragraph>Thank you for your hard work this semester!! </paragraph><paragraph>Hanna Martin</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The lowest assignment was dropped in my final grade calculation. The score on coursework does not reflect the drop because coursework doesn\u2019t support elaborate grading logic like this.\u00a0</paragraph></document>"
    },
    {
        "source": "No regrade request option for final exam marks. <document version=\"2.0\"><paragraph>I am not able to find any regrade request option in the gradescope for any of the questions in the final exam marks. How do we put them?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>First, make sure that you absolutely need a regrade request. A few points will likely not affect your letter grade at all. Please limit any such requests to major issues (for example, if the grader completely missed your answer because it was on a different page). </paragraph><paragraph>You can email me requests directly, but I may not be able to respond to them before I release final grades. If a request results in a change of letter grade, I will have to submit a change-grade request later. </paragraph></document>"
    },
    {
        "source": "HW4 Grade. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>I am wondering whether hw4 grade has been released as I only saw the grade for hw5 on my Canvas.</paragraph><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Unfortunately we are missing some part of hw4 and I can\u2019t release them before everything is in.<break/>But the final exam is fully graded and I should be able to push those grades out tomorrow.\u00a0</paragraph></document>"
    },
    {
        "source": "last minute error converting to pdf. <document version=\"2.0\"><file url=\"https://static.us.edusercontent.com/files/8ElemyOpbiU6GbAzqWl2rjHN\" filename=\"4705_f23_hw5.ipynb - Colaboratory.pdf\"/><paragraph>Hi I had a little situation when submitting hw5 a few minutes ago my jupter notebook stoped responding when I try to generate the pdf, I didn't submit the pdf successfully but I did submit the notebook (ipynb file), and I included the pdf file in this post. what should I do? do I resubmit?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Please resubmit, it\u2019s much easier for grading to have everything in one place. Obviously, we will count this as on-time.\u00a0</paragraph></document>"
    },
    {
        "source": "HW 5 Jupyter Notebook Input. <document version=\"2.0\"><paragraph>Hello! I was running into an issue earlier where my Jupyter Notebook would become unresponsive, so I would restart the kernel (<link href=\"https://edstem.org/us/courses/46417/discussion/4078889?answer=9381425\">https://edstem.org/us/courses/46417/discussion/4078889?answer=9381425</link>). <break/><break/>However, whenever I'm attempting to input something into my Jupyter Notebook (i.e. when it asks me if I want to replace a file and to answer with [y/n]) I'm unable to find a space to input a response. So far, I've just been working around this by deleting the files in ontonotes_srl.zip.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/yZaLiPqx2NxMJQOpdOswFed3\" width=\"658\" height=\"67.81615598885794\"/></figure><paragraph>For convenience's sake, is there a way to access an input box for this assignment?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>i believe you could just skip this box if you already get to the later steps in ur previous attempts!</paragraph></document>"
    },
    [
        {
            "source": "HW5 Submission. <document version=\"2.0\"><paragraph>Should we submit notebook and pdf in one zip file? Or should we upload two files separately into CourseWorks?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I just click \"add another file\" and loaded them both. Pretty sure if you submit a ZIP it will extract it out anyways.</paragraph><paragraph/></document>"
        },
        {
            "source": "HW5 Submission. <document version=\"2.0\"><paragraph>Should we submit notebook and pdf in one zip file? Or should we upload two files separately into CourseWorks?</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Two separate files please.\u00a0</paragraph></document>"
        }
    ],
    [
        {
            "source": "HW 5 Part 3 length-related questions. <document version=\"2.0\"><paragraph>For part 3, I kept getting the length errors. </paragraph><pre>RuntimeError: stack expects each tensor to be equal size, but got [128] at entry 0 and [161] at entry 13\n</pre><paragraph>I tried to look through many posts in the past and added codes to truncate (1) and skip long sentences (2). I even try to skip allllllllll. kinda stuck here</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Y1Buyvg3opMMU7Sx4t7FuhEd\" width=\"658\" height=\"281.77698695136417\"/></figure><paragraph>Can anyone tell me how to truncate the data / skip the too-long sentence correctly??</paragraph><figure><image/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>I truncated in the getitem method in 1.2.3 and that fixed that error for me</paragraph></document>"
        },
        {
            "source": "HW 5 Part 3 length-related questions. <document version=\"2.0\"><paragraph>For part 3, I kept getting the length errors. </paragraph><pre>RuntimeError: stack expects each tensor to be equal size, but got [128] at entry 0 and [161] at entry 13\n</pre><paragraph>I tried to look through many posts in the past and added codes to truncate (1) and skip long sentences (2). I even try to skip allllllllll. kinda stuck here</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Y1Buyvg3opMMU7Sx4t7FuhEd\" width=\"658\" height=\"281.77698695136417\"/></figure><paragraph>Can anyone tell me how to truncate the data / skip the too-long sentence correctly??</paragraph><figure><image/></figure></document>",
            "target": "<document version=\"1.0\"><paragraph>This should be handled on the level of the dataset, not the training loop.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "HW5 Part3 Training Loss, Accuracy, and Epoch. <document version=\"2.0\"><paragraph>My model reaches very low training loss(less than 0.19) and high accuracy(more than 0.94) after the first epoch. Is it normal to get this behavior?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I believe most people get a training accuracy in the 80s after the first epoch.</paragraph></document>"
    },
    {
        "source": "General Question About Google Cloud. <document version=\"2.0\"><paragraph>I am stopping my Google Cloud instance when not in use in order to save credits, but when I try to restart the VM, it gives me this error again: A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-west1-b zone.<break/><break/>Is there any way to move all these resources to another region, or is the only way forward to create a new VM entirely with identical hardware and re-clone everything again? Just wondering what best practice is here. I never run into the instance unavailable error with other cloud services.</paragraph><paragraph>Thanks in advance.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>https://edstem.org/us/courses/46417/discussion/4078868</paragraph></document>"
    },
    [
        {
            "source": "CUDA out of memory. <document version=\"2.0\"><paragraph>I am wondering if there are any quick solution for this??? I tried to restarted the kernel and it sometimes work but still getting a lot :(((</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/DEGZ9fDeWTAqAvmd0wEjcfL8\" width=\"658\" height=\"236.67193675889328\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Something that worked for me was changing the batch_size to 16. </paragraph></document>"
        },
        {
            "source": "CUDA out of memory. <document version=\"2.0\"><paragraph>I am wondering if there are any quick solution for this??? I tried to restarted the kernel and it sometimes work but still getting a lot :(((</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/DEGZ9fDeWTAqAvmd0wEjcfL8\" width=\"658\" height=\"236.67193675889328\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Same issue, any solution for this?</paragraph></document>"
        }
    ],
    {
        "source": "Part 6 Score. <document version=\"2.0\"><paragraph>I'm getting an F score of about 0.845 Is that reasonable?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, that seems fine!</paragraph></document>"
    },
    {
        "source": "HW5-PDF submission. <document version=\"2.0\"><paragraph>I printed many debug logs during the data loading and the training process. Then I realized that these cell outputs for debugging purposes makes the pdf saving process extremely long. I was wondering if it is okay to clear the output of some of these cells, including the train() cell, but mainly leave the evaluation result cells in output for pdf submission?</paragraph><paragraph/><paragraph/><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Okay that\u2019s fine. Maybe keep a comment that you removed the output because there were too many debugging messages.\u00a0</paragraph></document>"
    },
    {
        "source": "Jupyter Notebook Won't Load. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I was working on Part 5 when my Jupyter Notebook started being unresponsive. I've tried restarting the VM multiple times, but I can't get my notebook to load. It just shows a blank screen for a bit, and then it eventually crashes. Does anyone have any tips for how to fix this? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/fd14gXbGvcxm4PhFfhhUVRaH\" width=\"657.9999999999999\" height=\"111.6639072847682\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>here are a couple of ideas:<break/><bold>1. restart jupyter kernel:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>If the Jupyter Notebook itself is unresponsive, you might try restarting the Jupyter kernel. You can do this from the Jupyter interface or use the following command in a Jupyter cell:</paragraph><pre>pythonCopy code\nimport os\nos._exit(00)</pre></list-item></list><paragraph><bold>2. clear output cells:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Sometimes, large outputs in Jupyter cells can cause performance issues, so if you have large outputs, try clearing the output cells and running the notebook again.</paragraph></list-item></list><paragraph><bold>3. check disk space:</bold></paragraph><list style=\"unordered\"><list-item><paragraph>Check if there's enough disk space on your VM instance. You can use the following command in a Jupyter cell to check disk space:</paragraph><pre>pythonCopy code\n!df -h</pre></list-item></list></document>"
    },
    {
        "source": "HW5 - VM instance won't start. <document version=\"2.0\"><paragraph>I'm trying to reopen the VM instance I have but got this error.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/RsZ20QW7bVcVBdmJ3BTMBmFl\" width=\"558\" height=\"250\"/></figure><paragraph>I already have work done for this assignment on the current VM instance I have. Would opening a new instance make the current work I have disappear?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes \u2014 although maybe you can disconnect the GPU, start the instance, save the notebook, then start a new GPU instance in a different availability zone and upload the work.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Weird error for Hw 5 part 3. <document version=\"2.0\"><paragraph>Hi! I am trying to understand part 3's code so I am moving through the batches and trying to print the ids, but for some reason I encounter this error.</paragraph><pre>&lt;__array_function__ internals&gt; in reshape(*args, **kwargs)\n\n/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in reshape(a, newshape, order)\n    296            [5, 6]])\n    297     \"\"\"\n--&gt; 298     return _wrapfunc(a, 'reshape', newshape, order=order)\n    299 \n    300 \n\n/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)\n     55 \n     56     try:\n---&gt; 57         return bound(*args, **kwds)\n     58     except TypeError:\n     59         # A TypeError occurs if the object does have such a method in its\n\nValueError: cannot reshape array of size 146 into shape (1,128)\n</pre><paragraph>My part 1 works fine, so I don't really understand what's throwing this error. Why is my token tensor longer than 128 even though I reshaped the array? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I was running into what I think is the same issue, after some debugging it seems that there are simply just some sentences, once tokenized, have over 128 tokens. I've attached an example below. I'm not sure how to handle these cases, or if somehow I'm not preprocessing everything correctly in Part 1. The only idea I have right now to handle this is to simply not store the sentence in self.items if it has a token length &gt; 126 (to account for [CLS] and [SEP] tokens)</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/bAM8Hwhd4dmEeUx9yKH5mJ12\" width=\"758\" height=\"509.4375634517766\"/></figure></document>"
        },
        {
            "source": "Weird error for Hw 5 part 3. <document version=\"2.0\"><paragraph>Hi! I am trying to understand part 3's code so I am moving through the batches and trying to print the ids, but for some reason I encounter this error.</paragraph><pre>&lt;__array_function__ internals&gt; in reshape(*args, **kwargs)\n\n/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in reshape(a, newshape, order)\n    296            [5, 6]])\n    297     \"\"\"\n--&gt; 298     return _wrapfunc(a, 'reshape', newshape, order=order)\n    299 \n    300 \n\n/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py in _wrapfunc(obj, method, *args, **kwds)\n     55 \n     56     try:\n---&gt; 57         return bound(*args, **kwds)\n     58     except TypeError:\n     59         # A TypeError occurs if the object does have such a method in its\n\nValueError: cannot reshape array of size 146 into shape (1,128)\n</pre><paragraph>My part 1 works fine, so I don't really understand what's throwing this error. Why is my token tensor longer than 128 even though I reshaped the array? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I think you can truncate to 128 tokens based on #757 </paragraph></document>"
        }
    ],
    {
        "source": "attention mask. <document version=\"2.0\"><paragraph>Hi,<break/><break/>Should the attention mask in part 1 have a 1 in the positions for the CLS and SEP token? </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes</paragraph></document>"
    },
    [
        {
            "source": "BERT outputting nonsense after [SEP] token.. <document version=\"2.0\"><paragraph>When I trained my model and evaluated it via the decoding exercise and the token accuracy, my model seemed fine. However, my precision, and F1 scores are really poor. I output the labels my model is making to see what the problem is.</paragraph><paragraph>What they are supposed to be:</paragraph><pre>['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']</pre><paragraph>What my model is outputting:</paragraph><pre>['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'I-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O']</pre><paragraph>It seems that where the model is supposed to be outputting \"[PAD]\" tokens, it is instead outputting random nonsense. Since this area should be masked to 0 by the attention mask, I am confused as to what is happening here. Does anyone have any ideas/suggestions for what to look for or try? Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>My model did the same thing, I don't think this is an issue as long as your SEP token is in the right spot.</paragraph></document>"
        },
        {
            "source": "BERT outputting nonsense after [SEP] token.. <document version=\"2.0\"><paragraph>When I trained my model and evaluated it via the decoding exercise and the token accuracy, my model seemed fine. However, my precision, and F1 scores are really poor. I output the labels my model is making to see what the problem is.</paragraph><paragraph>What they are supposed to be:</paragraph><pre>['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']</pre><paragraph>What my model is outputting:</paragraph><pre>['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'I-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O']</pre><paragraph>It seems that where the model is supposed to be outputting \"[PAD]\" tokens, it is instead outputting random nonsense. Since this area should be masked to 0 by the attention mask, I am confused as to what is happening here. Does anyone have any ideas/suggestions for what to look for or try? Thanks!</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Yes multiplying the prediction matrix with the attention mask is a great approach.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Outputs seem to be shifted incorrectly.. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/DSObweK5xl7s9C5w4y4hnav1\" width=\"658\" height=\"599.1422018348624\"/></figure><paragraph>When I get to the decoding problem, I notice that my output labels seem correct; they just seem to be shifted incorrectly by 1. Did anyone else run into this? What was the reason behind it?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I figured this out. I didn't remove the CLS token, so they weren't aligned!</paragraph></document>"
    },
    {
        "source": "HW 5 Part 3 Training Loss and Accuracy. <document version=\"2.0\"><paragraph>Hello Professor,</paragraph><paragraph>My model was finally able to train properly!<break/>I got the following loss and accuracy after 2 epochs, and I just wanted to make sure my numbers are reasonable, especially since the second training epoch loss of 0.148 is a little lower than the expected value of 0.19.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/9Ti0o79gmlhnk5a34UEaCjAH\" width=\"658\" height=\"302.8427561837456\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That looks good.\u00a0</paragraph></document>"
    },
    {
        "source": "Going back to original tokens. <document version=\"2.0\"><paragraph>Hi! For part 6, I am trying to find a way to get the original list of tokens prior to tokenization. Is there an official way of doing so? I have tried using tokenizer.decode, but found that it is not accurate.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes it\u2019s not necessary to detokenize.\u00a0</paragraph><paragraph/></document>"
    },
    {
        "source": "HW 5 Incredibly high batch loss values. <document version=\"2.0\"><paragraph>Hi! I'm getting super high batch loss values when I train the model in Part 3. </paragraph><paragraph>I've spent hours trying to find the issue and have changed a few things, but still getting these kinds of values. Is there anything you recommend that I check in my code?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/rC5zla1iwC1bNRBfph8qgRl3\" width=\"658\" height=\"823.8373983739837\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>Hard to tell. Maybe there is something wrong with the target labels? \u00a0They could be mismatched.\u00a0</paragraph></document>"
    },
    {
        "source": "Moving dataset to GPU?. <document version=\"2.0\"><paragraph>Maybe I'm misreading this code, but is there a reason we don't move the entire dataset into the GPU? Given its footprint is small, I think it would eliminate what would normally be lots of expensive GPU &lt;&gt; CPU communication.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>You could try this. Let me know if it works?\u00a0</paragraph></document>"
    },
    {
        "source": "HW 5 Negative probabilities(?). <document version=\"2.0\"><paragraph>The model's output is a tensor of shape (1, 128, 53), where 128 is presumably the sentence length and 53 is a probability distribution over the labels per slot.</paragraph><paragraph>When I print this, some of those 53 probabilities are negative. Am I misunderstanding the output of the model, or is this normal?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The last layer is a linear layer, which means it has not been fed through a softmax layer. This is not supposed to be directly considered a probability distribution.</paragraph><paragraph/></document>"
    },
    {
        "source": "HW 5 [prd] in tag output. <document version=\"2.0\"><paragraph>When I print out my decoded tokens in part 2, some of them are \"[prd]\". Is anyone else getting that? I'm assuming there's an issue here. </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>[prd] is part of the role dictionary, and so without training it\u2019s one of the possible outputs. The tag was likely left over from me experimenting. It should never appear in the training data though and should not be predicted once the model is trained.\u00a0</paragraph></document>"
    },
    {
        "source": "HW 5 training. <document version=\"2.0\"><paragraph>While calculating accuracy, should we use weighted average accuracy or just average accuracy across iterations? \ufffc</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Just averaging across iterations will be fine.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "HW 5 Part 4 Model predicts 'O' tag. <document version=\"2.0\"><paragraph>I am working on part 4 and I noticed that the highest recommended tag is often <code>'O'</code> </paragraph><paragraph>Here is my result with the example given in 4. The <code>'B-V'</code> tag is accurate but it doesn't seem like the other tags are predicted. It is not because the other predicted tags are not in the <code>id_to_role</code> dictionary, but literally the <code>'O'</code> tag is getting predicted. </paragraph><paragraph>Does anyone know why this might be happening? The model inputs are <code>input_ids</code> and <code>attn_mask</code> processed similar to when we loaded in the dataset (part 1).  The input <code>pred_indicator</code> is also similar to the part 1 implementation, I just set <code>pred_indicator[pred_index] = 1</code></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/kd6n1woFDcjgQa7BpQ69X4sz\" width=\"130\" height=\"236\"/></figure><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Because the O tag is by far the majority tag, it seems reasonable that the model would learn to default to it. It may mean you need to train longer. What does the training loss and accuracy look like?\u00a0</paragraph></document>"
        },
        {
            "source": "HW 5 Part 4 Model predicts 'O' tag. <document version=\"2.0\"><paragraph>I am working on part 4 and I noticed that the highest recommended tag is often <code>'O'</code> </paragraph><paragraph>Here is my result with the example given in 4. The <code>'B-V'</code> tag is accurate but it doesn't seem like the other tags are predicted. It is not because the other predicted tags are not in the <code>id_to_role</code> dictionary, but literally the <code>'O'</code> tag is getting predicted. </paragraph><paragraph>Does anyone know why this might be happening? The model inputs are <code>input_ids</code> and <code>attn_mask</code> processed similar to when we loaded in the dataset (part 1).  The input <code>pred_indicator</code> is also similar to the part 1 implementation, I just set <code>pred_indicator[pred_index] = 1</code></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/kd6n1woFDcjgQa7BpQ69X4sz\" width=\"130\" height=\"236\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi. I had a similar issue as well. Not sure if we made the same mistake, but my mistake was that I called <code>tokenizer.tokenize(tokens)</code> which messed up the tokens since they were already tokenized. Once I deleted that line, everything worked as intended.</paragraph><paragraph/></document>"
        },
        {
            "source": "HW 5 Part 4 Model predicts 'O' tag. <document version=\"2.0\"><paragraph>I am working on part 4 and I noticed that the highest recommended tag is often <code>'O'</code> </paragraph><paragraph>Here is my result with the example given in 4. The <code>'B-V'</code> tag is accurate but it doesn't seem like the other tags are predicted. It is not because the other predicted tags are not in the <code>id_to_role</code> dictionary, but literally the <code>'O'</code> tag is getting predicted. </paragraph><paragraph>Does anyone know why this might be happening? The model inputs are <code>input_ids</code> and <code>attn_mask</code> processed similar to when we loaded in the dataset (part 1).  The input <code>pred_indicator</code> is also similar to the part 1 implementation, I just set <code>pred_indicator[pred_index] = 1</code></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/kd6n1woFDcjgQa7BpQ69X4sz\" width=\"130\" height=\"236\"/></figure><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>i am having this same issue (#779) but my loss and accuracy are the same as professor bauer after 2 epochs</paragraph></document>"
        }
    ],
    {
        "source": "GCP VM Instance Stops When Training. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I am currently training my model on part 3, and everything is working well. My issue is after about an hour and a half, the VM shuts down and times out, so all of my training progress gets lost. <break/><break/>Is there anything I can do to prevent this from happening? I saw someone on EdStem said to use the \"screen\" command, but the issue stemmed from the VM instance itself timing out.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>What do you mean the VM instance timed out? My problem was with my ssh session timing out so using screen to set the process to be in the background worked for me. </paragraph></document>"
    },
    {
        "source": "HW5. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I have been working primarily with Google Cloud for HW 5. I have been experiencing on and off issues. I am just checking to make sure Google Colab works and we don't need to do anything else but change to TCP4. </paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>So the assignment is meant to be solved on GCP and doesn\u2019t have to run anywhere else. But it should work on colab just fine, provided you get a GPU runtime.\u00a0</paragraph><paragraph>No changes to the actual code should be required.\u00a0</paragraph></document>"
    },
    {
        "source": "high loss and low accuracy values for part 3. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/meyXBAzKSFTemdk6NY233VdC\" width=\"616\" height=\"52\"/></figure><paragraph>Hi, I'm getting these values for part 3 after 2 epoch. Is this ok or are these off by too much? I'm not sure what could be wrong because everything up to this point, I've been getting the expected output. (similar loss value in part 2 for example). <break/><break/><break/>Some additional weird behavior I'm seeing.<break/><break/>1. The two epochs only take about 20-30 minutes to train for me, not several hours like is expected. The only thing I changed in the given train() code was computing the accuracy part.</paragraph><paragraph>2. this is epoch 2. The loss jumps up from 0.237 to 0.41 and stays around there until it's done.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/7ZUzWxoiMBuvuFqvvxkB0DV7\" width=\"643\" height=\"403.56456456456453\"/></figure><paragraph><break/><break/>3. I saved this model and used it for part 4 and 5. And I got 0.92227 accuracy for token-based accuracy, which doesn't seem terrible.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Hm the training loss seems a bit high, but if you get reasonable results during testing it may not be an issue.\u00a0</paragraph><paragraph>It\u2019s normal for the loss to jump around a little bit.\u00a0<break/></paragraph></document>"
    },
    {
        "source": "HW5 part 1.2.3. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm unclear as to what the parameter k is supposed to be. Can anyone clarify?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I'm not entirely sure, but I interpreted k as the index used to retrieve data from self.items.</paragraph><paragraph/></document>"
    },
    {
        "source": "k in 1.2.3 __getitem__(self,k). <document version=\"2.0\"><paragraph>is k in this section the actual item we need to process or the index of an item from our list of items?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>k is the index of the item that should be returned. Presumably, the data was just loaded into a list. It\u2019s just the index in this list.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Inconsistency in data size. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>In part 1.2.3, it is required that </paragraph><blockquote>Each sequence must be a pytorch tensor of shape (1,128).</blockquote><paragraph>However, in part 3, each batch are of the shape (32, 1, 128). To make sure that the shape is maintained correctly as (32, 128), do we change part 1.2.3 into a tensor of shape 128, or modify relevant codes in the dataloader in part 3?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph><strike>I had the same question! Did you find a fix for this?</strike></paragraph><paragraph>Nvm, looks like it's covered in #773!</paragraph><paragraph/></document>"
        },
        {
            "source": "Inconsistency in data size. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>In part 1.2.3, it is required that </paragraph><blockquote>Each sequence must be a pytorch tensor of shape (1,128).</blockquote><paragraph>However, in part 3, each batch are of the shape (32, 1, 128). To make sure that the shape is maintained correctly as (32, 128), do we change part 1.2.3 into a tensor of shape 128, or modify relevant codes in the dataloader in part 3?</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>I would change part 1.2.3.\u00a0</paragraph></document>"
        }
    ],
    [
        {
            "source": "Key error. <document version=\"2.0\"><paragraph>Hi,<break/><break/>I'm on the last part of part 2. I'm trying to take the argmax over every position. This is the part of the instructions I'm on (listed below). </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XGYpvxCnIzwevJjGfdK9PJzi\" width=\"642\" height=\"727.9095840867992\"/></figure><paragraph/><paragraph/><paragraph/><paragraph> However, when I do this my result looks like this. For some reason, some of my predictions are 0, and then when I try to use 0 with the id_to_role dictionary to decode the actual tokens, I get a key error because 0 is not a key in the id_to_role dictionary. Does anyone know what may be causing this? The loss I got from part 2 (3.89) was  close to the expected loss (3.97) so I think my parts 1 and 2 are correct so far.<break/></paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/tPbUkYCKfN6nC8kTtNBHfMMy\" width=\"657\" height=\"256.6577629382304\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Not a TA, but the ids of the roles are 1-indexed, so I added 1 to the argmax's. Not sure if this is correct though</paragraph></document>"
        },
        {
            "source": "Key error. <document version=\"2.0\"><paragraph>Hi,<break/><break/>I'm on the last part of part 2. I'm trying to take the argmax over every position. This is the part of the instructions I'm on (listed below). </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XGYpvxCnIzwevJjGfdK9PJzi\" width=\"642\" height=\"727.9095840867992\"/></figure><paragraph/><paragraph/><paragraph/><paragraph> However, when I do this my result looks like this. For some reason, some of my predictions are 0, and then when I try to use 0 with the id_to_role dictionary to decode the actual tokens, I get a key error because 0 is not a key in the id_to_role dictionary. Does anyone know what may be causing this? The loss I got from part 2 (3.89) was  close to the expected loss (3.97) so I think my parts 1 and 2 are correct so far.<break/></paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/tPbUkYCKfN6nC8kTtNBHfMMy\" width=\"657\" height=\"256.6577629382304\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I think that this is happening because this is before training . One of the posts says to return 'O' for values not in the dictionary </paragraph></document>"
        }
    ],
    {
        "source": "GCP Keeps on Crashing. <document version=\"2.0\"><paragraph>Hi, when I am trying to train my model on GCP, it keeps on crashing it would run for some time but then would throw an error, how can I fix this?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>What is the error?</paragraph></document>"
    },
    {
        "source": "HW5 Q2 loss unstable despite averaging. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>For HW2 Q2 I have tried the averaging different instantiations of the model trick mentioned in other posts. When averaging 5 results, the average loss ranges from about 3.84 to 4.06. When averaging 10 results, the error still goes around 3.91 to 4.01. Is this considered acceptable result?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That looks good to me. There is really no guarantee it will be exactly the expected value, but with more samples you should get closer.\u00a0</paragraph></document>"
    },
    {
        "source": "Regrade Request HW3. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I recently saw that I received a 25/30 on Part 3 for having too many dense layers. While I agree that this representation is incorrect according to the specifications of Part 3, I ended up submitting an experiment for changing the model (as was encouraged in Part 4 and performs better than the model specified in Part 3). In doing so, I changed the structure of what was asked from us in Part 3. While I understand, it is my fault for deviating from the instructions and I understand if I do not get the points back, I thought submitting a better version of the model asked from us in part 3 would suffice in its place. </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Please send an email so I have it in my inbox. I\u2019m processing these slowly.\u00a0</paragraph></document>"
    },
    {
        "source": "Hw 5 padding the sentence and tokens. <document version=\"2.0\"><paragraph>I'm a bit confused on how to pad the tokens in part 1.2.3. The instructions say, \"We need to process the sentence by adding \"[CLS]\" as the first token and \"[SEP]\" as the last token. The need to pad the token sequence to 128 tokens using the \"[PAD]\" symbol. This needs to happen both for the inputs (sentence token sequence) and outputs (BIO tag sequence).\"</paragraph><paragraph>Does that mean we need to add \"[CLS]\" , \"[SEP]\", and \"[PAD]\"  to the BIO tag sequence too?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s correct. Both the input and target need to be processed that way.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "HW 5 Part 2 Loss function runtime error. <document version=\"2.0\"><paragraph>I made the changes suggested in  #624 and it seemed like my model was created successfully (previously was getting the error described in #624). However, I'm still getting the same error again when computing the loss for one item.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XNNq78sWYLdWZwAcae01N4XT\" width=\"643\" height=\"92.86656200941916\"/></figure><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/GxUYrezphfDi50ljQMAto10f\" width=\"658\" height=\"43.822401614530776\"/></figure><paragraph/><paragraph>I am making sure to input the correct dimensions for output and targets, and I'm not sure how to fix this as I already made sure to edit the model code so it looks like this now. Does anyone know if there is any other ways to resolve the error?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/AdyrrMK5A6Zxuy5lSOF7uRch\" width=\"658\" height=\"442.60089686098655\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Update in case others have the same issue: I manually pushed the target tensor to GPU with <code>.cuda()</code> so that both target and outputs were on the GPU and it works now!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/gAkxSzfgxa77KNcf50ObfDyI\" width=\"680\" height=\"49.73134328358209\"/></figure><paragraph/></document>"
        },
        {
            "source": "HW 5 Part 2 Loss function runtime error. <document version=\"2.0\"><paragraph>I made the changes suggested in  #624 and it seemed like my model was created successfully (previously was getting the error described in #624). However, I'm still getting the same error again when computing the loss for one item.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XNNq78sWYLdWZwAcae01N4XT\" width=\"643\" height=\"92.86656200941916\"/></figure><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/GxUYrezphfDi50ljQMAto10f\" width=\"658\" height=\"43.822401614530776\"/></figure><paragraph/><paragraph>I am making sure to input the correct dimensions for output and targets, and I'm not sure how to fix this as I already made sure to edit the model code so it looks like this now. Does anyone know if there is any other ways to resolve the error?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/AdyrrMK5A6Zxuy5lSOF7uRch\" width=\"658\" height=\"442.60089686098655\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I believe targets is not saved to cuda. maybe do targets.to('cuda') before loss()... I did this step in SrlData getitem</paragraph></document>"
        }
    ],
    {
        "source": "HW5 Accuracy Calculation. <document version=\"2.0\"><paragraph>For the accuracy calculation, I just noticed that there is already some code in train() calculating the \"predictions\" and \"matching\" however I'm a bit confused at why the \"matching\" calculation does not seem to ignore the cases where [PAD] is predicted correctly. In my code I also stripped those matches out, are we not supposed to do that? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The code provided is incomplete. You will have to exclude the padded tokens before computing the accuracy.\u00a0</paragraph></document>"
    },
    {
        "source": "Decoding. <document version=\"2.0\"><paragraph>My epoch accuracy is 0.939 after training but my decoding output is the following. I checked all of my model inputs and outputs, so i think my model is just not predicting the right things. I'm not sure why this is happening when I have a pretty good training accuracy. I also tried using my model after 1 epoch (accuracy = 0.888) and have the same result. Any suggestions on debugging?</paragraph><pre>('A', '[CLS]')\n('U.', 'O')\n('N.', 'O')\n('team', 'O')\n('spent', 'O')\n('an', 'O')\n('hour', 'O')\n('inside', 'O')\n('the', 'O')\n('hospital', 'O')\n(',', 'O')\n('where', 'O')\n('it', 'O')\n('found', 'B-V')\n('evident', 'O')\n('signs', 'O')\n('of', 'O')\n('shelling', 'O')\n('and', 'O')\n('gunfire', 'O')\n('.', 'O')</pre></document>",
        "target": "<document version=\"2.0\"><paragraph>Oh my god. Ok, so I was correct that our models were trained properly. The pred_idx  that we're passing into label_sentence doesn't account for the fact that all indices are shifted right one to account for adding the [CLS] token. So to get the correct index for the predicate it's actually pred_idx + 1. </paragraph><paragraph/></document>"
    },
    {
        "source": "HW5 Part 4 - (a). <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For decoding in label_sentence, are we supposed to tokenize each token and send it to Bert for testing? Asking this because the word \"shelling\" is not tokenized in this expected output. Do we have to re-combine the tokenized words before outputting them? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/J1CdM8ltWESSXI9slO87Tnk9\" width=\"286\" height=\"310\"/></figure><paragraph>Thanks,</paragraph><paragraph>Samhit</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>I remember in a post professor told that Bert accepts tokenized text as input, so I think we need to send tokenized text to input and later convert the tokenized output labels back to labels for individual words and return the output.</paragraph></document>"
    },
    {
        "source": "HW5 - Part 3: Average accuracy. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For the average accuracy for each epoch calculation, can we just sum up all the accuracies for each batch (matching divided by predictions for that batch) and divide by the total number of steps, similar to average loss. (Or) should we sum up all the matchings, sum up all the predictions and divide the total_matchings by the total_predictions ?</paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I did the former using np.mean. </paragraph></document>"
    },
    {
        "source": "HW5 Part 6 - Same Span with different Tags. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I'm not sure if in the case where the prediction and target set both have the same span but with different tags, should we categorize it as FN or FP. Or both?</paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That situation would cause both a false negative and false positive.\u00a0</paragraph></document>"
    },
    {
        "source": "HW 5 Memory Issues + Part 3 training error. <document version=\"2.0\"><paragraph>Hello,</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/WvDGp9qVe9pUqEk3j8kvNFfY\" width=\"659\" height=\"150.16760828625237\"/></figure><paragraph>I'm running into some memory issues when trying to run train() and I don't know how to proceed. Before this error appeared, I also ran into the same issues as #773 and #766 with the not enough values to unpack.  Any help would be appreciated. </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I\u2019m not quite sure. What batch_size are you using? Did you make sure the BERT model is instantiated with a token length of 128?<break/></paragraph></document>"
    },
    [
        {
            "source": "HW5 Part3 Training Error. <document version=\"2.0\"><paragraph>I'm receiving this error and I'm not sure how to debug (I also don't have the unsqueeze calling in my getitem(). If anybody could help that would be great! Been stuck on this for a while:</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/HcI7RE7eVMU4b1BrjiyRSdvS\" width=\"658\" height=\"511.5568982880161\"/></figure><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>What is the actual input_shape (input_ids.shape). You could print it.\u00a0</paragraph></document>"
        },
        {
            "source": "HW5 Part3 Training Error. <document version=\"2.0\"><paragraph>I'm receiving this error and I'm not sure how to debug (I also don't have the unsqueeze calling in my getitem(). If anybody could help that would be great! Been stuck on this for a while:</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/HcI7RE7eVMU4b1BrjiyRSdvS\" width=\"658\" height=\"511.5568982880161\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I believe that the output tensors from __ getitem __ function is supposed to be in the shape of 128 (no need to reshape or unsqueeze). I only applied the reshape or unsqueeze to make it (1,128) for the individual tests before the \"train\" function since \"1\" refers to the batch size. In train(), the shape should be (32, 128) since the batch size is 32.</paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "HW 5 Submission. <document version=\"2.0\"><paragraph>are we supposed to submit the PDF from Print-&gt;Save as PDF as well as the FIle-&gt;Download As-&gt; Notebook?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s right.\u00a0</paragraph></document>"
    },
    {
        "source": "HW5 Part 6: Viable score. <document version=\"2.0\"><paragraph>Would it be possible to achieve an F1 score of 0.87 for evaluate_spans? Is this too high? Should we be including CLS and/or SEP?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>In the span based eval you should not include CLS or SEP (these are not part of any span).\u00a0</paragraph><paragraph>This seems high. But not impossible. What was your per token accuracy and loss?\u00a0</paragraph></document>"
    },
    {
        "source": "HW 5 Part 3 Training Error. <document version=\"2.0\"><paragraph>Hello,<break/><break/>For Part 3, when running Train(), my program is able to run for a few batches, but keeps crashing and outputting different errors, saying that it doesn't recognize a key. Is there an issue with the way in which I am invoking my get_item code? </paragraph><paragraph>My original loss from the end of part 2 is around 3.8-4.1 too.</paragraph><paragraph><break/>I truncated the token sequences before making them tensors, but I am still getting the follow errors:<break/><break/></paragraph><pre>KeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_2827/3364925475.py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_2827/2922944801.py in train()\n     19     epoch_acc=0\n     20 \n---&gt; 21     for idx, batch in enumerate(loader):\n     22         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    519             if self._sampler_iter is None:\n    520                 self._reset()\n--&gt; 521             data = self._next_data()\n    522             self._num_yielded += 1\n    523             if self._dataset_kind == _DatasetKind.Iterable and \\\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self._next_index()  # may raise StopIteration\n--&gt; 561         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    562         if self._pin_memory:\n    563             data = _utils.pin_memory.pin_memory(data)\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self.auto_collation:\n---&gt; 49             data = [self.dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self.dataset[possibly_batched_index]\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in &lt;listcomp&gt;(.0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self.auto_collation:\n---&gt; 49             data = [self.dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self.dataset[possibly_batched_index]\n\n/tmp/ipykernel_2827/2744729194.py in __getitem__(self, k)\n    120         tag_ids= []\n    121         for tag in padded[1]:\n--&gt; 122             tag_ids.append(role_to_id[tag])\n    123 \n    124 \n\nKeyError: 'B-ARGM-REC'\n</pre><paragraph/><paragraph/><pre>---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_2827/3364925475.py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_2827/2922944801.py in train()\n     19     epoch_acc=0\n     20 \n---&gt; 21     for idx, batch in enumerate(loader):\n     22         print(\"in da loop\")\n     23         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    519             if self._sampler_iter is None:\n    520                 self._reset()\n--&gt; 521             data = self._next_data()\n    522             self._num_yielded += 1\n    523             if self._dataset_kind == _DatasetKind.Iterable and \\\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self._next_index()  # may raise StopIteration\n--&gt; 561         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n    562         if self._pin_memory:\n    563             data = _utils.pin_memory.pin_memory(data)\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self.auto_collation:\n---&gt; 49             data = [self.dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self.dataset[possibly_batched_index]\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in &lt;listcomp&gt;(.0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self.auto_collation:\n---&gt; 49             data = [self.dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self.dataset[possibly_batched_index]\n\n/tmp/ipykernel_2827/2763776596.py in __getitem__(self, k)\n    119         tag_ids= []\n    120         for tag in padded[1]:\n--&gt; 121             tag_ids.append(role_to_id[tag])\n    122 \n    123 \n\nKeyError: 'B-ARG5'\n</pre></document>",
        "target": "<document version=\"2.0\"><paragraph>So I think the professor said that if you have a label that is not in the role_to_id dictionary (i.e. key errors like these), you replace it with 'O', so you would key into the 'O' when tag is not in the dictionary. I used a ternary operator for this, so I recommend using that! (formatted as role_to_id[tag if tag in role_to_id else 'O'], something like that)</paragraph><paragraph/></document>"
    },
    {
        "source": "HW-5 Model Accuracy and F1-score Validity. <document version=\"2.0\"><paragraph>Hi, <break/><break/>Just to confirm if they are valid scores,  I am getting the following after training for 2 epochs:<break/><break/><code>Training loss epoch: 0.053995941260483904</code></paragraph><paragraph><code>Training accuracy epoch: 0.983526443835309</code></paragraph><paragraph><code>Overall P: 0.7871418357806742</code> </paragraph><paragraph><code>Overall R: 0.7957545229120103</code> </paragraph><paragraph><code>Overall F1: 0.7914247481282417</code></paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That looks pretty good.\u00a0</paragraph></document>"
    },
    {
        "source": "Port Not Connecting for Jupiter. <document version=\"2.0\"><paragraph>Hi, I hope you are doing well</paragraph><paragraph/><paragraph>I'm trying to run the jupiter notebook on my browser, but I keep getting this error message even though my virtual machine successfully runs the command. I'm not sure how to proceed in this case, and I would love some guidance on this. Thank you!</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/P4UzL5fkhDaNL7g7ITvTSoSf\" width=\"658\" height=\"188.40473627556514\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/JItV8pBxE8A266LVh2Ty2ehb\" width=\"643\" height=\"437.1780986762936\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Double check the Jupyter notebook config file to check that the notebook is running on the external IP, not just 127.0.0.1. Also check the firewall config of the VM in the GCP console.\u00a0</paragraph></document>"
    },
    {
        "source": "hw 5 part 2 not enough values to unpack. <document version=\"2.0\"><paragraph>I'm running just the first part of part 2 where I just pick an item from my dataset and use it in the model. However, I'm running to this error: \"not enough values to unpack, expected 2 not 1\"</paragraph><paragraph/><paragraph>I double checked that each input to the model (ids, mask, and pred) are each tensors of length 128 -- does anyone know what might be the issue? </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/i2uu3lH3QASPPYPyXJpnX7dN\" width=\"493\" height=\"173\"/></figure><paragraph/><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/y8MKkPYHEG252SrpoUHdk5f3\" width=\"658.0000000000001\" height=\"437.8374291115312\"/></figure><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, I believe that the input tensors have to be of shape (1, 128), where 1 is your batch size.</paragraph></document>"
    },
    {
        "source": "HW 5 Part 2 Prediction Example. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>For Part 2, the example on the argmax prediction has '[CLS]' as the first token predicted. However, since we have not done any training by this point, it is expected that our example would not have '[CLS]' as its first token, correct? I just want to make sure I understand this part.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>So the model has not been fine tuned, but it has been pretrained in the masked LM objective. So I think it\u2019s likely that it would predict [CLS] for the first token.\u00a0</paragraph></document>"
    },
    {
        "source": "VM Instance not creating. <document version=\"2.0\"><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/DG2XdmQQM4zO4Vi9q5VvTrB3\" width=\"659\" height=\"107.71472081218275\"/></figure><paragraph>Hey everyone, </paragraph><paragraph>I got approved for 2 GPU quota, and its been over 15 minutes, but instance is still not finishing creating. Anyone else having this issue? </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>You can try hovering over the red exclamation point to see exactly what's going wrong, but I think that there might not be any GPUs available in the region that you're trying to create your VM instance. Prof Bauer talks about it in this question: #577 </paragraph><paragraph/></document>"
    },
    {
        "source": "HW5 Connection to Jupyter problem. <document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>I'm having a problem with accessing Jupyter notebook page. I followed the instructions and setup the firewall rules for jupyter notebooks, however when I tried to open the page [my vm ip]:8888 and it shows error message 'The site can't be reached' and 'refused to connect'. Is there anything I could do to fix this? Thank you so much for the help!!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hard to tell without a closer look. I suspect the Jupyter config may not be correct and the notebook server is only listening on the 127.0.0.1 IP, not the external ip of the VM. Make sure you copied the notebook config file correctly. \u00a0</paragraph></document>"
    },
    {
        "source": "can't reached the site. <document version=\"2.0\"><paragraph>I followed all the steps in the instructions, and cannot open the site. Can anyone tell me which step I did wrong? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/k67rpEijBdTX0Ir2WAENBNT5\" width=\"658\" height=\"248.30188679245282\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/aj4OlAY55UKGRjyrZxigswmI\" width=\"658\" height=\"306.8725663716814\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/8hPXtXhmpxf5gSkxtYYnfU3L\" width=\"492\" height=\"190\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>It looks like Jupyter is only running on the local IP. Make sure you copied the Jupyter configuration correctly. \u00a0</paragraph></document>"
    },
    {
        "source": "Cannot open Jupyter notebook using VM. <document version=\"2.0\"><paragraph>Hi, I've tried for a couple of hours to figure out how to open the Jupyter notebook from Google cloud VM. But it still does not work. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ybkbMdaybL2xjaXSGOA2d6Eu\" width=\"369\" height=\"279\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/6hKKliHyfTD0fZGcCHBSf8nA\" width=\"295\" height=\"264\"/></figure><paragraph>Is it possible to solely use Colab to finish homework 5? </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hey can you rejoin the zoom? Thanks!</paragraph><paragraph/></document>"
    },
    {
        "source": "HW 5 Unzipping Ontonotes. <document version=\"2.0\"><paragraph>Hello Professor Bauer and Teaching Staff, </paragraph><paragraph>I was wondering how to address this given that we can not respond \u2014 is the second screenshot a possible correct approach? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/uaa7PvBNnuA4dZZAgfT3BnRg\" width=\"658\" height=\"83.53918495297805\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/pOQAkit76VT4wqzDflqhTwBj\" width=\"658\" height=\"216.34920634920636\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Seems good. But it looks like you already had a version. \u00a0</paragraph></document>"
    },
    {
        "source": "model training time. <document version=\"2.0\"><paragraph>How long does \"a few hours\" mean for part 3?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, For the T4 GPU, I used <code>tqdm</code> and I am seeing as per the SS below, around 2 hrs per epoch</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/NP5GkdcaXXWwM7BRc3V5FIxN\" width=\"563\" height=\"34\"/></figure><paragraph/></document>"
    },
    {
        "source": "sentences with > 128 tokens. <document version=\"2.0\"><paragraph>how should we handle this case?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>In my implementation I just truncated these. But it may be better to just ignore the entire sentence.\u00a0</paragraph></document>"
    },
    {
        "source": "HW5 part 1.1. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>In HW5 part 1.1, is it that the \"tokenize_with_labels\" function is already completed and we don't need to do anything about it?<break/><break/>When I run \"tokenize_with_labels(\"the fancyful penguin devoured yummy fish .\".split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\".split(), tokenizer)\", I am already getting \"(['the', 'fancy', '##ful', 'penguin', 'dev', '##oured', 'yu', '##mmy', 'fish', '.'], ['B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'B-V', 'I-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'O'])\" as output.<break/><break/>Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, Professor Bauer left it completed for us. <link href=\"https://edstem.org/us/courses/46417/discussion/3992014\">#606</link></paragraph></document>"
    },
    [
        {
            "source": "blank notebook. <document version=\"2.0\"><paragraph>Has anyone had an issue with their notebook appearing blank? Its been working fine the last few days but now when i open the notebook its completely blank.</paragraph><paragraph>ssh into my instance is fine, its showing an active kernel, correct files are showing, it's showing my last save point...</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>have you try restarting the kernel?</paragraph></document>"
        },
        {
            "source": "blank notebook. <document version=\"2.0\"><paragraph>Has anyone had an issue with their notebook appearing blank? Its been working fine the last few days but now when i open the notebook its completely blank.</paragraph><paragraph>ssh into my instance is fine, its showing an active kernel, correct files are showing, it's showing my last save point...</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Does your ssh terminal say anything? I had this issue once, and i found that my jupyternotebook got untrusted. I used this <code>jupyter trust 4705-f23-hw5/4705_f23_hw5.ipynb</code>  to solve :)</paragraph><paragraph/><paragraph>[W 21:11:19.919 NotebookApp] Notebook 4705-f23-hw5/4705_f23_hw5.ipynb is not trusted<break/>^C<break/>tl3189@instance-2:~$ <code>jupyter trust 4705-f23-hw5/4705_f23_hw5.ipynb</code></paragraph></document>"
        }
    ],
    {
        "source": "hw5 part 2 loss. <document version=\"2.0\"><paragraph>similar question to #616: </paragraph><paragraph>My average loss is 3.99 but i've seen values as low as 3.73 and high as 4.14 after instantiating my model 10. Is this normal? I'm not really sure what I would change to get my loss consistently closer to the approximation. Is anyone able to explain why the loss should be close to the approximation and what would be affecting my values?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yeah I was getting similar results. Eventually I got a result, after reinstantiating the model many times, that was 0.03 off, and then I somehow managed to lose the result through restarting the kernel so that was frustrating. The professor said to just keep instantiating the model until you get something close to the desired result. It's not a very efficient way to handle this but I think it's the only way. Could be wrong though, so if anyone can confirm that would be helpful too.</paragraph><paragraph/></document>"
    },
    {
        "source": "How to fix this error in jupyter notebooks.. <document version=\"2.0\"><paragraph>Hi, <break/><break/>i was trying to follow the  setup instructions in jupyter notebook by running the code in each of the blocks. How do i fix the error in part 1 data preparation.  It says transformers module not found. I assume i have to install the transformers module somewhere, but I'm unfamiliar with Jupyter notebooks so not sure how to do fix this.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/tMdRS3aAtvxwQdB72nrTv0W1\" width=\"657\" height=\"1033.7201834862385\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>in Jupiter terminal \"!pip install transformers datasets\"</paragraph><paragraph>https://huggingface.co/docs/transformers/quicktour</paragraph></document>"
    },
    [
        {
            "source": "HW 5 Create firewall rule - Targets?. <document version=\"2.0\"><paragraph>What should I select?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/VB12v2S0HEnaOEDHhtiblHBm\" width=\"541\" height=\"151\"/></figure><paragraph>If 'Specified target tags\", what should I input:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/fTe3p8pOI1EtYWm9PIFHVjfi\" width=\"527\" height=\"81\"/></figure><paragraph>And why?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I picked all instances in the network and it worked. Not sure if this is correct though</paragraph></document>"
        },
        {
            "source": "HW 5 Create firewall rule - Targets?. <document version=\"2.0\"><paragraph>What should I select?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/VB12v2S0HEnaOEDHhtiblHBm\" width=\"541\" height=\"151\"/></figure><paragraph>If 'Specified target tags\", what should I input:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/fTe3p8pOI1EtYWm9PIFHVjfi\" width=\"527\" height=\"81\"/></figure><paragraph>And why?</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>All instances.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Jupyter token authentication is required.. <document version=\"2.0\"><paragraph>Hi,<break/><break/>I was following the steps for setting up HW5. as seen in this screenshot</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/yYgRmg0qHLfD61NwljmOU61s\" width=\"642\" height=\"343.65882352941173\"/></figure><paragraph><break/>After accessing from my local browser by running http://[your ip]:8888. I get this page. How should I proceed? I didn't see any instructions for how to handle this page in the HW directions.</paragraph><paragraph><break/><break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/OWw3dc43px11z2XgwTvq86f9\" width=\"657\" height=\"1162.1386861313867\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Try adding this to the Jupyter config:</paragraph><paragraph>c.NotebookApp.token = ''</paragraph><paragraph>Alternatively, just copy/paste the token that\u2019s being displayed on the terminal when you run Jupyter notebook.\u00a0</paragraph><paragraph/></document>"
    },
    {
        "source": "hw5 set up. <document version=\"2.0\"><paragraph>hi! in the future, I think a video tutorial for setting up this assignment (from gcp all the way to opening the notebook) would be extremely helpful. just wanted to suggest!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Thanks. I had an old tutorial from past semesters that was outdated, so I didn\u2019t distribute it this time. We can produce a new one for the upcoming spring iteration of the course.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "HW5 Part 3 Training Error - \"too many values to unpack\". <document version=\"2.0\"><paragraph>Hello all, </paragraph><paragraph>When trying to initially train the model in part 3, before modifying the train method at all, I get the below error (I also get it after modifying it).  The issue seems to be in the forward method of class SrlModel(). </paragraph><paragraph>Does anyone know what might be going wrong here?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/LelfaEd1RIpaxdmwpmVVzkge\" width=\"658\" height=\"213.42138364779873\"/></figure><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Can you print out the actual input shape? Seems like the shape tuple has more than two entries.\u00a0</paragraph></document>"
        },
        {
            "source": "HW5 Part 3 Training Error - \"too many values to unpack\". <document version=\"2.0\"><paragraph>Hello all, </paragraph><paragraph>When trying to initially train the model in part 3, before modifying the train method at all, I get the below error (I also get it after modifying it).  The issue seems to be in the forward method of class SrlModel(). </paragraph><paragraph>Does anyone know what might be going wrong here?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/LelfaEd1RIpaxdmwpmVVzkge\" width=\"658\" height=\"213.42138364779873\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I also observed this error, if I do <code>torch.tensor(token_ids, dtype=torch.long).unsqueeze(0)</code> , but removing \"unsqueeze\", makes it work fine. If you have the unsqueeze, try removing and retry </paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "Remaining OH for HW5?. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Wondering if there's any TA OH left for HW5? highly appreciate it! </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I will be holding one tmr</paragraph></document>"
    },
    {
        "source": "HW 5 - CUDA error. <document version=\"2.0\"><snippet language=\"py3\" runnable=\"true\" line-numbers=\"true\"><snippet-file id=\"code\">RuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_17324/2697911999.py in &lt;module&gt;\r\n----&gt; 1 model = SrlModel().to('cuda') # create new model and store weights in GPU memory\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\r\n    897             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n    898 \r\n--&gt; 899         return self._apply(convert)\r\n    900 \r\n    901     def register_backward_hook(\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self.children():\r\n--&gt; 570             module._apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self.children():\r\n--&gt; 570             module._apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self.children():\r\n--&gt; 570             module._apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    591             # `with torch.no_grad():`\r\n    592             with torch.no_grad():\r\n--&gt; 593                 param_applied = fn(param)\r\n    594             should_use_set_data = compute_should_use_set_data(param, param_applied)\r\n    595             if should_use_set_data:\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)\r\n    895                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\r\n    896                             non_blocking, memory_format=convert_to_format)\r\n--&gt; 897             return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\r\n    898 \r\n    899         return self._apply(convert)\r\n\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.</snippet-file></snippet><paragraph>I keep getting this error when I try to instantiate SRLModel, I'm unsure what to do. Here is the code block I'm running:</paragraph><snippet language=\"py3\" runnable=\"true\" line-numbers=\"true\"><snippet-file id=\"code\">model = SrlModel().to('cuda') # create new model and store weights in GPU memory</snippet-file></snippet></document>",
        "target": "<document version=\"2.0\"><paragraph>Nevermind I just restarted the notebook server and it worked</paragraph></document>"
    },
    {
        "source": "Incomplete Grade. <document version=\"2.0\"><paragraph>I recently sent an email to Professor Bauer to request an incomplete grade for this class. I was hoping to receive his approval given that the deadline is tonight. Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Responded by email.\u00a0</paragraph></document>"
    },
    {
        "source": "HW5 - Span-based Evaluation:. <document version=\"2.0\"><paragraph>Hi TAs and Prof., </paragraph><paragraph>Wanted to understand if my understanding of TP, FP, FN is correct because I'm getting very weird results for the span-based evaluation: </paragraph><list style=\"ordered\"><list-item><paragraph><bold>True Positives (TP)</bold>: Count a prediction as TP if the predicted span exactly matches a target span, including both the span and the label.</paragraph></list-item><list-item><paragraph><bold>False Positives (FP)</bold>: Count a prediction as FP if the predicted span does not match any span in the target. This includes:</paragraph><list style=\"unordered\"><list-item><paragraph>Predicted spans that don't exist in the target.</paragraph></list-item><list-item><paragraph>Predicted spans where the span is correct, but the label is incorrect.</paragraph></list-item></list></list-item><list-item><paragraph><bold>False Negatives (FN)</bold>: Count as FN if a span in the target:</paragraph><list style=\"unordered\"><list-item><paragraph>Does not exist in the predicted spans.</paragraph></list-item><list-item><paragraph>Exists in the predicted spans but with a different label.</paragraph></list-item></list></list-item></list><paragraph>Span-based valuation results: Overall P: 0.16827258962528463  Overall R: 0.7939763509332494  <bold>Overall F1: 0.27769208370387277</bold></paragraph><paragraph>Token-based valuation results: <bold>Accuracy: 0.9246983648015192</bold> (Looks decent)</paragraph><paragraph>Latest training epoch (after 2 epochs): <bold>Training loss epoch: 0.2138111023245621, Training accuracy epoch: 90.54548921959775%</bold> (Similar to what was given)<break/></paragraph><paragraph>Not sure why I'm getting such a low F-1 given other metrics seems to be correct? </paragraph><paragraph>Thanks in advance!</paragraph><pre>        # Calculate TP, FP, FN\n        for span, label in predicted_spans.items():\n            if span in target_spans:\n                if target_spans[span] == label:\n                    total_tp += 1  # Correct span and label\n                else:\n                    total_fp += 1  # Correct span but incorrect label\n            else:\n                total_fp += 1  # Span not in target\n\n        for span, label in target_spans.items():\n            if span not in predicted_spans:\n                total_fn += 1  # Span missing in predictions\n            elif predicted_spans[span] != label:\n                total_fn += 1  # Span present but labeled incorrectly in predictions</pre></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I think you might forget to only consider the span within [CLS_idx+1 : SEP_idx]. Otherwise, there would be too many FP leading to super low Precision and hence low F1.</paragraph><paragraph>In the case where the span exists in both prediction and target but with different tags, I'm not sure how to categorize this case (either FN or FN or both lol). We might need further clarifications.</paragraph></document>"
    },
    {
        "source": "HW5 Part 3 Implementation. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Part 3 asks us to \"modify the training loop... so that it also computes the accuracy for each batch and reports the average accuracy after the epoch.\" I just wanted clarification as to where exactly we implement this. Should this be under the part that says, \"# Compute accuracy for this batch\"? </paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes I would add it there.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "HW5 - 8888 in use when opening jupyter from browser. <document version=\"2.0\"><paragraph>Hello, </paragraph><paragraph>I have been trying to open jupyter from my browser, but it's not working. </paragraph><paragraph>My config should be correct. I've attached a screenshot just in case. </paragraph><paragraph>I've tried a few different IPs. </paragraph><paragraph>http://127.0.0.1:8923, http://10.208.0.2:8923, and http://34.165.248.255:8923. </paragraph><paragraph>I know it's not 8888, but according to the screenshot below, it seems to be in use. </paragraph><paragraph>I have a feeling this is the issue, as the firewall rule is configured for 8888.</paragraph><paragraph>The latter two IPs just load forever, while the first immediately fails (as expected as it is the loopback). </paragraph><paragraph>This is what I currently see. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YJnwJJIR4Vf517XbZj1nINsn\" width=\"658\" height=\"286.0472222222222\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/Nrb5OusGOiKVPSxi9RWCLhF1\" width=\"644\" height=\"402.5\"/></figure><paragraph/><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I seem to be having the same issue. There are no error messages in the terminal, but Jupyter won't open.</paragraph></document>"
        },
        {
            "source": "HW5 - 8888 in use when opening jupyter from browser. <document version=\"2.0\"><paragraph>Hello, </paragraph><paragraph>I have been trying to open jupyter from my browser, but it's not working. </paragraph><paragraph>My config should be correct. I've attached a screenshot just in case. </paragraph><paragraph>I've tried a few different IPs. </paragraph><paragraph>http://127.0.0.1:8923, http://10.208.0.2:8923, and http://34.165.248.255:8923. </paragraph><paragraph>I know it's not 8888, but according to the screenshot below, it seems to be in use. </paragraph><paragraph>I have a feeling this is the issue, as the firewall rule is configured for 8888.</paragraph><paragraph>The latter two IPs just load forever, while the first immediately fails (as expected as it is the loopback). </paragraph><paragraph>This is what I currently see. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YJnwJJIR4Vf517XbZj1nINsn\" width=\"658\" height=\"286.0472222222222\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/Nrb5OusGOiKVPSxi9RWCLhF1\" width=\"644\" height=\"402.5\"/></figure><paragraph/><paragraph/><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Odd. I assume restarting the vm instance would fix this.\u00a0</paragraph><paragraph>But you could also try to find which process is using that port and then kill it. To find the process try:</paragraph><paragraph>sudo netstat -nlp | grep :8888</paragraph></document>"
        },
        {
            "source": "HW5 - 8888 in use when opening jupyter from browser. <document version=\"2.0\"><paragraph>Hello, </paragraph><paragraph>I have been trying to open jupyter from my browser, but it's not working. </paragraph><paragraph>My config should be correct. I've attached a screenshot just in case. </paragraph><paragraph>I've tried a few different IPs. </paragraph><paragraph>http://127.0.0.1:8923, http://10.208.0.2:8923, and http://34.165.248.255:8923. </paragraph><paragraph>I know it's not 8888, but according to the screenshot below, it seems to be in use. </paragraph><paragraph>I have a feeling this is the issue, as the firewall rule is configured for 8888.</paragraph><paragraph>The latter two IPs just load forever, while the first immediately fails (as expected as it is the loopback). </paragraph><paragraph>This is what I currently see. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YJnwJJIR4Vf517XbZj1nINsn\" width=\"658\" height=\"286.0472222222222\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/Nrb5OusGOiKVPSxi9RWCLhF1\" width=\"644\" height=\"402.5\"/></figure><paragraph/><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I'm also running into the same issue! 8888 is available and I also followed the same instructions, but am unable to access the notebook. Has anyone been able to fix it?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/h0ghxDL5hWnaPD1uSsOa76sK\" width=\"611\" height=\"375.531561461794\"/></figure><paragraph> </paragraph></document>"
        }
    ],
    {
        "source": "HW 3 Regrade Requests. <document version=\"2.0\"><paragraph>I was just wondering, what is the procedure to request regrades for HW 3? The instructions on the home page say to email the TA who graded the assignment, but I can't see the name of the TA who graded my HW 3. Is there any other way to submit a regrade request?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Please email me directly. I forgot to include the TA names.</paragraph></document>"
    },
    {
        "source": "HW5- Dealing with label outside of the role_to_id dictionary. <document version=\"2.0\"><paragraph>When I'm training, I found cases where the label (e.g. 'B-ARGM-COM') is <bold>not a part of the dictionary created by the role_list.txt</bold> </paragraph><paragraph>The assignment didn't specify how to handle these specifically. Would treating them like ['PAD'] works? What would be the preferred approach? </paragraph><paragraph>Thanks in advance!<break/><break/>Training output: <break/>Batch loss:  4.183680534362793<break/>Current average loss: 4.183680534362793, Current average accuracy: 0.45558086560364464%<break/>Batch loss:  4.087785243988037<break/>Batch loss:  4.029718399047852<break/>Batch loss:  4.006991863250732</paragraph><paragraph>Error:</paragraph><pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_11409/3364925475.py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_11409/2737204925.py in train()\n<bold>     18</bold>     model.train()\n<bold>     19</bold> \n---&gt; 20     for idx, batch in enumerate(loader):\n<bold>     21</bold> \n<bold>     22</bold>         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\n<bold>    519</bold>             if self._sampler_iter is None:\n<bold>    520</bold>                 self._reset()\n--&gt; 521             data = self._next_data()\n<bold>    522</bold>             self._num_yielded += 1\n<bold>    523</bold>             if self._dataset_kind == _DatasetKind.Iterable and \\\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)\n<bold>    559</bold>     def _next_data(self):\n<bold>    560</bold>         index = self._next_index()  # may raise StopIteration\n--&gt; 561         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n<bold>    562</bold>         if self._pin_memory:\n<bold>    563</bold>             data = _utils.pin_memory.pin_memory(data)\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\n<bold>     47</bold>     def fetch(self, possibly_batched_index):\n<bold>     48</bold>         if self.auto_collation:\n---&gt; 49             data = [self.dataset[idx] for idx in possibly_batched_index]\n<bold>     50</bold>         else:\n<bold>     51</bold>             data = self.dataset[possibly_batched_index]\n\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in &lt;listcomp&gt;(.0)\n<bold>     47</bold>     def fetch(self, possibly_batched_index):\n<bold>     48</bold>         if self.auto_collation:\n---&gt; 49             data = [self.dataset[idx] for idx in possibly_batched_index]\n<bold>     50</bold>         else:\n<bold>     51</bold>             data = self.dataset[possibly_batched_index]\n\n/tmp/ipykernel_11409/835064567.py in __getitem__(self, k)\n<bold>     48</bold> \n<bold>     49</bold>         #labels to ids\n---&gt; 50         label_ids = [role_to_id[label] for label in label_sentence]\n<bold>     51</bold> \n<bold>     52</bold>         # Create attention mask\n\n/tmp/ipykernel_11409/835064567.py in &lt;listcomp&gt;(.0)\n<bold>     48</bold> \n<bold>     49</bold>         #labels to ids\n---&gt; 50         label_ids = [role_to_id[label] for label in label_sentence]\n<bold>     51</bold> \n<bold>     52</bold>         # Create attention mask\n\nKeyError: 'B-ARGM-COM'\n</pre></document>",
        "target": "<document version=\"1.0\"><paragraph>If a label doesn\u2019t appear on the role dictionary it should be replaced with O.\u00a0</paragraph></document>"
    },
    {
        "source": "HW5: Kernel death after Part2. <document version=\"2.0\"><paragraph>After running model here </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/JyX8HC8KONko9Oc8rDlx1AbV\" width=\"658.0000000000001\" height=\"123.41262580054897\"/></figure><paragraph>In this step or the prior step, I keep getting <bold>Kernal Death or Kernal Restart</bold> automatically </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/7Hy1xfDxCDjVe1lkkyZs2Oqc\" width=\"658\" height=\"162.22999080036797\"/></figure><paragraph>I tried starting my Notebook, restarting my SSH, and laptop, none of them works <break/>Are we allocated the right amount of memory for this task?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>What's the specific error message you encounter (if any)?</paragraph><paragraph>Are using a T4 GPU? </paragraph><paragraph>How much RAM did you set up for the VM?</paragraph></document>"
    },
    [
        {
            "source": "HW5 Setup. <document version=\"2.0\"><paragraph><bold>I encountered the following error when creating the instance as required.</bold> <break/></paragraph><paragraph><bold>Create VM instance \"instance-1\" and its boot disk \"instance-1\"</bold></paragraph><paragraph>A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-central1-a zone. Alternatively, you can try your request again with a different VM hardware configuration or at a later time. For more information, see the troubleshooting documentation.</paragraph><paragraph>What should i do? how to fix it?</paragraph><paragraph>I have changed and tried several zones. All unavailable.</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Yeah it can be tricky to find an instance. Keep trying (it\u2019s often easier during evenings/weekends).\u00a0</paragraph></document>"
        },
        {
            "source": "HW5 Setup. <document version=\"2.0\"><paragraph><bold>I encountered the following error when creating the instance as required.</bold> <break/></paragraph><paragraph><bold>Create VM instance \"instance-1\" and its boot disk \"instance-1\"</bold></paragraph><paragraph>A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-central1-a zone. Alternatively, you can try your request again with a different VM hardware configuration or at a later time. For more information, see the troubleshooting documentation.</paragraph><paragraph>What should i do? how to fix it?</paragraph><paragraph>I have changed and tried several zones. All unavailable.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I know this might not be particularly helpful but I just kept trying different zones until it worked!</paragraph></document>"
        }
    ],
    {
        "source": "Exam Q3. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>On the exam today, For Q3, I used capital L instead of lowercase l because i didn't want to confuse it for 1. I forgot to write that clarification down, is that ok?</paragraph><paragraph/><paragraph>Thanks</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s fine.\u00a0</paragraph></document>"
    },
    {
        "source": "AMR graph mandatory or optional?. <document version=\"2.0\"><paragraph>For any English-&gt;AMR question on the final, can we skip the graph and go straight to the PENMAN representation or do we have to draw the graph first?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Either output representation would be fine.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Ungraded exercise solution: AMR. <document version=\"2.0\"><paragraph>For the problem \"He admired Bob's love of math\", the solution is</paragraph><pre> (a/ admire\n\u00a0 \u00a0 :ARG0 (h / he)\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0:ARG1 (l / love\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 :ARG0 (b / Bob)\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 :ARG1 (m / math))) </pre><paragraph>It seems that \"love\" is arg1 of he, rather than arg1 of admire. Is this true? I feel like for \"He admires love\", love should be arg1 of admire.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I believe this is just a weird formatting issue. \"love\" is ARG1 of admire here. ChatGPT confirms this:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Esf2a3mdIA3mFD8o2b9ungNX\" width=\"758\" height=\"850.0829629629629\"/></figure></document>"
        },
        {
            "source": "Ungraded exercise solution: AMR. <document version=\"2.0\"><paragraph>For the problem \"He admired Bob's love of math\", the solution is</paragraph><pre> (a/ admire\n\u00a0 \u00a0 :ARG0 (h / he)\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0:ARG1 (l / love\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 :ARG0 (b / Bob)\n\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 \u00a0\u00a0 :ARG1 (m / math))) </pre><paragraph>It seems that \"love\" is arg1 of he, rather than arg1 of admire. Is this true? I feel like for \"He admires love\", love should be arg1 of admire.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I think you can think in such way:</paragraph><paragraph/><paragraph>He admired (Bob's) love (of math) -&gt; He (ARG0) admired love (ARG1).</paragraph><paragraph>Bob's love of math -&gt; Bob(ARG0)'s love of math(ARG1).</paragraph></document>"
        }
    ],
    {
        "source": "AMR format. <document version=\"2.0\"><paragraph>(y / yield-03</paragraph><paragraph> :ARG0 (f / fund</paragraph><paragraph> :mod (t / top)</paragraph><paragraph> :mod (m / money))</paragraph><paragraph> :ARG1 (o / over</paragraph><paragraph> :op1 (p / percentage-entity :value 9)</paragraph><paragraph> :degree (w / well))</paragraph><paragraph> :time (c / current))\" </paragraph><paragraph>do we need to know the exact number like '03' behind 'yield' in the exam2</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Its been confirmed that you do not need to know how to do AMR to this level of detail.</paragraph></document>"
    },
    {
        "source": "coverage for the final. <document version=\"2.0\"><paragraph>There was another thread saying how semantic parsing wasn't going to be on the final, but are there other sections that will not be covered on the final?</paragraph><paragraph>Or in other words which topics will be covered on the final?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>There's been a Page on CourseWorks with the entire topic list: https://courseworks2.columbia.edu/courses/179333/pages/exam-2-topic-overview</paragraph><paragraph/></document>"
    },
    {
        "source": "Ungraded Exercise IBM Model. <document version=\"2.0\"><paragraph>Is there a typo in part 3 of the IBM Model ungraded exercise? q(5 | 1, 5, 5) appears twice in the equation but I think of them is supposed to be q(1 | 5, 5, 5)?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, it's a typo, according to professor's comments in #678 </paragraph></document>"
    },
    {
        "source": "Ungraded Exercise: IBM Model 2. <document version=\"2.0\"><paragraph>I don't understand the answer to #670. How can I know that the alignment is inconsistent and crosses between noun phrases when \"e\" in the foreign sentence 2 to map to position 4 in the English sentence? In addition, is there any general strategy to find q probability? This is confusing since It looks like there are many possible cases.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>If the alignments for the nouns were different, you would get phrases that are not well nested. Of course this _could_ be the case, but seems highly unlikely given the general patterns implied by the other examples in the data.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Generative vs. Discriminative. <document version=\"2.0\"><paragraph>Hi, I'm recapping for Exam 2 and I've been struggling to understand the difference between Generative and Discriminative models. Are BERT and Naive Bayes discriminative or generative models? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I'm pretty sure that the fundamental difference between generative and discriminative models is that  generative models capture the joint probability P(x,y) while discriminative models capture the conditional probability P(y|x). Where y is a class and x is some input (what x is depends on the model we are working with I believe). What they actually do with the probability they capture varies between implementation of specific models. Generally though generative model is modelling the probability of a input and class occurring as a pair, this allows them to be better at modelling patterns within the data making them well suited to tasks needing the generation of new of data. Discriminative models instead capture the probability of class given some input, which makes them better at modelling the difference between classes and thus are often used for classification tasks. Also BERT is a discriminative model and Na\u00efve Bayes, despite being commonly used as a classifier, is a generative model.</paragraph></document>"
        },
        {
            "source": "Generative vs. Discriminative. <document version=\"2.0\"><paragraph>Hi, I'm recapping for Exam 2 and I've been struggling to understand the difference between Generative and Discriminative models. Are BERT and Naive Bayes discriminative or generative models? </paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>In a generative model, you model the distribution of the data, given some hidden variable, i.e. P(x|y). You then use Bayes rule to find the most likely y to explain your observation (input data) x.\u00a0</paragraph><paragraph>In a discriminative model you directly model the distribution P(y | x).\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Do ELMo embeddings use attention?. <document version=\"2.0\"><paragraph>While reviewing the ELMo slides, I realized that the \"s\" task-specific parameter looks the same as the \"\u03b1\" parameter mentioned in the slides for the attention mechanism. Does this mean that the ELMo representations also use the attention mechanism to generate the task-specific, contextualized embeddings? </paragraph><paragraph/><paragraph>Below are the relevant slides: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/15dM7WEPkJebweYhEOpG0la9\" width=\"658\" height=\"447.61904761904765\"/></figure><paragraph>Attention: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/n8g7Z6sT3XoQwxSpDfndiIRP\" width=\"657.9999999999999\" height=\"520.6504854368932\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I believe that in the context of ELMo the j's correspond to the layers in a stacked bidirectional RNN. For some tasks, researchers found that more heavily weighing the layers deeper into the model (where the embeddings have become more abstract) gives better results than just summing them all together equally, and sometimes they only use the top layer (layer L). This is different from the alpha parameter in attention, which corresponds to how relevant the key embedding of a token is to the query embedding of another token in a given layer. The attention mechanism involves a sum over N (the number of tokens), while the sum in the provided ELMo equation is over L (the number of bidirectional layers in the model).</paragraph></document>"
    },
    {
        "source": "Question on AMR. <document version=\"2.0\"><paragraph>Hi, I was wondering if there were any more resources to practice translating from AMR-to-English  in addition to the ungraded excercise.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I found this video to be really helpful. Especially the last 20 minutes. Start at 16:32 and watch in 2x speed. It goes through examples similar to the ungraded exercises. </paragraph><paragraph>https://www.youtube.com/watch?v=uCZ9nAe76Ss</paragraph></document>"
    },
    [
        {
            "source": "Representing task as GPT input problem. <document version=\"2.0\"><paragraph>Dumb quesiton but what does transformer return here for classification task ? Processed input representation with context ? Next word ? How does that translate to classificaiton ? Any help is appreciated, thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/wcWq4cFgHDJUIrTj0IE3vJDP\" width=\"542\" height=\"304.875\"/></figure><paragraph> </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>If you take a look at a GPT classifier (like GPT2ForSequenceClassification in <link href=\"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L1328\">https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L1328</link>), you will see the original head of the pre-trained model (which was used to train the model) is not used. Instead, a new head which is a singular Linear layer is used to convert the transformer outputs to logits. These then get converted to class scores via either MSELoss (regression), CrossEntropyLoss (binary classification), or BCEWithLogitsLoss (multiclass classification).</paragraph></document>"
        },
        {
            "source": "Representing task as GPT input problem. <document version=\"2.0\"><paragraph>Dumb quesiton but what does transformer return here for classification task ? Processed input representation with context ? Next word ? How does that translate to classificaiton ? Any help is appreciated, thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/wcWq4cFgHDJUIrTj0IE3vJDP\" width=\"542\" height=\"304.875\"/></figure><paragraph> </paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>The idea is that you augment the input with an Extract token, then you use the representation that GPT computes for this token (prior to the classification head that was used for pre training) and feed it into a classifier.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Attention similarity score. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>In the slides, it says that the way we compute the similarity score for attention is by doing matrix multiplication and scaling. I was wondering if there are other ways of doing this. </paragraph><paragraph>Thanks,</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>There are other ways to do this, but GPUs are really good at doing matrix multiplication as it is a highly parallelizable task. The alternative will dramatically increase the runtime needed to do attention. </paragraph></document>"
    },
    {
        "source": "[Important] Exam 1 Question. <document version=\"2.0\"><paragraph>Hello! I think the regrade window is closed, and that's honestly my bad, but I knew I got this question right. I assumed the grade I got was the grade I deserved.</paragraph><paragraph>However, the misgrade on Question 1 cost me a large number of points. Although it's technically my fault for missing the regrade deadline, as I am reviewing for the final exam, this -10 points on my midterm is just too large of a number for me to not try to ask for a regrade request even at this time of the semester.</paragraph><paragraph>As is apparent from the screenshots below, I did not use a table to show my work for the problem. <underline>Instead, I did it the other allowable way with equations.</underline> I did note to go to page 12 for my work on this problem, as this problem did not have an extra scratch page next to it on the exam.</paragraph><paragraph>This regrade request is not a petty few points. Getting these points back would solidly put my midterm grade in average zone. I would really appreciate if staff could make this change. Thank you so much for the consideration.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/R6gFNEm4TgdzCnddh305NR7t\" width=\"631\" height=\"453.87719298245617\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/TcJELXhOM146Nihb0WbCr5WW\" width=\"646\" height=\"348.0511551155115\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi please send this to my by email. There is a good chance it will get lost on Ed.\u00a0</paragraph></document>"
    },
    {
        "source": "Inputs and outputs of transformer component. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/awaWgCsWPMcm80HgVj8Xgg1T\" width=\"642\" height=\"361.125\"/></figure><list style=\"bullet\"><list-item><paragraph>Just to check my understanding =&gt; let's say we put 1 input token in, process and get a positional-encoded representation of the token</paragraph></list-item><list-item><paragraph>After that it's fed to attention block, which spits out a context vector of every ( is it every or is it every previous tokens ? ) in the input, with respect to the token that we are trying to guess. We do this by taking the weighted sum of those tokens and return that weighted sum (which is a context vector that contains similarity scores of each token)</paragraph></list-item><list-item><paragraph>This context vector (I assume the dimension is #of inputs *1) is then fed into a feed forward network.</paragraph></list-item></list><paragraph>=&gt; My question is: is the process above correct ? What does the feed forward network do ? Finally, I remember Dr. Bauer mentioned that we can have multiple context vector for the same token but I can't imagine how this is done, if anyone can give an example it would be amazing. Thanks</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I think one point of confusion is that there is no designated \u201ctarget\u201d token. All tokens in the input are passed into the encoder at the same time. The encoder produces a contextualized representation for each of the input tokens.\u00a0</paragraph><paragraph>The output will be a (batch_size, input_length, embedding_dim) tensor.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Meaning of f, e in MT setup, could be helpful for internalizing. <document version=\"2.0\"><paragraph>Hey everyone,</paragraph><paragraph>I was pretty confused about this naming convention for e, f (and the associated terms p(f|e), p(e) etc.) Kept thinking \"e\" stood for evidence in my attempts to internalize the content. Could not come up with a good reason why the Spanish text was called \"f\". I had almost given up in deciphering the naming method used here.</paragraph><paragraph/><paragraph>Turns out, historically, the setup/explanation of the MT task is usually made in the scenario where we are attempting to translate French (f) to English (e). [<link href=\"https://en.wikipedia.org/wiki/Statistical_machine_translation\">source1</link>, <link href=\"https://stanford.edu/~cpiech/cs221/apps/machineTranslation.html\">source2</link>]</paragraph><paragraph> It helped me to reason about the \"f\" as \"s\" in the lecture slides for Spanish, sharing in case it is helpful or if someone else was curious why they were called \"f\" and \"e\". </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>f is for french and e is for english in the examples</paragraph></document>"
        },
        {
            "source": "Meaning of f, e in MT setup, could be helpful for internalizing. <document version=\"2.0\"><paragraph>Hey everyone,</paragraph><paragraph>I was pretty confused about this naming convention for e, f (and the associated terms p(f|e), p(e) etc.) Kept thinking \"e\" stood for evidence in my attempts to internalize the content. Could not come up with a good reason why the Spanish text was called \"f\". I had almost given up in deciphering the naming method used here.</paragraph><paragraph/><paragraph>Turns out, historically, the setup/explanation of the MT task is usually made in the scenario where we are attempting to translate French (f) to English (e). [<link href=\"https://en.wikipedia.org/wiki/Statistical_machine_translation\">source1</link>, <link href=\"https://stanford.edu/~cpiech/cs221/apps/machineTranslation.html\">source2</link>]</paragraph><paragraph> It helped me to reason about the \"f\" as \"s\" in the lecture slides for Spanish, sharing in case it is helpful or if someone else was curious why they were called \"f\" and \"e\". </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>i think it means \"foreign\" and \"english\"?</paragraph><paragraph/></document>"
        }
    ],
    [
        {
            "source": "Semantic Parsing in Final. <document version=\"2.0\"><paragraph>Just to make sure: <bold><bold>Semantic Parsing</bold></bold> this whole part will not be tested in the final right? Thank you.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I just checked the final review recording, it will not be tested.</paragraph></document>"
        },
        {
            "source": "Semantic Parsing in Final. <document version=\"2.0\"><paragraph>Just to make sure: <bold><bold>Semantic Parsing</bold></bold> this whole part will not be tested in the final right? Thank you.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>It won't be tested. </paragraph></document>"
        }
    ],
    {
        "source": "Will the final review session slides be posted?. <document version=\"2.0\"><paragraph>Will the final review session slides be posted?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hey Jennifer, the slides should be here #630 ! Hope this helps</paragraph><paragraph>-SN</paragraph></document>"
    },
    [
        {
            "source": "Attention Matrices Q, K, and V. <document version=\"2.0\"><paragraph>I think the concept of attention as a soft lookup makes a lot of sense. You use the dot product as a sort of \"similarity\" score, and then use these as weights to fine-tune an aggregate of vectors. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/MtIefYahCXhRBy6KSWHGhamB\" width=\"374\" height=\"231\"/></figure><paragraph>This basically is storing information about what values to pay \"attention\" to in the context vector.</paragraph><paragraph>However, when this translates to attention matrices in transformers, I have a hard time understanding the concept. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/khtVPGCCDN3FMcPW8LpcXSj5\" width=\"658\" height=\"125.75675675675674\"/></figure><paragraph>We're still taking the similarity score of Q and K (this time as a matrix multiplication so we don't have to do it for every step), but what is V? What is the Value? In the above example, after calculating the score, we just multiply back the keys with high score (values and keys seem to be the same).</paragraph><paragraph>What exactly is the V matrix here? How do we calculate it? Is something different about Q and K?</paragraph><paragraph>When I googled this, it made it sound like Q, K, and V are all matrices that result from a matrix multiplication of the word embeddings with some weight matrix that is trained in the transformer. But that just made me more confused.</paragraph><paragraph>Any help would be appreciated.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I believe V is a matrix where each column is an input vector (following the positional encoding). The output is a matrix of vectors that have been adjusted for self-attention.</paragraph></document>"
        },
        {
            "source": "Attention Matrices Q, K, and V. <document version=\"2.0\"><paragraph>I think the concept of attention as a soft lookup makes a lot of sense. You use the dot product as a sort of \"similarity\" score, and then use these as weights to fine-tune an aggregate of vectors. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/MtIefYahCXhRBy6KSWHGhamB\" width=\"374\" height=\"231\"/></figure><paragraph>This basically is storing information about what values to pay \"attention\" to in the context vector.</paragraph><paragraph>However, when this translates to attention matrices in transformers, I have a hard time understanding the concept. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/khtVPGCCDN3FMcPW8LpcXSj5\" width=\"658\" height=\"125.75675675675674\"/></figure><paragraph>We're still taking the similarity score of Q and K (this time as a matrix multiplication so we don't have to do it for every step), but what is V? What is the Value? In the above example, after calculating the score, we just multiply back the keys with high score (values and keys seem to be the same).</paragraph><paragraph>What exactly is the V matrix here? How do we calculate it? Is something different about Q and K?</paragraph><paragraph>When I googled this, it made it sound like Q, K, and V are all matrices that result from a matrix multiplication of the word embeddings with some weight matrix that is trained in the transformer. But that just made me more confused.</paragraph><paragraph>Any help would be appreciated.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>V is a transformation matrix acting on the embeddings. It is is something different than Q, K. Maybe this, somewhat lengthy response in a different thread might help. </paragraph><paragraph><link href=\"https://edstem.org/us/courses/46417/discussion/4035163?answer=9287537\">https://edstem.org/us/courses/46417/discussion/4035163?answer=9287537</link></paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "Exam synchronously on Zoom. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I just wanted to know when we get some information on how the test is going to take place on zoom and general instructions for the exam. </paragraph><paragraph>Thanks</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>For those of you signed up for this in advance, I will distribute the link tomorrow morning. </paragraph><paragraph>You will get access to the exam on Gradescope at 1:10pm. You can then either print it, complete, and scan the final version back in -- or just use separate sheets of paper for your answers. </paragraph><paragraph>There will be extra time after the exam to finish scanning and uploading. </paragraph></document>"
    },
    {
        "source": "WordPiece and Bye-Pair Encoding. <document version=\"2.0\"><paragraph>Hi!<break/>In class we went through two types of tokenization that BERT uses (bye-pair encoding and WordPiece) are these things we should be able to replicate or is understanding the concept alone sufficient? Thank you! </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Understanding the concept is sufficient.\u00a0</paragraph></document>"
    },
    {
        "source": "amr structure. <document version=\"2.0\"><paragraph>Hi! I am just wondering is it acceptable  if we output sentence with a different structure than the solution provided during AMR translation (still have the same meaning\uff09</paragraph><paragraph>thank you</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>found answered in <link href=\"https://edstem.org/us/courses/46417/discussion/4037449\">#702</link> thanks</paragraph></document>"
    },
    [
        {
            "source": "BERT vs GPT?. <document version=\"2.0\"><paragraph>Hi, I'm confused on the difference between BERT and GPT and Transformer Models. Is the transformer model itself a combination of both BERT and GPT? If you are to choose between using BERT vs GPT is it just GPT if the task falls into one of the 4 categories (MCQ, Classification, Similarity, or Entailment) and BERT otherwise?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>not a TA so correct me if I am wrong:</paragraph><paragraph>BERT is implemented with a [encoder-only] transformer architecture, that's why it's called <bold>Bidirectional Encoder Representations from Transformers</bold>. </paragraph><paragraph>GPT is implemented with [decoder-only] transformer architecture, that's why it's called <bold>Generative Pre-trained Transformer.</bold> </paragraph><paragraph>So, in short, both of them are transformer models, I'd say it's a model architecture that's implemented by the above two models. </paragraph><paragraph>I am unsure of what exact model you want to choose between BERT and GPT. BERT is a masked model, which means that you can \"mask\" a word in a sentence and let BERT predict what the word would be(thus doesn't have to be a next word prediction). GPT is a generative model and thus can only predict the next word to appear. I don't think there are tasks that reply on specifically BERT or GPT. </paragraph></document>"
        },
        {
            "source": "BERT vs GPT?. <document version=\"2.0\"><paragraph>Hi, I'm confused on the difference between BERT and GPT and Transformer Models. Is the transformer model itself a combination of both BERT and GPT? If you are to choose between using BERT vs GPT is it just GPT if the task falls into one of the 4 categories (MCQ, Classification, Similarity, or Entailment) and BERT otherwise?</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>I think BERT is good for classification task and GPT is mainly used on generation task, generally speaking.</paragraph></document>"
        },
        {
            "source": "BERT vs GPT?. <document version=\"2.0\"><paragraph>Hi, I'm confused on the difference between BERT and GPT and Transformer Models. Is the transformer model itself a combination of both BERT and GPT? If you are to choose between using BERT vs GPT is it just GPT if the task falls into one of the 4 categories (MCQ, Classification, Similarity, or Entailment) and BERT otherwise?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I think BERT is better for understanding context because it considers left and right context, while GPT is more suitable for generative tasks. The inputs are also quite different for the two models, as the start/sep tokens are different for BERT and GPT(different input tokens depending on MC, similarity, entailment). </paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "ELMo Output. <document version=\"1.0\"><paragraph>Hello,<break/>What exactly is the output of running ELMo? My understanding was that it returned a word embeddings vector (but with context), similar to Word2Vec.<break/>But I got confused reviewing the \u201cnearest neighbors\u201d slide in Lecture 13; is the implication that ELMo is actually a sentence embeddings vector?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I think it might be the contextual embedding. For the \"nearest neighbors\", I think that refers to when you're trying to map the contextual embedding back into the lexeme during running time, you need to find the nearest contextual embedding to be its output lexeme.</paragraph><paragraph/></document>"
    },
    {
        "source": "AMR Translation. <document version=\"2.0\"><paragraph>Hi, While performing AMR translation is it okay if the sentence we make is structured differently than the original solution if they have the same meaning?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, AMR generalized away from the surface form of the sentence, so many different sentences can have the same AMR representation. </paragraph><paragraph/></document>"
    },
    {
        "source": "Context Size Query. <document version=\"2.0\"><paragraph>Hello! I am a little confused about whether context size is uni- or bi-directional in the scope of this class. By which I mean, for the sentence: \"The Quick Brown Fox\" with a context of 2, would the context for the word Quick be {\"Quick, The\", \"Quick, Brown\", \"Quick, Fox\"} or only {\"Quick, Brown\", \"Quick, Fox\"}.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I guess you could define this either way. Typically I would say something like \"a context window of +-2\". </paragraph><paragraph>But when it just says \"2-word window\" I would read this as +-1</paragraph><paragraph/><paragraph/></document>"
    },
    {
        "source": "AMR Translation to English. <document version=\"2.0\"><paragraph>For AMR translation to English such as in the ungraded exercise, does the verb tense of our answer matter as long as the overall semantic roles and meanings are captured? </paragraph><paragraph>For example, would \"Currently, the top money fund yields well over 9%\" be accepted versus the answer in the solution \"Currently, the top money fund is yielding well over 9%?\" </paragraph><paragraph>Thanks! </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>No, tense is one of the generalizations that AMR makes. It's not reflected in the AMR and therefore the English sentence could have any tense. </paragraph></document>"
    },
    {
        "source": "IBM arg max Question. <document version=\"2.0\"><paragraph>In lecture 18, the slides show that a_i = argmax q(j|i,l,m) * t (fi,ej)</paragraph><paragraph>I'm wondering is it equivalent to a_i = argmax q(j|i,l,m) * t (fi | ej)</paragraph><paragraph>In IBM exercise problem, we are also calculating the conditional probability instead of joint probability.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Ah, there's a typo in the slides. The t parameters are conditional probabilities, conditioned on e. So it should always be t (fi | ej).</paragraph></document>"
    },
    {
        "source": "exam 6 b. <document version=\"2.0\"><paragraph>hi i hope you are great,</paragraph><paragraph/><paragraph>I dont understand the solution for 6 B</paragraph><paragraph/><paragraph>In part A we calculated 3ed +dv parameters</paragraph><paragraph/><paragraph>In B we move from being giving embedding to, to do doing one hot.</paragraph><paragraph/><paragraph>so we add layer for one hot</paragraph><paragraph/><paragraph>so this would add: 3V * 3E parameters</paragraph><paragraph/><paragraph>i think</paragraph><paragraph/><paragraph>so wouldnt the total be:</paragraph><paragraph>(3V * 3E) + 3ed +dv parameters<break/><break/>i dont see how the actual answer is calculated</paragraph><paragraph/><paragraph>thank you so much!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>We use the same shared VxE embedding matrix for each of the tokens. We do compute the embedding matrix multiplication three times, but the matrix we multiply the one-hot encoding by is the same for each of these three multiplications, so there are only V*E new learnable parameters.</paragraph><paragraph/></document>"
    },
    {
        "source": "ARM Question. <document version=\"2.0\"><paragraph>Hi, I wonder how could we differentiate between ARG2 and ARG3 in AMR parsing? Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>You would need to look up the definition for ARG2 and ARG3 in the propbank framefile as these differ per predicate. Clearly not something you can do on the exam. </paragraph><paragraph/></document>"
    },
    [
        {
            "source": "Exam2 location and Time. <document version=\"2.0\"><paragraph>Just to confirm that the exam2 will start at <bold> 1:10pm Dec 11th</bold> in <bold>309 Havemeyer ?</bold></paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Yes, see #464</paragraph></document>"
        },
        {
            "source": "Exam2 location and Time. <document version=\"2.0\"><paragraph>Just to confirm that the exam2 will start at <bold> 1:10pm Dec 11th</bold> in <bold>309 Havemeyer ?</bold></paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Yes, that's correct -- the usual time and place. </paragraph></document>"
        }
    ],
    {
        "source": "Question about IBM Model2 q. <document version=\"2.0\"><paragraph>Hi, I'm a bit confused about IBM Model 2 after looking at the ungraded exercise. I saw that, for t(skwk|fly) and q(5 |1,5,5), the \"5\" in q corresponds to \"skwk\" and the \"1\" corresponds to \"fly\".</paragraph><paragraph>If we have a problem like this on the final, should the foreign or the english word come first in both t and q? In the ungraded exercise, the foreign word comes first in t but the english word comes first in q.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>For IBM Model 2, I believe it's:</paragraph><list style=\"number\"><list-item><paragraph>$t(f|e)$ for the probability that the output word $f$ is generated from the input word $e$.</paragraph></list-item><list-item><paragraph>$q(j|i,l,m)$ for the probability that the $i$-th alignment variable (i.e., $a_i$) is $j$ for input sentence length $l$ and output sentence length $m$. In other words, the probability that the $i$-th output word (i.e., $f_i$) is aligned to the $j$-th input word (i.e., $e_j$).</paragraph></list-item></list><paragraph>The probability of the whole sentence is $$\\prod_{i=1}^m q(a_i | i,l,m) t(f_i | e_{a_i}).$$</paragraph><paragraph/></document>"
    },
    {
        "source": "AMR Practice Question - why is the concept marked as -01, -03. <document version=\"2.0\"><paragraph>For \"</paragraph><paragraph>(y / yield-03</paragraph><paragraph> :ARG0 (f / fund</paragraph><paragraph> :mod (t / top)</paragraph><paragraph> :mod (m / money))</paragraph><paragraph> :ARG1 (o / over</paragraph><paragraph> :op1 (p / percentage-entity :value 9)</paragraph><paragraph> :degree (w / well))</paragraph><paragraph> :time (c / current))\" </paragraph><paragraph>I didn't get what does the \"-03\" mean in this case</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The -03 is just there to note that its the third sense/subsense of \"yield\" as defined in proppbank</paragraph><paragraph/></document>"
    },
    {
        "source": "AMR: Expected to know NER format. <document version=\"2.0\"><paragraph>For the exam, are we expected to know the format for NERs like time, person, date, etc?</paragraph><paragraph>(I mean this: time: (t/ time-12 :value \"4:00pm\"))</paragraph><paragraph>Or will we be given this formatting information on the exam? Just want to know if I should be placing it on my cheatsheet.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>You don't have to know the exact format. </paragraph></document>"
    },
    {
        "source": "Interpretation of Query, Key, Value Representations. <document version=\"2.0\"><paragraph>I understand that in the transformer model, query, key, and value vector representations are calculated using trainable weight matrices and the input embeddings. I'm curious how these different vector representaions can be interpreted. I'm wondering if there these embedding-like vectors carry different meanings of the same word? Like does the key form of a token supposedly represent a \"contextual meaning\" of a word whereas the query form captures a more central meaning of the word as a target (whatever that means)?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>This is a cool question. reminds me of a talk I had listened to either with Andrej Karpathy or Ilya Sutskever (can't remember which) discussing something similar. This is how I internalized some of that discussion. </paragraph><paragraph/><paragraph>At each attention layer, we have embeddings for each position. As we get deeper in the model, 2 things happen.</paragraph><paragraph>1. The embeddings get more complex, due to additional non-linearities.</paragraph><paragraph>2. They also get more contextualized. </paragraph><paragraph>The attention mechanism, with Q, K, V matrices (which are shared for each position's embedding) facilitate 2. And a nice analogy to how it does this is \"message passing\" or an \"online quorum\".   </paragraph><paragraph/><paragraph>When an embedding e_i is multiplied by Q(uery), q_i is generated. q_i, in my understanding, can be thought of as: <break/>\"As embedding e_i, here is a representation of the things I would like to know\"</paragraph><paragraph>So it's sort of a question that is asked by e_i, in the same way you might ask an online forum about what you'd like to know.</paragraph><paragraph>The k_i generated from the K(ey) is the natural analog to the above. It represents something along the lines:<break/>\"As embedding e_i, I can answers these kinds of questions\" </paragraph><paragraph>The score we calculate is then, kind of a matchmaking between askers and answerers. </paragraph><paragraph>Note that what the questions an embedding might be able to answer (it's k_i (key) value) and the actual answers to those questions might be different. </paragraph><paragraph>And that's where the V(alue) comes in. When two indices agree (via the score) that one can answer the other's question, it seeks the answer from the value vector. Thus v_i denotes something like:</paragraph><paragraph>\"As embedding e_i, here is the knowledge I possess that would be helpful to others\" </paragraph><paragraph/><paragraph>It happens that most of the time, the best answerer to a question an embedding might ask is itself. (i.e. the softmax is highest between its own query and key values) But attention basically presents an opportunity to see if there are other positions that possess something that might be helpful, which turns out to be super effective. (and the residual connections ensure that we don't lose track of what e_i was originally in this chatter) </paragraph><paragraph>So I think yes, the training does push these q,k,v vectors to capture different meaning, perhaps not semantically but contextually.</paragraph><paragraph>I know this about the world e_i (i.e. my semantics are set), What can I do with this?</paragraph><paragraph>1. Ask questions</paragraph><paragraph>2. List questions I can answer</paragraph><paragraph>3. Answer questions</paragraph><paragraph>Sorry for the long answer, I really like your question! </paragraph></document>"
    },
    {
        "source": "Important Conceptual Q. <document version=\"2.0\"><list style=\"unordered\"><list-item><list style=\"unordered\"><list-item><paragraph>Bayesian approaches: Model <bold>P(x|y) and P(y)</bold> <bold>using Baye\u2019s Rule</bold>. Assume observed data \u201cgenerated\u201d by some hidden class label. </paragraph><list style=\"unordered\"><list-item><paragraph>Naive Bayes</paragraph></list-item><list-item><paragraph>Gaussian Mixture Models</paragraph></list-item><list-item><paragraph>HMM</paragraph></list-item><list-item><paragraph>Applications: </paragraph><list style=\"unordered\"><list-item><paragraph>N-gram Language Model</paragraph></list-item><list-item><paragraph>CKY Parsing, PCFGs</paragraph></list-item><list-item><paragraph>Transition-based Dependency Parsing</paragraph></list-item><list-item><paragraph>IBM Model 2</paragraph></list-item><list-item><paragraph>JAMR</paragraph></list-item></list></list-item></list></list-item><list-item><paragraph>Discriminative approaches: Predicting <bold>P(y|x) directly</bold> with i<bold>nductive learning</bold>: given a bunch of input/output pairs, find h(x) that best approximates f(x) that maps inputs to outputs </paragraph><list style=\"unordered\"><list-item><paragraph>Linear and Log-linear model</paragraph></list-item><list-item><paragraph>SVM</paragraph></list-item><list-item><paragraph>Decision trees, random forrest</paragraph></list-item><list-item><paragraph>Applications: </paragraph><list style=\"unordered\"><list-item><paragraph>Neural Networks</paragraph></list-item><list-item><paragraph>RNN</paragraph></list-item><list-item><paragraph>Transformers</paragraph></list-item><list-item><paragraph>Summarization</paragraph></list-item><list-item><paragraph>Language and Vision</paragraph></list-item></list></list-item></list></list-item></list></list-item></list><paragraph/><paragraph>Hi TAs and Prof. Bauer, want to understand if my understanding of the different approaches and Applications are correct for all the models we covered. Please point out if I missed or misunderstood anything as well. </paragraph><paragraph/><paragraph>Thanks in advance!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Not sure about JAMR being a discriminative model. I think they are using a log-linear model in the original implementation. </paragraph><paragraph>You also seem to be mixing together models and applications. For example, summarization and language and vision would be an application, while RNN or transformer would be a model. </paragraph></document>"
    },
    {
        "source": "Ibm-Model 2. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>In this equation, what exactly are all the variables? (If translating from english to French, which is the english word and french (for both j, i, and l,m). </paragraph><paragraph><bold>q(j <bold>|</bold> i, l, m)</bold> </paragraph><paragraph/><paragraph>Also, are l and m necessarily equal. Can we also compare things which are 3 words and 5 words and find patterns? </paragraph><paragraph/><paragraph>Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>i is a token position in the source sentence (called \"foreign\" in the slides) and j is a token position in the target sentence (called \"English\" in the slides). l is the number of tokens in the target, m the number of tokens in the source. </paragraph><paragraph>l and m are not necessarily equal. You can have several source tokens aligned to the same target token, or even to nothing at all. You can also have unaligned tokens in the target language.  </paragraph></document>"
    },
    {
        "source": "Difference between k q and v for attention in transformers?. <document version=\"2.0\"><paragraph>Confused about the difference between Keys values and queries without the context  of transformers. if they are all input representations what makes them different? and do they change how  different they are for instance when calculating  cross attention (vs self attention) in the  decoder?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>In self attention, k, v, and q are all the token representations from the previous transformer layer.</paragraph><paragraph>In cross attention k and v are the token representations computed by the encoder, while q are the token representation from the previous decoder layer.\u00a0</paragraph></document>"
    },
    {
        "source": "Do Stacked RNNs need to be balanced?. <document version=\"2.0\"><paragraph>Hello!</paragraph><paragraph>For a stacked RNN model since we are encoding different vectors for each token, does it matter that the number of layers corresponding to each word in the text might not be the same or am I perhaps misunderstanding something? Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hm yeah I\u2019m not sure I understand the question. The number of layers is a hyper parameter of the model. It\u2019s the same for any input.\u00a0</paragraph></document>"
    },
    {
        "source": "Unfolded RNN vs traditional feed forward NN. What purpose does encoding hidden layers as an input parameter serve?. <document version=\"2.0\"><paragraph>Hello!</paragraph><paragraph>I am still a little confused on what exactly the difference is between the unfolded RNN and the traditional feed forward RNN is? I understand that we are now accounting for the vector encoded by the hidden layer (W*x) as a representation of the context to the left of the target word. Is the objective here that instead of each word having the semantic rep using 2 vectors (the word as a target and as a context word), we are combining into one vector? And if yes, what advantage does that hold over the regular NN model? Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Are you asking about unfolding for BPTT? The idea is that this provides a way to train the model and propagate later errors back to earlier input positions. There is no benefit in terms of the prediction itself and the model does not use more information than the not-unfolded RNN.\u00a0</paragraph></document>"
    },
    {
        "source": "Regarding Regualr Grammar. <document version=\"2.0\"><paragraph>According to definition in slides, A-&gt;aB or A-&gt;a is considered Regular Grammar. Is A-&gt;Ba also considered part of Regular Grammar?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>No</paragraph></document>"
    },
    {
        "source": "Question AMR Ungraded Assignment. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph> I'm confused about the AMR solution to the sentence \"We are expected to meet him at 4:00pm in the courtyard.\" Why are ARG1 and ARG0 both (w / we)? I appreciate your help\uff01\uff01</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><pre>(e / expect-01\n          :ARG1 (w / we)\n          :ARG2 (m / meet\n                    :ARG0 (w / we)\n                    :ARG1 (h / him)\n                    :time (t / time-12 :value \u201c4:00pm\u201d)\n                    :location (c / courtyard)))</pre><paragraph>\"We are expected to ....\", thus, with respect to the verb expect, \"we\" are being affected and thus become the patient/theme, hence tag ARG1.</paragraph><paragraph>What about the word \"meet\" ? Who is the person that takes this action ? The answer is \"we\", hence the tag ARG0 under meet.</paragraph></document>"
    },
    {
        "source": "vertbi ungraded. <document version=\"2.0\"><paragraph>hi i hope you are well</paragraph><paragraph/><paragraph>I dont quite understand why we say we take the max at every step for vertibi.  This is from the ungraded question solution:<break/><break/>\u03c0[0,Start] = 1</paragraph><paragraph>\u03c0[1,N] = \u03c0[0,Start] \u00b7 P(N|start) \u00b7 P(time |N) = 1 \u00b7 1/2 \u00b7 2/4 = 1/4<break/>\u03c0[1,V] = \u03c0[0,Start] \u00b7 P(V|start) \u00b7 P(time |V) = 1 \u00b7 1/2 \u00b7 1/6 = 1/12</paragraph><paragraph>\u03c0[2,N] = max( <bold>\u03c0[1,N] \u00b7 P(N|N) \u00b7 P(flies|N)</bold>, \u03c0[1,V] \u00b7 P(N|V) \u00b7 P(flies|N)) = max( (1/4 \u00b71/4 \u00b7 1/4), (1/12 \u00b72/3 \u00b7 1/4)) = 1/64<break/>\u03c0[2,V] = max( <bold>\u03c0[1,N] \u00b7 P(V|N) \u00b7 P(flies |V)</bold>, \u03c0[1,V] \u00b7 P(N|V) \u00b7 P(flies|N)) = max( (1/4 \u00b72/4 \u00b7 2/6), (1/12 \u00b7 <bold>0</bold> \u00b7 2/6)) = 1/24</paragraph><paragraph>\u03c0[3,V] = max( <bold>\u03c0[2,N] \u00b7P(V|N) \u00b7 P(like|V)</bold>, \u03c0[2,V] \u00b7 P(V|V) \u00b7 P(like|V)) = max( (1/64 \u00b7 2/4 \u00b7 2/6), (1/24 \u00b7 <bold>0</bold> \u00b7 2/6)) = 1/384<break/>\u03c0[3,Prep] = max( \u03c0[2,N] \u00b7P(Prep|N) \u00b7 P(like|Prep), <bold>\u03c0[2,V] \u00b7 P(Prep|V) \u00b7 P(like|Prep)</bold>) = max( (1/64 \u00b7 1/4 \u00b7 1), (1/24 \u00b7 1/3 \u00b7 1)) = 1/72</paragraph><paragraph>\u03c0[4,N] = max( \u03c0[3,V] \u00b7P(N|V) \u00b7 P(leaves|N), <bold>\u03c0[3,Prep] \u00b7 P(N|Prep) \u00b7 P(leaves|N)</bold>) = max( (1/384 \u00b7 2/3 \u00b7 1/4), (1/72 \u00b7 1 \u00b7 1/4 )) = <bold>1/288</bold><break/>\u03c0[4,V] = max( \u03c0[3,V] \u00b7P(V|V) \u00b7 P(leaves|V), \u03c0[3,Prep] \u00b7 P(V|Prep) \u00b7 P(leaves|V)) = max( (1/384 \u00b7 0 \u00b7 1/6), (1/72 \u00b7 0 \u00b7 1/6)) = 0</paragraph><paragraph>The most likely state sequence is N V Prep N.</paragraph><paragraph/><paragraph>I dont understand why we saying we take the max at each step.  It seems like we are work out each sequence from start to end and picking the one with the highest probability.  </paragraph><paragraph/><paragraph>thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I think the reason of taking the max at each step is to avoid the need to explicitly calculate and store probabilities for all possible sequences since that would be computationally expensive.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Question about IBM Model2. <document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>When finishing ungraded assignment, I got confused about this:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/xe5j1VNgno3DCLtlfso2IhnM\" width=\"312\" height=\"494\"/></figure><paragraph>How can we get this distribution? Why all the probabilities are 1?</paragraph><paragraph>And how can we get this: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/izLz9tZOzkpm45nofY9T3Hmz\" width=\"540\" height=\"298\"/></figure><paragraph>Thank you!</paragraph><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>And in this assignment, it seems that there is something wrong with the answer: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/f5pCRBQKQAnp6r4hrJfL5oGA\" width=\"705\" height=\"354.1706161137441\"/></figure><paragraph>or I have a misunderstanding about this part? This is my answer:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/fNaTLJB2s3Bi5MXQCiKBjrpR\" width=\"690.0000000000001\" height=\"502.5491624180627\"/></figure><paragraph/><paragraph/></document>"
        },
        {
            "source": "Question about IBM Model2. <document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>When finishing ungraded assignment, I got confused about this:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/xe5j1VNgno3DCLtlfso2IhnM\" width=\"312\" height=\"494\"/></figure><paragraph>How can we get this distribution? Why all the probabilities are 1?</paragraph><paragraph>And how can we get this: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/izLz9tZOzkpm45nofY9T3Hmz\" width=\"540\" height=\"298\"/></figure><paragraph>Thank you!</paragraph><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I think there are some typos in the solution. The last one should be q(1|5,5,5). </paragraph><paragraph/></document>"
        },
        {
            "source": "Question about IBM Model2. <document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>When finishing ungraded assignment, I got confused about this:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/xe5j1VNgno3DCLtlfso2IhnM\" width=\"312\" height=\"494\"/></figure><paragraph>How can we get this distribution? Why all the probabilities are 1?</paragraph><paragraph>And how can we get this: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/izLz9tZOzkpm45nofY9T3Hmz\" width=\"540\" height=\"298\"/></figure><paragraph>Thank you!</paragraph><paragraph/><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>All alignment possibilities are 1 because there is only one possible alignment per sentence pair, and for the same length these alignments are always the same. In the data, for each sentence pair of length 1, token 1 is aligned to 5, token 2 is aligned to 4, \u00a0etc</paragraph></document>"
        }
    ],
    {
        "source": "MT alignment probability understanding. <document version=\"2.0\"><paragraph>In the ungraded HW problem, we see an example of lexical divergence, resulting in non-1 translation probabilities. I was wondering in what scenario will there be a non-1 alignment probability and wanted to check if my understanding is correct.</paragraph><paragraph>If this is the example translation: He drinks coffee daily &lt;--&gt; on p'et kofe kazhdyy den</paragraph><paragraph>And this is the dictionary: He : on, drinks : p'et, coffee : kofe, daily : kazhdyy den</paragraph><paragraph>Will the alignment probabilities look something like this?</paragraph><paragraph>q(1 | 1, 4, 5) = 1, q(2 | 2, 4, 5) = 1, q(3 | 3, 4, 5) = 1, q(4 | 4, 4, 5) = 0.5, q(5 | 4, 4, 5) = 0.5.</paragraph><paragraph/><paragraph>Thank you in advance.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, if that one sentence was your only training data, you would get q(4 | 4, 4, 5) = 0.5, q(5 | 4, 4, 5) = 0.5.</paragraph><paragraph>More typically, you will have some inconsistent alignment  between multiple sentences, i.e. in some sentence pairs token 4 is aligned to position 4 and sometimes to position 5. </paragraph></document>"
    },
    {
        "source": "ELMo Understanding. <document version=\"2.0\"><paragraph>Hello, I am looking over the lecture slides again and wanted to make sure my understanding of ELMo was correct. Would it be that for a given sentence such as \"the cat is happy\" where you are analyzing the word \"cat\" ELMo would go forward for the context before the word \"cat\" (while attempting to predict \"cat\") and backwards for the words \"is happy\" (while attempting to predict \"cat\")?</paragraph><paragraph>Then finally both of these predictions would be combined to form an output?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That sounds almost correct. There are initially two different embedding for each token, one using the left context and one using the right context. Then, these are just concatenation.\u00a0</paragraph></document>"
    },
    {
        "source": "Exam1 Q6 b). <document version=\"2.0\"><paragraph>For Exam1 Question6 b), when using one-hot representation for input words and computing its embeddings, why does the embedding layer only add V*e parameters instead of 3V*e since there are 3 embeddings? How do we decide the embedding matrix is shared or not?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/R6ZRjFVdRCZkyV4CpMmzdOju\" width=\"658.0000000000001\" height=\"700.4516129032259\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>I think the embedding matrix weights are shared across each context word because we want to generalize a common matrix which encodes all the input word representations together.</paragraph></document>"
    },
    {
        "source": "Review session recording. <document version=\"2.0\"><paragraph>Hello! </paragraph><paragraph>I was wondering where we can find the recording of the review session from today (Sat. Dec 9th). I understand it just concluded, but wondering if it is already posted. </paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>i think its already posted :))</paragraph></document>"
    },
    {
        "source": "ungraded exercise - graph based dependency pasring. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>In the lecture notes, for the graph-based dependency parsing, we start from a completely connected graph and then find the maximum spanning tree from the fully connected graph as the dependency tree, so why the answer to the first question is \" The approach might create graphs that are disconnected\" given the generated tree is MST.</paragraph><paragraph>Thanks a lot!</paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>The approach described on the ungraded exercise does NOT use an MST algorithm. It simply greedily selects the highest scoring head for each token.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Attention as lookup?. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I understand attention as a concept generally (weighting hidden states depending on the decoding timestep) but I don't understand the \"attention as lookup\" idea. If the context vector is a aggregate of the hidden states, how does \"lookup\" fit in here? In the diagram below that I found, the query maps to all keys, so isn't it better to just describe this as a weighted sum? Why do we talk about attention as lookup? I feel like I'm missing something.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/6zNT4jav2LXOOBPTcNQRapGW\" width=\"522\" height=\"258.89273356401384\"/></figure><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>You are right that it feels like a weighted sum, at the end of the day there is a weighted sum that happens. Why this \"weighted sum\" is often though as a ~fuzzy~ lookup is because the mixing ratios in your weighted sum (for each embedding index) are determined/\"personalized\" by each position's query. </paragraph><paragraph/><paragraph>So people call this \"queried\"/\"personalized\" weighted sum that happens in attention a sort of lookup table by virtue of it being \"queryable\" like a lookup table. </paragraph></document>"
        },
        {
            "source": "Attention as lookup?. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I understand attention as a concept generally (weighting hidden states depending on the decoding timestep) but I don't understand the \"attention as lookup\" idea. If the context vector is a aggregate of the hidden states, how does \"lookup\" fit in here? In the diagram below that I found, the query maps to all keys, so isn't it better to just describe this as a weighted sum? Why do we talk about attention as lookup? I feel like I'm missing something.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/6zNT4jav2LXOOBPTcNQRapGW\" width=\"522\" height=\"258.89273356401384\"/></figure><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Very nice description, thanks. \u00a0</paragraph></document>"
        }
    ],
    {
        "source": "IBM Model 2: how to identify position when two of the same words in one sentence. <document version=\"2.0\"><paragraph>For ungraded assignment IBM Model 2 in example #4, isn't it possible for the \"e\" in the foreign sentence position 2 to map to position 4 in the English sentence?</paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Sure, but that would mean that the alignment is inconsistent and crosses between noun phrases. So while possible, it's not the most intuitive alignment. </paragraph></document>"
    },
    [
        {
            "source": "RNN Downstream Task. <document version=\"2.0\"><paragraph>Suppose we use RNN to some downstream task on a whole sentence (with input tokens x1,x2,....,xk). Do we use the final hidden state h_k or use the final output y_k (or both) to input in a linear model?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I think it depends on what you are trying to do with the sentence. Using hidden state can help understand the whole sentence. Final output can help understand what the sentence might llead to or its conclusion.</paragraph></document>"
        },
        {
            "source": "RNN Downstream Task. <document version=\"2.0\"><paragraph>Suppose we use RNN to some downstream task on a whole sentence (with input tokens x1,x2,....,xk). Do we use the final hidden state h_k or use the final output y_k (or both) to input in a linear model?</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Typically you would feed the hidden representations into the downstream classifier. Note that there is a difference between token classification tasks and sentence classification tasks. In sentence classification you would use the hidden state computed after the last time step.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "IBM Model 2 General Questions. <document version=\"2.0\"><paragraph>Hi, I am a little confused about the IBM Model 2, and I have some questions:<break/><break/>1. When calculating the probability <italic>q</italic>(<italic>j</italic>\u2223<italic>i</italic>), is the index <italic>i</italic> representing the position in the source language or the target language? </paragraph><paragraph>2. How does this choice impact the alignment and translation probabilities, and what considerations should be taken into account for optimal model performance?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>To answer your first question, it would make sense that i represents the position in the source language since q(j|i, l, m) represents the probability that the alignment variable i takes value j (from the slides). </paragraph><paragraph>In other words, what is the probability that the value of variable i actually takes the value of j?  For instance, if \"gato\" is at position i in some Spanish sentence and \"cat\" is at position j in the English sentence, then q(j | i, l, m) would represent the probability that the alignment variable i (representing the position of \"gato\" in the source language Spanish) takes the value j (the position of \"cat\" in the target language).</paragraph><paragraph/><paragraph><link href=\"https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/Collins_annotated.pdf\">https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/Collins_annotated.pdf</link></paragraph><paragraph/></document>"
    },
    [
        {
            "source": "AMR assignment. <document version=\"2.0\"><paragraph>I'm trying to better understand the usage of ARG0 and ARG1 in AMR. Could you explain when to use ARG0 as opposed to ARG1?</paragraph><pre> (a/ admire\n\u00a0 \u00a0 :ARG0 (h / he)\n</pre><paragraph>Here we use ARG0 following the verb admire.</paragraph><pre>(e / expect-01\n          :ARG1 (w / we)</pre><paragraph>Here we use ARG1.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Here's my interpretation (which may or may not be right): ARG0 is typically used to represent the entity or entities that perform the action denoted by the main verb.</paragraph><paragraph>In \"(a/ admire :ARG0 (h / he)),\" ARG0 is used to represent the agent, which is \"he\" (referring to someone) who is performing the action of admiring.</paragraph><paragraph>On the other hand, ARG1 is generally used to represent the entity or entities that undergo the action or are affected by it.</paragraph><paragraph>In \"(e / expect-01 :ARG1 (w / we)),\" now ARG1 is used to represent the patient or theme, which is \"we\" which refers to a group of people who are being expected.</paragraph><paragraph>Overall, use ARG0 to represent the agent or entities performing the action and use ARG1 to represent the entities undergoing or affected by the action.</paragraph></document>"
        },
        {
            "source": "AMR assignment. <document version=\"2.0\"><paragraph>I'm trying to better understand the usage of ARG0 and ARG1 in AMR. Could you explain when to use ARG0 as opposed to ARG1?</paragraph><pre> (a/ admire\n\u00a0 \u00a0 :ARG0 (h / he)\n</pre><paragraph>Here we use ARG0 following the verb admire.</paragraph><pre>(e / expect-01\n          :ARG1 (w / we)</pre><paragraph>Here we use ARG1.</paragraph></document>",
            "target": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/BFjpwUQw1nIZt2kzw2FnX1Qf\" width=\"632\" height=\"355.1395045632334\"/></figure><paragraph>I find that this example do a good job of understanding AMR for me. Notice that for line 2 and 3; we are tending to predicate b: beg -01. With respect to the word beg, we could see that \"I beg you\" would make \"I' the action-taker (hence ARG-0) and you the affected (hence ARG1). However, when we apply this recursive structure to the predicate \"excuse\", the roles are reveresed and so are ARG0 and ARG1 labels.</paragraph></document>"
        }
    ],
    {
        "source": "Can distillation work for generative NLP models like GPT?. <document version=\"2.0\"><paragraph>If so, why hasn't a distilled version of say GPT4 been developed? I would think a smaller, more portable version of GPT would be immensely useful for potential applications like on-device ChatGPT.</paragraph><paragraph/><paragraph>I found this Microsoft paper (<link href=\"https://arxiv.org/pdf/2108.13487.pdf\">https://arxiv.org/pdf/2108.13487.pdf</link>) that seems to indicate that at least on GPT3 similar training methodologies can lead to the new model outperforming the original model:</paragraph><paragraph>\"Brown et al. (2020) propose to directly use GPT-3 for downstream tasks, with the n given labeled instances and no fine-tuning. We refer to this strategy as raw GPT-3.<break/>We note that raw GPT-3 is expensive, as its cost goes linearly with the number of instances during inference. Also, it has a relatively high latency when deployed for real applications. However, even in terms of accuracy, we observe in the experiments from section 3.3 that the in-house models trained with GPT-3 labels can often outperform raw GPT-3. We argue that by using data labeled by GPT-3, we are essentially performing self-training: the predictions on unlabeled samples act as regularization on induced models and help improve the performance. In particular, for classification problems, we can theoretically upper-bound the error rate of the best in-house model using the labels generated by GPT-3.\"</paragraph><paragraph/><paragraph>I am just curious why if this is the case has it not been used or deployed in the real world.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Correct me if I misunderstood or if I am wrong. If you are asking about why we are not using labels from GPT4 to train models, I believe this is a (maybe widely) used approach for finetuning many LLM now: to use GPT4 feedback in finetuning as there has been evidence of positive correlation between human preference and GPT4 selection and always collecting human response is quite expensive. It's then collectively passed to a reinforcement strategy called DPO(direct policy optimization). I suggest checking out Prof. Rush's video here explaining how they did this on Zephyr(finetuned Mistral) <link href=\"https://www.youtube.com/watch?v=cuObPxCOBCw&amp;t=34s\">https://www.youtube.com/watch?v=cuObPxCOBCw&amp;t=34s</link> (paper here: <link href=\"https://arxiv.org/abs/2305.18290\">https://arxiv.org/abs/2305.18290</link>) </paragraph></document>"
    },
    {
        "source": "AMR Ungraded Assignment. <document version=\"2.0\"><paragraph>What's the purpose of ARG1 (w/we) after the line (e / expect -01)?</paragraph><paragraph>My understanding was that \"<bold>we</bold> are expected to meet him\" could be covered by:</paragraph><paragraph>(e / expect-01)</paragraph><paragraph>     :ARG1 (m/meet)</paragraph><paragraph>           :ARG0 (w/we) </paragraph><paragraph>           :ARG1 (h/him)    </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>I suppose this is a bit up to discussion, based on the propbank frameset for expect-01. Does this sense just cover \u201cARG0 expects an event ARG1\u201d, or would it instead be \u201cARG0 has an expectation of ARG1 to do ARG2\u201d.</paragraph><paragraph>I actually think your interpretation is better (and matches the one we came up with in class iirc).\u00a0</paragraph></document>"
    },
    {
        "source": "CVN Final Logistics. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>I hope you are doing well. I was wondering when we will get the CVN instructions for those of us taking the exam asynchronously. (Mostly just want to know the time frame in which we need to take the exam.)</paragraph><paragraph/><paragraph>Thanks in advance!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi,</paragraph><paragraph>The time window will open at 1:10pm on Monday and extend until Tuesday night, 11:59pm.\u00a0</paragraph></document>"
    },
    {
        "source": "Question about cheat sheet. <document version=\"2.0\"><paragraph>Dear Prof,</paragraph><paragraph>I'm wondering what kind of cheat sheet is accepted. Can we use printed cheat sheet? Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><heading level=\"2\"><link href=\"https://edstem.org/us/courses/46417/discussion/3807069\">#384</link></heading></document>"
    },
    {
        "source": "IBM model2. <document version=\"2.0\"><paragraph>1) for the probability q(j|i) in IBM model, is the dimension i standing for the dimension of source language or target language?</paragraph><paragraph>2) In order to make a one to many translation, does it mean the token number in source language (foreign language) is always more than the token number of target language (ie. English)?</paragraph><paragraph/><paragraph>3) If source language has 7 words and target language has 9 words, Can we use IBM Model?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>1) i is in the source language, j is in the target language. </paragraph><paragraph>2) Not necessarily, some words in the target language may simply not be aligned to anything in the source language. </paragraph><paragraph>3) Yes, see 2. </paragraph></document>"
    },
    {
        "source": "GPT API fine-tuning notes. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Want to ask for any helpful notes about using GPT 3.5-Turbo, GPT 4 API for fine-tuning? Any source available at Columbia? Thanks.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm, I'm not aware of a specific resource maintained at Columbia -- are you wondering about best practices etc.? I think most researchers are a little skeptical about using the API for fine-tuning since that doesn't give them full control of the process and transparency. So, in most recent work I have seen people tend to use open source models (like Llama). </paragraph><paragraph/></document>"
    },
    {
        "source": "Questions on Participation. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph/><paragraph>I want to know if there are any requirements for participation on Ed in order to get participation points, such as a certain number of times or should not be anonymous? </paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No require except for occasional posting. I can see you in the statistics tab, even when you post anonymously.\u00a0</paragraph></document>"
    },
    {
        "source": "how to count trainable parameters of model. <document version=\"2.0\"><paragraph>Hi, last exam we had a problem about how to count the trainable parameters of a model. I am still a bit confused about this concept. Can I ask the relationship between trainable parameters with input embedding, hidden layer, possible output words. And how this concept apply to like RNN or some other models.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Since feed-forward layers are fully connected, the weights between two layers can be expressed as a (e,d) matrix, where e is the dimensionality of the previous layer and d is the dimensionality of the successor vector.</paragraph><paragraph>For a simple Elman RNN model in the slides, the parameters consist of the weight matrices U and W, and a weight matrix to compute the output from the input. </paragraph><paragraph>For some of the more advanced models (like LSTMs and transformers etc.) things are more complicated because the internal architecture of the different components is more complex (think of the LSTM gates or the attention mechanism in the transformer). </paragraph><paragraph/></document>"
    },
    {
        "source": "GLUE and SuperGLUE. <document version=\"2.0\"><paragraph>Is there a reading material to understand in detail about how GLUE and SuperGLUE are computed? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>You could take a look at the glue and superglue websites:</paragraph><paragraph>https://gluebenchmark.com</paragraph><paragraph>https://super.gluebenchmark.com</paragraph><paragraph>This will not be in the exam (though it helps to have an idea how some of these problems can be solved with pretrained language models)\u00a0<break/></paragraph></document>"
    },
    [
        {
            "source": "Regarding BERT Pre-training. <document version=\"2.0\"><paragraph>There are two modeling objectives while training BERT. MLM and Next Sentence Prediction. Are these two tasks trained together? Specifically, I want to know how the loss function would look like.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Based on what I read online, I believe that these two tasks are trained together. If you look at the diagram below from the research paper introducing BERT, it seems like when they are feeding in the pre-training examples that they are masking two sentences and then concatenating them together with the [SEP] token. Then, the special classification token at the start is used to predict the NSP. Based on this, I believe that the loss function would probably be the combined MLM loss and NSP loss. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YxTR4IxTUeoS2623JnN0vB3Z\" width=\"528\" height=\"504\"/></figure><paragraph>Based off this medium article and the BERT paper:</paragraph><paragraph>https://arxiv.org/abs/1810.04805</paragraph><paragraph><link href=\"https://medium.com/@Suraj_Yadav/what-is-bert-how-it-is-trained-a-high-level-overview-1207a910aaed\">https://medium.com/@Suraj_Yadav/what-is-bert-how-it-is-trained-a-high-level-overview-1207a910aaed</link></paragraph><paragraph/><paragraph>Best Regards,</paragraph><paragraph>@fr.baebae</paragraph></document>"
        },
        {
            "source": "Regarding BERT Pre-training. <document version=\"2.0\"><paragraph>There are two modeling objectives while training BERT. MLM and Next Sentence Prediction. Are these two tasks trained together? Specifically, I want to know how the loss function would look like.</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Yes that\u2019s correct. Both objectives are used simultaneously and the losses are combined.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Ungraded Ex Graph-Based Dependency Parsing. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>In the objective function proposed in question c, I don't see how this object optimize for labeling the type of the edge. Do we need to think about this?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The problem assumes that dependencies are unlabeled \u2014 but it\u2019s probably a good idea to think about how you could modify the approach to deal with labeled edges.\u00a0</paragraph></document>"
    },
    {
        "source": "Incorrect file name submitted for HW4. <document version=\"2.0\"><paragraph>Hi Professor and TAs, I am just now realizing that the filename path for W2VMODEL_FILENAME in my lexsub_main.py submission is my local file path ('/Users/manasisoman/Downloads/GoogleNews-vectors-negative300.bin.gz') instead of ('GoogleNews-vectors-negative300.bin.gz'). I'm assuming this affects the autograder, so I just wanted to let you know that my Word2VecSubst class might not work as intended due to the file name error.</paragraph><paragraph>This is completely my bad, and I'm really sorry if this adds additional work to the TAs when grading. Let me know if there's anything I should do / if you think it's best if I resubmit the same code but with the different file name. </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>This will be fine. The TAs will be able to change the pathname during grading. \u00a0</paragraph></document>"
    },
    {
        "source": "lec17 predicate and function. <document version=\"2.0\"><paragraph>Hi, I feel not clear about the def of predicate and function in FOL models, why \"Student\" is a predicate not a function(on slide 12). In my understanding, predicate is a verb, argument represents each single word( not sure if I misunderstand these ). Thanks in advance!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>A predicate makes an assertion \u201cStudent(Mary)\u201d asserts that the individual designated by constant Mary is a student. The result is a truth value, either true or false.\u00a0</paragraph><paragraph>A function maps one constant to another one. \u201cBrother(Mary)\u201d returns another constant who is the brother of Mary. It\u2019s not asserting that Mary is a Brother.\u00a0</paragraph></document>"
    },
    {
        "source": "accidentally included code from external source in hw4. <document version=\"2.0\"><paragraph>Hi. Recently when I was doing hw4 part 6 I tried to solve the question by trying out a few methods including extracting word embedding from bert and using cosine similarity. when I was trying to use pytorch to manipulate the tensors since I don't have pervious experience with large language models and pytorch I consulted chatgpt. I worked on it for hours but in the end it didn't work, I did not include part6.predict in my files because I know my method didn't work but I did included whatever I had at the last minute in the code because I was hoping to get some advice from the TAs about why it didn't work and I leaved a comment stating that my code for part 6 does not work. yesterday while I was going through my code for part 6 again I realized I included some code from chat gpt. I am not sure about the policy about chatgpt regarding this class but I think I should report this because I am not sure if this is academic dishonest.  I am sorry for any trouble I have caused you and I will definitely learn from this in the future.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Thanks for coming forward and letting us know. We probably wouldn't have noticed, in all honesty. We will just grade your code as submitted without any issues. </paragraph><paragraph>Note though that the policy on the syllabus states \"<bold>AI-Tools/LLMs: </bold>The use of language model based AI to complete programming assignments is not permitted in this course, unless specified as part of an assignment.\"</paragraph><paragraph>So if you are working on homework 5, please do not use ChatGPT. <break/></paragraph><paragraph/></document>"
    },
    [
        {
            "source": "reentrancy in AMR. <document version=\"2.0\"><paragraph>What is reentrancy in the context of AMR and where in the slides is it explained?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi there, from my understanding reentrancy means that we are reusing the variables when something is referenced again in the sentence. I find this graph helps me a lot to understand the elements in AMR:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1Pi2ubxnTY7CMJ1y3qc3qu3x\" width=\"444\" height=\"362\"/></figure></document>"
        },
        {
            "source": "reentrancy in AMR. <document version=\"2.0\"><paragraph>What is reentrancy in the context of AMR and where in the slides is it explained?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>\"reentrancy\" simply refers to the idea that a vertex may have multiple incoming edges in the AMR graph. An entity may play multiple roles in relation to different predicates. </paragraph></document>"
        }
    ],
    {
        "source": "midterm regrade request. <document version=\"2.0\"><paragraph>I think this is a long shot, but I thought it was worth asking. I am redoing the my midterm and I noticed for question 1, I got points off for my chart, but I got the right answer. I am realizing that because of the lack of space, i could not write my full calculations, so I just wrote down the answer and not all the fractions that went into it. I did make a mistake  when starting the viterbi algorithm, I did not multiple the starting word with the corresponding pos value. This is a mistake, but I carried the mistake throughout the problem, which demonstrates an understanding of the algorithm. Looking at the rubric, I think i deserve 2 or 4 points off rather than 6 because I still got the final answer, I just made one wrong calculation that I carried throughout the problem. I know there is a lot of grading going on at the moment, and it is way past the midterm, so I understand if my request is denied. </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, sorry for the slow response. </paragraph><paragraph>The expectation was that you demonstrate understanding of the algorithm. Unfortunately, even though the answer is correct, your chart does not suggest that you computed the intermediate probabilities correctly (as previous_probability * transition_probability * emission_probability). A agree with the TAs' grading here. </paragraph><paragraph>Also note that the regrade period for exam 1 has passed. </paragraph></document>"
    },
    {
        "source": "window size for semantic relatedness and semantic similarity. <document version=\"2.0\"><paragraph>Hello!</paragraph><paragraph/><paragraph>I am not sure I entirely follow the logic behind wanting to keep the window size as large as possible to capture semantic relatedness (this alone makes sense since we want to have broader context for the target word) but keep the window small for semantic similarity? Is this simply because we dont want too many synonyms of hypernyms used in wrong contexts because as we broaden the context window, with more words substituting for the target word, we could have mismatches? Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>For small context windows, the idea is that other words that appear in the same context must be substitutes, so at least they need to have the correct part of speech. By capturing the immediate context, you will also likely capture some of the tokens context words have a direct semantic relation with the target (either arguments or the predicate of the target word, for example). So other words will share the same relation. </paragraph><paragraph>If you make the context bigger it becomes less likely for similar words to be exact substitutes. But they will still co-occur with the same content words, so they should be about the same topic and thus generally related. </paragraph></document>"
    },
    {
        "source": "blank midterm. <document version=\"2.0\"><paragraph>Hi is there a copy of the blank midterm? I want to use it as practice without the solutions</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I don't think there is a blank copy. The files section on Canvas only contains the solution copy. </paragraph><paragraph/></document>"
    },
    {
        "source": "Cross attention. <document version=\"2.0\"><paragraph>Hi, I wonder how does the cross attention work and its algorithm? From my understanding the cross attention is implemented in the decoder layer as the multi-head attention to the input representation. Is that correct?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, the only difference is between self-attention and cross-attention is that in the former all of key, query, and value vectors are from the same encoder embeddings whereas in the latter key and value from encoder embeddings while the query is constructed from decoder-side. </paragraph><paragraph>Check this neat illustration, especially the section \"Decoder Side\"</paragraph><paragraph><link href=\"http://jalammar.github.io/illustrated-transformer/\">http://jalammar.github.io/illustrated-transformer/</link></paragraph><paragraph/></document>"
    },
    {
        "source": "Question Towards Ungraded Exercise. <document version=\"2.0\"><paragraph>the bird ate a fly -----&gt; skwk e qta skwk et</paragraph><paragraph>It's unclear here whether the first skwk means bird or the second means bird.</paragraph><paragraph>As a result, q(1|2,5,5) should not be zero because  the first skwk (at position 1) may correspond to bird (in the 2nd position), right?</paragraph><paragraph/><paragraph>Similarly,  q(4|5,5,5) should not be zero because there's a possibility that in sentence 3, j =4 and i = 5.</paragraph><paragraph/><paragraph>Is my analysis correct?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>So since \"e\" must mean \"a\" and \"et\" must mean \"the\" (from example 1 and 2), in sentence 3 the nounphrase \"the bird\" must appear to the right of the verb while \"a fly\" must appear to the left of the verb. It would be very odd if the alignments were just crossing between noun phrases. </paragraph></document>"
    },
    {
        "source": "Weight Vector in Linear Models. <document version=\"2.0\"><paragraph>In question 5 of midterm, </paragraph><paragraph>Specify a set of concrete features and a corresponding weight vector w that will produce the correct classification for the documents in the training data.<break/><break/>In the solution the weight vector is given as (1,2,3). I did not understand how it has been calculated.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>The weights were selected to break ties between sentence 1 and 2 and sentence 2 and 3. </paragraph><paragraph>In sentence 2, both feature 1 and 2 would be active, so by assigning a higher weight to feature 2, the model selects the correct label. </paragraph><paragraph>Similarly in sentence 3, both feature 2 and 3 would be active, so you need to select a higher weight for feature 3 in order to predict the correct label. </paragraph><paragraph>There are many feature vectors that satisfy these conditions, it could have been (1, 10, 100)  or (1, 3, 9) just as long as weight 2 &gt; weight 1 and weight 3 &gt; weight 2. </paragraph></document>"
    },
    {
        "source": "AMR Question. <document version=\"2.0\"><paragraph>How to represent \"am/is/are\" in AMR</paragraph><paragraph/><paragraph>For instant, \"I am good at coding\". or \"I am so tired about the final exam\". How to use AMR to represent this?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I believe that you don't need to ever represent \"am/is/are\" in AMR. If you think about it AMR is to just convey the meaning away from the syntactical representation. Am/is/are do not add meaning to any sentence and they are more like linking some subject to its predicate. They are also just versions of \"be\", so you can just look here in the documentation to see: https://www.isi.edu/~ulf/amr/lib/popup/be.html</paragraph><paragraph>also this video was in that documentation: https://www.youtube.com/watch?v=h66NudhsJgM&amp;ab_channel=isinlp</paragraph><paragraph>As for your example I am not 100% but they way I would represent the first one would be</paragraph><pre>(c / code-01\n           :ARG0 (i / I)\n           :manner (g / good))\n</pre><paragraph>I am not sure if the top node could be \"excel\", \"proficient\" or something like that since the main concept is the skill level. </paragraph></document>"
    },
    {
        "source": "Weight Vectors inn Linear Models. <document version=\"2.0\"><paragraph>I am a little lost on 1) the importance and use of weight vectors in Linear Models where we are manually identifying features and 2) how we solve for these weight vectors.</paragraph><paragraph>There are 2 problems I am reviewing - the ungraded and the Exam1 problem, but I can't find the approach or reasoning to solve these. </paragraph><paragraph>Can someone please provide insight into how I go about solving this problem? Thank you.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>1) What do you mean by importance? Without the weight vector you would not be able to predict anything.</paragraph><paragraph>2) There is, in general, not an analytical way to find the optimal weight vector. Instead, training is done incrementally (using perceptron training for the linear model or gradient ascent on a loglinear model). </paragraph><paragraph>But on the specific problems we have seen, the weight vector was easy to find, mostly because there were only very few trainign examples. The trick is to look at which features will fire for each instance, and then select the weight vector so that the feature that is paired with the correct class has the highest score. </paragraph><paragraph>I think there was an Ed post asking about this specific question a while ago (when the midterm solutions first came out), but I couldn't find it right now. </paragraph><paragraph/></document>"
    },
    {
        "source": "GPT/BERT downstream task. <document version=\"2.0\"><paragraph>Suppose in we are given a task similar to what we did in CFG Parsing, ie. finding the Non terminal for each word, but using a BERT/GPT to do so.</paragraph><paragraph>We talk in class to decapitate the last layer of BERT/GPT, and add a classifier. </paragraph><paragraph>1) If we are asked about this kind of down stream task, do we need to specify the linear classifier (ie. it may be a multi-class logistic regression or a Feed forward network)? Do we need to specify the loss function or training objective of the task (ie. maximize the sum of log probability of each true Nonterminal label)?</paragraph><paragraph>2) Generally I remember we use CLS token or Extract token for downstream tasks. In this case, we need to classify each token's non terminal, do we need these CLS/Extract tokens to input them into linear classifier. I can think of several cases</paragraph><paragraph>a) only input CLS/Extract Token</paragraph><paragraph>b) forget about CLS/Extract, input BERT/Transformer embeddings for each word</paragraph><paragraph>c) Use a augmented input ie. [CLS,Vi] for each token where Vi is the embedding for each token.</paragraph><paragraph>Which method should I select?</paragraph><paragraph>Generally in any tasks, how should we select for (a), (b), (c)</paragraph><paragraph>3) To do So, there are two methods, one is fixing the original BERT/GPT's parameter fixed, the other is changing BERT/GPT's parameter according to downstream task. Which should we choose if we are given these types of problem?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>So for this kind of problem there are generally multiple approaches (all of which would give you full points on an exam). It sounds like your understanding here is sound. </paragraph><paragraph>1) Yes, you would have to specify in detail what the downstream classifier looks like. The question would ask for training objective and loss function specifically if that was expected. </paragraph><paragraph>2) It depends on the ask. For per-sentence or multi-sentence tasks, you would use the CLS or Extract token. For per-token tasks you would use the individual token embeddings. <break/>I think your approach c) is not super common -- I have seen this before, but the applications are limited. </paragraph><paragraph>3) It depends! On some tasks fine-tuning the model on the downstream task is necessary, but it's usually much more expensive. On other tasks, the pre-trained contextualized representations may be good enough (for example, because the task is somewhat similar to the pretraining objective). </paragraph><paragraph/></document>"
    },
    {
        "source": "Differences between GPT models. <document version=\"2.0\"><paragraph>So in lecture, the model we go over is called GPT (I assume this is GPT1). Are there major training/design differences between the different GPT models (GPT1 vs GPT2 vs GPT3 vs GPT3.5 vs GPT4), or are larger number versions typically just larger models trained on larger corpuses?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>As far as I understand, for the most part the models are identical just with exponentially more parameters and trained on larger and more varied data sets.  GPT3 adds instruction tuning. </paragraph><paragraph/></document>"
    },
    {
        "source": "HW3 and HW4 Grades Release Date. <document version=\"2.0\"><paragraph>Hi Teaching Team,</paragraph><paragraph>By when can we expect hw3 and hw4 grades to be released? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hey! I recommend checking out #618! Hope this helps!</paragraph></document>"
    },
    {
        "source": "OHs after Exam?. <document version=\"2.0\"><paragraph>Will there be reduced office hours after the final exam? Or will they continue as normal for students doing HW 5?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>They will likely be reduced, but some office hours will continue throughout reading week. </paragraph></document>"
    },
    {
        "source": "Question about final exam. <document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>Will we take the final exam at 309 Havemyer? Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>We will take the final at the lecture room on the day of if I recall correctly from what Professor Bauer said in class (same situation as midterm).</paragraph></document>"
    },
    {
        "source": "Homework 3 and 4 Grades. <document version=\"2.0\"><paragraph>Will homework 3 and 4 grades be released before homework 5 is due?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi! I recommend checking out #618! Hope this helps!</paragraph></document>"
    },
    {
        "source": "Participation Grade. <document version=\"2.0\"><paragraph>Hello!</paragraph><paragraph>Was wondering if participation on EdStem would suffice for a 10/10 on the participation grade. Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, the participation grade is all-or-nothing. Participation on Ed is sufficient to get this part of the grade. </paragraph></document>"
    },
    [
        {
            "source": "AMR translation. <document version=\"2.0\"><paragraph>While we translate AMR into sentences, could we have multiple correct sentences derived from the same ARM?<break/><break/>Thank you.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Not a TA, but since the AMR tree is just a complication of lemmas, there are many different ways to interpret the sentence while preserving the semantics of the sentence.<break/><break/><break/>As shown in the paper \"<bold><link href=\"https://aclanthology.org/W13-2322.pdf\">Abstract Meaning Representation for Sembanking</link> \":</bold></paragraph><list style=\"unordered\"><list-item><paragraph>AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences \u201che described her as a genius\u201d, \u201chis description of her: genius\u201d, and \u201cshe was a genius, according to his description\u201d are all as- signed the same AMR. </paragraph></list-item></list><paragraph/></document>"
        },
        {
            "source": "AMR translation. <document version=\"2.0\"><paragraph>While we translate AMR into sentences, could we have multiple correct sentences derived from the same ARM?<break/><break/>Thank you.</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Yes, that\u2019s a crucial idea behind AMR: many sentences can have the same annotation. AMR is abstracting from the specific surface form.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "AMR Solution. <document version=\"2.0\"><paragraph>For the AMR problem, are we supposed to know the two approaches discussed in class to parse (JAMR and bauers method)? I was wondering if there is more than one correct solutions. In other words, does my answer have to be identical to the solution? I get the general idea of how to parse (main verb up top, recursively find arguments or concepts to further break down) but I am confused about the step by step 'formula' or details. Just a little concerned about how to solve this type of problem on a test.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>There is no specific algorithm for doing these translations \u2014 and since you don\u2019t know the full annotation guidelines, chances are you won\u2019t be able to get English-&gt;AMR exactly \u201ccorrect\u201d. Note that even trained annotators only get about a 90% agreement.\u00a0</paragraph><paragraph>For AMR-&gt; English, there are clearly many different sentences (all with the same meaning).\u00a0</paragraph><paragraph>What we are after is simply \u201cwho does what to whom, when, where, why\u2026\u201d. \u00a0As long as the basic predicate/argument structure (including reentrancy) reflects the sentence, your solution would count as correct.\u00a0</paragraph></document>"
    },
    {
        "source": "Clarification on Semantic roles, theta roles, deep cases. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>Just wondering if these terminologies are interchangeable?</paragraph><paragraph>Thanks </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>For the most part. They originate from a slightly different branch of the literature, but they essentially mean the same.</paragraph></document>"
    },
    {
        "source": "Question towards dependency parsing. <document version=\"2.0\"><paragraph>My question is towards the Arc-standard system and Oracle algorithm.</paragraph><paragraph/><paragraph>In Oracle algorithm we perform Right-Arc only if there's no relationship pointing to the right element. Is this rule giving us a unique sequence of actions?</paragraph><paragraph/><paragraph>In Arc Standard rule, do we have this rule for right-arc? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I\u2019m not quite sure I understand this question.\u00a0<break/>In the arg standard oracle algorithm, you perform the right_arc transition if 1) there is a dependency edge between the first word on the stack and the first word on the buffer and 2) all dependents of the first word on the buffer (according to the gold tree) have already been connected in the partially derived tree up to this point.\u00a0</paragraph><paragraph>This oracle algorithm is deterministic, so it produces one specific sequence of actions. It\u2019s possible that there are multiple sequences that result in the same tree. However, the oracle algorithm will produce only one of them.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "AMR Ungraded Exercise Q. <document version=\"2.0\"><paragraph>Q3: Why is arg0 h/he and arg1 l/love not in the same indentation? </paragraph><paragraph>Q4: Clarify difference and definition of arg1 and arg2? Whys is we arg1 and meet arg2? </paragraph><paragraph/><paragraph>Thanks in advance!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Yes, I had the same question on Q3. I assumed that 'he' and 'love' would both be arguments for admire. I also was hoping to get some clarity about using pronouns as arguments. AMR is meant to omit details, but do pronouns not count as details?</paragraph></document>"
        },
        {
            "source": "AMR Ungraded Exercise Q. <document version=\"2.0\"><paragraph>Q3: Why is arg0 h/he and arg1 l/love not in the same indentation? </paragraph><paragraph>Q4: Clarify difference and definition of arg1 and arg2? Whys is we arg1 and meet arg2? </paragraph><paragraph/><paragraph>Thanks in advance!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>same here...some clarifications would be nice</paragraph></document>"
        }
    ],
    {
        "source": "Logical Form and MT Questions. <document version=\"2.0\"><list style=\"unordered\"><list-item><paragraph>Logical form:</paragraph><list style=\"unordered\"><list-item><paragraph>What are unbound variables?</paragraph></list-item><list-item><paragraph>Does vocabulary or signature always mean arity value?</paragraph></list-item></list></list-item><list-item><paragraph>Machine translation:</paragraph><list style=\"unordered\"><list-item><paragraph>For back translation, why create synthetic \u201cback-translation\u201d from French to English instead of English to French directly?</paragraph></list-item><list-item><paragraph>IBM: why P(f,a| e, <bold>m</bold>)?</paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/1GGbaiIX2BxVuaBe5gCyQnyo\" width=\"618\" height=\"152.48185776487662\"/></figure><paragraph/><list style=\"unordered\"><list-item><paragraph>BLUE: </paragraph><list style=\"unordered\"><list-item><paragraph>what does it mean by recall is ignored?</paragraph></list-item><list-item><paragraph>Using the example in class exp(log(0.8)+log(0.5)*(1/2)) =0.78 and not 0.6325</paragraph></list-item></list></list-item></list></list-item></list></document>",
        "target": "<document version=\"2.0\"><paragraph>Logical form (note this is not part of the material for the final): </paragraph><list style=\"unordered\"><list-item><paragraph>What are unbound variables?<break/>Bound variables are bound to a quantifier \"forall x.student(x)-&gt;smart(x)\". Unbound variables appear without a quantifier. </paragraph></list-item><list-item><paragraph>Vocabulary/Signature refers to the collection of all predicates. But each predicate has a specific arity (number of arguments) and that information is part of the signature. </paragraph></list-item></list><paragraph>MT:</paragraph><list style=\"bullet\"><list-item><paragraph>The assumption is that you have more mono-lingual data for french. </paragraph></list-item><list-item><paragraph>m is the number of tokens in the F sentence. The q parameters depend on m, so the entire distribution depends on m. </paragraph></list-item></list><paragraph>BLUE: </paragraph><list style=\"bullet\"><list-item><paragraph>Blue score only measure n-gram precision, i.e. how many of the system output/predicted ngrams appear in any of the reference translations. It does not measure n-gram recall, i.e. how many of the ngrams in the references appear in the system output. </paragraph></list-item><list-item><paragraph>Thanks, this may be a typo. </paragraph></list-item></list></document>"
    },
    {
        "source": "HW 5 Part 2 Loss Function. <document version=\"2.0\"><paragraph>I am having trouble getting the correction dimensions from the cross_entropy loss(input, target) function in part 2.<break/></paragraph><paragraph>For the input parameter, I tried to mimic the 'outputs.transpose(2,1)' code in the given training cell for part 3, but I am not sure why this tensor must be transposed if the model output already returns the logit tensor anyway.</paragraph><paragraph>For the target parameter, I am very confused as to what 'input_pos'  is referring to in the hint \"# complete this. Note that you still have to provide a (batch_size, input_pos)  tensor for each parameter, where batch_size =1\" .</paragraph><paragraph/><paragraph>Is the 'input_pos' the index of the role our sentence is predicated on form role_ids? The pytorch documentation keeps referring to the target parameter in reference to the \"class indice\" but I was not sure what this meant in this context, or if \"input_pos\" was the class indice.</paragraph><paragraph/><pre>ValueError: Expected input batch_size (1) to match target batch_size (2).\n</pre></document>",
        "target": "<document version=\"1.0\"><paragraph>The input should be (1,128) since there are 128 input positions. The output will be (1,128,num_labels).\u00a0<break/></paragraph></document>"
    },
    {
        "source": "What word embedding method is used for transformers?. <document version=\"2.0\"><paragraph>Is word2vec used to create the embeddings that get fed into a transformer typically? Does it vary based on transformer architecture and downstream task?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Transformer models usually have a trainable embedding layer (as in the neural language model, for example), but this layer is initialized with static embedding obtained from word2vec or glove.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Does it normal to have 0 id in my untrained model prediction result. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>For the single prediction result on the untrained model, after doing argmax, for some indexes, I got 0 id,  while 0 is not presented in the keys of \"id_to_role\", so I am wondering if it is normal for our model to predict 0 at this stage?</paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Yes that\u2019s possible.\u00a0</paragraph></document>"
        },
        {
            "source": "Does it normal to have 0 id in my untrained model prediction result. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>For the single prediction result on the untrained model, after doing argmax, for some indexes, I got 0 id,  while 0 is not presented in the keys of \"id_to_role\", so I am wondering if it is normal for our model to predict 0 at this stage?</paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>gocha</paragraph></document>"
        }
    ],
    [
        {
            "source": "Structure of Conceptual Questions. <document version=\"2.0\"><paragraph>Hello, was just wondering what is to be expected of the structure of the conceptual questions. Someone of Ed mentioned that they would be similar to the last question on the first exam. Is this accurate? Or would these questions be more theoretical?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Just saw the newly posted ungraded exercises, so that kind of answers my question</paragraph></document>"
        },
        {
            "source": "Structure of Conceptual Questions. <document version=\"2.0\"><paragraph>Hello, was just wondering what is to be expected of the structure of the conceptual questions. Someone of Ed mentioned that they would be similar to the last question on the first exam. Is this accurate? Or would these questions be more theoretical?</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>I\u2019d recommend watching the lecture from this Wednesday! Professor Bauer throws out some examples of what conceptual questions could look like (I.e. using RNNs/Transformers to compute WSD the same way as the Lesk algorithm)</paragraph></document>"
        }
    ],
    {
        "source": "Homework 4 Solutions. <document version=\"2.0\"><paragraph>Hi- I was wondering if it would be possible to have a solution set released for homework 4 before the final if the grades will not be released in time. Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, I can likely get these up -- but note that there is significant variation in how things can be implemented. </paragraph></document>"
    },
    {
        "source": "Missing B-ARGM-REC role. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>In the training corpus, there is an example that contains an unknown role </paragraph><paragraph><code>As    long    as    these    two    countries    stand    together    ,    then    it    seems    that    other    countries    could    not    do    anything    to    them    .\n O    O    O    B-ARG0    I-ARG0    I-ARG0    B-V    <bold>B-ARGM-REC</bold>    O    O    O    O    O    O    O    O    O    O    O    O    O    O</code></paragraph><paragraph>May I ask how to handle this one? Also, for part 1.1, it looks like the function \"tokenize_with_labels\" is finished, do we need to add anything to the function? Do I miss anything here?</paragraph><paragraph>Thanks! </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The completed tokenize_with_labels function was an oversight :) I forgot to remove it-- feel free to use my version. </paragraph><paragraph>I filtered out roles that appeared very infrequently (iirc less than 1k occurences in the dev data). So if you find a label in the corpus that is not in the list, you can just ignore it. I removed both the B and I tags in those cases, so simply ignoring the individual tags will not lead to inconsistent annotations. </paragraph></document>"
    },
    {
        "source": "HW Grading Timeline. <document version=\"2.0\"><paragraph>Hi--I wanted to ask if the teaching staff had an approximate guideline of when HW 3 and 4 would be finished grading. I just want to figure out if we would know any more of our grades before the final and if I should start working on HW5.</paragraph><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>We should have homework 3 done very soon (missing for one block of grades from one TA). Homework 4 won't be done in time, unfortunately. </paragraph></document>"
    },
    {
        "source": "HW5 Q2 single loss for untrained model. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>For part 2, by running a single input on the untrained model, I got a loss value to be 4.164388656616211. Given the desired initial loss is 3.970291913552122, I am wondering if my initial loss value is acceptable.</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That seems a bit high. Try to instantiate the model multiple times, then compute the initial loss and average. </paragraph></document>"
    },
    {
        "source": "LSTM for SRL. <document version=\"2.0\"><paragraph>Is the following logic correct?<break/><break/>If the input consists of 4 words and the task involves assigning BIO tags to each word indicating their semantic roles, the number of outputs generated by the model would typically be 4 times the number of possible BIO tags or semantic role categories.</paragraph><paragraph>For instance, considering a simplified set of BIO tags:</paragraph><list style=\"unordered\"><list-item><paragraph><code><bold>B-Agent</bold></code>: Beginning of an Agent role.</paragraph></list-item><list-item><paragraph><code><bold>I-Agent</bold></code>: Inside an Agent role.</paragraph></list-item><list-item><paragraph><code><bold>B-Patient</bold></code>: Beginning of a Patient role.</paragraph></list-item><list-item><paragraph><code><bold>I-Patient</bold></code>: Inside a Patient role.</paragraph></list-item><list-item><paragraph><code><bold>O</bold></code>: Outside any role.</paragraph></list-item></list><paragraph>If each word in the input sequence is assigned one of these tags (B-Agent, I-Agent, B-Patient, I-Patient, or O), the number of possible outputs for each word would be 5 (including O).</paragraph><paragraph>Therefore, for an input sequence of 4 words, the total number of outputs would be 4 multiplied by the number of possible tags for each word, which in this case would be 4 * 5 = 20 outputs. These outputs correspond to the predicted BIO tags for each word in the sequence, indicating their assigned semantic roles or lack thereof (as denoted by O).</paragraph><paragraph>Therefore, in the following LSTM, there should be more than 4 output probabilities, correct?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/zYZ9TvMJXpDCImZf7xhedURg\" width=\"644\" height=\"480.1028277634961\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>Sorry for the slow response. I'm not sure I understand the question here. Yes, there will be number_of_possible_labels output units. So if you unroll the RNN, there would be number_of_tokens * number_of_possible_labels output units. During decoding, that isn't typically done though -- you only consider one time-step at a time/ </paragraph><paragraph>Are you just asking why it only shows P(Arg0) for the first token, and not all of them? The figure is demonstrating the training process, so P(BArg0) is the output that matters for computing the loss on the training data. </paragraph></document>"
    },
    {
        "source": "SRL Path From Argument to Target. <document version=\"2.0\"><paragraph>What do we get out of finding the path from argument to target here?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/HQyT3uhCnoJed8bH0hUAYlXT\" width=\"331\" height=\"286\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>The parse tree path is a relevant feature for determining if the candidate constituent is an argument and what its label may be. </paragraph></document>"
    },
    {
        "source": "Helpful Resource for Transformers, GPT, and BERT. <document version=\"2.0\"><paragraph><link href=\"https://jalammar.github.io/illustrated-gpt2/\">https://jalammar.github.io/illustrated-gpt2/</link></paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>thank you so much! this seems really cool</paragraph></document>"
    },
    {
        "source": "HW5 - Opening Jupyter from browser. <document version=\"2.0\"><paragraph>Hi! I was setting up my instance and got to the last steps but I can't connect to the notebook in my browser :(<break/><break/>The firewall rules are set per the spec, and the notebook looks like its up and running when I run:</paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"true\"><snippet-file id=\"code\">jupyter notebook &gt;&gt; jup_notebook.log 2&gt;&amp;1 &amp; tail -f jup_notebook.log</snippet-file></snippet><paragraph>But I'm never able to connect, has anyone else had this issue?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Did you change the jupyter config file? </paragraph></document>"
    },
    {
        "source": "12/06 lecture exam review. <document version=\"2.0\"><paragraph>Hi! i am wondering if tomorrow's lecture will be an exam 2 review session (as indicated in the syllabus)? Thanks! </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>#609 </paragraph></document>"
    },
    {
        "source": "Review Session Virtual Options?. <document version=\"2.0\"><paragraph>Will the Saturday final review session have a Zoom option as well? Also, will it be recorded for us to review after the session ends?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, we will try to broadcast it on zoom and record. </paragraph></document>"
    },
    {
        "source": "Review Session. <document version=\"2.0\"><paragraph>Is there a final review session today and if there is when and where is it being held?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>There will be a review session on Saturday (tentatively at 4pm). Room TBD. </paragraph><paragraph>We will also have a review session in class tomorrow. </paragraph></document>"
    },
    {
        "source": "HW 5 1.2.1. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I am currently trying to populate the self.items array by reading in the entire tsv file into an array, and parsing every 4 lines into their tokens and bio_tags.<break/><break/>Although this method makes sense to me logically, it is very inefficient as the file has over a million lines. <break/><break/>Is there anything I can do to optimize this parsing? Is there a better approach I could take maybe?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>So there is no (efficient) way around storing the data in memory at some point. It\u2019s probably better to only store the final dictionary rather than the initial lines from the file.\u00a0<break/>I would read the file line-by-line rather than all at once and keep count of the lines. After every four lines, process the data you have read to create a new item.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Suggested reading for linguistics. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Thanks Prof. Bauer and the TA's for a wonderful semester! I was wondering if there are any books/articles on linguistics that can help us understand natural language processing better, especially for those without a linguistic background. Thank you very much!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Are you more interested in textbooks or in casual reading? </paragraph><paragraph>One of the books that got me interested/started in linguistics was Stephen Pinker's \"The Language Instinct\". It's not completely up-to-date in terms of the consensus within cognitive/neuro science, but still a fascinating read. </paragraph><paragraph/></document>"
        },
        {
            "source": "Suggested reading for linguistics. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Thanks Prof. Bauer and the TA's for a wonderful semester! I was wondering if there are any books/articles on linguistics that can help us understand natural language processing better, especially for those without a linguistic background. Thank you very much!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>not a TA but did my undergrad in linguistics major, I can suggest some textbooks we read for some courses in my ugrad:</paragraph><paragraph>1. Intro to linguistics: Language Files: Materials for an Introduction to Language and Linguistics</paragraph><paragraph>2. Intro to Phonology/Phonetics: Zsiga, Elizabeth, C. (2013) The Sounds of Language: An Introduction to Phonetics and Phonology. Malden: Wiley-Blackwell.[I actually found this book quite interesting]</paragraph><paragraph>3. Intro to historical linguistics: Lyle Campbell, <italic>Historical Linguistics: An Introduction, </italic>3rd ed.</paragraph><paragraph>4. Intro to syntax/semantics: <italic><bold>syntax</bold></italic>: Sportiche, Dominique, Hilda Koopman and Edward Stabler 2014. An Introduction to Syntactic Analysis and Theory. Wiley Blackwell ; <bold><italic>semantics</italic></bold>: Elbourne, Paul. 2011. Meaning: A Slim Guide to Semantics. Oxford University Press</paragraph><paragraph>5. Pragmatics: <link href=\"https://plato.stanford.edu/entries/pragmatics/\">https://plato.stanford.edu/entries/pragmatics/</link> </paragraph><paragraph>while these maybe too specific and too academic, feel free to take a look at the linguistics branch that corresponds to your interest in nlp. Hope this helps :)</paragraph></document>"
        }
    ],
    {
        "source": "HW 5 part 1.1. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I wanted to get a head start on hw 5 after finishing hw4.<break/><break/>I noticed for part 1.1, where we are to map the tokenized sentence to the proper BIO tags, that the code given produces the gold output shown in the prompt. <break/><break/>Was this code left completed on purpose?</paragraph><pre>def tokenize_with_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n    Word piece tokenization makes it difficult to match word labels\n    back up with individual word pieces. This function tokenizes each\n    word one at a time so that it is easier to preserve the correct\n    label for each subword. It is, of course, a bit slower in processing\n    time, but it will help our model achieve higher accuracy.\n    \"\"\"\n\n    tokenized_sentence = []\n    labels = []\n    \n    for word, label in zip(sentence, text_labels):\n\n        # Tokenize the word and count # of subwords the word is broken into\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n\n        # Add the tokenized word to the final tokenized word list\n        tokenized_sentence.extend(tokenized_word)\n\n        # Add the same label to the new list of labels `n_subwords` times\n                  \n        if label.startswith(\"B\"):\n            labels.append(label)\n            labels.extend([\"I-\"+label.split(\"-\",1)[1]] * (n_subwords-1))\n        else: \n            labels.extend([label]* n_subwords)\n            \n\n    return tokenized_sentence, labels\n</pre></document>",
        "target": "<document version=\"1.0\"><paragraph>Ha, yes it was supposed to be removed. Feel free to use this part, obviously.\u00a0</paragraph></document>"
    },
    {
        "source": "HW4 part2 &3 attempted number less than total. <document version=\"2.0\"><paragraph>Hi Professor and TAs,</paragraph><paragraph>When testing HW4 part 2 and part 3, I noticed that the number of attempts shown is slightly smaller than total, but this situation does not occur in part 4, 5 &amp; 6. I'm curious about what could lead to the failure of some attempts. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/aqsq6SbOaxiyWvFEiV7KIMY9\" width=\"566\" height=\"360\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>So several people have made this observation and I haven\u2019t been able to figure out why this is happening. I suspect some form of encoding issue that causes the Perl script to ignore some lines.\u00a0</paragraph></document>"
    },
    {
        "source": "HW 4 Submission Question. <document version=\"2.0\"><paragraph>Hello! I submitted at 12 a.m. and not 11:59 p.m. Will my score by deducted by 20%?</paragraph><paragraph>Also, I am not really sure how it is scored, but say there is an error in my syntax that results in a compile error. How is that graded?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No that falls within the gray area.\u00a0</paragraph></document>"
    },
    {
        "source": "hw3 grade. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>just want to get an idea about when will the grade of hw3 be released? thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I\u2019m hoping tomorrow or Wednesday. There\u2019s one part missing.\u00a0</paragraph></document>"
    },
    {
        "source": "P6) LLM API/HF API. <document version=\"2.0\"><paragraph>For part 6, would I be allowed to call another HF models api/the openai api to test how it performs?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes. That\u2019s a good idea.\u00a0</paragraph></document>"
    },
    {
        "source": "Output Tokens from BERT. <document version=\"2.0\"><paragraph>Do we need to process any tokens outputted from the BERT model like .replace(\"_\", \" \") and .lower() or can we assume the tokens outputted by BERT are singular lower-case words? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Technically the tokens outputted by BERT are wordpieces. But since the output is filtered by matching against the candidates you can expect the remaining predictions to be individual lower-case words.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Part 6 Runtime. <document version=\"2.0\"><paragraph>I call part 2,3,4,5 in part 6 and it takes a really long time for my code to run (over an hour). Is that okay, or do I need to come up with a part six that has a quicker runtime to get the points?  </paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I had this problem too! I just decided to modify my code in a slightly different way because I didn't want to risk it :)</paragraph></document>"
        },
        {
            "source": "Part 6 Runtime. <document version=\"2.0\"><paragraph>I call part 2,3,4,5 in part 6 and it takes a really long time for my code to run (over an hour). Is that okay, or do I need to come up with a part six that has a quicker runtime to get the points?  </paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Not a TA, but according to the assignment, \"Any small improvement (or conceptually interesting approach) will do to get credit for part 6.\" So I think you're totally fine even if the \"performance,\" however this is being defined, isn't better. See #594.</paragraph><paragraph/></document>"
        },
        {
            "source": "Part 6 Runtime. <document version=\"2.0\"><paragraph>I call part 2,3,4,5 in part 6 and it takes a really long time for my code to run (over an hour). Is that okay, or do I need to come up with a part six that has a quicker runtime to get the points?  </paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>A little late, but one thing that could cause this is if your code for part 6 (or main) were creating the model multiple times (if you're using BERT or Word2Vec as base). </paragraph></document>"
        }
    ],
    {
        "source": "HW4 Test Warning. <document version=\"2.0\"><paragraph>I got the warning like this:<break/>/Users/.../venv-metal/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020</paragraph><paragraph> warnings.warn(</paragraph><paragraph>when I tries to run the code python lexsub_main.py lexsub_trial.xml  &gt; smurf.predict.<break/>Would this affect my implementation or setup? How to fix it?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Um since it\u2019s just a warning I don\u2019t see why it would affect the output. But I\u2019m confused why urllib would be needed for anything\u2026</paragraph></document>"
    },
    {
        "source": "HW 4 Part 2. <document version=\"2.0\"><paragraph>would the result like this:<break/>Total = 298, attempted = 298</paragraph><paragraph>precision = 0.092, recall = 0.092</paragraph><paragraph>Total with mode 206 attempted 206</paragraph><paragraph>precision = 0.131, recall = 0.131<break/>good enough for this part?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>someone posted these exact stats in a previous post and Dr.Bauer approved it, I think you're all good.</paragraph></document>"
    },
    {
        "source": "HW4 Outputs. <document version=\"2.0\"><paragraph>Are these acceptable outputs? Did you guys get similar?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/OGgEl3kQp5ebAL5JYt28IYDR\" width=\"287\" height=\"454\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>See #585. They have similar scores, and Professor Bauer said they look normal. </paragraph></document>"
    },
    [
        {
            "source": "HW4 Part 6 Score. <document version=\"2.0\"><paragraph>Hi everybody!</paragraph><paragraph>I was wondering if it is okay if our part 6 score is significantly worse than some of the other predictors as long as it is conceptually interesting/unique.</paragraph><paragraph>Thanks for the help!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi David! I'm not super sure, but the assignment says, \"Any small improvement (or conceptually interesting approach) will do to get credit for part 6\". I assumed we could have a good approach that performs only as well as one of the other methods. Hope this helps!</paragraph><paragraph/></document>"
        },
        {
            "source": "HW4 Part 6 Score. <document version=\"2.0\"><paragraph>Hi everybody!</paragraph><paragraph>I was wondering if it is okay if our part 6 score is significantly worse than some of the other predictors as long as it is conceptually interesting/unique.</paragraph><paragraph>Thanks for the help!</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>As long as the approach is interesting/different from the other parts, it\u2019s okay if it doesn\u2019t outperform the other approaches.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "punctuation as tokens. <document version=\"2.0\"><paragraph>Does the Lesk algorithm consider punctuation such as \"?\" and \",\" as tokens when calculating overlap?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No punctuation should be ignored for part 3. \u00a0</paragraph></document>"
    },
    {
        "source": "ARM Ungraded exercises solution. <document version=\"2.0\"><paragraph>Hi - </paragraph><paragraph>I am wondering whether the ARM exercises solution has been posted because I was not able to find it. Tysm!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Posted in Pages now.</paragraph></document>"
    },
    [
        {
            "source": "import collections?. <document version=\"2.0\"><paragraph>Are we allowed to import collections?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Yes according to <link href=\"https://edstem.org/us/courses/46417/discussion/3974511\">https://edstem.org/us/courses/46417/discussion/3974511</link></paragraph><paragraph/></document>"
        },
        {
            "source": "import collections?. <document version=\"2.0\"><paragraph>Are we allowed to import collections?</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>That\u2019s fine.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "part 6. <document version=\"2.0\"><paragraph>hi i hope you are great.  I have an idea for part 6 that involves importing a new library in hw.  is that okay?</paragraph><paragraph/><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes for part 6 this is fine. Make sure you list any necessary dependencies in a comment.\u00a0</paragraph></document>"
    },
    {
        "source": "Hw 4 Part 6. <document version=\"2.0\"><paragraph>Do we have to write explanation for the part 6 of HW 4 or is it sufficient to just include the code.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>An explanation in a comment would be useful for the graders.\u00a0</paragraph></document>"
    },
    {
        "source": "Final exam weighting. <document version=\"2.0\"><paragraph>Sorry if this was announced already, but I was wondering in there would be a weighting on the final between material covered before and after exam 1?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>It will be approximately 60% new material, 40% old material.\u00a0</paragraph></document>"
    },
    {
        "source": "HW4: part3 result better than part2. <document version=\"2.0\"><paragraph>For some reason my part3 result </paragraph><pre># Total = 298, attempted = 298\r\n# precision = 0.112, recall = 0.112\r\n# Total with mode 206 attempted 206\r\n# precision = 0.155, recall = 0.155\r\n</pre><paragraph>is better than part2 result</paragraph><pre># Total = 298, attempted = 298\n# precision = 0.074, recall = 0.074\n# Total with mode 206 attempted 206\n# precision = 0.107, recall = 0.107</pre><paragraph>Does this result looks normal?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, that looks normal. </paragraph></document>"
    },
    {
        "source": "HW4 Part 2 Scores Question. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am getting the following scores for part 2, but they are quite low compared to the other ones I have seen in Ed. Is this within the reasonable range?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/obgeTLgIPn8AdbMDni8yCZe6\" width=\"658\" height=\"200.8473804100228\"/></figure><paragraph>This is how I am implementing it. Is there something I'm overlooking?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/IUbap1O12XlGKIgVSFrLgnOz\" width=\"643\" height=\"665.7610619469026\"/></figure><paragraph>Thanks in advance!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>You need to use the .count() method on the lexemes to obtain the frequency. </paragraph></document>"
    },
    {
        "source": "HW5 Missing role_list.txt. <document version=\"2.0\"><paragraph>Where should we download rolelist.txt? After unzipping ontonotes_srl.zip, I only got <code>propbank_dev.tsv</code>  <code>propbank_test.tsv</code>  <code>propbank_train.tsv</code><break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/6Wdi0mNahcWvciFzByJStbmC\" width=\"594\" height=\"340.1357142857143\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I think you are the first person working on this. :) </paragraph><paragraph>I added the rolelist.txt file to the zip file. Please download it again. </paragraph></document>"
    },
    {
        "source": "HW4 Part 3 Details. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I just want to confirm my understanding of calculating overlap in simple lesk is correct:</paragraph><paragraph>1. Suppose a word appears in the definition of synset m times and in the context of the target word n times. Then when calculating the overlap, we count this as 1 overlap, instead of min(m, n)?</paragraph><paragraph>2. We need to discard the occurrences of the target word in the definition/context</paragraph><paragraph>With the above understanding (and the score based approach in #477) I get the following result for part 3:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/GQNVd1EsqqOCinkE7izO3dxt\" width=\"658\" height=\"73.82439024390244\"/></figure><paragraph>which is significantly better than the result for part 2:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/rDBhgO1WdCqtQUqliKuLSCMB\" width=\"644\" height=\"72.81091617933723\"/></figure><paragraph>Is this expected behavior, or did I do something wrong?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, both of these points are correct. Point 1 is a bit up to interpretation, but in my implementation I simply used set intersection, so duplicate words were discarded. </paragraph><paragraph>The results look good tome. </paragraph></document>"
    },
    [
        {
            "source": "Same score for part 4 and part 5. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I got exactly the same score for part 4 and part 5 . Is that an issue or Ok?</paragraph><paragraph>\"(base) qmn@qmndeMBP hw4_files % perl score.pl part5.predict gold.trial</paragraph><paragraph>Total = 298, attempted = 298</paragraph><paragraph>precision = 0.115, recall = 0.115</paragraph><paragraph>Total with mode 206 attempted 206</paragraph><paragraph>precision = 0.170, recall = 0.170</paragraph><paragraph>(base) qmn@qmndeMBP hw4_files % perl score.pl part4.predict gold.trial</paragraph><paragraph>Total = 298, attempted = 298</paragraph><paragraph>precision = 0.115, recall = 0.115</paragraph><paragraph>Total with mode 206 attempted 206</paragraph><paragraph>precision = 0.170, recall = 0.170'</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Ew7p4AXzJjIGhAD1OiKN70ni\" width=\"643\" height=\"207.32242990654206\"/></figure><paragraph>Also, Am I run part 5 correctly? </paragraph><figure><image/></figure><figure><image/></figure><figure><image/></figure><figure><image/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>I got the same result, see #520 </paragraph></document>"
        },
        {
            "source": "Same score for part 4 and part 5. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I got exactly the same score for part 4 and part 5 . Is that an issue or Ok?</paragraph><paragraph>\"(base) qmn@qmndeMBP hw4_files % perl score.pl part5.predict gold.trial</paragraph><paragraph>Total = 298, attempted = 298</paragraph><paragraph>precision = 0.115, recall = 0.115</paragraph><paragraph>Total with mode 206 attempted 206</paragraph><paragraph>precision = 0.170, recall = 0.170</paragraph><paragraph>(base) qmn@qmndeMBP hw4_files % perl score.pl part4.predict gold.trial</paragraph><paragraph>Total = 298, attempted = 298</paragraph><paragraph>precision = 0.115, recall = 0.115</paragraph><paragraph>Total with mode 206 attempted 206</paragraph><paragraph>precision = 0.170, recall = 0.170'</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Ew7p4AXzJjIGhAD1OiKN70ni\" width=\"643\" height=\"207.32242990654206\"/></figure><paragraph>Also, Am I run part 5 correctly? </paragraph><figure><image/></figure><figure><image/></figure><figure><image/></figure><figure><image/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>Same to you.</paragraph><pre>PART4:\nTotal = 298, attempted = 298\nprecision = 0.115, recall = 0.115\nTotal with mode 206 attempted 206\nprecision = 0.170, recall = 0.170\n\nPART5:\nTotal = 298, attempted = 298\nprecision = 0.115, recall = 0.115\nTotal with mode 206 attempted 206\nprecision = 0.170, recall = 0.170\n</pre></document>"
        }
    ],
    [
        {
            "source": "HW 4 Part 3 Score. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I have written my code for part 3, and I followed the logic as recommended in the hw and EdStem posts, but I am getting a low performance as shown below:</paragraph><paragraph/><paragraph>Any tips on how I can improve this performance?</paragraph><paragraph>I checked, and my tokenization is correct. I dont include the target word in the full context to make it easier to parse later too.<break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/BhT6tC0TOsKeTnQkEs8AfrL4\" width=\"281\" height=\"72\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/VgCEmTAKuPQ2HIFrmqacQWnE\" width=\"277\" height=\"538\"/></figure><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi, this might not fix everything but I noticed \"use_up\" in your prediction - make sure to replace all \"_\" in the predictions with a space \" \". You can do this by using the replace() function, for example: word.replace(\"_\",\" \")</paragraph><paragraph/></document>"
        },
        {
            "source": "HW 4 Part 3 Score. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I have written my code for part 3, and I followed the logic as recommended in the hw and EdStem posts, but I am getting a low performance as shown below:</paragraph><paragraph/><paragraph>Any tips on how I can improve this performance?</paragraph><paragraph>I checked, and my tokenization is correct. I dont include the target word in the full context to make it easier to parse later too.<break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/BhT6tC0TOsKeTnQkEs8AfrL4\" width=\"281\" height=\"72\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/VgCEmTAKuPQ2HIFrmqacQWnE\" width=\"277\" height=\"538\"/></figure><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I\u2019m getting similarly low numbers for this part. I\u2019m combining left/right context, tokenizing the context, getting synsets for the target word with the appropriate POS. Then I strip stop words from the context, iterate through the synsets, obtain definition, examples, hypernyms for each, tokenize the definition, remove stop words from definition and each example.  </paragraph><paragraph>Next, I look for overlap in the definition, incrementing counter each time a match is found. Then I check for overlap in each example (tokenizing each example first), obtain examples and definition for each hypernym, remove stop words and tokenize those, iterate through them and again search for overlap. This in the end gives me a dict of synsets with total overlap counts for definition, examples, hypernym examples and definition.</paragraph><paragraph>If the dict is empty (no overlap), I iterate through the synsets, then through the lemmas in each synset and store the lemma count for each synset in another dict. I take the max from this dict to get the most frequent synset, then iterate through the lemmas in this synset and store their counts. The lemma with the maximum count I take as the synonym.</paragraph><paragraph>If there is a tie, I do the same thing, only checking the synsets that have the highest (tied) score, going through the lemma counts for the most frequent synset, choosing the max as the synonym.</paragraph><paragraph>I think there may be a problem with how I\u2019m handling no overlap/ties, but I\u2019m not sure. I didn\u2019t try to implement the professor\u2019s suggested algorithm.</paragraph><paragraph>Question:  If the scores for this part are low/incorrect, will we be graded on our code/algorithm process and be given partial credit?</paragraph></document>"
        }
    ],
    [
        {
            "source": "HW4 Part 2 (illegal division by zero). <document version=\"2.0\"><paragraph>I have seen several posts about \"Illegal division by zero at score.pl line 109\" on Windows but I haven't seen many solutions. I tried to use the lines written in #476 but I still get the same error when I run <code>perl score.pl wn_frequency_predictor gold.trial</code>. I would appreciate any guidance from people who have been able to solve this issue!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I got this error when the file name was incorrect.  Did you mean wn_frequency_predictor.predict?</paragraph></document>"
        },
        {
            "source": "HW4 Part 2 (illegal division by zero). <document version=\"2.0\"><paragraph>I have seen several posts about \"Illegal division by zero at score.pl line 109\" on Windows but I haven't seen many solutions. I tried to use the lines written in #476 but I still get the same error when I run <code>perl score.pl wn_frequency_predictor gold.trial</code>. I would appreciate any guidance from people who have been able to solve this issue!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I think first check your file name and whether you change the main when you test for each part. And did you do \"python lexsub_main.py lexsub_trial.xml &gt; wn_frequency_predictor.predict \"  before you run score.pl? (I had the same error and it turns out to be this issue.)</paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "Part 3 Synonyms. <document version=\"2.0\"><paragraph>In Part 3, we need to find the synset with the greatest overlap between the context and the definition. </paragraph><paragraph>When we get this synset, how should I decide which word to pull out? Because a synset is a collection of lemmas, most of these synsets will have numerous words. Am I supposed to use the p2 results to return the most common lemma in that synset?</paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes you would then choose the lemma with the highest .count() value. Take a look at #477 for an optional alternative approach to selecting a lemma and breaking ties.\u00a0</paragraph></document>"
    },
    {
        "source": "HW5 GCP GPU Quota. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>When getting quota for GPU, I am facing 2 problems:</paragraph><list style=\"number\"><list-item><paragraph>There is no link that takes me to the GPU Quota page when the initial VM creation failed. The \"Learn More\" takes me to a documentation page, not a place to request for GPU quota. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/EM2bQ6uKwdzaCqwC8d2nF1Mk\" width=\"603\" height=\"294.63037974683544\"/></figure><paragraph/></list-item><list-item><paragraph>I am able to find the page to request GPU quota but no quota can be selected (but can be seen)</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/j9VzSpVxdydS8QarRnKGIVgl\" width=\"618\" height=\"143.55518945634265\"/></figure><paragraph>Any suggestions on how to set up the VM instance?</paragraph></list-item></list></document>",
        "target": "<document version=\"2.0\"><paragraph>So that error message indicates that no GPUs are available in your availability zone. Please try a different one. </paragraph><paragraph>One the Quota page, filter for the \"NVIDIA T4 GPUs\" quota. You should see a list of all availability zones and a Limit of 0 on each. Select the zone you need (I had luck with us-east4). Then select the checkbox and click \"Edit Quotas\" on top of the page. </paragraph><paragraph/></document>"
    },
    {
        "source": "HW4 part 5 clarification. <document version=\"2.0\"><paragraph>Hey Staff,</paragraph><paragraph>I don't think I am understanding what I am supposed to do in part 5:</paragraph><paragraph>The instructions are:<break/></paragraph><list style=\"unordered\"><list-item><paragraph>Obtain a set of candidate synonyms (for example, by calling <code>get_candidates</code>).</paragraph></list-item><list-item><paragraph>Convert the information in <code>context</code> into a suitable masked input representation for the DistilBERT model (see example above). Make sure you store the index of the masked target word (the position of the [MASK] token).</paragraph></list-item><list-item><paragraph>Run the DistilBERT model on the input representation.</paragraph></list-item><list-item><paragraph>Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target position (i.e. the position of the masked word). Return this word.</paragraph></list-item></list><paragraph>So firstly, call get_candidates and store it in a list.</paragraph><paragraph>Secondly, Take the context, find the index with the context word and swap the word with [mask].</paragraph><paragraph>Thirdly, run the model on the context and get back the 10 best words based on the model.</paragraph><paragraph>Lastly, -here is where I don't fully understand- from the set of candidate synonyms in part 1 choose the highest scoring word for the masked word. But then where does the BERT model come in?</paragraph><paragraph>Here is my code:</paragraph><pre>def predict(self, context : Context) -&gt; str:\n\n    # 1. <italic>TODO Obtain a set of candidate synonyms (for example, by calling get_candidates).<break/></italic>    cs_list = set(get_candidates(context.lemma, context.pos))\n\n    # 2. <italic>TODO Convert the information in context into a suitable masked input representation for the DistilBERT model (see example above).<break/></italic>    #... <italic>TODO Make sure you store the index of the masked target word (the position of the [MASK] token).<break/></italic>    index = 0\n    context_list = context\n    for i in range(0, len(context_list)):\n        if context_list[i] == context.lemma:\n            index = i\n            context_list[i] = '[MASK]'\n\n    input_toks = self.tokenizer.encode(context_list)\n    input_mat = np.array(input_toks).reshape((1, -1))\n\n    # 3. <italic>TODO Run the DistilBERT model on the input representation.<break/></italic>    outputs = self.model.predict(input_mat)\n    predictions = outputs[0]\n    best_words = np.argsort(predictions[0][5])[::-1]  # Sort in increasing order\n    best_tokens = self.tokenizer.convert_ids_to_tokens(best_words[:10])\n\n    # 4. <italic>TODO Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target<break/></italic>    #... <italic>TODO position (i.e. the position of the masked word). Return this word.</italic></pre><paragraph>What exactly am I supposed to do in the last part?</paragraph><paragraph/><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I would iterate through the predicted tokens suggested by BERT (in decreasing order of their probability). For each, you check if it appears in the candidate list. If it does, return the candidate. If not continue to the next predicted token. </paragraph></document>"
    },
    {
        "source": "Part 5 Time. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I noticed another Ed Post that indicated Part 5 should not take a long time. However, it seems to be taking at least 5 to 6 minutes for me, and I'm not sure if this is normal? Thanks so much! </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>That seems fine to me. There may be a bottle-neck in how you sort the predictions. </paragraph></document>"
    },
    {
        "source": "Error with Word2Vec embeddings. <document version=\"2.0\"><paragraph>When I try to test my genism download, I had the same issue as #502, so I edit my embeddings import statement to not include the .gz extension. When I do that, I don't get an error, but when I try to run v1 = model.wv['computer'], I get the below error</paragraph><paragraph>AttributeError: 'KeyedVectors' object has no attribute 'wv'</paragraph><paragraph>Do you know why this could be happening?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I saw that the workaround for this is just below on the instructions, please disregard this</paragraph></document>"
    },
    {
        "source": "HW 5: Google Cloud and GPU. <document version=\"2.0\"><paragraph>Dear Teaching Team,<break/><break/>Just out of curiosity, when we it's said that HW 5 \"requires a GPU with sufficient memory\", what magnitude are talking about? Is it completely unfeasible to run this sort of projects on a personal computer? I'm curious because I want to try to make some projects over winter break.<break/><break/>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>You will need about 16GB of GPU memory to fine-tune BERT. \u00a0</paragraph></document>"
    },
    {
        "source": "question about part 2 breaking ties. <document version=\"2.0\"><paragraph>In part2, if there is a tie, I just choose the word appears first (I made a dictionary to record all lemmas in all sysnets and call item() ). I got 0.074 and 0.107. If I choose the word appears last, I got 0.035 and 0.01. Why it differs so much? Is this score acceptable?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I'm actually surprised it makes such a huge difference. But then again, there aren't that many test instances. </paragraph></document>"
    },
    {
        "source": "Allowed imports?. <document version=\"2.0\"><paragraph>Hi,<break/><break/>Are we allowed to import extra packages? I imported string, re, and defaultdict</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, as long as they are part of the python standard library. </paragraph></document>"
    },
    {
        "source": "HW4 Part5. <document version=\"2.0\"><paragraph>In part5, do we need to consider the case where none of the prediction word is in the candidates(getting from part1)?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, I don't think this happens with this data, but it's a good idea to have a way of handling it. Maybe simply select the most likely BERT prediction :)? </paragraph></document>"
    },
    {
        "source": "score for part 3. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm getting </paragraph><paragraph>\"Total = 298, attempted = 298</paragraph><paragraph>precision = 0.111, recall = 0.111</paragraph><paragraph>Total with mode 206 attempted 206</paragraph><paragraph>precision = 0.150, recall = 0.150\"</paragraph><paragraph>for part 3, which I believe is so much higher because simple lesk algorithm should not outperform the WordNet frequency baseline.. Is this expected?</paragraph><paragraph>I post the output file just for reference.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/qQwBT3dsuBlzmq5gb3pgzQXB\" width=\"494\" height=\"1062\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>Professor Bauer responded to another student about part 3 outperforming part 2! #510 #585 </paragraph></document>"
    },
    {
        "source": "Bert Model weird output. <document version=\"2.0\"><paragraph>Hi,<break/><break/>When I run python lexsub_main.py lexsub_trial.xml  &gt; part5.predict <break/><break/>i get this message printed over and over below. However, my code doesn't error out (is this just a warning?)</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/dQuWfcCvCOMTQTEVF2f8EdOM\" width=\"642\" height=\"148.26143790849673\"/></figure><paragraph><break/>Then when I do </paragraph><paragraph>perl score.pl part5.predict gold.trial</paragraph><paragraph>I get results that are similar to what other students are getting for part 5.</paragraph><paragraph/><paragraph><break/><break/></paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>This is just a warning that you can safely ignore. </paragraph></document>"
    },
    {
        "source": "HW4 Overlap Definition. <document version=\"2.0\"><paragraph>I am little confused that, <code>set</code> and <code>list</code>, which data type we should use for calculate overlap score. </paragraph><paragraph>For example, the context is <code>[\"insitution\"]</code> and gloss of a \"bank\" sense is </paragraph><pre>depository financial <bold>institution</bold>, bank, banking company\n\na financial <bold>institution</bold> that accepts deposits and channels the money\n</pre><paragraph>There are two <code>\"institution\"</code> in the gloss. In this case, the overlap score is 1 or 2?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I recommend just using a set. The overlap score would then be 1. <break/>You could make an argument that counting duplicates accounts for some words being more important, but I think you would have to control for the varying lengths of the glosses. </paragraph></document>"
    },
    {
        "source": "HW4 part5 output. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For my part 5 output, it prints the process to the .predict file. Do I need to remove it from the .predict file?</paragraph><paragraph>Thank you</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Sn3Msi5iYBiESMb9mORNYFVj\" width=\"658\" height=\"214.63333333333333\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>For me, adding 'verbose=0' inside model.predict() helped remove these extra lines from the .predict file. So, the line of code would look something like:</paragraph><paragraph>outputs = self.model.predict(input_mat, verbose=0)</paragraph></document>"
    },
    {
        "source": "Part 4 Low Accuracy. <document version=\"2.0\"><paragraph>Hello! At the moment for HW 4 part 4, I'm getting a prediction and accuracy of 0.102. Does anyone have any suggestions on slightly increasing these values? Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><heading level=\"2\"><link href=\"https://edstem.org/us/courses/46417/discussion/3938121\">#495</link></heading><paragraph/></document>"
    },
    {
        "source": "HW 4 Part 5 tokenizer.encode. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I wanted to sanity check some behavior I noticed. Should the tokenizer.encode add unnecessary #'s and split the sentence somewhat incorrectly in some cases such as the below? The third line starting with ['[CLS]',....] is what I get when I print the result of calling tokens.convert_ids_tokens on tokenizer.encode(first sentence shown below). <break/><break/>I believe the extraneous #'s might be affecting how my model performs but I'm not sure if it's expected.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/f2rro4BhcCDV8gEDF7E9PxeC\" width=\"658\" height=\"42.5951690821256\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>This is the subword wordpiece tokenization. Bert is trained on this type of tokenization, so the input is necessary for the encoder to work properly.\u00a0</paragraph></document>"
    },
    {
        "source": "HW4 Part 3. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am working on part 3 right now and I have a question. I went into office hours early which helped me get started and get a basic idea of what we are doing for this part, but I'm bit confused. I've pasted my code below. For the code under my part B comment, shouldn't it go under the \"for lemma in lemmas\" loop from part A? Or am I understanding something wrong. Also for the tie breaker stuff that was mentioned in <link href=\"https://edstem.org/us/courses/46417/discussion/3922878\">#477</link>, where would this fit into my code? Like should I be handling the tie breaker stuff after my part C? Thanks.<break/></paragraph><pre>def wn_simple_lesk_predictor(context : Context) -&gt; str:\n    stop_words = stopwords.words('english')\n    # For part a, tokenize the context and finding the overlap between the synset + synset hypernyms definitions\n    # and examples\n    tokens = set()\n    context_lr = context.left_context + context.right_context\n    context_string = \" \".join(context_lr)\n    for token in tokenize(context_string):\n        if token not in stop_words:\n            tokens.add(token)\n    lemmas = wn.lemmas(context.lemma, context.pos)\n    for lemma in lemmas:\n        syn = lemma.synset()\n        definition = syn.definition() + \" \" + \" \".join(syn.examples())\n        for hypernym in syn.hypernyms():\n            definition += hypernym.definition() + \" \" + \" \".join(syn.examples())\n        definition_set = set(word.lower() for word in tokenize(definition) if word not in stop_words and word.isalpha())\n        overlap = tokens.intersection(definition_set)\n\n    # For part b, loop through the synset get the count go synset.lemma.count\n    for synset in overlap:\n        for lemmas in synset.lemma:\n\n\n\n    # Part c loop through synset.lemma, get l.count for l in synset.lemma\n    for l in synset.lemma:\n        count = 0\n\n\n    return None #replace for part 3</pre></document>",
        "target": "<document version=\"1.0\"><paragraph>I\u2019m not sure what the loop under your part b comment I supposed to do.\u00a0</paragraph><paragraph>Once you have the overlap (in the \u201cfor lemma in lemmas\u201d loop) you can compute the score for the current lexeme (the lemma variable) as described in #477. Then store the lemma and the score in a dictionary.\u00a0</paragraph><paragraph>Finally (after the main for loop) select the candidate with the max score.\u00a0<break/></paragraph></document>"
    },
    {
        "source": "python: can't open file 'perl': [Errno 2] No such file or directory. <document version=\"2.0\"><paragraph>Hi I hope you are great.</paragraph><paragraph/><paragraph>I have beeen testing my files non stop for days.</paragraph><paragraph/><paragraph>I'm not sure what i did but i just got the error</paragraph><paragraph/><paragraph>python: can't open file 'perl': [Errno 2] No such file or directory</paragraph><paragraph/><paragraph>the perl command has been working for days so</paragraph><paragraph>i am not sure what is going on</paragraph><paragraph>thank you</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I suspect you are trying to run Perl from inside the Python interpreter.\u00a0</paragraph></document>"
    },
    {
        "source": "BertPredictor predict() outputting one more word. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/fIw2cH58d95an1iXf2Y1l9a7\" width=\"658\" height=\"288.973293768546\"/></figure><paragraph>Hi Professor and TAs,</paragraph><paragraph>I tested my algorithm for pt 5 with Professor's test codes (in main). However, there's always an extra word at the end of my outputs (for example, it's supposed to be</paragraph><pre>['tight', 'close', 'loose', 'deep', 'big', 'tighter', 'tied', 'firm', 'hard']\n</pre><paragraph>instead of</paragraph><pre>['tight', 'close', 'loose', 'deep', 'big', 'tighter', 'tied', 'firm', 'hard', <bold>'down'</bold>]\n</pre><paragraph/><paragraph>I am also confused about why I have to manually add the <code>add_special_tokens = True</code> in my test code </paragraph><pre>input_toks = tokenizer.encode(\"If your money is tight, don't cut corners\", <bold>add_special_tokens=True</bold>)\n</pre><paragraph>To make it print the above outputs, if I don't add it, then the output is </paragraph><paragraph><code>[',', '.', '-', ';', 'and', '/', '...', '!', ':', 'to']</code>.</paragraph><paragraph>Any hint or direction would be greatly appreciated, thank you in advance.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm, my example code suggests that there should be 10 tokens predicted, not 9 (as shown in the example). So your output looks good. </paragraph><paragraph>I think with the special tokens here is what's going on: With special tokens, the tokenizer will automatically insert [CLS] and [SEP]. Then the the target word \"tight\" would be at index 5. Without special tokens it would be at index 4, and then the \",\" would be at index 5. </paragraph><paragraph>Daniel </paragraph><paragraph/><paragraph/></document>"
    },
    {
        "source": "Still getting divison by zero error. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I know other students also asked about this but I still struggle with it.</paragraph><paragraph>I got </paragraph><paragraph>\"(base) ....... hw4_files % perl score.pl smurf.predict gold.trial</paragraph><paragraph>Illegal division by zero at score.pl line 109.\"</paragraph><paragraph>even when trying with smurf.predict.</paragraph><paragraph>The main method now is </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/tsdfKpe0nvfgRu0249OPmZBF\" width=\"658\" height=\"158.06703910614524\"/></figure><paragraph>BTW I'm using Mac. I also tried import io but it still not working</paragraph><paragraph/><paragraph>Thanks a lot!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>hi did you do the piping step before creating smurf.predict, i think that might be the problem</paragraph></document>"
    },
    {
        "source": "HW 4 Desired Results. <document version=\"2.0\"><paragraph>For HW 4, is our goal to have the scores be as close to the expected values or do we want to have the scores for each part to be as high as possible on each individual task? I believe my scores have typically been around the estimates given in the HW4 documentation, but just want to understand whether we should prioritize increasing the score for each part or getting the \"right\" score</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>My advise is to not obsess about the score  -- use the provided scores as a reference to check if your code is working correctly in general. </paragraph><paragraph>The evaluation for hw4 is relatively objective because we are using the \"official\" evaluation script provided for the lexical substitution task. So you can trust that better results are better :)</paragraph></document>"
    },
    {
        "source": "Error in Part -5 evaluation. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I ran the evaluation code on the part5.predict file. I got the following error.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/VOtrwB20swRBWUZ1YZXJxkIf\" width=\"522\" height=\"1082\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>Please take a look at the actual output file and see if anything looks off. If you are running this on windows, you may be encountering an issue with the line endings. #476 </paragraph></document>"
    },
    {
        "source": "Hw4 - Part 6. <document version=\"2.0\"><paragraph>If we create a conceptually different approach for Part 6, however, the performance is worse, should we include part 5 or Part 6 in the main part of our final submission? Thank you so much!!!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>It doesn't really matter. Let's say you should run the part that gives you the best performance -- in your case part 5. </paragraph></document>"
    },
    [
        {
            "source": "HW 5: Part 5 output. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>This is my output when I run part 5. It prints some \"error in line\" from lines 1 - 600 before printing the scores. However, my precision seems to be slightly better than the word2vec approach as mentioned in the assignment. Can you please confirm if I should be worried about this \"error in line\" part or is it fine as long as my precision is reasonable. </paragraph><paragraph>Thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/hT0CG3AYjttLKqMA0XY01mST\" width=\"658\" height=\"239.27272727272728\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>Same problem here. Did you solve it?</paragraph><paragraph/></document>"
        },
        {
            "source": "HW 5: Part 5 output. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>This is my output when I run part 5. It prints some \"error in line\" from lines 1 - 600 before printing the scores. However, my precision seems to be slightly better than the word2vec approach as mentioned in the assignment. Can you please confirm if I should be worried about this \"error in line\" part or is it fine as long as my precision is reasonable. </paragraph><paragraph>Thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/hT0CG3AYjttLKqMA0XY01mST\" width=\"658\" height=\"239.27272727272728\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>Hello! Looks like this can be prevented using what someone else mentioned above, but I'd also point out that it looks like these extra lines don't effect the actual effectiveness of the scoring script</paragraph></document>"
        }
    ],
    {
        "source": "part 5 get candidates. <document version=\"2.0\"><paragraph>Hi I hope you are great. For this part of part 5:</paragraph><paragraph/><list style=\"unordered\"><list-item><paragraph>Obtain a set of candidate synonyms (for example, by calling <code>get_candidates</code>).</paragraph></list-item></list><paragraph>can i just copy over the logic, or do i have to actaully call the get_candidates function.  </paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>It's actually easier to copy the logic because you can then score the candidates on-line instead of having to store them all in a list. </paragraph></document>"
    },
    {
        "source": "Error when installing gensim. <document version=\"2.0\"><paragraph>Hello! I have been trying to \"pip install gensim\" into my Python environment, but I keep getting this error. Does anyone happen to have encountered this/resolved it? Thank you so much for any help!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/j5RLj7kacrtVtmhTJYUUPNjc\" width=\"657.9999999999999\" height=\"266.94661921708183\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>Can you copy/paste the entire output? It looks cut-off to the right and top.</paragraph></document>"
    },
    {
        "source": "part5 example. <document version=\"2.0\"><paragraph>Hi I hope you are great.</paragraph><paragraph/><paragraph>I was wondering for the part 5 example, to go through the example, do I just create a random .py file and run the file?</paragraph><paragraph/><paragraph>thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Correct me if I'm misunderstanding your question, to do the introductory lines of code that introduce questions 5 you should be able to use your terminal (at least for mac). If you open a terminal and type python then hit enter, you should be able to type in the lines and get the expected returns. Anything with &gt;&gt;&gt; is something you type in, the next line is the return you should get in the terminal, if any. This would be the easier option, but yes you could also creat a .pay and run the file in your ide.</paragraph></document>"
    },
    {
        "source": "Part 3. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/4z7EP3s61QkdZJdfSpI0djTf\" width=\"550\" height=\"160.19417475728156\"/></figure><paragraph>Hi  I hope you are doing great.  My part 3 results are above.  I am not sure how to improve this.  Is there anything you can suggest?  Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I would double check that you are tokenizing the text groupings properly for the intersection check. I think it was left+right context vs definition+examples+hypernyms examples+hypernyms definitions</paragraph></document>"
    },
    [
        {
            "source": "HW4 Score. <document version=\"2.0\"><paragraph>Does my following scores for part 3&amp;4&amp;5 looks reasonable? My Bert score is much lower than the word2vec model. So I am not sure is something that I did wrong. Thank you!</paragraph><paragraph>perl score.pl wn_simple_lesk.predict gold.trial<break/>Total = 298, attempted = 298 <break/>precision = 0.039, recall = 0.039 <break/>Total with mode 206 attempted 206 <break/>precision = 0.068, recall = 0.068</paragraph><paragraph>perl score.pl word2vec.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.115, recall = 0.115<break/>Total with mode 206 attempted 206<break/>precision = 0.170, recall = 0.170</paragraph><paragraph>perl score.pl bert.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.098, recall = 0.098<break/>Total with mode 206 attempted 206<break/>precision = 0.126, recall = 0.126<break/><break/>Then as a result, I tried to use Bert Large for my part 6, which also have a lower score. Is this fine? Do we need to actually have a model 6 that has the highest score over all models? </paragraph><paragraph>perl score.pl bertlarge.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.099, recall = 0.099<break/>Total with mode 206 attempted 206<break/>precision = 0.146, recall = 0.146</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>My word2vec scores were better than my bert scores too. So for part 6 I decided to work off the word2vec method and make some small improvements to push the score up. I would imagine they expect your part 6 to be able to at least marginally beat your part 3, 4, and 5 scores since you have the flexibility to mix and match the techniques.</paragraph></document>"
        },
        {
            "source": "HW4 Score. <document version=\"2.0\"><paragraph>Does my following scores for part 3&amp;4&amp;5 looks reasonable? My Bert score is much lower than the word2vec model. So I am not sure is something that I did wrong. Thank you!</paragraph><paragraph>perl score.pl wn_simple_lesk.predict gold.trial<break/>Total = 298, attempted = 298 <break/>precision = 0.039, recall = 0.039 <break/>Total with mode 206 attempted 206 <break/>precision = 0.068, recall = 0.068</paragraph><paragraph>perl score.pl word2vec.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.115, recall = 0.115<break/>Total with mode 206 attempted 206<break/>precision = 0.170, recall = 0.170</paragraph><paragraph>perl score.pl bert.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.098, recall = 0.098<break/>Total with mode 206 attempted 206<break/>precision = 0.126, recall = 0.126<break/><break/>Then as a result, I tried to use Bert Large for my part 6, which also have a lower score. Is this fine? Do we need to actually have a model 6 that has the highest score over all models? </paragraph><paragraph>perl score.pl bertlarge.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.099, recall = 0.099<break/>Total with mode 206 attempted 206<break/>precision = 0.146, recall = 0.146</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Even my Word2Vec scores were better than the BERT scores. Infact, the Word2Vec scores were higher than the scores produced by any of the other three techniques.\u00a0</paragraph></document>"
        },
        {
            "source": "HW4 Score. <document version=\"2.0\"><paragraph>Does my following scores for part 3&amp;4&amp;5 looks reasonable? My Bert score is much lower than the word2vec model. So I am not sure is something that I did wrong. Thank you!</paragraph><paragraph>perl score.pl wn_simple_lesk.predict gold.trial<break/>Total = 298, attempted = 298 <break/>precision = 0.039, recall = 0.039 <break/>Total with mode 206 attempted 206 <break/>precision = 0.068, recall = 0.068</paragraph><paragraph>perl score.pl word2vec.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.115, recall = 0.115<break/>Total with mode 206 attempted 206<break/>precision = 0.170, recall = 0.170</paragraph><paragraph>perl score.pl bert.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.098, recall = 0.098<break/>Total with mode 206 attempted 206<break/>precision = 0.126, recall = 0.126<break/><break/>Then as a result, I tried to use Bert Large for my part 6, which also have a lower score. Is this fine? Do we need to actually have a model 6 that has the highest score over all models? </paragraph><paragraph>perl score.pl bertlarge.predict gold.trial<break/>Total = 298, attempted = 298<break/>precision = 0.099, recall = 0.099<break/>Total with mode 206 attempted 206<break/>precision = 0.146, recall = 0.146</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Just adding on to the others above\u2014I had the same word2vec performance, and Bert exactly tied that. The simple lesk performance you have looks a bit low though, I'd make sure you try out Dr. Bauer's recommendation about breaking ties in #477 </paragraph></document>"
        }
    ],
    {
        "source": "potential file issue. <document version=\"2.0\"><paragraph>Hi,  I hope you are doing great. </paragraph><paragraph>I accidentally exported and converted my main file to a txt.  I brought it back to the correct folder and changed the extension back to .py. It seems to be functioing normally, but when I look at the file in the hw4 folder it no longer shows, the .py extension, but it shows the exstension in my ide.  Is this okay?  </paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The file you submit on Courseworks should have a .py extension. </paragraph></document>"
    },
    {
        "source": "Problem 3 Question. <document version=\"2.0\"><paragraph>Hi, when we are returning values in Q3 predictions, should they be strings or synsets?</paragraph><paragraph>for example should I return the string \"streak\" or \"Synset('streak.n.01')\"?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/9FhLdKsgbvfMMcOqG9p7bdVH\" width=\"644\" height=\"179.2\"/></figure><paragraph>Here is an example of it returning the string \"go\" and the synset \"streak.n.01\".</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I believe the return should just be a string, e.g. \"streak\".</paragraph><paragraph>so when you check your .predict file it will look like:</paragraph><paragraph>run.v 300 :: streak</paragraph></document>"
    },
    {
        "source": "Tensorflow not Working. <document version=\"2.0\"><paragraph>Hello! I hope you are doing well. I'm currently trying to work on the assignment, but I'm having issues with my tensorflow. I'm getting this reccuring message, and I'm not sure what commands I have to run to get my tensorflow working on my Mac. Any feedback would be greatly appreciated!</paragraph><paragraph/><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/645HpoQze0H6Jek5ggiEQ9at\" width=\"658\" height=\"279.27536231884056\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>It looks like you are using the tensorflow-metal accelerated version. For this assignment you should be good with the CPU version. </paragraph><pre>$ pip uninstall tensorflow-metal\n$ pip install tensorflow\n</pre><paragraph/></document>"
    },
    {
        "source": "Hw4 part4 and part5 score. <document version=\"2.0\"><paragraph>Hello Professor,</paragraph><paragraph>Is it possible for Part 4 and Part 5 to have the same score, even if their predictions are not the same? Thank you for your help.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/C8YEJelSJYNf8ZvvvHRjJUEC\" width=\"598\" height=\"211.6131805157593\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>I got the same result, see <link href=\"https://edstem.org/us/courses/46417/discussion/threads/520\">#520</link> </paragraph></document>"
    },
    {
        "source": "Question about final exam. <document version=\"1.0\"><paragraph>Dear Prof. and TAs,<break/>I\u2019m wondering whether we can get sample paper of previous final exam or not? And I\u2019m curious about the question patterns, will it be the same as the midterm exam? Thanks!<break/></paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Unfortunately I don't have a large enough trove of exam problems to be able to release a sample exam. </paragraph><paragraph>I will try to share some individual example problems (as ungraded exercises).</paragraph></document>"
    },
    {
        "source": "Can I submit my .predict files as output by my Windows pc?. <document version=\"2.0\"><paragraph>I have completed my assignment 4 on Windows pc and my perl score.pl script outputs precision results that I am satisfied with. However, when I try to run the same script on my mac book the script is not working. I am aware that this difference is due to Unix file endings as explained in one of the previous posts. What machine will be grading our .predict files? Can I submit my .predict files as output by my Windows pc?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes that should be okay -- worst case we can convert the file endings. </paragraph></document>"
    },
    {
        "source": "Results too high?. <document version=\"2.0\"><paragraph/><paragraph>Hello,</paragraph><paragraph>I was wondering if for part 4 these results were too high. Thanks!</paragraph><paragraph/><paragraph>perl score.pl part4.predict gold.trial</paragraph><paragraph/><paragraph>Total = 298, attempted = 298</paragraph><paragraph>precision = 0.136, recall = 0.136</paragraph><paragraph>Total with mode 206 attempted 206</paragraph><paragraph>precision = 0.218, recall = 0.218</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hu, the results with mode are really quite good. </paragraph><paragraph>Unless you somehow modified the evaluation script, it's difficult to \"cheat\" on this problem, so I would trust the good results. </paragraph></document>"
    },
    {
        "source": "Hw4 part2 and part3 score. <document version=\"2.0\"><paragraph>Hello Professor and TA,</paragraph><paragraph/><paragraph>It this score good for part 2 and part 3? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/5HrfOwylocA8BSBRoB7VilKY\" width=\"632\" height=\"213.25153374233128\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>These look reasonable. </paragraph></document>"
    },
    {
        "source": "score.pl and files. <document version=\"2.0\"><paragraph>In regards to #476, I implemented the solution suggested here in the main function and I am still met with the error of illegal division by 0 on line 109. I previously tried to include the io.open line within the functions and was met with the same error. Here is a snippet of my code. Am I doing this correctly? Thanks!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/kVeesDNPTuwftutImwXIs93o\" width=\"658\" height=\"221.43298969072166\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>Nevermind, I got it</paragraph></document>"
    },
    {
        "source": "Hw4 Part 4 & 5 Score. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>Are these score okay for Part 4 and part 5? My bert model does not perform better than word2vec unfortunately, and I am not quite sure how I can improve it.</paragraph><paragraph>Best,</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/z9IzlGxyCcszhwNEJvuPH7ek\" width=\"658\" height=\"130.078612716763\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>That's expected. My theory here is that both w2v and Bert are really good at selecting a candidate, but that unfortunately the candidate set obtained from wordnet is too limited. </paragraph></document>"
    },
    {
        "source": "Hw4 Part 5: Bert predictor only run in virtual environment. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>For hw4 part 5, I have to create a virtual python environment and reinstall all the dependencies to run Bert Predictor (otherwise I would run into some Keras package/ dependency issues in my local main machine). When I run the code in the virtual environment, it works. I want to make sure that this is okay and it is not something with my code that cause the issue. Thank you so much! </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Sure, you can assume the TAs environment when grading the assignment is set up properly. </paragraph></document>"
    },
    {
        "source": "Part 3 Score. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I see on courseworks that part 3 isn't expected to outperform part 2. However, based on what I've seen from other students' posts on Ed, I'm worried my output is too low. Do these scores seem acceptable? Thank you!  </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/HSI4P7WQtoT6E8hhwtTiHjCk\" width=\"658\" height=\"64.22467771639043\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>It's difficult to say based on the scores themselves. They don't look suspiciously low to me. </paragraph></document>"
    },
    [
        {
            "source": "Part 4 Key Error. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a </paragraph><paragraph>KeyError: \"Key 'moving picture' not present.\"</paragraph><paragraph>it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? </paragraph><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>There may be a way to fix this, but when I chose to skip those words with a space in between, I got the following results:</paragraph><paragraph>Total = 298, attempted = 298; precision = 0.115, recall = 0.115</paragraph><paragraph>Total with mode 206 attempted 206; precision = 0.170, recall = 0.170</paragraph></document>"
        },
        {
            "source": "Part 4 Key Error. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a </paragraph><paragraph>KeyError: \"Key 'moving picture' not present.\"</paragraph><paragraph>it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? </paragraph><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Note that in part 1 instruction, \"WordNet will represent such lemmas as \"turn_around\", so you need to remove the .\". Hence, from my understanding, you should update part 1 to convert word with space in between into _ instead. After that, the error should be fixed.</paragraph></document>"
        },
        {
            "source": "Part 4 Key Error. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a </paragraph><paragraph>KeyError: \"Key 'moving picture' not present.\"</paragraph><paragraph>it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? </paragraph><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I think you can try:</paragraph><paragraph> if word not in self.model.key_to_index: proceed to next word</paragraph><paragraph>to skip the word that is not in the pre-trained model.</paragraph></document>"
        },
        {
            "source": "Part 4 Key Error. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a </paragraph><paragraph>KeyError: \"Key 'moving picture' not present.\"</paragraph><paragraph>it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? </paragraph><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>For skipping, depending on the version I think the syntax I found worked was <code>if word not in self.model</code>. FWIW for part 6 I tried some more creative approaches for handling compound words that aren't in the model, but didn't find anything that improved (or even matched) the performance when just skipping them.</paragraph></document>"
        }
    ],
    {
        "source": "HW4 Part3. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For the simple less predictor, when using the shortcut in #477 to break ties, should we still expect there to be some ties between lexemes? When I implemented the shortcut, there were about 41/300 instances where there were still ties. </paragraph><paragraph>For example, when the target word was 'cross' in context 51, the lexemes (Synset('crisscross.n.01'), Lemma('crisscross.n.01.crisscross')) and (Synset('crisscross.n.01'), Lemma('crisscross.n.01.mark')) were tied. I randomly picked crisscross. I think this tie is occurring because when I used count() to calculate their frequencies, I got 0, and since they are in the same synset, they had the same aggregate score.</paragraph><paragraph>These are my results for part 3:</paragraph><paragraph>Total = 298, attempted = 298; precision = 0.109, recall = 0.109</paragraph><paragraph>Total with mode 206 attempted 206; precision = 0.150, recall = 0.150</paragraph><paragraph>Also, I was surprised to find that when I always used the tie breaking shortcut on all lexemes, it slightly outperformed my original results:</paragraph><paragraph>Total = 298, attempted = 298; precision = 0.112, recall = 0.112</paragraph><paragraph>Total with mode 206 attempted 206; precision = 0.155, recall = 0.155</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I didn't actually check how many remaining ties there are. But yes, I would expect there are some. </paragraph></document>"
    },
    {
        "source": "Lecture 19 Text Summarization Slides. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I don't see the slides for today's lecture (Text Summarization) on courseworks. When can we expect to see them published? Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>They\u2019re up now in the Files section on courseworks.\u00a0</paragraph></document>"
    },
    {
        "source": "Lecture Slides 18 pdf. <document version=\"2.0\"><paragraph>Would it be possible to upload the lecture 18 slides as a PDF to Courseworks?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>They\u2019re up now (in the files section. I\u2019ll link them from modules later)\u00a0</paragraph></document>"
    },
    {
        "source": "Homework Grading. <document version=\"2.0\"><paragraph>Hi, I am wondering if hw4 will be our last coding assignment. The syllabus says the course grade will be based on 5 programming assignments and the lowest grade will be dropped. </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>In lecture, I believe the professor mentioned that there will be a fifth coding assignment, but that this will be due after the final. Also, since the lowest grade will be dropped, I think this assignment will be optional.</paragraph></document>"
    },
    [
        {
            "source": "HW4 Part5. <document version=\"2.0\"><paragraph>How long should it take to run part 5 of hw4? My laptop is taking upwards of 20 minutes and I wanted to know if that was normal or if there are some issues within my code.</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>That seems rather slow. It probably has to do with how you sort the BERT predictions. </paragraph></document>"
        },
        {
            "source": "HW4 Part5. <document version=\"2.0\"><paragraph>How long should it take to run part 5 of hw4? My laptop is taking upwards of 20 minutes and I wanted to know if that was normal or if there are some issues within my code.</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I think you may load the <break/><break/>word2vec = Word2VecSubst('GoogleNews-vectors-negative300.bin')\n <break/><break/>in your loop function. I did not notice this for the first time I ran this.<break/>Move it outside and just call the predict_nearest in your loop</paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "HW4 Part 2 & 3 Score. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>Is this score good for Part 2 and part 3. My lesk model (part3) slightly outperformed wn_frequency model (part2), and I want to make sure it is okay. Thank you so much!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/6qs8W986n2v6GXcKkn5JgNG6\" width=\"658\" height=\"105.89209302325581\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>yes, that looks good .</paragraph></document>"
    },
    {
        "source": "Hw4: Update tokenize function?. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>For HW4, can I update tokenize function and create helper method for my code?</paragraph><paragraph>Tysm!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes. </paragraph></document>"
    },
    {
        "source": "Hw4 Part2 Instruction Clarification. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs -</paragraph><paragraph>Can I get some clarification on how to find \"the possible synonym with the highest total occurence frequency (according to WordNet)\". Does highest total occurence frequency mean occurence frequency of this sense of 'synonym' in the SemCor corpus? Can I simply get this frequency by using the .count() function? Tysm!!!!</paragraph><paragraph>Best</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, you can simply get this using .count()</paragraph><paragraph>The only thing to watch out for is that a synonym may occur in two different synsets.</paragraph></document>"
    },
    {
        "source": "Part3 Breaking ties. <document version=\"2.0\"><paragraph>Hi, I'm struggling to break the tie in part 3. I believe I have the correct overlap score, but I think I'm making errors in determining the values for 'b' and 'c' in @477. My logic is as follows:<break/>1. Create tokens of the contexts</paragraph><paragraph>2. Find synsets using context</paragraph><paragraph>3. For each synset in the synsets:</paragraph><paragraph>     3.a. Calculate overlap_score for the synset</paragraph><paragraph>     3.b. Retrieve synset lemmas.</paragraph><paragraph>     3.c. For each lemma in the synset:</paragraph><paragraph>          3.a.i. if this lemma is the same as the target(b case) -&gt; add it to b variable</paragraph><paragraph>          3.a.2. all the other lemmas will fall under the c case. -&gt; Increase c with each count</paragraph><paragraph>     3.d. Calculate the total score and select the highest.</paragraph><paragraph>However, this approach yields a low output (approximately 0.06). Could you let me know where I might be making mistakes? </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, from my understanding of @477, it seems that your logic is correct, but just want to clarify some things in your logic. </paragraph><paragraph>In steps 3c-3d, are you calculating an aggregate score for each lexeme? or each synset? </paragraph><paragraph>And in step 3.a.2, when you calculate 'c' for \"all other lemmas,\" are you calculating a separate 'c' for each lemma?    </paragraph></document>"
    },
    {
        "source": "hw4 part6 improvement. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/hff78IFQVHZJz2owOpvzI6mo\" width=\"658\" height=\"162.20891364902508\"/></figure><paragraph>For part 6, the precision and recall improve in the total = 298, but in mode 206, they decrease. Can this result be considered an improvement?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I'd consider it an improvement. </paragraph><paragraph>But more importantly, we will not grade part 6 according to absolute improvement as long as you tried a different approach (one that is qualitatively different than the other parts). </paragraph></document>"
    },
    {
        "source": "hw4 part2 scores question. <document version=\"2.0\"><paragraph>Hi this is a general question about the varying scores that people are posting for this section. For part2, aren't the predictions deterministic, since we are just outputting the most frequently occurring possible substitute? So why are people getting different scores for this section? Or am I misunderstanding the prompt... Much thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>There are a few implementation decisions that may lead to different scores:  </paragraph><list style=\"bullet\"><list-item><paragraph>Are you implementing the scoring method proposed in #477 or did you implement tie breaking differently?</paragraph></list-item><list-item><paragraph>How did you preprocess the definitions and example sentences? </paragraph></list-item></list><paragraph/></document>"
    },
    {
        "source": "Question about lecture16 PPT. <document version=\"2.0\"><paragraph>Dear Prof. and TAs,</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ntMhA7YIm99V7DmDsQSf8HIQ\" width=\"658\" height=\"285.602495543672\"/></figure><paragraph>I'm wondering how this graph was derived, I can't find any definition about the dependency structure or replacement rules on the PPT, could you please explain how the graph was derived to me? Thanks!</paragraph><paragraph>(starting from this:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/OccSdevkdeClbeMUTvhoEdUr\" width=\"643\" height=\"265.2453567937439\"/></figure><paragraph>)</paragraph><paragraph/><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>The next step here would be to use Rule 5 (the full grammar is a few slides past this one) to convert the S2 edge into a VP2 edge. Then replace the VP2 edge using rule 6. </paragraph></document>"
    },
    [
        {
            "source": "File 'GoogleNews-vectors-negative300.bin' or 'bin.gz'. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>When I downloaded the file, it was named 'GoogleNews-vectors-negative300.bin', not 'GoogleNews-vectors-negative300.bin.gz'. </paragraph><paragraph>I attempted to convert it to the .gz format but encountered an error: gzip.BadGzipFile: Not a gzipped file (b'30'). </paragraph><paragraph>Is it possible to use the 'GoogleNews-vectors-negative300.bin' file directly in lexsub_main.py?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Just imported it as the '.bin' file, it is a trained model.</paragraph></document>"
        },
        {
            "source": "File 'GoogleNews-vectors-negative300.bin' or 'bin.gz'. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>When I downloaded the file, it was named 'GoogleNews-vectors-negative300.bin', not 'GoogleNews-vectors-negative300.bin.gz'. </paragraph><paragraph>I attempted to convert it to the .gz format but encountered an error: gzip.BadGzipFile: Not a gzipped file (b'30'). </paragraph><paragraph>Is it possible to use the 'GoogleNews-vectors-negative300.bin' file directly in lexsub_main.py?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Yes, I think on some systems / browsers the file is automatically unzipped. Using the .bin file is fine. </paragraph></document>"
        }
    ],
    {
        "source": "hw4 part 2 & 3. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/p1pZ6nvUevHAU2fnItWAnzq4\" width=\"658\" height=\"148.77260981912144\"/></figure><paragraph>May I ask if the results in the picture are reasonable results for frequency and simple leak predictor?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>These look reasonable to me!</paragraph></document>"
    },
    {
        "source": "Error Using the Tokenize Method. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/9iP94YyAsDGgVXoqGueLSOPH\" width=\"644.0000000000001\" height=\"158.9225806451613\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/N4CHXpfBCUbt1J7yBR9k8dks\" width=\"644\" height=\"89.07135016465423\"/></figure><paragraph>Hi, when trying to use the tokenize method, I get an error saying it doesn't recognize the item \"string\". Did anyone else encounter this problem?</paragraph><paragraph>Would it be wrong to simply tokenize by using .split(\" \")?</paragraph><paragraph/><paragraph/><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>feel free to import string.</paragraph><pre>import string\n</pre><paragraph>You could simply split by whitespaces, but that would not handle punctuation at all, such as commas and periods. </paragraph></document>"
    },
    {
        "source": "hw4 part 5. <document version=\"2.0\"><paragraph>Hi there, </paragraph><paragraph>I am doing the part5 and I am not sure about how to handle the case when the candidate from WordNet is a multi-word like \"at long last\", in this case, how can we compare the candidate with the output tokens of the DistillBert model, as I think the prediction result from the DistillBert model is just a single token and doesn't include the multi-word case. </paragraph><paragraph>Thanks a lot in advance!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>In my implementation I skipped these candidates.\u00a0</paragraph></document>"
    },
    {
        "source": "hw4 part1 Question. <document version=\"2.0\"><paragraph>For part 1, shall we return a list or a set?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Return a set, as shown in the example. Sorry for the misleading description.\u00a0</paragraph></document>"
    },
    {
        "source": "HW4 part 2. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am also confused what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". In particular I am confused what it means by \"if the word and the target appear together in multiple synsets\" Is it simply counting frequency of synonym occurrence in the list from part 1?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Let\u2019s say the target word is \u201cbright\u201d and both \u201cbright\u201d and \u201clight\u201d appear in synset s1 and s2. To get the score for the candidate \u201clight\u201d you need to sum the count for &lt;light, s1&gt; and &lt;light, s2&gt;.\u00a0</paragraph></document>"
    },
    {
        "source": "HW4 part2 result question. <document version=\"2.0\"><paragraph>Hi dear prof. and TAs,</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1qF3l9gM1tdx0QZNZG35R7l7\" width=\"516\" height=\"124\"/></figure><paragraph>After finishing part2, I got this result, it seemed that it was lower than 10%, is it OK?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Hm that looks a little low from what I remember. Maybe take a look at the predict file and see if you notice anything suspicious.\u00a0</paragraph></document>"
    },
    {
        "source": "HW4 Part 6 question with using BERT-Large. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For part 6, I tried to use a larger version of the BERT model to increase the prediction \"accuracy\" of the replacement for [MASK] label. The version I used is BERT-large, also included in the transformer package. After making this replacement, the predictions for the [MASK] label all becomes labels like [unused818]. Why would this happen?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Are you also using the corresponding tokenizer? It\u2019s important to use the same tokenizer that the model was pretrained on.\u00a0</paragraph></document>"
    },
    {
        "source": "HW4 Part 2 Scoring. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I'm currently working on part 2 and had a clarification question: am I meant to be looking at the word_form or the lemma of the given context? It seems to make more sense to me to use the lemma, but I'm getting the following results that don't seem quite right:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/SlqnYBNXqQOQLfFHquHz3fsh\" width=\"658\" height=\"74.08592592592592\"/></figure><paragraph>My results when using the word_form are even further off, so I'm a bit confused about where I'm going wrong. Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>You should be using the lemma. Look at the output file and see if you notice anything suspicious.\u00a0</paragraph></document>"
    },
    {
        "source": "Exact same performance for Word2VecSubst and BertPredictor. <document version=\"2.0\"><paragraph>When I evaluate both implementations, I get the following results:</paragraph><paragraph>(tf) sameerkhanna@Sameers-MacBook-Pro hw4_files % perl score.pl word_vec.predict gold.trial<break/> Total = 298, attempted = 298 precision = 0.115, recall = 0.115 Total with mode 206 attempted 206 precision = 0.170, recall = 0.170 </paragraph><paragraph>(tf) sameerkhanna@Sameers-MacBook-Pro hw4_files % perl score.pl bert.predict gold.trial<break/> Total = 298, attempted = 298 precision = 0.115, recall = 0.115 Total with mode 206 attempted 206 precision = 0.170, recall = 0.170</paragraph><paragraph/><paragraph>I checked and confirmed the prediction files are indeed predicting different things. Is this result ok, or are we supposed to get better performance using BERT? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes this is expected.</paragraph></document>"
    },
    {
        "source": "hw 4 pt 2 results. <document version=\"2.0\"><paragraph>I want to confirm that my results for hw 4 part 2 wn_frequency test look reasonable. The assignment says they should both be around 10% and mine is 9% and 13%. </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/KNbulNJdB4LNUxKjjTLy22yn\" width=\"658\" height=\"149.98529411764707\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>I think this is fine.\u00a0</paragraph></document>"
    },
    {
        "source": "Illegal Instruction 4. <document version=\"2.0\"><paragraph>Hi, I'm getting an error \"Illegal Instruction: 4\" when I try to run the line python lexsub_main.py lexsub_trial.xml  &gt; smurf.predict. Everything else in the instructions has worked up until this point, so not entirely sure what could be happening. Thanks so much! </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Can you post the entire error message you are getting? </paragraph></document>"
    },
    {
        "source": "Hw4 part 1. <document version=\"1.0\"><paragraph>The documentation mentions removing the _ in some of the words. By removing, does that mean delete the underscore (which combines the words) or replace the underscore with a space?</paragraph><paragraph>Ex: CanisFamiliaris or Canis Familiaris</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>In the test data, multiword expressions show up with white spaces \" \". But note that this won't help you for the gensim and Bert approach (because multiword expressions are not in the lexicon for these models). </paragraph><paragraph/></document>"
    },
    {
        "source": "HW 4 Part 6 Question. <document version=\"2.0\"><paragraph>As mentioned earlier in <link href=\"https://edstem.org/us/courses/46417/discussion/3921880\">#476</link>, Windows laptops have an issue with the newlines. To circumvent this, I made it so that my python file directly opens and writes a file. For the final submission, is it fine to keep my code written in that manner so it automatically creates a part6.predict file when we run this line</paragraph><pre>python lexsub_main.py lexsub_trial.xml</pre><paragraph>or does it need to be runnable in the following format (as in the HW documentation)?</paragraph><pre>python lexsub_main.py lexsub_trial.xml  &gt; part6.predict\n</pre><paragraph>I would prefer to have it so that my python file is able to open and write the file itself instead of storing the printed lines in a file, because the newlines in Windows do not behave appropriately the second way.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>It\u2019s fine to leave it the way you changed it, with the output being written directly to a file.\u00a0</paragraph></document>"
    },
    {
        "source": "problem with python. <document version=\"2.0\"><paragraph>Hi I was running the python file for assignment 4 and I got an unexpected error. I always ran python file using pycharm but this time I got an pop up window that says python quited unexpected. I retried a few times but this window appears every time I try to run. I am using python 3.9.7 and mac. I am not very familiar with python, can anyone give me some suggestion about how to solve this? any advice helps, Thank you so much.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/8pPhF1DeOcCqujCtcq1XFGzb\" width=\"658\" height=\"527.716\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Not sure what's going on -- the issue seems to be the pycharm IDE. Maybe try a different IDE or editor? </paragraph><paragraph>You could also try to install a separate Python environment, rather than the one that comes preinstalled with macOS. I recommend looking into Anaconda (or miniconda). </paragraph></document>"
    },
    {
        "source": "HW4 Part 3 Acceptable Score. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>For part 3, what is an acceptable precision/recall score? My implementation produces the following result: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ChBkam7HmQ6v4l7GWWmeHDua\" width=\"476\" height=\"108\"/></figure><paragraph>I was wondering whether this is acceptable or not.</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes it\u2019s expected that the scores for part 4 and 5 produce very similar results. The score for part 3 also looks fine to me, but maybe post publicly for comparison?\u00a0</paragraph></document>"
    },
    {
        "source": "biLM representations of different senses. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I was reviewing the slides and I have a question about biLM embeddings. From my understanding, biLM generates different embeddings for the same word if it has different senses. How does biLM differentiate the two senses? I would guess that even if two words have the same sense but appear in slightly different contexts, their embeddings would be slightly different, but not enough to consider them to have different senses. Does biLM store embeddings for each of the instances of a word and then classify them into groups depending on their similarities? </paragraph><paragraph>Thanks in advance!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The representations computed by a bidirectional language model do not differentiate between discrete word senses. Rather, each token representation will be unique based on the actual sentence context that the word appears in. Tokens that appear in similar contexts will have similar senses. The premise is that there are no discrete senses and that contextualized embeddings capture fine-grained sense distinctions more accurately. After all, the sense annotations in WordNet (and other resources, like VerbNet, PropBank and FrameNet) are coarse and somewhat artificial -- they are created by human annotators. </paragraph><paragraph>If you <underline>wanted</underline> to use contextualized word representations to obtain representations for discrete senses (as annotated in WordNet), you could do the following. For each lexeme, obtain contextualized embeddings for all annotations of the lexeme in SemCor. Then average these embeddings (which should, after all, be reasonably similar anyway). </paragraph><paragraph/></document>"
    },
    {
        "source": "Are we allowed to use cheat sheets for Final Exam?. <document version=\"2.0\"><paragraph>Are we allowed to use cheat sheets for Final Exam?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>\"One double-sided letter-sized \"cheat sheet\" will be allowed\" (on Courseworks)</paragraph></document>"
    },
    {
        "source": "HW4 Part 4. <document version=\"2.0\"><paragraph>When working on part 4, I noticed that some candidate synonyms (obtained from the <code>get_candidates</code> method) does not exist in the word2vec model. For example, for the word film.n, I got the following error:</paragraph><pre>KeyError: \"Key 'moving picture' not present\"\n</pre><paragraph>I'm wondering that when processing multi-word expressions in part 1, by removing '_', should we change (for example) the word <code>turn_around</code> to <code>turn around</code> or <code>turnaround</code>? If it's the first case, when computing the embedding of this phrase, should the embedding be the average of the embedding of each word? If it's the second case, this word will still be shown as not present. How should we process this case?</paragraph><paragraph>Also, I noticed that words like <code>of</code> cannot be found in the KeyedVector. Should we consider using the <code>stopword</code> to remove words like this?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Check out this thread: <link href=\"https://edstem.org/us/courses/46417/discussion/3950973\">#517</link></paragraph></document>"
    },
    [
        {
            "source": "HW 4 part 2 Issue with score.pl. <document version=\"2.0\"><paragraph>I was able to get the score.pl to work with our smurf.predict, and it correctly gives 0 for everything. However, in part 2, we implemented a version that should be better. I checked my output part2.predict file and it did look like the words seemed to line up occasionally with the gold.trial. However, I still receive 0 for both the precision and recall (even though my part2.predict file looks more accurate) when I run the following command:</paragraph><paragraph>perl score.pl part2.predict gold.trial</paragraph><paragraph>The only parts that I updated were the part1 and part2 code so far, and I changed the prediction to use part 2's wn_frequency_predictor method to generate the part2.predict file. Can anyone help me understand where I am going wrong and how to move forward?</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>I suspect this is an issue of file endings. The Perl script expects Unix file endings, but you may be running your code on windows, in which case Python produces windows file endings by default. \u00a0<break/>You a modify the code to write to a file directly rather than printing the predictions to console. That would allow you to control the formatting and encoding of the file. Try something like this:</paragraph><paragraph>import io<break/>f = io.open('file.presidict\u2019, 'w', newline='\\n')<break/></paragraph></document>"
        },
        {
            "source": "HW 4 part 2 Issue with score.pl. <document version=\"2.0\"><paragraph>I was able to get the score.pl to work with our smurf.predict, and it correctly gives 0 for everything. However, in part 2, we implemented a version that should be better. I checked my output part2.predict file and it did look like the words seemed to line up occasionally with the gold.trial. However, I still receive 0 for both the precision and recall (even though my part2.predict file looks more accurate) when I run the following command:</paragraph><paragraph>perl score.pl part2.predict gold.trial</paragraph><paragraph>The only parts that I updated were the part1 and part2 code so far, and I changed the prediction to use part 2's wn_frequency_predictor method to generate the part2.predict file. Can anyone help me understand where I am going wrong and how to move forward?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Thank you so much professor!!!</paragraph></document>"
        }
    ],
    {
        "source": "HW4 Part 3. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am a bit confused about the following statements in the description for part 3<break/><break/><italic>If this is the case (or if there is a tie), you should select the most frequent synset (i.e. the Synset with which the target word forms the most frequent lexeme, according to WordNet). Then select the most frequent lexeme from that synset as the result.</italic></paragraph><paragraph>How do we define the most frequent lexeme and the most frequent synset? As far as I understand, WordNet does not contain any information on the frequency of each synset. Do we need to rely on any external corpus to get this frequency? I would really appreciate some explanations on this, preferably with an example. Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Wordnet contains frequency information about individual lexemes (in the SemCor corpus).\u00a0</paragraph><paragraph>So if your candidate lemma is bright.an and you \u00a0find three lexemes for synset 1, 2, and 3, let\u2019s call them &lt;bright-s1&gt; &lt;bright-s2&gt; &lt;bright-s3&gt;, then the \u201cmost frequent synset\u201d is the one for the most frequent of these lexemes.\u00a0</paragraph><paragraph>There is a shortcut/trick for doing the tie-braking. I will post in another thread.\u00a0</paragraph></document>"
    },
    {
        "source": "Ungraded Exercise AMR. <document version=\"2.0\"><paragraph>Hi! Hope everyone is having a good break, just wondering when the solutions to the ungraded AMR exercise will become available :)</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I will post them early this week. </paragraph></document>"
    },
    {
        "source": "'keras.engine' when running Bert. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I try to run the Bert model by using </paragraph><pre>&gt;&gt;&gt; import transformers \n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; model = transformers.TFDistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')</pre><paragraph>and encounter this error:</paragraph><paragraph>File \"/Users/.../anaconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py\", line 70, in  </paragraph><paragraph>from keras.engine import data_adapter </paragraph><paragraph>ModuleNotFoundError: No module named 'keras.engine'</paragraph><paragraph>RuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):<break/>No module named 'keras.engine'</paragraph><paragraph>How to resolve this error? I tried to update my tensorflow and transformers, and keras is also installed in my computer, but none of them worked. </paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Sorry for the slow response. This looks like a version incompatibility between tensorflow and transformers. Make sure you have the latest version of each. </paragraph><paragraph>pip install --upgrade keras<break/>pip install --upgrade transformers</paragraph><paragraph>Is anyone else encountering the same issue? </paragraph></document>"
    },
    {
        "source": "HW4 Part 6. <document version=\"2.0\"><paragraph>For Part 6, should we be creating another function separate from the functions from the first 5 parts? For instance, if I want to refine the model in Part 4, could I just define another function inside the <code>Word2VecSubst</code> class?</paragraph><paragraph>Also, do we only need to refine one of the 5 models? Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes this should be a new function (though it can duplicate code or call other functions.</paragraph><paragraph>The refinement soils be to one of the 5 models or a combination of multiple models. We are more interested in new/creative ways of attempting this task rather than just getting the best performance.\u00a0</paragraph></document>"
    },
    {
        "source": "Combination of Lecture-based Conceptual Qs. <document version=\"2.0\"><list style=\"unordered\"><list-item><paragraph>ML for Dependency Parsing: Max of 3 untyped choices: $|R| \\times 2 + 1$ ? I didn\u2019t understand what R is and why is it multiplied by 2 and added by 1?</paragraph></list-item><list-item><paragraph>What does it mean by Left arc transition focus on relating head to adjacent, while right arc focusing on relating the head? So is buffer typically considered head?</paragraph></list-item><list-item><paragraph>GPT/BERT: does auto-regressor and auto-encoder classification suggest how the models are originally trained or the type of tasks they can do?</paragraph></list-item><list-item><paragraph>PropBank: how to differentiate between Arg2 and Arg3? Seems quite similar to each other.</paragraph></list-item><list-item><paragraph>Span-graph based SRL predictions: what is p, a, l in this case? can you give a more detailed explaination of the Unary scores, label score, and combined score and how they relate and interact with each other?</paragraph></list-item><list-item><paragraph>For the PropBank SRL Results: the last paper on swapping out ELMo, it\u2019s still based on RNN network or Span-graph? If span-graph is it the Word &amp; character representation layer (x) that is being replaced?<break/><break/>Thanks in advance!</paragraph></list-item></list></document>",
        "target": "<document version=\"2.0\"><paragraph>Sorry for the slow response. Let me address these one-by-one</paragraph><list style=\"number\"><list-item><paragraph><bold>R</bold> is the set of relation labels. Each label can appear with two transitions, arc_left and arc_right. The +1 is for the shift transition which does not require a label. </paragraph></list-item><list-item><paragraph>Not sure I understand the question. With arc_left the head is the first word on the buffer and the dependent is the first word on the stack. With arc_right, it's the other way around. </paragraph></list-item><list-item><paragraph>Yes, the terms auto-regressor and auto-encoder refer to the pre-training objective. </paragraph></list-item><list-item><paragraph>Arg2 - Arg4 have to be interpreted differently for each verb sense). They do not generalize across different predicates. The definition for what the Arg2-Arg4 roles mean is part of the propbank annotation (in the \"frame file\" for that verb). </paragraph></list-item><list-item><paragraph>p, a, l stand for predicate, argument, and label. We didn't go too much into detail for this specific model -- if you are interested in the detail I suggest looking at the He et al. 2018 paper. <link href=\"https://aclanthology.org/P18-2058/\">https://aclanthology.org/P18-2058/</link></paragraph></list-item><list-item><paragraph>The last row is the span-graph based approach, but using pre-trained ELMo to compute contextualized word representations. And yes, this would replace the \"Word &amp; Character representation layer\" in the figure in lecture 15 slide 48. </paragraph></list-item></list></document>"
    },
    {
        "source": "midterm distribution. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm just wondering where can i find the class distribution of midterm. Thank you very much</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi this should be available on Gradescope, but was also included in the Announcement I sent out when the grades were released. </paragraph></document>"
    },
    [
        {
            "source": "Hw4 Part 2 Clarification. <document version=\"2.0\"><paragraph>Hi, I'm a bit unsure of what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". Would you be able to give an example of this?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I am also struggling to understand what is meant here.</paragraph></document>"
        },
        {
            "source": "Hw4 Part 2 Clarification. <document version=\"2.0\"><paragraph>Hi, I'm a bit unsure of what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". Would you be able to give an example of this?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Similar to the get_candidates method, as you iterate through all the lemmas, store the counts for each word encountered to get the most frequent synonym.</paragraph></document>"
        }
    ],
    {
        "source": "HW 4 Wordnet Installation. <document version=\"2.0\"><paragraph>Really insignificant but just wanted to make sure I didn't set up something incorrectly but when I run :</paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"true\"><snippet-file id=\"code\">l1 = wn.lemmas('break', pos='n')[0]\ns1 = l1.synset()\ns1.hypernyms()</snippet-file></snippet><paragraph>I should get: \"[Synset('happening.v.01')]\"</paragraph><paragraph>but instead, get:\"[Synset('happening.n.01')]\"</paragraph><paragraph/><paragraph>Everything else looks okay but just wanted to make sure \ud83d\ude05<break/></paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>\"[Synset('happening.n.01')]\" seems good to me actually because your initial search was for a noun.\u00a0</paragraph></document>"
    },
    {
        "source": "Final exam material. <document version=\"2.0\"><paragraph>Will the final be cumulative or will it be the part covered only after the midterm.  </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>(not a TA) They mentioned the final will be <link href=\"https://edstem.org/us/courses/46417/discussion/3807069?comment=8777621\">cumulative</link></paragraph><paragraph/></document>"
    },
    {
        "source": "Final Exam Schedule. <document version=\"2.0\"><paragraph>Hi - </paragraph><paragraph>I noticed that the final exam schedule date published on SSOL is different from the one noted on Canvas. I want to confirm which one is the correct schedule:</paragraph><paragraph>Canvas/ syllabus: 12/11 (class time)</paragraph><paragraph>SSOL: 12/18/23 </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/JQ7Q8ahK5k8pYHRtC6mPe2HA\" width=\"658\" height=\"115.83762057877814\"/></figure><paragraph>Best,</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The syllabus is correct -- exam 2 will be on the last day of class (12/11, 1:10pm). </paragraph><paragraph>The registrar automatically schedules a final exam slot, but we are not using it. </paragraph></document>"
    },
    [
        {
            "source": "Hw4 Release date. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs -</paragraph><paragraph>I am wondering when will HW4 be released so I can plan my Thanksgiving plan accordingly?</paragraph><paragraph>Thank you so much!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>(Not a TA) I think in lecture he said it would be released on Friday </paragraph></document>"
        },
        {
            "source": "Hw4 Release date. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs -</paragraph><paragraph>I am wondering when will HW4 be released so I can plan my Thanksgiving plan accordingly?</paragraph><paragraph>Thank you so much!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I'm still working on it, apologies. But it should come out today or maybe tomorrow. </paragraph></document>"
        }
    ],
    {
        "source": "Lecture 16: Semantic Parsing - Abstract Meaning Representation Slides Courseworks. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I was wondering when the slides for Lecture 16 (the slide deck for the second half of today's lecture) would be uploaded to courseworks? I don't see them currently under lecture notes. Thank you very much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Lecture 16 is now available on Courseworks. </paragraph></document>"
    },
    {
        "source": "Exam question. <document version=\"2.0\"><paragraph>Dear TAs and Professor,</paragraph><paragraph>I noticed that my exam grade on canvas is different from that is on the grade scope. Will the grade on canvas be changed later? Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes I will synchronize these once all regrades have been processed.\u00a0</paragraph></document>"
    },
    {
        "source": "Prof. Bauer's OH Today. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>A few of us are on the zoom link sent via the announcement, wondering if we are in the right place or is there a change in OH? </paragraph><paragraph/><paragraph>Thanks</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I was running late, but I think I got to talk to you eventually (as well as everyone else who was waiting). Sorry for that. </paragraph></document>"
    },
    {
        "source": "final crib sheet. <document version=\"2.0\"><paragraph>Hi i hope you are doing great.  I think I remember from class discussion that on the final we are allowed to have a cheat/crib sheet.  If so, how long can the sheet be and are there any rules about  what can and can't be on it, if it has to written or typed etc.  Thank you!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><heading level=\"2\"><link href=\"https://edstem.org/us/courses/46417/discussion/3807069\">#384</link></heading><paragraph/></document>"
    },
    {
        "source": "nlp internships. <document version=\"2.0\"><paragraph>Hi I'm sorry if this is an inappropriate  use of ED, but I am trying to get into the NLP field and I was wondering if anyone knew of any NLP internships (particularly with mitigating bias) that are specifically  for undergrads?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi there! <break/><break/>A little late, but I believe looking into an Ontology/Knowledge Engineering internship at Amazon could be helpful. I was an intern at Alexa AI last year and while not directly related to NLP, my work was somewhat adjacent to it. I believe one of the other interns in our team was working directly with NLP, so if you show interest in the topic maybe an NLP project can be assigned to you. In any case, it could be a nice way to get some exposure to it.<break/><break/>Hope this helps</paragraph></document>"
    },
    {
        "source": "Couseworks grade not updated after regrade. <document version=\"2.0\"><paragraph>Hello Teaching staff,</paragraph><paragraph>Just letting you know that my courseworks grade is not yet updated after the regrade is reflected on gradescope.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, these don't automatically synchronize. I will update them after the end of the regrade period. </paragraph></document>"
    },
    {
        "source": "HW3 - Finally debugged!. <document version=\"2.0\"><paragraph>Hi TAs and Prof. Bauer, </paragraph><paragraph>This is Millie and I know I asked for a lot of help on HW3. The code seems all correct but was just not running properly. Turns out, there may be problems with my machines. I transported everything onto Google Colab and finally got very reasonable results. </paragraph><paragraph><bold>Training Model Loss:</bold> </paragraph><paragraph>Done loading data. Epoch 1/5 18996/18996 [==============================] - 80s 4ms/step - loss: 0.4917 Epoch 2/5 18996/18996 [==============================] - 79s 4ms/step - loss: 0.3149 Epoch 3/5 18996/18996 [==============================] - 78s 4ms/step - loss: 0.2796 Epoch 4/5 18996/18996 [==============================] - 78s 4ms/step - loss: 0.2597 Epoch 5/5 18996/18996 [==============================] - 80s 4ms/step - loss: 0.2462</paragraph><paragraph><bold>Evaluation:</bold> </paragraph><paragraph>5039 sentence. </paragraph><paragraph>Micro Avg. Labeled Attachment Score: 0.7465165107845009 </paragraph><paragraph>Micro Avg. Unlabeled Attachment Score: 0.7942181887591316 </paragraph><paragraph>Macro Avg. Labeled Attachment Score: 0.7558937632762938 </paragraph><paragraph>Macro Avg. Unlabeled Attachment Score: 0.8047567682996984</paragraph><paragraph/><paragraph>So happy to finally get the program to work, but perhaps suggest issues with my machine I may need to look into for future HWs.</paragraph><paragraph>Overall, thank you again for all your patience and help throughout the process!! </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>How odd! No other change? I\u2019d be very curious to learn what the issue was in the end.\u00a0</paragraph></document>"
    },
    {
        "source": "Grading Question. <document version=\"2.0\"><paragraph>Hi, I hope you are doing great.</paragraph><paragraph/><paragraph>I was wondering if I could ask if students could be given the option to weigh the final more heavily in the grade calculations.  I was wondering if I could make the final worth 50% of my grade and drop my midterm grade.</paragraph><paragraph/><paragraph>I  understand if this is not possible.  I am not trying to make excuses or to get special treatment.  I am not yet sure what I misunderstood with the midterm material, but I think I can get a lot better on the final.</paragraph><paragraph/><paragraph>Thank you for your consideration!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Unfortunately this is not really an option, unless there is a significant reason for a lower midterm performance (which would typically mean health-related -- and even then I have entertained options like this one only in extreme cases). </paragraph><paragraph>The final exam is curved, so everyone is affected by the midterm exam in the same way. </paragraph></document>"
    },
    {
        "source": "Midterm solution. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Will midterm solutions be posted? Would love to learn how to improve for the final. </paragraph><paragraph/><paragraph>Thanks</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>See #446</paragraph></document>"
    },
    {
        "source": "mid question 1. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>Just a quick question, if the answer to Question 1 mid-exam is \"v-&gt;d-&gt;adj-&gt;n\"?</paragraph><paragraph>Thanks a lot!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, the POS sequence is V-&gt;D-&gt;Adj-&gt;N.</paragraph></document>"
    },
    {
        "source": "Midterm. <document version=\"2.0\"><paragraph>Incredibly embarrassed that I have to ask, but what is the likelihood of failing the class if I got a 38 on the midterm :( </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>My experience is that people only fail this class if they stop submitting work. The homework problems alone are worth 40% of the grade. </paragraph><paragraph>A 38% is not great, but I would think of it as a diagnostic tool to see how you can improve for exam 2.</paragraph><paragraph>I will release a sample solution for the exam today. Maybe take a look and compare to your exam. If there are any remaining questions, we can go through your exam together in office hours and analyze which concepts you should revise and how to prepare better for exam 2. </paragraph></document>"
    },
    {
        "source": "exam question. <document version=\"2.0\"><paragraph>Hi I hope your are doing great.  Are the correct answers for the exam available somewhere?  I see the deductions, but I am trying to figure out where I went wrong.</paragraph><paragraph/><paragraph>Thank you</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes, I will release a sample solution. I just didn\u2019t get a chance to type it up.\u00a0</paragraph></document>"
    },
    {
        "source": "Exam issue. <document version=\"2.0\"><paragraph>Hello, I think my exam for question 4 was correct, but the text was blurred. It only scanned part of my answer on it. Is there any way to deal with this issue?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yea please submit a regrade request through Gradescope so we can process these more efficiently. Worst case, I can try to chase down the physical copy.\u00a0</paragraph></document>"
    },
    {
        "source": "Submission file name. <document version=\"2.0\"><paragraph>Hi! I totally missed the part of the assignment instructions for HW 3 where we were supposed to name our file &lt;uni&gt;_homework3 -- is this big enough of an issue to resubmit or is there any way to resubmit without losing credit?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>No, not worth resubmitting. We will figure it out. </paragraph></document>"
    },
    {
        "source": "HW3 resubmission. <document version=\"2.0\"><paragraph>Hello! I just realized I submitted my code with a line I meant to delete that was leftover from testing. It was a single line in the train_model.py file. </paragraph><paragraph>Is there any way for me to resubmit without the 20 point late penalty? </paragraph><paragraph>Sorry for the late post!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes, please go ahead and resubmit today.\u00a0</paragraph></document>"
    },
    {
        "source": "Submission. <document version=\"2.0\"><paragraph>Should every py file, h5 file, and vocab file be in the same directory in the zip file? or should there be a data folder as well?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Not a TA. I just included all 6 .py, 1 h5 file, and 2 vocab file into one folder and zip it.</paragraph></document>"
    },
    {
        "source": "HW3 Part 4: my evaluate.py doesn't recognize dependency relation. <document version=\"2.0\"><paragraph>I get a score of .76 and .77 for my micro and macro unlabeled attachment scores, however, my labeled attachment score is 0. Actually, I get a division by zero error without adjusting the main.py in evaluate.py. This appears to be because the program doesn't recognize any of my dependency relations in each edge. </paragraph><paragraph>What should the argument for left and right arc look like exactly? I'm currently keying into the output label dictionary, but this doesn't seem to be working.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Nevermind, found the answer, it turns out that if you just print the target_labeled list you can check to see the correct format for the labeled dependency relations.</paragraph></document>"
    },
    {
        "source": "Runtime for part 4 is 1 hour but no infinite loop, should I submit ?. <document version=\"2.0\"><paragraph>I let my part 4 run around 45 minutes and it's not finished so I cancelled it. When I try running it on smaller data set that I made, it produces 0.5 -&gt; 0.6 accuracy (around 100 sentences). Took it to an OH, and the TA was kind to walked through my code but we still coulnd't figure out what's wrong. (My loss value for part 3 is around 0.3 after 5th epoch). I also ran out of GPU so testing is gonna be slower, should I just submit ? Appreciate any advice.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Sounds like it\u2019s working \u2014 there are some reason why part 4 may be slow.\u00a0</paragraph><paragraph>Check that you have eager execution turned off.\u00a0</paragraph><paragraph>Another possible issue might be that the sorting of transitions is slow. Make sure you use np.argsort.\u00a0</paragraph></document>"
    },
    {
        "source": "Google Colab GPU Limit. <document version=\"2.0\"><paragraph>Has anyone got this message in google colab and know how to get around it?  </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/jaFzssZ8FhtrZTXxlr3nf1SM\" width=\"658\" height=\"315.45762711864404\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>maybe try another Google account where you still have credits?</paragraph><paragraph/></document>"
    },
    {
        "source": "HW3 Part 4 empty stack clarification. <document version=\"2.0\"><paragraph>When checking the legality for arc-right, the condition requires that the stack is not empty, does this mean that the stack contains no root node as well or would it be empty (not including root) i.e. its length equals 1? Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>right_arc is permitted if the stack contains root. It is not permitted if the stack is empty.\u00a0</paragraph><paragraph>Left_arc however is not permitted if the top of the stack is root.\u00a0</paragraph></document>"
    },
    {
        "source": "Bizarre Python Error for HW3 Part 4. <document version=\"2.0\"><paragraph>I keep getting this error while running the decoder and then Python crashes:</paragraph><pre>UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown\n\nwarnings.warn('resource_tracker: There appear to be %d '</pre><paragraph>For reference, I am on an M1 Mac but am using CPU TensorFlow.</paragraph><paragraph>Any ideas what could be causing this? I've checked my Part 2 and Part 3 and they look mostly right. Not sure what's going on here.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>This is just a warning you get because Python didn\u2019t shut down properly. It\u2019s not the reason for why it crashes. Are there any other error messages? \u00a0This happens in part 4 only? It\u2019s odd because if it was a tensorflow issue it should have also shown up in part 3.\u00a0</paragraph></document>"
    },
    {
        "source": "Dhaarna OH. <document version=\"2.0\"><paragraph>Does anyone know if Dhaarna OH is supposed to be in the CS TA room today? I've been here for 10 minutes but the only person here is the whiteboard with NLP written on it</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I think there was an announcement on canvas they got moved to tmrw :(</paragraph></document>"
    },
    {
        "source": "TensorFlow GPU on Linux (with just conda + NVIDIA drivers). <document version=\"2.0\"><paragraph>Hi all, </paragraph><paragraph>If you're working on a Linux machine and don't want to install specific CUDA and cuDNN libraries system-wide, you might find figuring out your TensorFlow installation a bit tricky like I did. I am sharing the installation method that I have found most desirable for my needs (on Linux with just conda + NVIDIA drivers and no cuda installation system-wide) in hopes of it being useful.</paragraph><paragraph/><paragraph>You most likely have NVIDIA drivers installed, you can just verify it via </paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"false\"><snippet-file id=\"code\">nvidia-smi</snippet-file></snippet><paragraph/><paragraph>If the above command does not display a nice output of your GPU processes, then you should figure out installing NVIDIA drivers first. </paragraph><paragraph/><paragraph>Setup a new conda environment with Python 3.10 (due to TF compatibility, we will not use newest Python versions):</paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"false\"><snippet-file id=\"code\">conda create -n tf python=3.10</snippet-file></snippet><paragraph>Next, review the TF compatibility website as well as the conda-forge packages for cudatoolkit and cudnn to find the newest version of TF that you can install via conda alone. As of today, this is 2.11, which came out about a year ago. The tradeoff with this installation method is that you won't be able to use the shiniest TF release but you will have the ease of installation that comes with conda.</paragraph><paragraph><link href=\"https://www.tensorflow.org/install/source#gpu\">https://www.tensorflow.org/install/source#gpu</link></paragraph><paragraph><link href=\"https://anaconda.org/conda-forge/cudatoolkit/files?version=\">https://anaconda.org/conda-forge/cudatoolkit/files?version=</link></paragraph><paragraph><link href=\"https://anaconda.org/conda-forge/cudnn/files?version=\">https://anaconda.org/conda-forge/cudnn/files?version=</link></paragraph><paragraph>Install the cudatoolkit and cudnn packages first:</paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"false\"><snippet-file id=\"code\">conda install -c conda-forge cudatoolkit=11.2.2 cudnn=8.1.0</snippet-file></snippet><paragraph>Configure conda activate scripts to set your $LD_LIBRARY_PATH (which is where TF looks for your cuda/cudnn libraries) whenever this environment is activated: </paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"false\"><snippet-file id=\"code\">mkdir -p $CONDA_PREFIX/etc/conda/activate.d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh</snippet-file></snippet><paragraph>Now you need reload the conda environment for the above changes to go in effect. Easiest way is to close your terminal and reopening&amp;reactivating the environment.</paragraph><paragraph>Next, install the latest TF version that you've identified that can be installed via this method (2.11) as of today:</paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"false\"><snippet-file id=\"code\">pip install tensorflow==2.11</snippet-file></snippet><paragraph>This is it! You can verify by running:</paragraph><snippet language=\"py3\" runnable=\"false\" line-numbers=\"false\"><snippet-file id=\"code\">python -c \"import tensorflow as tf; print('GPUs Available: ',tf.config.list_physical_devices('GPU'))\"</snippet-file></snippet><paragraph>which should print a ton of TF warnings/messages but also:</paragraph><pre>GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n</pre><paragraph>which confirms the installation is successful!</paragraph><paragraph>Hope it's useful!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Nice, thanks for the detailed instructions.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 Colab. <document version=\"2.0\"><paragraph>Hi Everyone!</paragraph><paragraph>I'm having a problem running my code for part 3 on Colab. When I run it on my computer (Mac) it works fine, but when I try to run it on Colab, I get this error:</paragraph><paragraph>ValueError: cannot reshape array of size 33816560 into shape (1899519,91)</paragraph><paragraph>From this line:</paragraph><paragraph>outputs = np.load(sys.argv[2])</paragraph><paragraph>Does anyone know if there are extra steps you need to take to run on Colab?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Did you rerun the feature extractor on colab or upload the .npy file from your local machine? It could be an encoding issue.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "HW 3 submitting with tf.compat.v1.disable_eager_execution(). <document version=\"2.0\"><paragraph>If we used it for part 4, should we include tf.compat.v1.disable_eager_execution() in our submission? Or should that be left out in the submission and only used when we are testing our own code?</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>To my knowledge, it shouldn't matter that you have tf.compat.v1.disable_eager_execution() in your code because you only need to submit the .py files, .vocab files, and the .h5 model file. Unless this line is in one of these files (which it probably wouldn't be since you would be using an external file such as a .ipynb notebook file ffor testing), this shouldn't matter or cause any issues.</paragraph><paragraph/></document>"
        },
        {
            "source": "HW 3 submitting with tf.compat.v1.disable_eager_execution(). <document version=\"2.0\"><paragraph>If we used it for part 4, should we include tf.compat.v1.disable_eager_execution() in our submission? Or should that be left out in the submission and only used when we are testing our own code?</paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>You can leave that line in your code when you submit.\u00a0</paragraph></document>"
        }
    ],
    [
        {
            "source": "Part 4 : Infinite Loop. <document version=\"2.0\"><paragraph>Hello, has anyone else run into a problem where they get stuck in an infinite loop of shifting and right-arcing? I am not sure why this is, I'm thinking perhaps my checks are incorrect but I'm not sure where to start with debugging.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>(Not a TA) I have run into a similar issue. I recommend you check your right-arc implementation to see if the correct word is removed from the buffer.</paragraph></document>"
        },
        {
            "source": "Part 4 : Infinite Loop. <document version=\"2.0\"><paragraph>Hello, has anyone else run into a problem where they get stuck in an infinite loop of shifting and right-arcing? I am not sure why this is, I'm thinking perhaps my checks are incorrect but I'm not sure where to start with debugging.</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Isolate the sentence for which this happens, then run the decoder on this sentence only and trace the steps.\u00a0</paragraph></document>"
        },
        {
            "source": "Part 4 : Infinite Loop. <document version=\"2.0\"><paragraph>Hello, has anyone else run into a problem where they get stuck in an infinite loop of shifting and right-arcing? I am not sure why this is, I'm thinking perhaps my checks are incorrect but I'm not sure where to start with debugging.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I would also try to check that you properly mapped the actions to their probabilities before sorting.</paragraph></document>"
        }
    ],
    {
        "source": "Worse scores after fixing bug?. <document version=\"2.0\"><paragraph>Hi,<break/><break/>I believe i found an issue in my code for get_input_representation() method.<break/></paragraph><paragraph>My old program had the representation as </paragraph><paragraph>[ stack[-3], stack[-2], stack[-1] buffer[-3], buffer[-2], buffer[-1] ].</paragraph><paragraph>so I fixed it to be: </paragraph><paragraph> [ stack[-1], stack[-2], stack[-3], buffer[-1], buffer[-2], buffer[-3] ]</paragraph><paragraph>However, even with the incorrect representation, my old program was giving me loss values </paragraph><paragraph>of 0.5 (epoch 1) down to 0.28 (epoch 5). and attachment scores between 0.75-0.8</paragraph><paragraph><break/>My fixed program is giving me loss values of 0.55 (epoch 1) down to 0.38 (epoch 5) and attachment scores of around 0.67-0.74. <break/><break/>So it seems like my model got worse even though I fixed the input representation. Can someone explain how this is possible? <break/><break/></paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The specific representation shouldn\u2019t matter as long as it\u2019s consistent between training and deciding. There\u2019s some randomness in the training process which might explain this difference.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 Part 3 Loss Value of 0. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I know there have been several posts about loss values, but I haven't seen anyone else mention loss values of 0. Whenever I run my code, the loss value only shows as 0. All my code from part 2 is running as predicted, and I don't see any obvious errors in the build_model function. Does anyone have any idea what might be causing this behavior?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hey Anonymous,</paragraph><paragraph>If you are getting loss values of 0 it is most likely because your input feature vector is not in the correct format and doesn't match up at all with how you are training it. I would recommend looking specifically at the example the professor gives you in the spec with <code>\"dog eats a\" and \"the\" and &lt;ROOT&gt; are on the stack</code>. Pay attention to the order of how he mentions it and then also how it is put in root and whatnot. IE: it is stack for the first 3 and then buff for the last 3. Also, make sure it is numpy of length (6,)</paragraph><paragraph>Your Favorite Nguyen,</paragraph><paragraph>Nguyen Quoc Tran &lt;3</paragraph></document>"
    },
    {
        "source": "necessity of cuDNN, cuFFT, cuBLAS, and NUMA. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>My current build for tensorflow on WSL for windows 11 successfully runs on Nvidia GPU, but fails to support cuDNN, cuFFT, cuBLAS, and NUMA (see image below), despite numerous attempts to fix these issues. In this homework my runtime is acceptable (slightly over 10 mins on part 4 without eager execution) and the score is greater than 0.7, but I wonder if in future assignments these libraries would be necessary to ensure performance. Thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/bKe8pPeMwatDoHS6lRqciTix\" width=\"658\" height=\"141.74397797660012\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>No I don\u2019t think you will need those.\u00a0</paragraph></document>"
    },
    {
        "source": "Part4 tf.compat.v1.disable_eager_execution(). <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Once I add tf.compat.v1.disable_eager_execution(), the runtime for part 4 is reduced to less than a minute when using just the CPU. In contrast, the runtime is approximately 2 hours without tf.compat.v1.disable_eager_execution(). Does this speed improvement seem plausible? The accuracy appears fine, but I'm concerned about whether such a significant runtime reduction does not negatively affect the implementation.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes that seems reasonable. :)\u00a0<break/>Essentially with eager execution, rather than using the compiled computation graph, the model is evaluated layer by layer. This is good for debugging and implementing certain control flow mechanism, but it slows things down considerably.\u00a0</paragraph></document>"
    },
    {
        "source": "Cannot use Tensorflow on Apple M1 Ventura. <document version=\"2.0\"><paragraph>Hi, I'm using Anaconda's virtual environment, and on that environment, I installed Tensorflow. The installation of Tensorflow was successful, but I could not use it and got the same error \"illegal hardware instruction\" in post #343. I tried both Python 3.9 and 3.11, and neither works. I wonder how I can solve this issue.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/pf24ghfixutGF6HR6P5pw7X0\" width=\"658\" height=\"127.77441860465116\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Hmm that\u2019s different from the increasing loss issue other people have encountered. Which version of tensorflow is this?</paragraph><paragraph>You could always switch to colab.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 Loss. <document version=\"2.0\"><paragraph>Question to understand things better: I ran my model locally (M2 Max Ventura) and it gave me a loss that was increasing in value and way greater than 1. When I ran the same code on Google Colab, the model trained as expected and gave me a more reasonable loss that got less than 0.2956 after the 3rd epoch. I am curious why the training loss varies based on what device we train the model on even though the training data and model structure stays the same. Is it that tensorflow computes calculations differently across different device packages? </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>To be honest I don\u2019t quite understand what\u2019s going on there. It\u2019s definitely an issue in the M1/M2 version of tensorflow. I initially thought only the GPU (tensorflow-Metal) version was affected, but it seems to actually affect the CPU version as well.\u00a0</paragraph><paragraph>I think the most recent version of the Adam optimizer is at fault. There is a way to use a legacy version \u2014 maybe something to look up?\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 loss. <document version=\"2.0\"><paragraph>I was wondering if I should be concerned with my model loss (around 0.39 after epoch 5) as it's a bit higher than what other people have got. Despite this, I get a Labeled Attachment Score of around 0.7 and an Unlabeled Attachment Score of around 0.75. Should I still look at my model more closely? Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That seems okay.\u00a0</paragraph></document>"
    },
    {
        "source": "oh today tony yu. <document version=\"2.0\"><paragraph>hi wondering if the OH will be in the CS TA room or online? </paragraph><paragraph>Thanks! </paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>It will be in the CS TA room.</paragraph></document>"
    },
    {
        "source": "Part 4 Logic. <document version=\"2.0\"><paragraph>Hello,<break/><break/>I have two questions regarding part 4:<break/><break/>1) When looking for legal transitions, are we to check if state.stack is empty, or if the stack that is presented in the 1x6 input representation vector is empty (or are both of these stacks the same structure)?<break/><break/>2) How are we to update the current state after we find a legal transition? My current implementation  uses conditional statements to apply a transition on the state given a legal action, but my output is returning None.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>1) The 1x6 input representation vector should include the top three elements of the stack (or stack[-1], stack[-2], stack[-3]) in the first three indices. Thus, if the first three elements of the vector are all equal to the index for &lt;NULL&gt;, the state.stack would be empty, but I think it's more straightforward to just check if state.stack is empty.</paragraph><paragraph>2) The State class comes with functions that allow you to perform shift, left_arc, and right_arc, so calling those functions with the dependency label should update your state.</paragraph></document>"
    },
    {
        "source": "hw3 pt3 & pt4 output. <document version=\"2.0\"><paragraph>Hi, I get the following outputs in pt3 and pt4, not sure if they are okay or should I update my code to improve the result?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/bIH4K82VG9335I1N5MsV4LJF\" width=\"658\" height=\"173.02962962962962\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/bT3edKgGehgdGp787s1PfoXC\" width=\"658\" height=\"182.06796116504853\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>This looks great! Better than some other results I have seen.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "HW3 Part 4 Avg Runtime. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I was wondering what a good time/step would be for part 4 to make sure it won't take forever to run.</paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>(Not a TA)You could use the trick in #361. And it would take several minutes. </paragraph></document>"
        },
        {
            "source": "HW3 Part 4 Avg Runtime. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I was wondering what a good time/step would be for part 4 to make sure it won't take forever to run.</paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I'd say 5-10 minutes if you turn off eager-executions </paragraph></document>"
        }
    ],
    [
        {
            "source": "How layers and density are chosen (HW3 Part 3). <document version=\"2.0\"><paragraph>This is more of a conceptual question, but how were the densities and number of layers chosen here? For example, why 100 and then 10? And why only 2 hidden layers? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/CAxhjxxX33dPKuCQD32gnUIl\" width=\"658\" height=\"186.21252796420583\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>I am also curious about the conceptual basis of this model as well, can you please explain how the \"Dense\" layers work? </paragraph><paragraph>Based on the documentation, it appears that they apply a weight and an activation function to the previous layer's output, but I am not certain my understanding is correct.</paragraph></document>"
        },
        {
            "source": "How layers and density are chosen (HW3 Part 3). <document version=\"2.0\"><paragraph>This is more of a conceptual question, but how were the densities and number of layers chosen here? For example, why 100 and then 10? And why only 2 hidden layers? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/CAxhjxxX33dPKuCQD32gnUIl\" width=\"658\" height=\"186.21252796420583\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>My understanding is that the number of hidden layers is up to personal choice and what you think is best for the problem you are trying to solve; basically it is arbitrary, three hidden layers could have probably done the job or even one, but finding a good balance between accuracy and training time is the main goal, and whenever you add hidden layers the training time goes up, especially for larger datasets like the one we are working with. A \"Dense\" layer is basically a normal neural network layer, similar to what we have seen in lecture. The values for these layers (such as 100 or 10) correspond to the number of nodes/input size of the layer, which depends on the value of the output of the previous layer/the general input. For \"Dense\" layers, you tend to have a lot more freedom on these values, as they usually can be any value, but remember, the more nodes you have, the more the training time increases, albeit while also most likely increasing accuracy.</paragraph></document>"
        }
    ],
    {
        "source": "Part3. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/B8kYK9fPV0ZX14usydeN5DIB\" width=\"658\" height=\"233.2762836185819\"/></figure><paragraph>When I'm trying to train the model, I get the Value Error above for the data shapes. </paragraph><paragraph>In my part 2, I return the np arrays of shape (6,) and (91,) which I think is right. These get put into the npy files by the code provided. </paragraph><paragraph>Do you know why these shapes could be incorrect and what I can do to fix this?? </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I see this was answered with #368, disregard!</paragraph><paragraph><link href=\"https://edstem.org/us/courses/46417/discussion/3797602\">https://edstem.org/us/courses/46417/discussion/3797602</link> </paragraph></document>"
    },
    [
        {
            "source": "HW3 Part 3 Loss Values. <document version=\"2.0\"><paragraph>Hi everybody,</paragraph><paragraph>I know there have been a couple of posts about the loss values already but I think all of them have been about getting extremely large values or showing values as low as .2.</paragraph><paragraph>My loss values just steadily stay around .43 which is neither extremely high nor as low as many people are getting. Does anyone have any ideas on how to get it lower or are these values also ok? (For reference I'm running on CPU on my local Windows machine).</paragraph><paragraph>Thanks!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/oXMXzrmeMmZwf3X5TSqPGU8c\" width=\"658\" height=\"177.77676016027476\"/></figure><paragraph/><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>That seems okay to me. I would try to see what happens when you continue with this model in part 4.\u00a0</paragraph></document>"
        },
        {
            "source": "HW3 Part 3 Loss Values. <document version=\"2.0\"><paragraph>Hi everybody,</paragraph><paragraph>I know there have been a couple of posts about the loss values already but I think all of them have been about getting extremely large values or showing values as low as .2.</paragraph><paragraph>My loss values just steadily stay around .43 which is neither extremely high nor as low as many people are getting. Does anyone have any ideas on how to get it lower or are these values also ok? (For reference I'm running on CPU on my local Windows machine).</paragraph><paragraph>Thanks!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/oXMXzrmeMmZwf3X5TSqPGU8c\" width=\"658\" height=\"177.77676016027476\"/></figure><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I was also getting similar pattern and loss values when running locally on CPU. (I'm on a Macbook whose OS is quite old). I was able to get ~0.25 loss value by the end of 5 epoch when running the same code on Google Colab, so I was suspecting that there may be potential issues with the Keras or tensorflow installed on my local computer. </paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "Part 2 output. <document version=\"2.0\"><paragraph>Hello,<break/><break/>I just wanted to ensure that my part 2 code is working as expected.<break/><break/>After completing get_input_representation, my vector of length 6 is outputted as:<break/>[4. 4. 4. 2. 4. 4.]</paragraph><paragraph>Indicating that the stack is empty, and the top (only) item in the buffer is an unknown token.</paragraph><paragraph>My intuition makes me believe that the top item in the buffer should be \"root\", not an unknown token, can you please correct my intuition if it its wrong?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yeah. If there is only one token left, that one token should be root.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 part 0. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Every time I run the command for part 0, I get a different <code>words.vocab</code> file. I have not been able to get it to match the spec (the first few lines of mine look like this): </paragraph><pre>&lt;CD&gt;    0\n&lt;NNP&gt;    1\n&lt;UNK&gt;    2\n&lt;ROOT&gt;    3\n&lt;NULL&gt;    4\nsilly    5\nvaluing    6\nembezzling    7\n</pre><paragraph>Is this intended behavior?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hey Anonymous,</paragraph><paragraph>That is the intended behavior. If you look at the spec it actually directly says \"(Your indices may differ from this example because the get_vocab script outputs a different index mapping each time it is run.)\".</paragraph><paragraph>You can also look into how it is getting made and you will see that it will be a little diff each time. Because of the iteration. </paragraph><paragraph>Happy to help,</paragraph><paragraph>Nguyen Quoc Tran &lt;3</paragraph></document>"
    },
    {
        "source": "HW3 Zip File. <document version=\"2.0\"><paragraph>For the files we submit, should there/can there be a subfolder \"data\" containing the model and vocab files?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes that would be a good idea.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Low Score for HW3 Part 4. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0.28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/s6g5LpF4UyGJU3KTacs8LjXZ\" width=\"658\" height=\"110.70616113744076\"/></figure><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi! I had a similar issue and resolved it by recloning hw3 and running everything from scratch and then running the evaluate/decode commands. I'm not sure if I accidentally corrupted a data file or something or if this is the same issue as what I had, but hopefully this helps!</paragraph></document>"
        },
        {
            "source": "Low Score for HW3 Part 4. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0.28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/s6g5LpF4UyGJU3KTacs8LjXZ\" width=\"658\" height=\"110.70616113744076\"/></figure><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>If your loss values look normal and only the attachment scores look abnormally low, this might be an indication that something is off with your one-hot encoding. The mapping of indices to output actions in <code>self.output_labels</code> (inside of <code>decoder.py</code>) performs the one-hot encoding in a very particular way, so I would make sure that it's consistent with how you performed one-hot encoding in <code>get_output_representation()</code> of <code>extract_training_data.py</code>.</paragraph></document>"
        },
        {
            "source": "Low Score for HW3 Part 4. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0.28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/s6g5LpF4UyGJU3KTacs8LjXZ\" width=\"658\" height=\"110.70616113744076\"/></figure><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>It\u2019s also possible that there is an issue with the decoder. Even though your transitions are predicted correctly, the way you select the best transition from the predicted probabilities may not be (make sure you are sorting in reverse order).\u00a0</paragraph></document>"
        },
        {
            "source": "Low Score for HW3 Part 4. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0.28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/s6g5LpF4UyGJU3KTacs8LjXZ\" width=\"658\" height=\"110.70616113744076\"/></figure><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I also had that extremely low score. I ran my decoder evaluation function on colab and the numbers were fixed!</paragraph></document>"
        }
    ],
    [
        {
            "source": "Running HW 3 Part 3 on Colab. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I'm trying to test my part 3 but I can't figure out how to run the given command on Colab since input_train.npy, target_train.npy, and model.h5 don't exist in data. </paragraph><paragraph>!python '/content/drive/MyDrive/hw3_files/train_model.py' '/content/drive/MyDrive/hw3_files/data/input_train.npy' '/content/drive/MyDrive/hw3_files/data/target_train.npy' '/content/drive/MyDrive/hw3_files/data/model.h5' </paragraph><paragraph>is the code I'm using but I get this error and I'm not sure how to fix it. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Ww4t76tleDfHYhtrNf87mJ1v\" width=\"658\" height=\"84.21394460362941\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>input_train.npy and target_train.npy should exist from p2 -- I ran part 2 locally and then uploaded those files into my folder on Drive. It's okay that model.h5 doesn't exist yet in the Drive when running p3 since it will be created after training.</paragraph></document>"
        },
        {
            "source": "Running HW 3 Part 3 on Colab. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I'm trying to test my part 3 but I can't figure out how to run the given command on Colab since input_train.npy, target_train.npy, and model.h5 don't exist in data. </paragraph><paragraph>!python '/content/drive/MyDrive/hw3_files/train_model.py' '/content/drive/MyDrive/hw3_files/data/input_train.npy' '/content/drive/MyDrive/hw3_files/data/target_train.npy' '/content/drive/MyDrive/hw3_files/data/model.h5' </paragraph><paragraph>is the code I'm using but I get this error and I'm not sure how to fix it. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Ww4t76tleDfHYhtrNf87mJ1v\" width=\"658\" height=\"84.21394460362941\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>You can use </paragraph><pre>import os\nprint(os.getcwd())\n</pre><pre>!ls ./drive/MyDrive/...\n</pre><paragraph>to check the path.</paragraph></document>"
        }
    ],
    {
        "source": "Giant loss value on Colab. <document version=\"2.0\"><paragraph>Hello, I am using Colab w GPU so I assume it's not a hardware issues. </paragraph><paragraph>My model is just like what's laid out in the description, and I even changed it according to other posts on Edstem, so can it be because of part2 ? (I am not sure how to test this). I don't know why the loss value is 9 digits and gets worse after each iteration.</paragraph><paragraph>Any pointers on this would be so appreciated. Thanks all.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/kSlOPihc39V7WbUIiAB7IXa2\" width=\"642\" height=\"96.75960813865862\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/8XbVFZsC848SSqhxaL07ZRs7\" width=\"642\" height=\"200.25688073394494\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes I suspect there is something wrong with your training data, unfortunately. I would recommend trying it out on a single training sentence so that you can verify that the resulting input and output representations are correct. Just copy the sec0.conll file and delete everything except for one sentence.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 Part 2 \"None\". <document version=\"2.0\"><paragraph>How are we supposed to deal with \"None\" in the get_input_representation function? Should we assign it the POS &lt;NULL&gt; or is there another value that would make more sense here? Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>None represents the ROOT POS. If you see the code, you will observe that it is added to the front of each sentence to serve as the Root.</paragraph></document>"
    },
    {
        "source": "hw3 final result inconsistency. <document version=\"2.0\"><paragraph>This isn't about the inconsistency between mac and windows that other people have mentioned. </paragraph><paragraph>But, when i run the final evaluate script, roughly half the time there's some error in the execution (tensorflow or threading probably) and it either fails or doesn't terminate. When it does work, I get acceptable results, but I'm concerned that I might be graded on a failing execution. Is that okay as long as it eventually works?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/aoCzBpM3r9aGfX8YNu1Nwqdt\" width=\"658\" height=\"181.89903846153845\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>What\u2019s the error message you get? The results you posted look good.\u00a0</paragraph></document>"
    },
    {
        "source": "Order of Precedence. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Part 4 is timing out even when I disable eager execution. When attempting to improve my model, I was wondering if there should be an order of precedence between the different cases in get_input_representation. For instance, if a certain word is unknown and it has a special tag, which special case should it fall under? Thanks so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The special tag should take priority in that case.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 Part 3. <document version=\"2.0\"><paragraph>In the build_model function, what is the pos_types argument used for?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>It is not used at all. Refer to this Ed post - https://edstem.org/us/courses/46417/discussion/3712855</paragraph></document>"
    },
    {
        "source": "file to submit. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I just wanna check that we should submit </paragraph><paragraph>6 .py files</paragraph><paragraph>2 .vocab files</paragraph><paragraph>1 .h5 file</paragraph><paragraph>right?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 Part4 Different result. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/9sc5hXFHZecsYG1UsX2aGrwJ\" width=\"572\" height=\"195.82978723404256\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/xl6DD2KY2CvQoNOYCgj04TrK\" width=\"572\" height=\"163.16065573770493\"/></figure><paragraph>Hello Professor and TA.</paragraph><paragraph>I ran this part of my code and got two different results on Mac and Windows computers, I would like to ask if this result is acceptable? Thank you.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hm people have reported similar observations. I don\u2019t really have any idea why. Did you train to the same loss on both machines?\u00a0</paragraph><paragraph>Either way, the results look good. \u00a0</paragraph></document>"
    },
    {
        "source": "HW 3 - Part 4 - Scores. <document version=\"2.0\"><paragraph>I just wanted to confirm, do these scores look reasonable?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/oUz6NclQ0Kpz62H5mfkCmkLH\" width=\"658\" height=\"115.5945945945946\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>The labeled scores seem a little low. Take a look at the output of decoder.py (the actual dependency trees) to see if there is any systematic issue you see with the labels.\u00a0</paragraph></document>"
    },
    {
        "source": "Extremely low LAS. <document version=\"2.0\"><paragraph>Hi Professor and TAs,</paragraph><paragraph>The accuracy of my decoder seems to be extremely low (~0.03). I've checked my methods carefully but am struggling to figure out where I went wrong. Any hint/tip would be greatly appreciated! Thank you.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/xtXF6XPzqgMYeTlU6SWxNoAO\" width=\"658\" height=\"285.98853211009174\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/gWV1s9JJzVeg9UgoEcNchoXS\" width=\"643\" height=\"479.95058517555265\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/FCJdR2140kM2CQV25LEkftvW\" width=\"643\" height=\"196.8875968992248\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Check the actual output of the decoder (the dependency trees printed by decoder.py). You may be able to see some systematic issue.\u00a0</paragraph></document>"
    },
    {
        "source": "Part4 prediction method. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For part 4, when I make a prediction, can I use predict_on_batch instead of predict?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>You will have to make predictions for one input at a time, unfortunately.\u00a0</paragraph></document>"
    },
    {
        "source": "last day of class. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I want to double-check that whether the last day of this course is Mon 12/11 for exam 2?</paragraph><paragraph>Thanks.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That's correct. </paragraph></document>"
    },
    {
        "source": "HW 3 Part 4 Output. <document version=\"2.0\"><paragraph>Hello TAs and Professor,</paragraph><paragraph>Is the following output acceptable?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/BJYViaUQyM5Oi2eReutXNSzZ\" width=\"658\" height=\"173.22593320235757\"/></figure><paragraph>Thank you,</paragraph><paragraph>Shimon</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, that looks okay to me. </paragraph></document>"
    },
    [
        {
            "source": "Extremely Large Loss Values. <document version=\"2.0\"><paragraph/><paragraph>Hi,</paragraph><paragraph>My losses are EXTREMELY large. I think my build_model is fine because I compared it to a previous post (which you had told them that theirs looked fine). I\u2019ve pretty much boiled it down to how I am handling input/output representation. The vectors are both correctly formatted with the correct lengths respectively and everything compiles, but the loss results are clearly terrible. Wsa just wondering if my thought process of focusing on input/output was on the right track and if there are any insights/tips anyone has. Below is the summary of the losses (2020 Macbook Pro on Google Colab):</paragraph><paragraph>Epoch 1/5 18996/18996 [==============================] - 146s 8ms/step - loss: 86050283520.0000 </paragraph><paragraph>Epoch 2/5 18996/18996 [==============================] - 143s 8ms/step - loss: 1137886625792.0000 </paragraph><paragraph>Epoch 3/5 18996/18996 [==============================] - 151s 8ms/step - loss: 4919651205120.0000 </paragraph><paragraph>Epoch 4/5 18996/18996 [==============================] - 147s 8ms/step - loss: 14147200745472.0000 </paragraph><paragraph>Epoch 5/5 18996/18996 [==============================] - 146s 8ms/step - loss: 32414466310144.0000</paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hm, that almost looks to me like an issue with the model. The predicted vector will all have values between 0 and 1, and the target should be 1-hot. The reported loss is actually the average loss of the batches, not the sum of losses of the epoch. </paragraph></document>"
        },
        {
            "source": "Extremely Large Loss Values. <document version=\"2.0\"><paragraph/><paragraph>Hi,</paragraph><paragraph>My losses are EXTREMELY large. I think my build_model is fine because I compared it to a previous post (which you had told them that theirs looked fine). I\u2019ve pretty much boiled it down to how I am handling input/output representation. The vectors are both correctly formatted with the correct lengths respectively and everything compiles, but the loss results are clearly terrible. Wsa just wondering if my thought process of focusing on input/output was on the right track and if there are any insights/tips anyone has. Below is the summary of the losses (2020 Macbook Pro on Google Colab):</paragraph><paragraph>Epoch 1/5 18996/18996 [==============================] - 146s 8ms/step - loss: 86050283520.0000 </paragraph><paragraph>Epoch 2/5 18996/18996 [==============================] - 143s 8ms/step - loss: 1137886625792.0000 </paragraph><paragraph>Epoch 3/5 18996/18996 [==============================] - 151s 8ms/step - loss: 4919651205120.0000 </paragraph><paragraph>Epoch 4/5 18996/18996 [==============================] - 147s 8ms/step - loss: 14147200745472.0000 </paragraph><paragraph>Epoch 5/5 18996/18996 [==============================] - 146s 8ms/step - loss: 32414466310144.0000</paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I am facing the same issue, was anyone able to solve it?</paragraph></document>"
        }
    ],
    {
        "source": "Tensorflow Installation Issue. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/4vIs8UiILanMza3LsiSazMdG\" width=\"497\" height=\"149.1\"/></figure><paragraph>Hi! I get the following error when trying to run <code>$ python -m pip install tensorflow</code>. I have deduced that it is an issue with Windows 11. Is there an easier workaround for this besides importing the code into Colab when I want to train the model?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, this is due to window limiting its path to &lt;260 characters iirc. What you have to do is moving tensorflow folder to another directory with shorter path or enable long-path. </paragraph><paragraph>Step: </paragraph><paragraph>Open \"Run\" (The window app), you should be able to look it up w window search bar. Type \"%systemroot%\\syswow64\\regedit\". This should pops up sth called \"Registry Editor\", which resembles a big directory. You should follow this path into the subfolders: Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem. You can either paste the path or just manually click on them until you get to  \"FileSystem\".</paragraph><paragraph>At this point, you should see \"LongPathEnabled\", click on it and set value from 0-&gt;1. If this works for you, lmk so I can put it on my academic highlight mixtapes for gradschool</paragraph><paragraph/></document>"
    },
    {
        "source": "HW3 Part 4: difference in output with tf.compat.v1.disable_eager_execution(). <document version=\"2.0\"><paragraph>Hello, I was curious as to why I have a different output when running with eager execution disabled. When it is disabled, there is no constant stream of outputted text, as opposed to when it is enabled. Is this to be expected? </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>You mean the progress-bar information isn't showing up? </paragraph><paragraph/></document>"
    },
    {
        "source": "HW3 Part 4 prediction input. <document version=\"2.0\"><paragraph>I'm a little confused on the input being used in this part.  When it comes to parsing sentences and making predictions on the transitions, should we be extracting the input representation of a given state of the sentence using get_input_representation that we created in part 2?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That's right, you'll be using get_input_representation on the current state of the parser. </paragraph></document>"
    },
    {
        "source": "HW 3 Part 4 Running Time. <document version=\"2.0\"><paragraph>I'm using Colab for this assignment. For part 4, I already turned off \"eager execution\" for TensorFlow, but it has been running for 40 minutes and still has not finished. I wonder approximately how long it takes to run each of these two executions on Colab:</paragraph><paragraph>1. python decoder.py data/model.h5 data/dev.conll</paragraph><paragraph>2. python evaluate.py data/model.h5 data/dev.conll</paragraph><paragraph>Will the evaluation step take a shorter amount of time? If it's running too long, does it indicate there is an error? Thanks in advance!\"</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That seems a bit long. Make sure you are correctly updating the parser state after each prediction, otherwise you may end up in an infinite loop. You could try to print out the current state and see if it repeats. </paragraph></document>"
    },
    [
        {
            "source": "HW03 Part3 Loss Result. <document version=\"2.0\"><paragraph>Hello Professor and TA.</paragraph><paragraph>I am running the same code and getting different loss results on Windows and Mac computers. windows computer has a lower loss. what should I do in this case? Thank you for being so helpful and understand. </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/i5cHiE2cIPZ3SZsRPt2rdHnX\" width=\"658\" height=\"226.69351230425056\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/yqvcA8FKQhOxiH5nLGC1gkVa\" width=\"658\" height=\"208.65781710914456\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>It may based on the calculation ability about the chip? I guess.</paragraph></document>"
        },
        {
            "source": "HW03 Part3 Loss Result. <document version=\"2.0\"><paragraph>Hello Professor and TA.</paragraph><paragraph>I am running the same code and getting different loss results on Windows and Mac computers. windows computer has a lower loss. what should I do in this case? Thank you for being so helpful and understand. </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/i5cHiE2cIPZ3SZsRPt2rdHnX\" width=\"658\" height=\"226.69351230425056\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/yqvcA8FKQhOxiH5nLGC1gkVa\" width=\"658\" height=\"208.65781710914456\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>Assuming that the second screenshot is from mac, I have very similar results. Also very interested in knowing the answer. </paragraph></document>"
        },
        {
            "source": "HW03 Part3 Loss Result. <document version=\"2.0\"><paragraph>Hello Professor and TA.</paragraph><paragraph>I am running the same code and getting different loss results on Windows and Mac computers. windows computer has a lower loss. what should I do in this case? Thank you for being so helpful and understand. </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/i5cHiE2cIPZ3SZsRPt2rdHnX\" width=\"658\" height=\"226.69351230425056\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/yqvcA8FKQhOxiH5nLGC1gkVa\" width=\"658\" height=\"208.65781710914456\"/></figure></document>",
            "target": "<document version=\"1.0\"><paragraph>Hu, I assume that is running on the CPU in both cases (I.e. not GPU)\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "HW 3 Using Colab. <document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>To run the program on Colab, do we have to create a notebook and then upload all the files into it? </paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I don't know if I'm doing it the most efficient way, but I put all my files into a Google Drive folder and then am accessing them through my notebook.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/7mwoC0RKFeEvdYEADx6t7kNz\" width=\"699\" height=\"81.51603498542273\"/></figure></document>"
    },
    {
        "source": "Error: \"Cast string to float not supported\". <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>When running:</paragraph><pre>python train_model.py data/input_train.npy data/target_train.npy data/model.h5\n</pre><paragraph>I get the error:</paragraph><pre>Error: \"Cast string to float not supported\"\n</pre><paragraph>This is my code:</paragraph><pre>def build_model(word_types, pos_types, outputs):\n    # TODO: Write this function for part 3\n    model = Sequential()\n\n    # Embedding Layer\n    model.add(Embedding(input_dim=word_types, output_dim=32, input_length=6))\n\n    # Flatten Layer\n    model.add(Flatten())\n\n    # 2 Layers, 100, 10\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n\n    # Output Layer with softmax\n    model.add(Dense(91, activation=keras.activations.softmax))\n\n    model.compile(keras.optimizers.legacy.Adam(learning_rate=0.01), loss=\"categorical_crossentropy\")\n    model.summary()\n    return model\n</pre><paragraph>This is the full error:</paragraph><pre>Compiling model.\n/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super().__init__(name, **kwargs)\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 6, 32)             484896    \n                                                                 \n flatten (Flatten)           (None, 192)               0         \n                                                                 \n dense (Dense)               (None, 100)               19300     \n                                                                 \n dense_1 (Dense)             (None, 10)                1010      \n                                                                 \n dense_2 (Dense)             (None, 91)                1001      \n                                                                 \n=================================================================\nTotal params: 506207 (1.93 MB)\nTrainable params: 506207 (1.93 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nDone loading data.\nEpoch 1/5\n2023-11-05 12:36:07.078331: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\nTraceback (most recent call last):\n  File \"/Users/omernauer/Desktop/Columbia Studies/Fall 2023/NLP/hw3_files/train_model.py\", line 52, in &lt;module&gt;\n    model.fit(inputs, outputs, epochs=5, batch_size=100)\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\n\nDetected at node sequential/Cast defined at (most recent call last):\n  File \"/Users/omernauer/Desktop/Columbia Studies/Fall 2023/NLP/hw3_files/train_model.py\", line 52, in &lt;module&gt;\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1783, in fit\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1126, in train_step\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 589, in __call__\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 654, in _run_internal_graph\n\n  File \"/Users/omernauer/miniconda3/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 751, in _conform_to_reference_input\n\nCast string to float is not supported\n         [[{{node sequential/Cast}}]] [Op:__inference_train_function_760]\n\n</pre><paragraph>Any help is appreciated!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I suspect theirs is an issue in your input matrices. Are you correctly replacing the tokens from the input string with integer indices?\u00a0</paragraph></document>"
    },
    {
        "source": "Part2 stack & buffer representation. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>From what I understand, in part 2, the task is to represent the top 3 words from both the stack and the buffer in a single array. </paragraph><paragraph>['the', 'root', 'null', 'dogs', 'eats', 'a'].</paragraph><paragraph>What if the buffer has less than 3 items? For instance, if we only have two words, 'dog', and 'eats', should it be </paragraph><paragraph>['the', 'root', 'null', 'dog', 'eats', 'null'] or </paragraph><paragraph>['the', 'root', 'null', 'null', 'dog', 'eats']?</paragraph><paragraph/><paragraph>Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I believe the array should essentially be <code>[stack[-1], stack[-2], stack[-3], buffer[-1], buffer[-2], buffer[-3]]</code>.</paragraph><paragraph>But you need to make sure those indices exist. If they don't, you should replace any non-existent ones with <italic>&lt;NULL&gt;</italic>'s representation (i.e., <code>4</code> from the <italic>words.vocab</italic> output in Part 1). You also have to check for the special symbols (e.g., <italic>&lt;CD&gt;</italic>) by looking at the part of speech tag.</paragraph><paragraph>Finally, you want to map the string words to their index value from the word-to-index dictionary in the attribute <code>word_vocab</code>. The actual <code>state.stack</code> and <code>state.buffer</code> values are just a word's index in the sentence (e.g., <code>3</code> to mean the third word in the sentence).</paragraph></document>"
    },
    {
        "source": "HW4 Output. <document version=\"2.0\"><paragraph>Hi ,</paragraph><paragraph>Does my output needs to be under 70? Or is this fine.  </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/p61R8SZ5hwNI38cILer8qPZo\" width=\"657.9999999999999\" height=\"154.5480427046263\"/></figure><paragraph>Thank you!</paragraph><paragraph>Best,</paragraph><paragraph>Adrian</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>These look great!\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Large Loss Values in Part 3. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Like a few others have mentioned, I'm running into large loss values when attempting to train the model in Part 3. During the first epoch, the loss value appears to continue reducing until around 1.95, at which point it begins to steadily increase. What could be causing this behavior? Thanks so much!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi, I had a similar issue training my model locally on my M1 Macbook Air where the number went down to 0.6 before it exploded into a really large value (~200). I trained my model on Google colab and it seems to fix my problem though. My loss values are now more reasonable, it was around 0.6 at the end of epoch 1 and I ended with 0.29 at the end of epoch 5. Hope this helps!</paragraph></document>"
        },
        {
            "source": "Large Loss Values in Part 3. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Like a few others have mentioned, I'm running into large loss values when attempting to train the model in Part 3. During the first epoch, the loss value appears to continue reducing until around 1.95, at which point it begins to steadily increase. What could be causing this behavior? Thanks so much!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>If you are not trying to use GPU acceleration on an M1/M2 unit, maybe there is something wrong with your training data?</paragraph></document>"
        }
    ],
    [
        {
            "source": "Shapes are incompatible from model.fit. <document version=\"2.0\"><paragraph>Hi! I'm getting this error from the \"model.<bold>fit</bold>(inputs, outputs, epochs=5, batch_size=100)\" line in train_model.py and am not sure where it may be coming from (probably from my 6 inputs and 91 outputs dimensions), but am not sure how to resolve it-- I would appreciate any insight on this, thank you!!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/NLCfpUu8CoovWCP0VSlwcHzF\" width=\"658\" height=\"179.2282471626734\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>You can also try feature_vector = feature_vector.reshape(1, -1). I threw this into my code and it seems to have fixed something for now.</paragraph></document>"
        },
        {
            "source": "Shapes are incompatible from model.fit. <document version=\"2.0\"><paragraph>Hi! I'm getting this error from the \"model.<bold>fit</bold>(inputs, outputs, epochs=5, batch_size=100)\" line in train_model.py and am not sure where it may be coming from (probably from my 6 inputs and 91 outputs dimensions), but am not sure how to resolve it-- I would appreciate any insight on this, thank you!!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/NLCfpUu8CoovWCP0VSlwcHzF\" width=\"658\" height=\"179.2282471626734\"/></figure></document>",
            "target": "<document version=\"2.0\"><paragraph>Yes, the first dimension required is generally the batch size, so instead of feeding an input of dimensionality (,6) you should feed an input of (1,6). You can get this using .reshape(1,-1). </paragraph><paragraph>Similarly, the output will be an array of size (1,91). </paragraph></document>"
        }
    ],
    [
        {
            "source": "Question about hw3 part4. <document version=\"2.0\"><paragraph>Dear TAs,</paragraph><paragraph>When I was working on hw3 part4, I met this problem when running evaluation program: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/I296l283KYbakmbDLhYaA8gk\" width=\"574\" height=\"77.88247863247864\"/></figure><paragraph>I'm wondering what will cause this error? Thank you!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I just modified my codes and I noticed that the scores seem to be too low:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/2j3w7pgIUbCK59vxrisuXmzb\" width=\"564\" height=\"144.91666666666666\"/></figure><paragraph>I'm wondering where should I modify my codes? Thanks!</paragraph><paragraph/></document>"
        },
        {
            "source": "Question about hw3 part4. <document version=\"2.0\"><paragraph>Dear TAs,</paragraph><paragraph>When I was working on hw3 part4, I met this problem when running evaluation program: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/I296l283KYbakmbDLhYaA8gk\" width=\"574\" height=\"77.88247863247864\"/></figure><paragraph>I'm wondering what will cause this error? Thank you!</paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi dear TAs and professor:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/P2OWTjTLG2InVXCtWUebC586\" width=\"564\" height=\"147.07177033492823\"/></figure><paragraph>After modification, I finally got this output, does it reach the assignment's requirement? Thank you!</paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "Issues installing Tensorflow for GPU on Windows. <document version=\"2.0\"><paragraph>When I tried installing the tensorflow 1 GPU version on my windows 11 laptop that has a nvidia GPU, I get prompted with the following error</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/BaaTr7ocK5VR7AjTBLgt4fih\" width=\"658\" height=\"720.7995758218451\"/></figure><paragraph>And because the latest version of tensorflow requires linux environment on windows, I installed WSL and minicando3, then installing tensorflow ontop of that, but it's not recognizing my GPU, or skipping the process of checking all together </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/yWvff3CYjRwBDNX8DzJR115J\" width=\"658\" height=\"151.26436781609195\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm, sorry I don't really have a good answer to this as a non-windows user. If you can't figure it out, you could try Google Colab. </paragraph><paragraph>It's actually possible to run training on the CPU as well, so if it runs without GPU support that's okay for now. For homework 4 you will likely use GCP anyway. </paragraph><paragraph/></document>"
    },
    {
        "source": "HW3 Part 4. <document version=\"2.0\"><paragraph>Hello! <break/><break/>I've been working on part 4 for a while now and I can't seem to solve the following error<break/><underline>This is my model</underline>:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/v30gvpauBeeE9lGxfIEOg8yy\" width=\"495\" height=\"170\"/></figure><paragraph><underline>And when I try to call model.predict:</underline></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/VheYJNzTT7MXSYp7MXk9ybGD\" width=\"527\" height=\"184\"/></figure><paragraph><underline>I get the following error:</underline></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/4fMJ8qlaV54ONA4dg8hyG8GL\" width=\"643\" height=\"93.14365746298519\"/></figure><paragraph><break/>I double-checked the output of currentState, and it is indeed an np array of integers of length 6. I don't understand what is going on with the dimensions (192) and (32). Could you please help me understand what's going on? Thanks in advance for your help</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi the predict method expects the input to be of shape (1,6), that is a 2D matrix (the first dim is the batch size), instead of (6,). Try to reshape the result if get_input_representation using .reshape(1,-1).\u00a0</paragraph></document>"
    },
    {
        "source": "May:  Zoom. <document version=\"2.0\"><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/xbAN85bj54NRFrOxQN2fQ1V4\" width=\"658\" height=\"243.05490848585688\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>Solved in OH!</paragraph></document>"
    },
    {
        "source": "Runtime of decoder.py and evaluate.py. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and the TAs, </paragraph><paragraph>Is it normal to have decoder.py to run for more than 30min? I can't remember exactly how long but somewhere in 30-40min range. I was using the accelerator on M2 chip. </paragraph><paragraph>I also tried on Colab with T4 GPU. The step time seems to be longer on Colab (~20ms/step) than locally on M2 chip (~10ms/step). </paragraph><paragraph>Best,</paragraph><paragraph>Ansen Gong</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I recommend turning off eager execution. \u00a0<break/>tf.compat.v1.disable_eager_execution()</paragraph></document>"
    },
    {
        "source": "HW3 Part 2 - Output. <document version=\"2.0\"><paragraph>Hello, <break/><break/>I would like to confirm whether I understood the task for <code>get_output_representation</code>. It's asking us to make a one-hot representation of the transition. Does that mean that we should make a function that maps each of the 91 possible transitions to a unique index in the output array, leaving the rest as 0?<break/><break/>If this is right, I am guessing there are multiple ways of doing this, and it won't matter as long as we're consistent.<break/><break/>Thanks for your time and the clarifications<break/><break/><underline>EDIT</underline>: <break/><break/>Now I'm realizing that function <code>make_output_labels</code> already creates a dictionary of all possible transitions. Maybe we can take each value in the dictionary (which represents the index and goes from 0 to 90)?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s correct. You can use the output labels dictionary to find the numeric representation for the transition. Then turn it into a one-hot vector.\u00a0</paragraph></document>"
    },
    {
        "source": "hw3 part1. <document version=\"2.0\"><paragraph>Hi, sorry to ask a silly question, I can't type </paragraph><paragraph>\"\\$ python get_vocab.py data/train.conll data/words.vocab data/pos.vocab\"</paragraph><paragraph>in the terminal in my VScode, </paragraph><paragraph>I can only type </paragraph><paragraph>\"python get_vocab.py data/train.conll data/words.vocab data/pos.vocab\".</paragraph><paragraph>All command contain <bold>\"$</bold>\" is unable to run on my terminal.</paragraph><paragraph>Btw, I find every time I run </paragraph><paragraph>\"python get_vocab.py data/train.conll data/words.vocab data/pos.vocab\",</paragraph><paragraph>I get a different <bold>words.vocab</bold>, is it normal?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The $ is a common prompt character, not part of the actual command. :)</paragraph><paragraph>Getting different words.vocab files is expected. The types are collected in a set, which is unordered so when they are written to the file, the order is arbitrary. </paragraph><paragraph/></document>"
    },
    {
        "source": "Regarding trying to reach SOTA. <document version=\"2.0\"><paragraph>The current state of the art for dependency parsing is ~97 ( <link href=\"http://nlpprogress.com/english/dependency_parsing.html\">http://nlpprogress.com/english/dependency_parsing.html</link>). Feel free to experiment with additional features to improve the parser. <break/><break/>Dear Prof. Bauer and TAs,<break/><break/>Does this experimentation part hold any marks? I would like to do it but probably later sometime due to the number of assignments lined up this week\ud83d\ude05.<break/><break/>Thank you,<break/>Shreyas.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I feel like if you could build state of the art in a week or two you'd have a future in NLP research</paragraph></document>"
    },
    [
        {
            "source": "Adam not working on Colab. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>As I am trying to migrate the code to Colab, I have encountered this issue on the optimizer Adam below: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/QdxzZGwKNUBdbG4ksa7Cjvdg\" width=\"658\" height=\"302.6193548387097\"/></figure><paragraph>My Colab uses Tensorflow version 2.14.0. Have there been similar issues? And if so, what were the solutions?</paragraph><paragraph>Best,</paragraph><paragraph>Ansen</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hm -- probably a version incompatibility. Try using tf.keras.optimizers.legacy.Adam.</paragraph></document>"
        },
        {
            "source": "Adam not working on Colab. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>As I am trying to migrate the code to Colab, I have encountered this issue on the optimizer Adam below: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/QdxzZGwKNUBdbG4ksa7Cjvdg\" width=\"658\" height=\"302.6193548387097\"/></figure><paragraph>My Colab uses Tensorflow version 2.14.0. Have there been similar issues? And if so, what were the solutions?</paragraph><paragraph>Best,</paragraph><paragraph>Ansen</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I met the same issue when I disabled eager execution before fitting. I guess that after disabling it the backpropagation is also blocked.</paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "HW03 Part2. <document version=\"2.0\"><paragraph>Hello Professor and TA,</paragraph><paragraph>I'm curious about the output result of part 2. Below is what I've obtained. Is it the expected result? Thank you for your help and response.</paragraph><paragraph/><paragraph>[None, 'BUSH', 'AND', 'GORBACHEV', 'WILL', 'HOLD', 'two', 'days', 'of', 'informal', 'talks', 'next', 'month', '.']<break/>[None, 'NNP', 'CC', 'NNP', 'MD', 'VB', 'CD', 'NNS', 'IN', 'JJ', 'NNS', 'JJ', 'NN', '.']<break/>[None, '.', '.', 'GORBACHEV', 'AND', 'BUSH']<break/>[3, 4, 4, 4, 5106, 4]</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The first line are the words, second line are the POS -- what is the third line? </paragraph><paragraph>And the last line is the actual input representation? For which state?</paragraph></document>"
    },
    [
        {
            "source": "Printing Twice. <document version=\"2.0\"><paragraph>Hi, I hope you are doing well.</paragraph><paragraph/><paragraph>My output looks like:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/UabeT5ZjzagbmgTZFadYPZqr\" width=\"658\" height=\"203.26336633663368\"/></figure><paragraph/><paragraph>I see that the scores are very bad, but i also see that it is printing twice.  Is that supposed to happen?</paragraph><paragraph>Thank you so much</paragraph><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>The scores are different though. First one is Micro, the other is macro LAS. They are calculated differently in the code.</paragraph></document>"
        },
        {
            "source": "Printing Twice. <document version=\"2.0\"><paragraph>Hi, I hope you are doing well.</paragraph><paragraph/><paragraph>My output looks like:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/UabeT5ZjzagbmgTZFadYPZqr\" width=\"658\" height=\"203.26336633663368\"/></figure><paragraph/><paragraph>I see that the scores are very bad, but i also see that it is printing twice.  Is that supposed to happen?</paragraph><paragraph>Thank you so much</paragraph><paragraph/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I think it is supposed to happen because I am also getting two separate sets of scores, one for the micro averages and other for the macro averages of the labeled and unlabeled attachments. </paragraph><paragraph>Also, Micro LAS and Macro LAS are calculated and printed separately in the evaluate.py file.</paragraph></document>"
        }
    ],
    {
        "source": "HW3 Part 2. <document version=\"2.0\"><paragraph>Hello, <break/><break/>For the get_input_representation method, if our stack/buffer is shorter than 3 words, should we replace the corresponding elements in the array we'll return with the index of \"&lt;NULL&gt;\" in the word vocab?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, that is what I did, and it worked for me.</paragraph></document>"
    },
    {
        "source": "words in extract_tarining data. <document version=\"2.0\"><paragraph>Hi i hope you are great</paragraph><paragraph/><paragraph>I am having issues with my code so I printed out words in extract training data.</paragraph><paragraph/><paragraph>This is what printed:</paragraph><paragraph/><paragraph>[None, '``', 'Feeding', 'Frenzy', \"''\", '-LRB-', 'Henry', 'Holt', ',', '326', 'pages', ',', '$', '19.95', '-RRB-', ',', 'a',</paragraph><paragraph>'highly', 'detailed', 'account', 'of', 'the', 'Wedtech', 'scandal', ',', 'begins', 'on', 'a', 'reassuring', 'note', '.']</paragraph><paragraph/><paragraph>I have a few questions about this parameter,</paragraph><paragraph/><paragraph>is None the root?</paragraph><paragraph>what are -LRB- and  -RBR-</paragraph><paragraph>thank you so much!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes, None will always appear in position 0 and represents the root symbol.\u00a0<break/>-LRB- stands for the symbol ( \u2014 left round bracket.\u00a0<break/></paragraph></document>"
    },
    {
        "source": "Error in Obtaining Vocabulary. <document version=\"2.0\"><paragraph>Hi after running </paragraph><pre>$python get_vocab.py data/train.conll data/words.vocab data/pos.vocab\n\n\n</pre><paragraph>i do not get the screenshot on Courseworks, instead i get the following: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/r1YwRWyOhNubFVKT02TJUSmW\" width=\"185\" height=\"235\"/></figure><paragraph>Is there something I did wrong with uploading the information? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The order of tokens in the vocabulary may come out differently since we are keeping track of words in a set, which is unordered.\u00a0</paragraph></document>"
    },
    {
        "source": "Mac M1 vs Google Colab: Training Model Loss Function. <document version=\"2.0\"><paragraph>Hello, <break/><break/>I had different results when I ran part 3 locally vs Google Colab. </paragraph><paragraph>I am using MacBook Pro (M1 chip) , macOS : Sonoma and I got a very high loss value which is &gt;700 and it kept increasing on every epoch. </paragraph><paragraph>However, when I ran my part 3 at Google Colab not only its significantly faster in time but my loss value is also significantly lower, which is &lt;0.5 and decreasing on every epoch. <break/><break/>I am curious on why this happened, and since I get a very drastic output when running on both platforms which one is more reliable? <break/><break/>Thank you </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Same questions^</paragraph></document>"
    },
    {
        "source": "Error: zsh: illegal hardware instruction. <document version=\"2.0\"><paragraph>Hi, when trying to run:</paragraph><pre>python extract_training_data.py data/dev.conll data/input_dev.npy data/target_dev.npy\n</pre><paragraph>I get the error: </paragraph><pre>zsh: illegal hardware instruction  python extract_training_data.py data/dev.conll data/input_dev.npy \n</pre><paragraph>Is anyone else getting this error or knows how to solve this?</paragraph><paragraph>Thanks</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>What architecture are you running this on, and are you using the CPU or GPU version of tensorflow? Is this really an error or just a warning?</paragraph></document>"
    },
    {
        "source": "Hw3 Part 4 understanding predictions. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>I am having trouble understanding the output of model.predict. When I print predict out, the result is a 2D array consisting of numbers. How can I tell 1) which transition the model predict 2) the probability of the prediction? Appreciate any guidance/ pointer.</paragraph><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The output is a (batch_size, vocab_size) array. Since you are just passing a single input you can just take the first entry from the array in the batch dimension (index 0).<break/>Then you end up with a 1D array of length vocab_size. </paragraph><paragraph>The probability values represent the probability of each word. \u00a0For each dimension you get a probability value. You need to sort the dimensions according to the probability, which should result in an array of integer dimensions sorted in decreasing order of the probability. This can be done using np.argsort</paragraph></document>"
    },
    {
        "source": "HW 3 PART 2. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm still a little confused here, just wanna check I correctly understanding this.</paragraph><paragraph>For part2:</paragraph><paragraph>the word and pos contains every possible word and corresponding tags from the input file </paragraph><paragraph>and state can be any middle transition state for one sentence?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>In get_input_representation(self, words, pos, state), words and pos refer to the actual input sentence and corresponding pos sequence. State is the current (intermediate) state in the transition sequence. </paragraph><paragraph>state.stack and state.buffer contain token positions (that is, you can treat them as indices in the words and pos list). </paragraph></document>"
    },
    {
        "source": "Hw3 Pt 4 Accuracy. <document version=\"2.0\"><paragraph>For labeled attachment score, I'm getting around 0.68 and 0.75 for unlabeled attachment score. Are these scores sufficient for the assignment? If they're not sufficient, what's the best way to go about testing the code to find places to improve?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That sounds pretty good to me!</paragraph></document>"
    },
    [
        {
            "source": "Very low accuracy. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I just ran evaluate.py on my decoder, but my accuracy is super low, as copied below:</paragraph><paragraph>Micro Avg. Labeled Attachment Score: 0.04646650580831323</paragraph><paragraph>Micro Avg. Unlabeled Attachment Score: 0.20907440113430015</paragraph><paragraph>Macro Avg. Labeled Attachment Score: 0.052479354272750564</paragraph><paragraph>Macro Avg. Unlabeled Attachment Score: 0.21695312916782164</paragraph><paragraph>I think there's something really wrong in my code for it to be this low, so does anyone have any suggestions on where to even start debugging or has anyone come across this? Totally understand this is hard to help with without seeing my code, but I'd appreciate any pointers. Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I would look at the output produced by decoder to see if there are any obvious systematic mistakes (such as disconnected trees, or everything depending directly on 0-root). With low scores like these, I think something systematic is likely. At the very least, you should be able to identify if the issue is the model or the decoder. </paragraph><paragraph>Then you can try to find possible causes in your code. </paragraph></document>"
        },
        {
            "source": "Very low accuracy. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I just ran evaluate.py on my decoder, but my accuracy is super low, as copied below:</paragraph><paragraph>Micro Avg. Labeled Attachment Score: 0.04646650580831323</paragraph><paragraph>Micro Avg. Unlabeled Attachment Score: 0.20907440113430015</paragraph><paragraph>Macro Avg. Labeled Attachment Score: 0.052479354272750564</paragraph><paragraph>Macro Avg. Unlabeled Attachment Score: 0.21695312916782164</paragraph><paragraph>I think there's something really wrong in my code for it to be this low, so does anyone have any suggestions on where to even start debugging or has anyone come across this? Totally understand this is hard to help with without seeing my code, but I'd appreciate any pointers. Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I had the same issue and to fix it I printed out the state.stack and the state.buffer and every action I was checking the validity of in the while loop. I found a lot of implementation issues when I did this where certain arcs or shifts weren't done properly and once I fixed these, the accuracy increased.</paragraph><paragraph/></document>"
        }
    ],
    {
        "source": "Possible Error in HW3 Q2. <document version=\"2.0\"><paragraph>In the instructions it says: </paragraph><paragraph>Take a look at the file extract_training_data.py <break/><bold>States:</bold> The input will be an instance of the class State, which represents a parser state. The attributes of this class consist of a stack, buffer, and partially built dependency structure deps. --<bold>stack and buffer are lists of word ids (integers).--\"</bold></paragraph><paragraph>Based on this it means that the values on the stack and buffer are the word positions in the sentence, and I don't see if there is a way to reverse engineer it to find the word itself. I have been at a few OHs and no one can figure out and TAs think it could be a wording mistake because it would make more sense that the values on the stack and buffer are the words themselves. </paragraph><paragraph>Can you please clarify if there is a mistake or not?</paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi Omer, </paragraph><paragraph>Responding here -- I also saw your email. </paragraph><paragraph>The description is correct. Let's say the input sentence is 0-root 1-the 2-cat 3-sleeps. The initial configuration would look like this </paragraph><paragraph>stack: [0]  buffer: [3,2,1]  </paragraph><paragraph>To find the specific tokens you need to do two lookup steps. First, find the word at the specific index, then lookup the integer representation of that word in the vocabulary dictionary.  For example, for token 1, you would use <italic>word_vocab[words[1]]</italic> to get the integer representation of \"the\". </paragraph><paragraph>Does that help?</paragraph><paragraph>Daniel </paragraph><paragraph/></document>"
    },
    {
        "source": "HW3 Output labeling. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Can I label my outputs in a different one-hot format?</paragraph><paragraph>Thank you </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I initially tried to use my own scheme, but I ran into an issue because of this during the decoder and evaluation stage since I utilize self.output_labels from the Parser class which is defined as dict([(index, action) for (action, index) in extractor.output_labels.items()]) and imposes a certain structure on the output. It looks like this: \"{0: ('shift', None), 1: ('left_arc', 'tmod'), 2: ('right_arc', 'tmod'), 3: ('left_arc', 'vmod'), 4: ('right_arc', 'vmod')...}\". My code ended up working after following the same scheme as shown here. It may be possible to do it with your own output ordering scheme, but alternating left and right arcs like done in self.output_labels worked best in my case. Hope this helps!</paragraph><paragraph/><paragraph/></document>"
    },
    {
        "source": "Q2 Finding root. <document version=\"2.0\"><paragraph>How do I find root?</paragraph><paragraph><italic>\"So for example, if the next words on the buffer are \"dog eats a\" and \"the\" and &lt;ROOT&gt; are on the stack , the return value should be a numpy array numpy.array([4047, 3, 4, 8346, 8995, 14774]). Here 4 is the index for the &lt;NULL&gt; symbol, 3 is the index for the &lt;ROOT&gt; symbol, 4047 is the index for \"the\" and 8346, 8995, 14774 are the indices for \"dog\", \"eats\" and \"a\".\"</italic></paragraph><paragraph>Based on this quote, if root is sitting on the stack, and I call state.stack[root index] would it return the string \"&lt;ROOT&gt;\"?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Both the stack and the buffer contain word ids.<break/>You'd want to extract the word (string) using its id. Then, think about how to incorporate word_vocab in order to switch between the word and the mapped index!</paragraph></document>"
    },
    {
        "source": "Office Hours During Fall Break. <document version=\"2.0\"><paragraph>Hello!<break/></paragraph><paragraph>I just wanted to confirm if and which office hours will be available during fall break since the homework is due the day after? Thank you!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, I will hold office hours on Monday and Tuesday on Zoom. I may have to schedule a different time slot than usual on Monday -- I will send an announcement through Courseworks. </paragraph><paragraph>I don't think the TAs will hold office hours Mon/Tue.</paragraph></document>"
    },
    {
        "source": "Part 4 Warning. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.  When I run python evaluate.py data/model.h5 data/dev.conll, I get the warning below:</paragraph><paragraph/><paragraph>(base) venkatramamoorthi@dyn-160-39-55-55 hw3_files % python evaluate.py data/model.h5 data/dev.conll 2023-10-31 09:18:52.036024: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use a</paragraph><paragraph>vailable CPU instructions in performance-critical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropri ate compiler flags. 2023-10-31 09:18:57.104034: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled 2023-10-31 09:18:57.227919: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2/bias/Assign' id:87 op device:{requested: ' ', assigned: ''} def:{{{node dense_2/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/bias, dense_2/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. Evaluating. (Each . represents 100 test dependency trees) /Users/venkatramamoorthi/opt/anaconda3/lib/python3.8/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.</paragraph><paragraph>state_updates<code>will be removed in a future version. This property should not be used in TensorFlow 2.0, as</code>updates` are appl ied automatically. updates=self.state_updates, 2023-10-31 09:18:57.453587: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2/Softmax' id:93 op device:{requested: '', a ssigned: ''} def:{{{node dense_2/Softmax}} = Softmax<link href=\"dense_2/BiasAdd\">T=DT_FLOAT, _has_manual_control_dependencies=true</link>}}' w as changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error i n the future. Either don't modify nodes after running them or create a new session.</paragraph><paragraph>I let this run for 4  hours and it stayed like this.  I worked with a TA and we moved:</paragraph><paragraph><code>import tensorflow as tf</code><break/><code>tf.compat.v1.disable_eager_execution()</code></paragraph><paragraph/><paragraph>between files.  If I dont have the lines above, it just keeps outputting lines like:<break/><break/>1/1 [==============================] - 0s 15ms/step<break/><break/>Is there anything else I can try?</paragraph><paragraph>Thank you so much!</paragraph><paragraph/><paragraph>Based on another question in the ed, i think something larger is wrong with my code. it seems someone had this error but their code compiled.  I am unsure of how to troubleshoot this.  thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm, so the warnings won't affect your code, as far as I can tell. </paragraph><paragraph>The output is expected. 15 ms/step seems still very slow, but should allow you to evaluate the model. After the progress bars, does it fail to compute the actual scores? </paragraph></document>"
    },
    {
        "source": "M1 Chip Computer Warnings during HW3 Part 4. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm running the decoder and evaluator on an M1-chip computer, and I'm encountering numerous warnings in part 4. While the decoder and evaluator functions execute correctly and produce results, I'm noticing that the accuracy is significantly low. Due to the warnings, I'm uncertain about whether the issues mentioned in the warnings may be having some impact on my performance that may be attributed to my M1 and some error in my initial setup.</paragraph><paragraph>Here are some examples of the warnings I get: \"</paragraph><paragraph>2023-10-30 23:08:58.971460: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled 2023-10-30 23:08:59.002409: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2/kernel/Assign' id:82 op device:{requested: '', assigned: ''} def:{{{node dense_2/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/kernel, dense_2/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. 2023-10-30 23:08:59.094512: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_1/bias/v/Assign' id:265 op device:{requested: '', assigned: ''} def:{{{node dense_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/bias/v, dense_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. Evaluating. (Each . represents 100 test dependency trees) /Users/anaconda3/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: <code>Model.state_updates</code> will be removed in a future version. This property should not be used in TensorFlow 2.0, as <code>updates</code> are applied automatically. updates=self.state_updates, 2023-10-30 23:08:59.187791: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_2/Softmax' id:93 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = Softmax<link href=\"dense_2/BiasAdd\">T=DT_FLOAT, _has_manual_control_dependencies=true</link>}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\"</paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Are you just running the CPU version on an M1, or are you attempting to use the tensorflow-metal GPU acceleration? There are known issues with tensorflow-metal, so I would recommend to just use the CPU version for now. </paragraph><paragraph>The warnings should not affect performance in that case. </paragraph></document>"
    },
    {
        "source": "Cumulative Exam 2. <document version=\"2.0\"><paragraph>Hi! Thank you so much for a great course so far! I was wondering if the second exam includes what was on the first exam or if the scope is just what was covered after? Thank you!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I think it will be cumulative, based on this comment - https://edstem.org/us/courses/46417/discussion/3807069?comment=8777621</paragraph></document>"
    },
    {
        "source": "followup to best transition. <document version=\"2.0\"><paragraph>Hi I hope you are great.</paragraph><paragraph/><paragraph>I asked this question on ED:</paragraph><paragraph>Once we figure out the best transition in Part 4, how should we store it?</paragraph><paragraph>I'm unsure of how finding the best transition connects with the given part:</paragraph><pre>        result = DependencyStructure()\n        for p,c,r in state.deps:\n            result.add_deprel(DependencyEdge(c,words[c],pos[c],p, r))\n        return result</pre><paragraph/><paragraph/><paragraph/><paragraph>The answer was:</paragraph><paragraph/><paragraph>The best transition is found and used to update the parser's state within the <code><bold>while</bold></code> loop of the <code><bold>parse_sentence</bold></code> method. There's no need to separately store the best transition because it is immediately applied to the parser's state. So the result here is already the best transition</paragraph><paragraph/><paragraph>I have my while loop with my logic to see if its a legal transition.  I'm not sure how to update the parser's state once i get the best transition</paragraph><paragraph/><paragraph>I think i should call the legal transition' function from the state class, prior to </paragraph><paragraph/><paragraph> </paragraph><pre>        result = DependencyStructure()\n        for p,c,r in state.deps:\n            result.add_deprel(DependencyEdge(c,words[c],pos[c],p, r))\n        return result</pre><paragraph>thank you again!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph> Hm, I'm not sure I understand your follow-up question. </paragraph><paragraph>Once you have the best transition, you can call the corresponding method on the parser state. </paragraph><paragraph>for example, result.shift() or result.left_arc(\"nsubj\"). This will change the state of result -- then you proceed with the next prediction based on the new state. </paragraph></document>"
    },
    {
        "source": "HW3 Part 2. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I would like some clarification on get_input_representation(self, words, pos, state). First, are words and pos basically used to check if any entry in the buffer/stack needs to be replaced by a tag? Secondly, are words and pos structured such that pos[i] is the pos tag of word[i]? If there is no tag, does pos[i] equal to None? Also, regarding to the comment in post #315, can we assume if we see None(or 0, as described in the instructions) in the stack or buffer, we can replace it with &lt;ROOT&gt;?</paragraph><paragraph>Thank you</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>You're given a list of words that are just words, prepended with None<break/>You're given a list of POS that correspond 1:1 with the words<break/>Stack and buffer contain a list of indexes where 0 is root, but these index the word/POS list. I think the wording of the assignment is awkward for that<break/>You need to identify which words are being referenced and when, but also handle finding their actual IDs to create the vectors. you can use the POS to check if some words need to be converted to a special ID corresponding to a category of words, like NNP and CD.</paragraph><paragraph/></document>"
    },
    {
        "source": "part 4 best transition. <document version=\"2.0\"><paragraph>Hi I hope you are great.</paragraph><paragraph/><paragraph>Once we figure out the best transition in Part 4, how should we store it?</paragraph><paragraph/><paragraph>I'm unsure of how finding the best transition connects with the given part:</paragraph><paragraph/><pre>result = DependencyStructure()\n        for p,c,r in state.deps:\n            result.add_deprel(DependencyEdge(c,words[c],pos[c],p, r))\n        return result</pre><paragraph/><paragraph>thank you so much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The best transition is found and used to update the parser's state within the <code><bold>while</bold></code> loop of the <code><bold>parse_sentence</bold></code> method. There's no need to separately store the best transition because it is immediately applied to the parser's state. So the result here is already the best transition</paragraph><paragraph/></document>"
    },
    {
        "source": "HW3 Part 4 - name 'extractor' is not defined error. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I am getting a strange error when attempting to run:</paragraph><pre>python evaluate.py data/model.h5 data/dev.conll\n</pre><paragraph>When I try and run it on Google Colab, I get the following output:</paragraph><pre>Evaluating. (Each . represents 100 test dependency trees)\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n/content/evaluate.py in &lt;module&gt;\r\n     45             words = dtree.words()\r\n     46             pos = dtree.pos()\r\n---&gt; 47             predict = parser.parse_sentence(words, pos)\r\n     48             labeled_correct, unlabeled_correct, num_words = compare_parser(dtree, predict)\r\n     49             las_s = labeled_correct / float(num_words)\r\n\r\n/content/decoder.py in parse_sentence(self, words, pos)\r\n     38 \r\n     39             transition = False\r\n---&gt; 40             features = self.extractor.get_input_representation(words, pos, state).reshape(1,-1)\r\n     41             status = self.model.predict(features)\r\n     42             while transition == False:\r\n\r\nNameError: name 'extractor' is not defined\n</pre><paragraph>This is very confusing, since running decoder.py does not bring up any errors at all. I suspected that it might be my indentation, but I went over it and it seems fine to me. </paragraph><paragraph>I have not touched evaluate.py. I even redownloaded it just to make sure. What could be causing this? Any help would be greatly appreciated.</paragraph><paragraph>Thank you.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I assume this might be an import problem. Make sure your working directory on your google colab is correct.<break/><break/>It's hard to solve this kind of problem on ed discussion. Could you go to one of the OH and ask the TA for help?</paragraph></document>"
    },
    {
        "source": "HW3 Part 4 - Difficulty Reading In Feature Into Prediction. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I am getting a error I do not understand when attempting to predict using extractor.get_input_representation(). Right now, I have it so I read it in through:</paragraph><pre>status = self.model.predict(extractor.get_input_representation(words, pos, state))\n</pre><paragraph>However, when this line is run, I receive the following error:</paragraph><pre>ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\r\n    \r\n    Input 0 of layer \"1stHidden\" is incompatible with the layer: expected axis -1 of input shape to have value 192, but received input with shape (None, 32)\n</pre><paragraph>I do not understand what might be causing this. extractor.get_input_representation() returns a numpy array of shape (6,), which is what I expect. Here is how my model is set up for reference:</paragraph><pre>def build_model(word_types, pos_types, outputs):\r\n    m = Sequential()\r\n    m.add(Embedding(word_types, 32, input_length=6, name=\"Embedding\"))\r\n    m.add(Flatten(name=\"FlattenEmbed\"))\r\n    m.add(Dense(100,activation=\"relu\", name=\"1stHidden\"))\r\n    m.add(Dense(10, activation=\"relu\", name=\"2ndHidden\"))\r\n    m.add(Dense(outputs, activation=\"softmax\", name=\"Output\"))\r\n\r\n    m.compile(keras.optimizers.Adam(lr=0.01), loss=\"categorical_crossentropy\")\r\n    return m\n</pre><paragraph>Any help regarding what might be causing this would be greatly appreciated.</paragraph><paragraph>Thank you.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi the predict method expects the input to be of shape (1,6), that is a 2D matrix (the first dim is the batch size), instead of (6,). Try to reshape the result if get_input_representation using .reshape(1,-1).\u00a0</paragraph></document>"
    },
    {
        "source": "Homework 3 Part 4. <document version=\"2.0\"><paragraph>For the part 4, I want to get the input representation, so I wrote something like <code>input_representation = self.extractor.get_input_representation(words, pos, state)</code> but it throws an error <code>Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 192, but received input with shape (None, 32)</code> So I change my code to <code>input_representation = self.extractor.get_input_representation(words, pos, state).reshape(1,-1)</code> and it works. But I just want to make sure this is a necessary step to do to reshape or it indicates something went wrong with my previous code? </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes that\u2019s fine. By reshaping this way you are turning the d-dim vector (i.e a 1-D Matrix) into a (1,d) 2-D matrix. This allows the training data to be stacked together in batches of 192 of shape (192, d).\u00a0</paragraph></document>"
    },
    {
        "source": "Homework 3 Part 2. <document version=\"2.0\"><paragraph>1. To get output representation, can I use <code>return keras.utils.to_categorical(self.output_labels[output_pair], num_classes=91)</code> to get the output matrix? Or I have to implement this manually? </paragraph><paragraph>2. To get input representation, when we considering the special case of root, I am confused about do we compare the word with <code>None</code> or <code>'None'</code>? i.e. with quotation mark or without? So, I am confused about how the stack is initialized for the special case &lt;ROOT&gt;?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1JhbXnCy76TTNUABepZQFgeu\" width=\"658\" height=\"268.0740740740741\"/></figure><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>1. Yes that sounds okay. </paragraph><paragraph>2. I think the entry in the word list is just the value None (not a string). Note that the stack contains word positions, so for the root it will contain the value 0. In the initial state, it will be [0]. </paragraph></document>"
    },
    [
        {
            "source": "Loss values for HW3?. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>Is it possible to know (or share) the approximate loss values people are getting for their training? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>When I was training I started at like a loss of 2.5 and it went down to about 0.25 by the end of the 5 epochs. Hope this helps.</paragraph></document>"
        },
        {
            "source": "Loss values for HW3?. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>Is it possible to know (or share) the approximate loss values people are getting for their training? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi, I got: 0.4751 0.3072 0.2737 0.2544 0.2413</paragraph><paragraph>I'm not sure whether this is the expected answer</paragraph><paragraph>]</paragraph></document>"
        },
        {
            "source": "Loss values for HW3?. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>Is it possible to know (or share) the approximate loss values people are getting for their training? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Thanks all, at time of writing my model wasn't converging so I asked but now it's consistent.</paragraph></document>"
        }
    ],
    {
        "source": "HW3 part 4. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I am trying to run the evaluation (python evaluate.py data/model.h5 data/dev.conll) for the model and I found it took quite a long time. On average it takes 9ms / step on a A6000 gpu and it went for nearly two hours. Is this normal evaluation time or something is wrong in my setup?<break/><break/>For reference my final results are as follows:<break/><break/>5039 sentence.</paragraph><paragraph>Micro Avg. Labeled Attachment Score: 0.7391242832055795<break/>Micro Avg. Unlabeled Attachment Score: 0.786863415811507</paragraph><paragraph>Macro Avg. Labeled Attachment Score: 0.7480423144749794<break/>Macro Avg. Unlabeled Attachment Score: 0.7964001075386273<break/><break/>Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Did you disable eager execution? I found that it speeds up the last step significantly</paragraph></document>"
    },
    {
        "source": "HW3 part 2. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>In HW3 part 2 get_input_representation(self, words, pos, state), I am not sure what the pos variable can be used for, is it supposed to be used in the function?<break/><break/>Also, I found that the list of words for a sentence always start with none, for example:<break/>words: [None, '``', 'Feeding', 'Frenzy', \"''\", '-LRB-', 'Henry'...]. Is this normal and we leave it as it is?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>For the input representation you need the POS to replace some of the input tokens with special symbols. </paragraph><blockquote>Note that you need to account for the special symbols (&lt;CD&gt;,&lt;NNP&gt;,&lt;UNK&gt;,&lt;ROOT&gt;,&lt;NULL&gt;) in creating the input representation. Make sure you take into account states in which there are less than 3 words on the stack or buffer.</blockquote><paragraph>The first token (position 0) corresponds to the \"root\" symbol. None is included in the list of input tokens so that the first \"actual\" token has the index 1.</paragraph></document>"
    },
    [
        {
            "source": "Annotated Dependency Tree Bank. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>When starting HW 3, I wanted to make sure I had a solid understanding of Dependency Parsing.<break/><break/>To clarify, we manually create an annotated dependency tree that is the gold tree from a treebank.</paragraph><paragraph>I am confused as to how this dependency tree is different from the one we are trying to construct. If we already have the gold tree, why are we training this model to construct transitions if the transitions are already from the gold tree?<break/><break/>Are we using the model to create transitions on unseen inputs that are not from the gold tree?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I believe you are training it to construct dependency trees for any input, not just the training set.</paragraph><paragraph/></document>"
        },
        {
            "source": "Annotated Dependency Tree Bank. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>When starting HW 3, I wanted to make sure I had a solid understanding of Dependency Parsing.<break/><break/>To clarify, we manually create an annotated dependency tree that is the gold tree from a treebank.</paragraph><paragraph>I am confused as to how this dependency tree is different from the one we are trying to construct. If we already have the gold tree, why are we training this model to construct transitions if the transitions are already from the gold tree?<break/><break/>Are we using the model to create transitions on unseen inputs that are not from the gold tree?</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>I think your understanding is correct here. We have the manually annotated treebank. We obtain transition sequences for these trees using the oracle algorithm. The transition sequence for each training sentence make up the training data. We are training the model to predict good transition sequences for unseen sentences.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "target_dev.npy and target_train.py. <document version=\"2.0\"><paragraph>Hi I hope you are doing great!</paragraph><paragraph/><paragraph>I think I have completed part 2.</paragraph><paragraph/><paragraph>I just ran these lines:</paragraph><paragraph/><paragraph>python extract_training_data.py data/train.conll data/input_train.npy data/target_train.npy</paragraph><paragraph/><paragraph>python extract_training_data.py data/dev.conll data/input_dev.npy data/target_dev.npy</paragraph><paragraph/><paragraph>So now I have target_dev.npy and target_train.py as files in my data folder.</paragraph><paragraph/><paragraph>I am using atom and when I clicked on the files, I got a message that: 'Atom will be unresponsive while loading such large files\"</paragraph><paragraph/><paragraph>I procceded and target_dev.npy has a bunch of lines like:  ??</paragraph><paragraph>and target_traim.np seems to just be blank.</paragraph><paragraph/><paragraph>Is this normal?</paragraph><paragraph/><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes. These are binary files, so you can\u2019t just open them in a text editor. However target_train.npy should not be empty \u2014 I don\u2019t know if there is an issue with your code or if it simply doesn\u2019t show the content in Atom. What\u2019s the actual file size?\u00a0</paragraph><paragraph/></document>"
    },
    {
        "source": "Teacher Forcing. <document version=\"2.0\"><paragraph>Hello,<break/><break/>I had a quick clarification question regarding what \"teacher forcing\"  is.<break/><break/>In the slides today, we saw the outputs for an RNN LM being fed independently into each other, as well as outputs being pipelined together in a Generator. <break/><break/>My question is in which application of RNNs would we consider the inputs to be \"teacher-forced?\" </paragraph><paragraph>I am under the assumption that \"teacher-forcing\" means to over-write an incorrect model prediction output with the correct training data as an input in pipelining, but we mentioned teacher-forcing in the RNN LM context as well.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>So the generator and RNN language model is trained exactly the same way. But you are right that you can\u2019t really speak of \u201cteacher forcing\u201d in the context of the language model.\u00a0</paragraph></document>"
    },
    {
        "source": "Different evaluation result in Colab and Pycharm. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>when I ran the same code in both Colab and PyCharm on the \"dev.conll\" set to evaluate the model performance, it gave me very low accuracy on PyCharm (roughly Micro Avg. Unlabeled Attachment Score 0.05), but the code run in Colab gave me an accuracy of \"Micro Avg. Unlabeled Attachment Score: 0.77,\" so I am not sure what happened here.</paragraph><paragraph>Also, I further trained the model on the \"input_dev\" and \"target_dev\" sets and named the model \"model_dev.h5\". However, when I ran the original model which is not further trained on the \"input_dev\" and \"target_dev\" sets on \"test.conll\" file, it returned the \"ZeroDivisionError: float division by zero\", but if I ran the evaluation by using the \"model_dev.h5\" model, it worked just fine.  I am also unsure what is wrong here.</paragraph><paragraph>Thanks a lot in advance.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>It was solved by reinstalling all libraries.</paragraph><paragraph/></document>"
    },
    {
        "source": "indicies question. <document version=\"2.0\"><paragraph>Hi i hope you are doing great.  I get that the words map to indicies.  But do the particular indicies have a meaning, or are they just a mapping of a word to an arbitrary numeric value.</paragraph><paragraph/><paragraph>Thank you!</paragraph><paragraph> </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>They are just arbitrary numeric values (since you need to represent the input to the neural network numerically).\u00a0</paragraph></document>"
    },
    {
        "source": "The usage of \"pos_types\" parameter in build_model function in HW3. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>I am doing the HW3 part 3, the input parameter \"pos_types\" for function \"build_model\" seems a bit redundant and is not used in constructing the neural network. Just want to confirm if it is right or if I missed something. </paragraph><paragraph>Thanks a lot in advance!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That's correct, that parameter is not used. </paragraph></document>"
    },
    {
        "source": "Ungraded Exercise for HMM Questions Part c and d. <document version=\"2.0\"><paragraph>for c, wouldn't the tri-grams be on applied to the transition probabilities rather than the emission probabilities? Meaning it should be p(V|N, start) rather than the example with P(START time flies)? </paragraph><paragraph>In that case, would it be P(V|N, start) = P(V,N,start) / sum of P(x, N, start)</paragraph><paragraph>and we should do it for as the full set?</paragraph><paragraph>P(N| N, start)</paragraph><paragraph>P(Prep| N, start)</paragraph><paragraph>P(N| V, start)</paragraph><paragraph>P(Prep| V, start)</paragraph><paragraph>P(Prep|V,N)</paragraph><paragraph>P(N|V,N)</paragraph><paragraph>P(Prep|N,V)</paragraph><paragraph>P(N|N,V)</paragraph><paragraph>P(V|N,V)</paragraph><paragraph>P(N|Prep, V)</paragraph><paragraph>P(N|Prep, N)</paragraph><paragraph>For part (d), I don't really understand why don't the solution use a count variable but instead uses: </paragraph><paragraph>pi[k,t] += pi[k-1][s]</paragraph><paragraph>is this assuming the first p[0,start] is initialized as 1 and that 1s will keep accumulating? </paragraph><paragraph>what is sum_s pi[n, s]  ? is sum_s a function or variable here? </paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi Millie,</paragraph><paragraph>I think we discussed this during office hours. Please ask again if I misremember.\u00a0</paragraph><paragraph>Daniel\u00a0</paragraph></document>"
    },
    {
        "source": "Ungraded Excercises Parsing. <document version=\"1.0\"><paragraph>In step 11, one might think that a RIGHT-ARC is possible. However, since there is no subsequent connection between \u201croot\u201d and \u201ctoday,\u201d we cannot actually make that move. This makes this example unique. Am I interpreting this correctly?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/f2SPv24ydo354Z9Olx9MKEGa\" width=\"1126\" height=\"367\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>I think we cannot perform right arc when sent still have a child (today), therefore we have to shift first). Once the child (today) is processed, we can right arc root to sent</paragraph></document>"
    },
    [
        {
            "source": "Basic doubt regarding probabilities. <document version=\"2.0\"><paragraph>Given that this is our data in Ungraded Exercise: Naive Bayes<break/><break/>Email1 (spam): buy car Nigeria profit<break/>Email2 (spam): money profit home bank<break/>Email3 (spam): Nigeria bank check wire<break/>Email4 (ham): money bank home car<break/>Email5 (ham): home Nigeria fly<break/><break/>I want to calculate simple P(buy).<break/><break/>One way I go is P(buy) = count(buy)/total_no_words = 1/19<break/><break/>The other way to go is using total probability theorem,<break/><break/>P(buy) = Summation P(buy|x)<italic>P(x) = P(buy|spam) * P(spam) + P(buy|ham) * P(ham)<break/> = 1/12 (considering the answer is right) * 3</italic>/5 + 0/7 * 2/5 = 1/20<break/><break/>Where am I going wrong here? I definitely know that total probability theorem is not wrong.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>It's a bit late in the night, so I can't really think this through fully, but consider this:</paragraph><paragraph>Email1 (spam): buy * infinity</paragraph><paragraph>Email2 (ham): something</paragraph><paragraph>In this case, using the first method to calculate P(buy), we get 1, because inf/(1+inf) goes to 1.</paragraph><paragraph>But it can be seen that P(buy) should be 1/2, because if only half of all emails are spam (in this data), then it should not be guaranteed that we see \"buy\". So it seems like the count(buy)/total_words method of calculation is invalid, although I can't think of why right now - maybe that method requires P(buy) to be independent of P(class = x)?</paragraph></document>"
        },
        {
            "source": "Basic doubt regarding probabilities. <document version=\"2.0\"><paragraph>Given that this is our data in Ungraded Exercise: Naive Bayes<break/><break/>Email1 (spam): buy car Nigeria profit<break/>Email2 (spam): money profit home bank<break/>Email3 (spam): Nigeria bank check wire<break/>Email4 (ham): money bank home car<break/>Email5 (ham): home Nigeria fly<break/><break/>I want to calculate simple P(buy).<break/><break/>One way I go is P(buy) = count(buy)/total_no_words = 1/19<break/><break/>The other way to go is using total probability theorem,<break/><break/>P(buy) = Summation P(buy|x)<italic>P(x) = P(buy|spam) * P(spam) + P(buy|ham) * P(ham)<break/> = 1/12 (considering the answer is right) * 3</italic>/5 + 0/7 * 2/5 = 1/20<break/><break/>Where am I going wrong here? I definitely know that total probability theorem is not wrong.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I think that P(HAM) and P(SPAM) need to be considered on a per word basis to use the law of total probability in this way. Thus, in this case, P(HAM) = 7/19 and P(SPAM) = 12/19, </paragraph><paragraph>P(buy) = 1/19,</paragraph><paragraph>P(buy|HAM)P(HAM) + P(buy|SPAM)*P(SPAM) = (1/12)*(12/19) = 1/19.</paragraph><paragraph>I believe this is because the definition of the Law of Total Probability states the events you are conditioning over must partition the same sample space as the event you're trying to find the probability of, and that the law of total probability seems to fail in the original example because the sample space over documents and the sample space over words are being conflated. If all documents contained the same number of words, I don't think this would have been an issue.</paragraph></document>"
        }
    ],
    [
        {
            "source": "ungraded linear model and feature problem #2. <document version=\"2.0\"><paragraph>I am confused about how to tackle this subproblem:</paragraph><paragraph>b) Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus. </paragraph><paragraph>Can anyone provide a hint? The solution didn't offer me any clue. </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hello! My understanding when I was going through it is that we have a really small lexicon and set of possible labels, so our rules can be super specific. Because our rules are so specific, we can make it so they don't have any false positives, hence we probably don't even need to try to weight them differently. </paragraph></document>"
        },
        {
            "source": "ungraded linear model and feature problem #2. <document version=\"2.0\"><paragraph>I am confused about how to tackle this subproblem:</paragraph><paragraph>b) Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus. </paragraph><paragraph>Can anyone provide a hint? The solution didn't offer me any clue. </paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Not sure if my approach is correct, so take it with a huge pinch of salt. Basically I try to approach it like a super simplify Softmax. It would great that TA can point out where I\u2019m doing wrong.\u00a0</paragraph><paragraph>Based on how the feature function is set up in the solution, as long as the weights are all greater than 0, then all then all POS should correctly assigned.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/glntbIQKlg56NN2buMi7IUc9\" width=\"2244\" height=\"2904\"/></figure></document>"
        },
        {
            "source": "ungraded linear model and feature problem #2. <document version=\"2.0\"><paragraph>I am confused about how to tackle this subproblem:</paragraph><paragraph>b) Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus. </paragraph><paragraph>Can anyone provide a hint? The solution didn't offer me any clue. </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>My intuition is that since the feature functions are defined based on what has appeared in the training corpus, the POS tag/token combination that matches what appeared in the training corpus will always have more 1s in the features comparing to another POS tag/token that hasn't appeared. Therefore to make sure that the POS tag/token combination gets the highest score among all possible POS tag choices, we should set all positive numbers for the weights. </paragraph></document>"
        }
    ],
    {
        "source": "Clarification on Linear Interpolation. <document version=\"2.0\"><paragraph>Hi, I'm studying the different smoothing methods and wondering what value we should be using for the unigram probabilities in linear interpolation. Should we use  count(word_i)/(number of tokens in the lexicon including END) or count(word_i)/(number of tokens in the lexicon not including END)</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I believe that we typically include the END token but do not include the START token, similar to how we calculated it for HW 1. Hope that helps!</paragraph></document>"
    },
    {
        "source": "Constituent Labels Clarification. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I just wanted to clarify a few things about the slide on constituent labels. Since the slide states that constituents should have one non-bracketed word, would this mean the constituents in the example are the NP consisting of DetP and noun \"girl\", VP consisting of verb \"likes\" and NP, and the NP consisting of AdjP and noun \"boys\"? Or is the logic behind this that all phrases XP containing X (regardless of how many non-bracketed words there are) would be a constituent with X as the head?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/G74gMGGoZCTSHsP0BU3xNENF\" width=\"658\" height=\"488.82428940568474\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>One assumption here is that the splits are binary and are chose in such a way that the single non-bracketed word is the head.\u00a0<break/>This approach (called X-bar theory) is not always consistently used. The S constituent, for example, does not have an un-bracketed head. Instead, the head of that constituent is the verb of the VP.\u00a0</paragraph></document>"
    },
    {
        "source": "hmm-ungraded assignment-a. <document version=\"2.0\"><paragraph>for part a, why we have 2 routes of red back pointer, one start from \"leaves\" and one start from \"like\"</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>This may be an oversimplification but, if you check the lecture slides for 05 hmms, toward the middle it goes over the Viterbi algorithm and explains it as d^n paths for n words and d tags. Essentially a matrix of possibilities to work out all possible paths forward then the most probable paths backward. For the red paths I believe it shows multiple because you want to see the most\u00a0probable path from node to node where one exists &gt;0. <break/>Not until you\u2019ve moved front to back then back to front do you pick the most probable singular path. Until then you want to keep your options on the table, per say.</paragraph></document>"
    },
    {
        "source": "Number of feature vectors. <document version=\"2.0\"><paragraph>What's the process of knowing how many features to list out for the feature function assignment? The solution has 8, but I'm assuming that depends based on what our features are. </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Not a TA, but in lecture the instructor made it sound like it was kind of an art form of picking the right features that capture the right aspects and perform well. I'm pretty sure the dimensionality is just based on however many features we determine are sufficient for the given task. </paragraph></document>"
    },
    {
        "source": "Request for Guidance on Zoom Examination Process. <document version=\"2.0\"><paragraph>Dear Professor and TA,</paragraph><paragraph>Unfortunately, I've been falling sick and have decided to opt for the Zoom alternative for tomorrow's exam after discussing with the professor. Could you provide any instructions or guidelines on how this will be conducted? I greatly appreciate your assistance.</paragraph><paragraph>Thank you, </paragraph><paragraph>Xinyue</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Please log into the regular classroom Zoom meeting. I will have the exam set up in Gradescope for you. You will have to either print it and then complete the template, or respond on separate sheets of paper. One you are done, please scan your work (I recommend using a smartphone scanner app, such as GeniusScan), and upload to Gradescope. During the exam, please keep your camera turned on. One of the TAs will proctor the exams on Zoom.\u00a0</paragraph></document>"
    },
    {
        "source": "Complexity of retrieving a single parse tree.. <document version=\"2.0\"><paragraph>In lecture07, it says that the complexity of retrieving any single parse tree is O(N^2). Why is it not O(2*N)?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>You start at [0,n,S] and then follow the backpointers deterministically. The table has n^2 entries for the spans, and at most you will touch each cell once.\u00a0</paragraph></document>"
    },
    {
        "source": "HMM - Ungraded Exercise part c. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/g4IPGsifnCRAfzqDAlr285ft\" width=\"658\" height=\"133.5641791044776\"/></figure><paragraph>Hello, I wonder how could we get sum_z P(u, v, z) in this specific case? What other z tokens can it take except \"flies\"? Thanks in advance!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>In principle, the z can range over all words in the vocabulary. Some words may not be possible at all in the context u,v thought. \u00a0Note that the computation for P(START time flies) is just an example. You would have to do this for all trigrams that the model could generate.\u00a0</paragraph></document>"
    },
    {
        "source": "Katz's Backoff. <document version=\"2.0\"><paragraph>Hi, I'm looking at the formula for Katz' backoff for bigram models. and I was wondering if there was a typo in the formula?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/7KpyLKwS2QPEznmLjk5cHAX5\" width=\"595\" height=\"226.42050390964377\"/></figure><paragraph>should it be SUM v: count(v,w) Pmle(v) in the denominator of the backoff? I don't understand what the variable u is here.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>In my opinion, the variable 'u' here is different from 'w', it is used to describe the condition : 'count(v,u) = 0'. P_mle is used to describe the viewed variable 'w' and 'u' is the hidden(not viewed) variable.</paragraph></document>"
    },
    [
        {
            "source": "Perplexity Formula. <document version=\"2.0\"><paragraph>Do we include the counts for the start and end markers when determining M for perplexity calculations using the formula given in the lecture notes? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Correct me if I'm wrong, but I believe you count the end markers and not the start markers (at least this is what I remember doing in the homework).</paragraph><paragraph/></document>"
        },
        {
            "source": "Perplexity Formula. <document version=\"2.0\"><paragraph>Do we include the counts for the start and end markers when determining M for perplexity calculations using the formula given in the lecture notes? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I agree with Alexandra, the logic was that we count everything that can be generated by our model. In our ngram model, the START token were never generated therefore we don't include it in the perplexity calculations. </paragraph><paragraph/></document>"
        },
        {
            "source": "Perplexity Formula. <document version=\"2.0\"><paragraph>Do we include the counts for the start and end markers when determining M for perplexity calculations using the formula given in the lecture notes? </paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>The student answers are correct. You count the end markers but not the start markers.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Clarification on ungraded exercise: Linear Models and Feature Functions. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>I am a bit confused about the answer to the feature functions to this question. It defines eight feature functions, does it mean that for each:</paragraph><math>\\left(w_{i-1},\\ w_i,\\ t_{i-1},\\ t_i\\right)</math><paragraph>, should we compute all the eight feature functions for the single input? If this is the case, for the input <bold>(time, flies, N, V) from sentence 1</bold>, the resulting feature vector would be <bold>(0, 0, 0, 0, 0, 0, 0, 1),</bold> which only has one dimension to be 1. So I am a bit confused about what the <bold>\"specifying the conditions under which each dimension of the feature vector is 1.\"</bold> in the question means. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/eD4hZjESUfEex24swpQM4zPj\" width=\"658\" height=\"483.54954954954957\"/></figure><paragraph>Thanks a lot in advance!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hello! Obligatory not a TA but was just thinking through the same question\u2014</paragraph><paragraph>this is the formula from the ungraded exercise, plus a few details I added when I was going through it to make it clearer for myself. We have a set of possible labels <bold>T = {N, V, P}</bold>, a <bold>d</bold>-dimensional vector of weights <bold>v</bold> (<bold>d</bold> being 8 for the feature function here), and our feature function converts its inputs <bold>w_i, w_i-1, t_i-1</bold>, and <bold>t_i</bold> to a <bold>d</bold>-dimensional <italic>feature vector</italic>.</paragraph><math>t_i^*=\\arg\\max_{t_i\\in T}\\sum_{j=1}^d\\Phi_j(w_{i-1},\\ w_i,\\ t_{i-1},\\ t_i)\\cdot v_j</math><paragraph>So the argmax is basically a for loop <code>for t_i in T</code> , the sum is basically <code>sum(dimension for dimension in feature_function)</code>, so for each context, we consider each label, then sum up the result of applying each dimension of the feature function on it.  </paragraph><paragraph>Basically, we sum up all the weighted values from each dimension, so the more dimensions that are set to one, the higher our unweighted chance to pick that label. Based on the way the individual functions that build up our feature function are written, <bold>(time, flies, N, N)</bold> and <bold>(time, flies, N, P)</bold> would both be <bold>(0, 0, 0, 0, 0, 0, 0, 0).</bold> If we sum up the values from each dimension, we get 1 when we try <bold>V</bold> and 0 when we try <bold>N</bold> or <bold>P</bold>, so our model picks <bold>t_i* =</bold> <bold>V</bold>. </paragraph><paragraph>This ended up being a bit long winded but hope that helps! And teaching staff please correct anything I got wrong here. </paragraph></document>"
    },
    {
        "source": "Question about definition of token. <document version=\"2.0\"><paragraph>In the lecture 3 slides, a token is defined as \"total number of word occurrences\", but during lecture, when we went over the exam material, I believe it was defined as \"an instance of word in particular context\".</paragraph><paragraph>To clarify, is a token a number or an instance of a word?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I believe usually when we refer to tokens we are referring to the instance of a word in a particular context. If you look at the definition of \"tokenization\" presented in lecture three, it is the process of segmenting text (a sequence of characters) into a sequence of tokens (words). From my understanding, the number of tokens is the number of unique words in a particular context. </paragraph></document>"
    },
    {
        "source": "Homework 2 Submission. <document version=\"2.0\"><paragraph>I did assignment 2 this week and submitted it on Monday. Now when I check canvas, it's listed as a past assignment and doesn't show that I submitted. I was wondering if someone could check Courseworks from the instructor side and verify that you can see my submission? </paragraph><paragraph>My name is Sean Harrison and my uni is sph2146</paragraph><paragraph>I really appreciate your help!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi Sean,</paragraph><paragraph>I can see your submission on Courseworks.\u00a0</paragraph><paragraph>Daniel\u00a0</paragraph></document>"
    },
    {
        "source": "Incomplete Answer for Linear models Ungraded Exercises. <document version=\"2.0\"><paragraph>Dear Professor and TA, may I ask if there is a full version of the Linear Model exercise answer? I cannot see the answer for b) except the first line, and all for c) is missing. Many thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The answer for b) is just the weight vector itself.\u00a0<break/>For c) I don\u2019t have a written up answer, and I didn\u2019t have time to write it up today. However, it\u2019s not critical in terms of exam prep.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "N-gram. <document version=\"2.0\"><paragraph>I was doing n-gram ungraded exercise. I was wondering why vocabulary size of a bigram is all the types and END. </paragraph><paragraph>Why is END included and not START? </paragraph><paragraph>Why don't we calculate the vocabulary size by the number of every bigram seen, but instead look at individual words? </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>professor has answered a similar question, just see #51 </paragraph></document>"
        },
        {
            "source": "N-gram. <document version=\"2.0\"><paragraph>I was doing n-gram ungraded exercise. I was wondering why vocabulary size of a bigram is all the types and END. </paragraph><paragraph>Why is END included and not START? </paragraph><paragraph>Why don't we calculate the vocabulary size by the number of every bigram seen, but instead look at individual words? </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am not a TA but I believe that END is included because you will need to predict and include <code>END</code> in order to mark the sentence boundaries. For instance, consider the sentence \"time flies like flies END\". You will actually have to deliberately make the decision of END in that sentence. As opposed to START which might not be necessarily needed because you are looking at sets of words. </paragraph><paragraph>As for your second question, I believe that we look at individual words + END because the individual words could result in more bigrams than what is seen Blike pairings and stuff. Bigrams are from individual words.</paragraph></document>"
        }
    ],
    [
        {
            "source": "ungraded viterbi question. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.</paragraph><paragraph/><paragraph>part b states:</paragraph><paragraph/><paragraph>Use the <bold>Forward algorithm</bold> to compute the probability of the surface sequence \"<italic>time flies like leaves</italic>\".</paragraph><paragraph/><paragraph>I get how that works in general, but the table ended with this:</paragraph><paragraph/><paragraph>\u03c0[4,N] = \u03c0[3,V] \u00b7P(N|V) \u00b7 P(leaves|N) + \u03c0[3,Prep] \u00b7 P(N|Prep) \u00b7 P(leaves|N)) = (0.004918 \u00b7 2/3 \u00b7 1/4), (0.021267 \u00b7 1 \u00b7 1/4 )) = <bold>0.00614</bold><break/>\u03c0[4,V] = \u03c0[3,V] \u00b7P(V|V) \u00b7 P(leaves|V) + \u03c0[3,Prep] \u00b7 P(V|Prep) \u00b7 P(leaves|V) = 0 // because of the 0 transition probabilities</paragraph><paragraph>so if 4,V was a possibility, we would just pick whichever had the higher probability between </paragraph><paragraph>\u03c0[4,N] and \u03c0[4,V] as our final answer, right?</paragraph><paragraph/><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Opps, I thought you were talking about the part a Virterbi algo. </paragraph><paragraph>where you would pick the higher probability between \u03c0[4,N] and \u03c0[4,V] if they were both non-zero. This is because you want to select the state of the position of the sequence that has the most likelihood. In this context, there is no transition probability for leaves to be a verb which means it is most likely a noun in that context</paragraph><paragraph>I think you can look from slide 37 onwards in the slide deck lecture05-hmm_pos_tagging.pdf for more information, or you can look on page 4 of this page: https://web.stanford.edu/~jurafsky/slp3/A.pdf</paragraph></document>"
        },
        {
            "source": "ungraded viterbi question. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.</paragraph><paragraph/><paragraph>part b states:</paragraph><paragraph/><paragraph>Use the <bold>Forward algorithm</bold> to compute the probability of the surface sequence \"<italic>time flies like leaves</italic>\".</paragraph><paragraph/><paragraph>I get how that works in general, but the table ended with this:</paragraph><paragraph/><paragraph>\u03c0[4,N] = \u03c0[3,V] \u00b7P(N|V) \u00b7 P(leaves|N) + \u03c0[3,Prep] \u00b7 P(N|Prep) \u00b7 P(leaves|N)) = (0.004918 \u00b7 2/3 \u00b7 1/4), (0.021267 \u00b7 1 \u00b7 1/4 )) = <bold>0.00614</bold><break/>\u03c0[4,V] = \u03c0[3,V] \u00b7P(V|V) \u00b7 P(leaves|V) + \u03c0[3,Prep] \u00b7 P(V|Prep) \u00b7 P(leaves|V) = 0 // because of the 0 transition probabilities</paragraph><paragraph>so if 4,V was a possibility, we would just pick whichever had the higher probability between </paragraph><paragraph>\u03c0[4,N] and \u03c0[4,V] as our final answer, right?</paragraph><paragraph/><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I might be mistaken, but I think you actually sum them instead of taking the max, at least for the forward algorithm. As seen in the forward algorithm, you loop through each word in the sentence, summing probabilities of each incoming sequence of POS, until reaching the final word. However, after the loop, you can see that we return the sum of the probabilities. </paragraph><math>\\sum_s^{ }\\pi\\left[n,s\\right]</math><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/pP4oU1p5WkfPHCuFHdwNi3hm\" width=\"496\" height=\"350.52193995381066\"/></figure><paragraph/></document>"
        }
    ],
    {
        "source": "Feature functions, feature vector. <document version=\"2.0\"><paragraph>I'm having some trouble understanding the solution for ungraded exercise #6. What's the logic behind specifying conditions under which each dimension of the feature vector is 1?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>From my understanding, feature value = 1 when the feature is true for all training data in the corpus, and 0 otherwise. For example, for all training data, start are always followed by N. Therefore, when we set feature factor N follow start, the feature function will = 1.\u00a0</paragraph></document>"
    },
    {
        "source": "arc-eager transition problem. <document version=\"2.0\"><paragraph>Hi, I am a bit confused about using \"reduce\" in arc-eager transition, are we only allowed to use it  at the end or we can use it anytime once we want to drop a token in the stack?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>It can be used anytime (but reducing too early may lead to incorrect results / bad parse trees, of course). </paragraph></document>"
    },
    {
        "source": "Exam Requirement clarification. <document version=\"2.0\"><paragraph>Do we need to memorize the formule of the partial derivative of the Log Likelihood to prepare for the mid-term exam? Thank you.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>No calculus will be required on the exam. You certainly don't have to know the formula for the partial derivatives of the log likelihood function. </paragraph><paragraph>You should know the formulas for ngram probabilities, smoothing, HMMs, etc. My advise would be to focus on how these models work, rather than specifically memorizing the formulas. You should then be able to reconstruct the formulas if you need them. </paragraph></document>"
    },
    {
        "source": "Ungraded Exercise - Show arc-standard sequences are not unique. <document version=\"2.0\"><paragraph>Does anyone know the transitions different than the one on questino a? I have been thinking for a long time about the solution to this question. Is there any other arc-standrard transitions that can generate the same dependency tree?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>It's very possible that the dependency tree in part a) has a unique transition sequence. Try to construct a minimal example for which there are multiple sequences. </paragraph></document>"
    },
    {
        "source": "Derivation vs Parse Trees. <document version=\"2.0\"><paragraph>Just to double check, are derivation trees and parse trees  essentially the same thing? </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Essentially :) </paragraph><paragraph>The derivation tree specifies the order of production application. The nodes in the derivation tree (including the terminals) correspond to specific productions. </paragraph><paragraph>In the parse tree, the internal nodes correspond to phrase labels, and the terminal nodes correspond to  tokens in the sentence. </paragraph><paragraph>Because each production introduces a nonterminal (phrase label) derivation trees and parse trees are essentially identical (except for the terminals, which the derivation tree does not have). </paragraph></document>"
    },
    {
        "source": "calculator in exam 1. <document version=\"2.0\"><paragraph>It states that \"You should bring a calculator\" on the exam 1 topics. How necessary is this? Are we going to answer some hard calculation question?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>In lecture he mentioned it's helpful for computing the probabilities to arrive at an answer. I believe he also mentioned something along the lines of, if you don't have access to a calculator you would need to write out the calculations to show you would otherwise know how to do the multiplication if you had a calculator.</paragraph></document>"
    },
    {
        "source": "Confusion about lecture 11 Log-Linear Models as Neural Networks. <document version=\"2.0\"><paragraph>Hi there, </paragraph><paragraph>In lecture 11 slide 2, it describes that the log-linear models can be considered as a neural network with one hidden layer. But it says the input of the neural network is:</paragraph><math>X\\ =\\ \\left[\\phi\\left(x,y\\right)_1,\\phi\\left(x,y\\right)_2,\\ ...,\\ \\phi\\left(x,y\\right)_d\\right]</math><paragraph>I am not sure why the input is this form. I can understand that for the log-linear model, we take the original input and each potential class label <code>y</code>  as the inputs to the feature function and generate a new d-dimensional input. But in the neural network scenario, since the output contains the probabilities for each class label <code>y</code> , so why do we still need to take the output of the feature function as the input of the neural network? Or does it mean that we should input each class label <code>y</code>  to the feature function and generate a corresponding feature vector to the input of the neural network? </paragraph><paragraph>I might make the question a bit hard to understand, sorry about that, and I attached a screenshot of the slide as well. </paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/1JHyIP0gw9e66Fyh8fqG6KT1\" width=\"643\" height=\"486.3093434343434\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Thanks, this is not a great slide and potentially misleading. The analogy is a bit more loose than the slide makes it appear. In the log-linear model, we have the same weight vector for all classes. In the neural network, you can have different weights for each output unit. </paragraph><paragraph>Ordinarily in a neural network, you would <bold>not</bold> use the feature function approach. </paragraph><paragraph/></document>"
    },
    {
        "source": "Projectivity for Transition Based Parsing. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/M1suisfsmVj84T3JzDvZbLPV\" width=\"658\" height=\"485.643994211288\"/></figure><paragraph>Hi, </paragraph><paragraph>Why can transition based parsing with arc-standard or arc-eager only produce projective dependency structures? </paragraph><paragraph>Thank you! </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I don't know the details specifically but the intuition I observed is that these algos essentially looks at stuff on your immediate left and right and form dependency between them (this is why we have Maxent tagger function, which can establish dependency anywhere in the setence). For example, normally you have NP (5-&gt;7) -&gt; N (5-&gt;6), VP (6-&gt;7); this means that NP (which spans word_5 -&gt; word_7) can be broken down into N and VP which spans word 6 and 7 respectively. </paragraph><paragraph>What a non-projective algo would do is that it will still see NP but now NP is (5-&gt;8), and NP is made up of N(5-&gt;6) and VP (7-&gt;8). There would be sth else in between them (6-&gt;7) but the algo somehow skipped over that to connect N and VP together into NP despite them being separated by sth in the middle. In your questions, arc-standard and arc-eager only looks at the immediate left/right neighbor, they can't \"skip\" a word like I mention =&gt; they are projective. Lmk if this helps</paragraph><paragraph/></document>"
    },
    {
        "source": "Question about regularization. <document version=\"2.0\"><paragraph>I understand that the idea behind regularization is to subtact spikey parameter vectors from the log likelihood function, but can someone explain why we are multiplying by lamba/2 (what is lambda here?)</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>objective_function = min(loss) + lambda * max(regularization)<break/><break/>lambda is one of those common parameters along with the learning rate alpha that you tuned usually via cross-validation. it balances the trade off between minimizing loss and maximizing the margin.</paragraph><paragraph>using a boundary maximization model like SVM as example, the purpose of a high lambda value put more emphasis on the margin boundaries in an attempt to make the model more generalizable to unseen data. could however potentially lead to more errors if it is too large.<break/></paragraph><paragraph>conversely, a small lambda value will de-emphasize the regularization (thereby the margin-boundaries), thus more emphasis on the loss-function, which could make the model overfits to the training data. less generalizable to unseen data. <break/></paragraph><paragraph>lambda / 2 could just be it will simplify the derivative of the regularization term. when differentiating theta^2 using the power rule, the 2 being pull down will nicely cancel off with the 1/2, thereby leaving the derivative of a clean lambda * theta for the purpose of computing the gradient descent.</paragraph></document>"
    },
    {
        "source": "bar notation. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.</paragraph><paragraph/><paragraph>In a previous post about dependcy parcing an answer was:</paragraph><paragraph>The bar | separates the first or last symbol from the remainder of the stack or buffer. w \u2223\u03b2 stands for a buffer whose first token is w (read left-to-right) and \u03c3\u2223w stands for a stack with top (the top is conventionally shown on the right). </paragraph><paragraph/><paragraph>Is it just convention that the first element of the buffer is on the left of the bar. while the first element of the stack is on the right of bar?  So, they will always be depicted this way?</paragraph><paragraph/><paragraph>Since they both operate as stacks, I am a little confesued as to why they are depicted differently</paragraph><paragraph/><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes, that\u2019s the convention. For the buffer, it makes sense to have the first word on the left, as the buffer initially contains all tokens in the sentence from left to right.\u00a0</paragraph><paragraph>For the stack, I think the idea is that you can easily visualize the shift operation if the top of the stack is on the right, and the tokens involved in a dependency edge will be adjancent.\u00a0</paragraph></document>"
    },
    {
        "source": "N-gram Vocab Clarification. <document version=\"2.0\"><paragraph>I was reviewing the lecture 4 notes for ngram language models and have a couple of questions regarding the terminology. </paragraph><paragraph>1. When we used N for the number of tokens, are tokens just the unique set of all unigrams excluding START?</paragraph><paragraph>2. For the tokens, do we include STOP, and why is it that we choose to exclude start/stop/both?</paragraph><paragraph>3. When we are talking about the size of the lexicon, are we referring to the number of tokens or the total number of words present?</paragraph><paragraph>4. When we did additive smoothing, for constant V (size of vocabularly), is that just the total number of words in the file?</paragraph><paragraph/><paragraph>Thank you for the clarifications!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I could be wrong on these, so take them with a grain of salt.</paragraph><paragraph>1. N is the total number of words in the training data, not unique words but the total number of words.</paragraph><paragraph>2. I believe you include STOP and exclude START.  See #51.</paragraph><paragraph>3. The size of the lexicon is the number of unique words (or types--the number of distinct words).</paragraph><paragraph>4. V is the number of types (unique words) in the corpus.  Since you are adding one to each count in the numerator in Laplacian smoothing (over-counting every word by 1), you need to do the same for the denominator, which is why you add the size of the lexicon.</paragraph></document>"
    },
    {
        "source": "Question Regarding Overfitting. <document version=\"2.0\"><paragraph>Hi, I just had a question about overfitting - the definition/when it occurs wasn't really clear to me from the lecture/slides, so i was wondering if someone could explain it. Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Overfitting means when your model fits too well that it loses generalization ability. </paragraph><paragraph>Say you have a bunch of (x, y)s and you want to fit a model to predict y from x. You chose a very complicated model with tons of parameters and it worked very well: it reached 100% accuracy rate on this training set of (x,y). Now you receive another set of (x_new, y_new) and you are using it as your test set, your model is likely not going to do well on this new set of data because it's very specially designed for (x, y). This problem is overfiting: the model tried so hard on the training dataset that it can't generalize to other circumstances. </paragraph><paragraph>Let me know if this make sense. </paragraph><paragraph/></document>"
    },
    {
        "source": "Exam Content: Lexical Semantics. <document version=\"2.0\"><paragraph>Will any of the content from Wednesday's lecture on Lexical Semantics be on the midterm? I don't see it explicitly mentioned on the topics sheet, but just wanted to confirm since we did cover the material in class.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The last topic covered on the midterm will be lecture 11.\u00a0</paragraph></document>"
    },
    {
        "source": "Ungraded Exercise: n-gram language models. <document version=\"2.0\"><paragraph>The size of the vocabulary (excluding the [START] and [END] markers) is 6. Was wondering why the denominator of the laplace bigram probability has 7 in it? Why is the [END] marker considered in the count of the vocabulary?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The probability distribution needs to assign a probability over all possible events so that they sum to 1.0. The possible events are the possible next words given the left context. END is one of these possibilities, so it should receive some probability. Essentially we are treating it as just another possible event.\u00a0</paragraph></document>"
    },
    {
        "source": "Ungraded Exercise: Linear Models and Feature Functions part b. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I do not fully understand why (1,1,1,1,1,1,1,1) is the answer for part b</paragraph><paragraph>Thank you</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The features are designed in such a way that only features with the correct tag will be activated for any of the tokens in the data. So there is never any ambiguity between features that would encoder a preference for different tags.\u00a0</paragraph></document>"
    },
    {
        "source": "HMM as language models advantages over an n-gram model. <document version=\"2.0\"><paragraph>Hi there,</paragraph><paragraph>I am reviewing the lecture notes and lecture 05 page 40, asks us what the advantage of using HMM as a language model over the plain word n-gram model. </paragraph><paragraph>I am actually not sure about the answer to this question, any hints would be very appreciated. </paragraph><paragraph>Thanks a lot in advance.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>HMMs explicitly model a limited amount of syntactic structure (verbs typically follow nouns, etc.).\u00a0</paragraph><paragraph>HMMs don\u2019t assume direct dependencies between words which makes the model generalize better. For example, it may have never seen the sequence \u201cthe green cheese\u201d during training, but knows that green is an adjective and cheese is a noun. During training, the model has learned that nouns can follow adjectives, so this sentence should receive a high probability.\u00a0</paragraph></document>"
    },
    {
        "source": "Ungraded Exercises And Midterm Question. <document version=\"2.0\"><paragraph>Hi TAs and Professor,</paragraph><paragraph>It seems that the part b and part c in the solutions of 'Linear models and feature functions' are missing, would you please provide a full edition? And I'm wondering whether there will be a piece of exam paper for reference, thank you!</paragraph><paragraph>Sincerely, </paragraph><paragraph>Yuning</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>What do you mean by a \"piece of exam paper\"? I don't know if i will be able to provide answers to the last part by tonight, unfortunately. Sorry I missed this one. </paragraph></document>"
    },
    {
        "source": "Ungraded Exercise 1: Naive Bayes. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>I hope you are all doing well! I had a question regarding the probability calculations for Naive Bayes. In the exercise, I noticed that the probabilities don't divide by P(B), and I was wondering why that was the case?</paragraph><paragraph/><paragraph>For example, this calculation (P(class=<italic>ham</italic>, Nigeria) = P(class=<italic>ham</italic>) * P(Nigeria | class=<italic>ham</italic>) = 2/5 * 1/7 = 2/35) doesn't divide by P(Nigeria). Should we always do this in Naive Bayes? How should we approach problems like this in the midterm?</paragraph><paragraph/><paragraph>Best,<break/>Aishwarya</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Because we are usually only interested in selecting one of the classes, not computing the exact conditional probability, it\u2019s sufficient to compute the joint probability. The denominator for computing the conditional probability is the same for all classes, so you can think of this as a constant factor (in the slides, this was called alpha).\u00a0</paragraph></document>"
    },
    {
        "source": "Midterm Exam Time and Position. <document version=\"2.0\"><paragraph>Hi Professor,</paragraph><paragraph>I'm wondering whether can I take the exam on zoom? And what is the time for us to finish the exam? Do I need to open the camera or download any software to monitor my exam? Thanks!</paragraph><paragraph>Sincerely,</paragraph><paragraph>Yuning</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>In general, all regular (non-CVN) students are expected to take the exam in-person.</paragraph><paragraph>If you need an exception, for example because you are sick, please reach out to me by email.\u00a0</paragraph><paragraph>The exam will be in 309 Havemeyer at 1:10pm and will take 65min.\u00a0</paragraph></document>"
    },
    {
        "source": "midterm position. <document version=\"2.0\"><paragraph>want to make sure if the exam place is in the 309 HAVEMEYER </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes correct.\u00a0</paragraph></document>"
    },
    {
        "source": "Part d of Ungraded Exercise - HMM. <document version=\"2.0\"><paragraph>Hello everyone,</paragraph><paragraph>For the solution to part d of the question as in the title, \"s\" is a bit unclear in this context. So I added an extra line to the solution like the below, please correct me if I'm wrong:</paragraph><pre>Input: An input sequence w, consisting of tokens w[1] ... w[n]\n//initialization\nInitialize pi[0,start] = 1\nFor each tag t in T, initialize pi[0,t] = 0\n\n//main loop\nfor k = 1 ... n:\n    for each tag t in T:\n      <bold><underline>for each tag s in T:</underline></bold>\n        if pi[k-1, s] &gt; 0 and  P(t|s) * P(w[k] | t) &gt; 0:\n          pi[k,t] += pi[k-1][s]   \nreturn sum_s pi[n, s]\n</pre><paragraph>Thank you :)</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, you're right. This looks more complete. </paragraph></document>"
    },
    {
        "source": "Ungraded Exercise Question: Transition-based dependency parsing. <document version=\"2.0\"><paragraph>Hi, I have few questions of this ungraded assignment:</paragraph><paragraph>1). For question a, do we must follow the format write in the answer, or it's okay that we write in other way, i.e.: {shift, left-arc_{nsubj}, shift, right-arc_{jobj}, shift, shift, shift, ....}, or just draw the graph like it shows in lecturenote.</paragraph><paragraph>2) For question a again, since it's an \"arc-standard\" transition, so if I remembered correctly, we should finish left-arc then use right-arc, but in this example, we do use a right-arc before we finish left-arc, does that mean it's not a strict rule?</paragraph><paragraph>3) For question b, can we use \"arc-eager\" transition or we still have to use \"arc-standard\" transition? If we have to use \"arc-standard\" transition, I got no idea how to do write a different one.</paragraph><paragraph>Thanks in advance!!!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>1) as long as you write down the required sequence of transitions, that would be okay on the exam. But note that it\u2019s difficult to give partial credit if we don\u2019t have intermediate dependency structures (drawing the graph for those is fine as long as the transitions are also listed). \u00a0</paragraph><paragraph>2) that\u2019s not a strict rule \u2014 in fact this relates to your question about part b). In some cases you can reach the correct final dependency tree by creating the left and right children in either order.\u00a0</paragraph><paragraph>3) for part b) you should still use arc standard. I don\u2019t remember if the example sentence in a) can be created in different ways, but you can try to come up with a new (minimal) dependency structure that has an ambiguous transition sequence.\u00a0</paragraph></document>"
    },
    {
        "source": "Linear Model and Feature Functions Questions for Review and Ungraded Exercise. <document version=\"2.0\"><list style=\"unordered\"><list-item><paragraph>Weight bias didn\u2019t understand when the professor was explaining the difference between when setting j = 0 to d vs 1 to d?</paragraph></list-item><list-item><paragraph>For feature function: </paragraph><list style=\"unordered\"><list-item><paragraph><bold>Input:</bold> n-dimensional vector where x is an input object and y is a possible output</paragraph></list-item><list-item><paragraph><bold>Output:</bold> n-dimensional vector feeds into another indicator function, where value is 0 or 1?</paragraph></list-item></list></list-item></list><list style=\"bullet\"><list-item><paragraph>Ungraded exercise: </paragraph><list style=\"bullet\"><list-item><paragraph>Can you please upload the solution for the last portion \"More advanced\"? </paragraph></list-item><list-item><paragraph>For (a), is the answer here pretty open-ended, for instance, if I were to add a condition where wi = Fruit, Ti =N, Ti-1 = V, it would be correct as well? For (b), the vectors are all 1s because all the conditions are true based on the example? </paragraph></list-item></list></list-item></list><paragraph>Thank you! </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>1. Often, linear models are formulate like this: </paragraph><math>(\\sum_{i=1}^dx_iw_i) + b</math><paragraph>Where b is a bias. Instead, it's convenient to just set w_0 = b and then add x_0 =1, so you get </paragraph><math>\\sum_{i=0}^dx_iw_i = (\\sum_{i=1}^d x_i w_i) + 1 \\cdot w_0</math><paragraph>2. Not sure what you mean here. The feature function maps an (x,y) pair to a feature vector. x is the original input object, and y is one possible prediction candidate. The value of the feature function is a vector. The individual values are typically (but not always) binary indicator features. </paragraph><paragraph>3. Not sure I will get to the writeup for the last part before the exam, but it's not really relevant. For a) and b) Correct, there are many different feature functions. The weight vector can be all 1 because only features for the correct output (POS) will fire.</paragraph><paragraph/></document>"
    },
    {
        "source": "Transition-based Parsing Questions for review. <document version=\"2.0\"><list style=\"unordered\"><list-item><paragraph>Why does the divide line mean? e.g. $(\\sigma, w_i | \\beta, A) =&gt; (\\sigma | w_i, \\beta, A)$?</paragraph></list-item><list-item><paragraph>Why during the arc left operation word deleted from the stack side but not the buffer side?</paragraph></list-item><list-item><paragraph>Why during the arc right operation,$w_i$ moved from top of the stack to the buffer?</paragraph></list-item><list-item><paragraph>What is r\u2019 and w in this case? difference between Ad and A? does it has something to do with not moving root to the right until everything\u2019s been parsed essentially? does it mean this rule doesn\u2019t apply to the arc eager example?</paragraph></list-item></list><figure><image src=\"https://static.us.edusercontent.com/files/0ioQRbMXHPTAb7X4AKwzuFB3\" width=\"618\" height=\"95.76953642384106\"/></figure><list style=\"unordered\"><list-item><paragraph>What is lambda in the score equation for the graph-based approach?</paragraph></list-item></list></document>",
        "target": "<document version=\"1.0\"><paragraph>1. The bar | separates the first or last symbol from the remainder of the stack or buffer. w \u2223\u03b2 stands for a buffer whose first token is w (read left-to-right) and \u03c3\u2223w stands for a stack with top (the top is conventionally shown on the right).\u00a0</paragraph><paragraph>2. You only delete the dependent but not the head! If you delete the head, then neither of the tokens would be able to participate in further dependency relations. You would never be able to create more than one edge.\u00a0</paragraph><paragraph>3. The reason the head shifts back to the buffer is that there is no other operation to shift it back. \u00a0You don\u2019t yet know if you may want to attach additional children to the right or to the left of that token, or if you want the token to receive a head to the left or to the right. You can always shift it back to the stack, but you can only move it to the buffer using the right-arc operation.\u00a0</paragraph><paragraph>4. A is the partially built dependency structure that is part of this state. Ad is the gold dependency tree. w\u2019 ans r\u2019 are variables referring to the different tokens and relation labels in Ad. What the condition says is that, before you are allowed to do a right-arc with beta[0] as a dependent, all tokens w\u2019 that are dependents of beta[0] in the gold tree Ad must have already been introduced in A.\u00a0</paragraph><paragraph>5. Lambda is a scoring function computed by some machine learning model, for example a log linear model or a neural net.\u00a0</paragraph></document>"
    },
    {
        "source": "Typo in slide of Arc-right. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I'd like to make sure that there is a typo in the following slide from lecture 8:</paragraph><paragraph>Arc-right should build an edge from the top word on the stack to the next word on the buffer; while the arc-left builds an edge from the next word on the buffer to the top word on the stack.</paragraph><paragraph>Thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ZoJTUfHmAavLUMyCJqf5ngAy\" width=\"450\" height=\"353\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes, good catch. Thank you.\u00a0</paragraph></document>"
    },
    {
        "source": "CVN Instructions for Midterm. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I hope you are doing well.</paragraph><paragraph>When will the CVN instructions for the midterm be released? I am mostly curious about the time frame we have to take the exam within. Most classes I have taken so far provide a 48 hour time period within which we need to take the exam via Proctorio (~ same amount of time as in person students). Is it the same for this course?</paragraph><paragraph>Knowing this will be immensely helpful in planning the next couple of days out.</paragraph><paragraph>Thanks in advance! </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I will send out an email in a few minutes. You have the option of either taking the exam synchronously on Zoom, or using the CVN Proctorio system (like you have done in the past). </paragraph><paragraph>For the asynchronous option, the exam will be open until Wednesday night. </paragraph></document>"
    },
    {
        "source": "Logit Function. <document version=\"2.0\"><paragraph>Hi, I had a question about the relationship between the logit and sigmoid function presented in lecture. I am a bit confused as to why we are able to interpret </paragraph><math>z\\ =\\ \\sum_{ }^{ }x\\cdot w</math><paragraph>as the log-odds. I understand how logit and sigmoid are inverses of each other, however this fact seemed like it is making an assumption. Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes, we are assuming that the log-odds are a linear function of the x values. Logistic regression is sort of the simplest model you can assume if you want the output to be a probability, but there are other options.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Ungraded HW dependency parsing question b. <document version=\"2.0\"><paragraph>Question b asked us to show that the arc standard sequences are not unique, and there is no answer to this question on the posted solution page. Could you explain the way to answer this question? Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>You would have to provide two different transition sequences resulting in the same dependency tree. </paragraph></document>"
        },
        {
            "source": "Ungraded HW dependency parsing question b. <document version=\"2.0\"><paragraph>Question b asked us to show that the arc standard sequences are not unique, and there is no answer to this question on the posted solution page. Could you explain the way to answer this question? Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>If it helps, looking specifically at transitions we can do to build the relations between <code>he</code>,<code>sent</code> and <code>sent</code>,<code>her</code> made things clearer for me.</paragraph></document>"
        }
    ],
    {
        "source": "Ungraded Exercise HMM. <document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/o5QOzIRsR1bStoNfpNtuldwg\" width=\"658\" height=\"133.5641791044776\"/></figure><paragraph>In this question, Why do we need to compute the conditional probabilities?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Because the parameters of the trigram model are specified as conditional probabilities. In other words, you want P(flies | START, time) instead of P(START time flies). </paragraph></document>"
    },
    {
        "source": "Question on Ungraded Exercise: CKY parsing. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>I am wondering if the order of the right hand side matter here. For example:</paragraph><paragraph>S -&gt; NP V<break/>VP -&gt; V NP</paragraph><paragraph>We record S -&gt; NP (cell 1-2) and V (cell 2-3) in the solution, however we do not record rule VP -&gt; V (cell 2 - 3) NP (cell - 2). Just want to make sure that VP -&gt; V NP # VP -&gt; NP V.</paragraph><paragraph>Thank you,</paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, the order matters!</paragraph><paragraph>Otherwise, you would be able to swap the left and right constituent and that would give you \"ungrammatical\" sentences. Instead of \"the dog eats the bone\" you would be able to do things like \"the dog the bone eats\". </paragraph></document>"
    },
    {
        "source": "COVID during Exam 1. <document version=\"2.0\"><paragraph>Hi Professor and TAs,</paragraph><paragraph>I just tested positive for COVID and have been running a fever and experiencing symptoms since yesterday. I am not sure if I will be able to take Monday's exam in-person due to the isolation protocol and my current symptoms - I was wondering how or when I can take the exam in this case? I apologize if this is the wrong place for this question, and I am happy to follow up through email and copy my class dean/advisor and provide a doctor's note if needed. Thank you for any help.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi sorry for the slow response. Typically email is better for questions like this one. </paragraph><paragraph>If you feel well enough to take the exam, you can take it on Zoom. </paragraph><paragraph>Otherwise, we can try to schedule a make-up exam once you have recovered. </paragraph></document>"
    },
    {
        "source": "Midterm Topics. <document version=\"2.0\"><paragraph>In Monday's lecture, Professor went through a list of topics to know for the midterm but I wasn't able to find the file. Where can I find that document?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>This is listed under the Pages section in Courseworks.</paragraph></document>"
    },
    {
        "source": "HW2 Return Type. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I just realized that in my verify_grammar code for part 1 of the coding assignment, I return 2 things - a boolean true or false and a sentence that pinpoints exactly what causes the PCFG to not be in CNF because the assignment said to print out an error or confirmation message. Depending on how this code will be tested, would it be better to resubmit and take the 20 point deduction (same worth as the part) or is it possible to get partial credit on this?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>This is certainly not worth a 20 point deduction. I don\u2019t recommend resubmitting.</paragraph></document>"
    },
    {
        "source": "hw2 submission. <document version=\"2.0\"><paragraph>Hi, I made two submissions for hw2 as I realized that I didn't upload the most updated zip file for my first submission, which was on time. The second submission was 40 min late.</paragraph><paragraph>I made some minor updates to my codes and did more testing in my second submission, but there shouldn't be that much of a difference. Is it OK to grade whichever submission that gives me the most points? If late penalty was gonna apply to my second submission, could you grade my first submission instead? Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>No worries. That\u2019s well within the grace period.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Question on N-gram ungraded excercises?. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>I am wondering why END symbol is included in the calculation of V, but not START sumbol?</paragraph><paragraph>Tysm!!!</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>The START symbol is never predicted by the model and should therefore not have a probability. It\u2019s therefore not a word in the lexicon. In other words, the START symbol is not one of the possibilities for the next word that the model would consider.\u00a0</paragraph></document>"
        },
        {
            "source": "Question on N-gram ungraded excercises?. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>I am wondering why END symbol is included in the calculation of V, but not START sumbol?</paragraph><paragraph>Tysm!!!</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>I would also like to share that It is essential for the model to predict END symbol too, because the model must know how significant it is for END token to occur with other tokens (certain tokens, such as \u2018.\u2019 (Full stop)). </paragraph><paragraph>\u00a0That\u2019s why it can be considered just as any other token, as it helps with the predictions.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Syntax Error when Evaluating Parser. <document version=\"2.0\"><paragraph>Hello NLP Teaching Team.</paragraph><paragraph>I am encountering this SyntaxError when I attempt to evaluate my parser, although am unsure why it is incorrect? I am using Spyder (Python 3.10).</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/GCYMOWpwjEtBN9qaM7EorlU0\" width=\"574\" height=\"161.40409683426444\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>This happens because you are running the command in the Python console, not the terminal.\u00a0</paragraph><paragraph>One way of running it from within Python is to use<break/>\u201c! python evaluate_parser.py \u2026\u201d</paragraph><paragraph>The ! runs a shell command in the Python console.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Submission Format. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Sorry, just to check, should we submit the assignment as 3 separate zip files, or put them all into a folder and then compress the entire folder? Thanks in advance! </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I believe we are to take those 3 files and put them into a singular .zip folder:<break/><break/>\"Pack these files together in a zip or tgz file as described on top of this page. <bold>Please follow the submission instructions precisely!</bold> \"</paragraph></document>"
        },
        {
            "source": "Submission Format. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Sorry, just to check, should we submit the assignment as 3 separate zip files, or put them all into a folder and then compress the entire folder? Thanks in advance! </paragraph><paragraph/></document>",
            "target": "<document version=\"1.0\"><paragraph>Put them all in a folder and then zip and submit the folder.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Tips on Error Checking Grammars. <document version=\"2.0\"><paragraph>Hi everyone, </paragraph><paragraph>I see the assignment says to try and create small toy grammars to error check with and I was just wondering if there are any good examples or tips when it comes to making these grammars (i.e. covering all the potential edge cases).</paragraph><paragraph>Thank you!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Not sure about edge cases, but you want some limited amount of ambiguity in the grammar for your test sentence. Three example from the lecture notes is a good starting point.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2: Tricky Bug on p3,p4 - Couldn't figure out with one of the TAs. <document version=\"2.0\"><pre>    def parse_with_backpointers(self, tokens):\n        \"\"\"\n        Parse the input tokens and return a parse table and a probability table.\n        \"\"\"\n        # TODO, part 3\n        n = len(tokens)\n\n        # Create parse table and probability table as dictionaries\n        table = {}\n        probs = {}\n\n        # Initialize the tables\n        for i in range(n):\n            for j in range(i, n):\n                table[(i, j)] = {}\n                probs[(i, j)] = {}\n\n        # Fill in the diagonal of the parse table with nonterminals that can produce terminals\n        for i in range(n):\n            word = tokens[i]\n            for lhs, rhs, prob in self.grammar.rhs_to_rules.get((word,), []):\n                table[(i, i)][lhs] = word\n                probs[(i, i)][lhs] = math.log(prob)\n\n        # Fill in the upper triangle of the parse table\n        for length in range(2, n + 1):\n            for i in range(n - length + 1):\n                j = i + length -1\n#                 j = i +length\n                for k in range(i, j):# i\n                    for lhs1 in table[(i, k)]:\n                        for lhs2 in table[(k +1, j)]: #k+1\n                            # Check if there are rules that can combine lhs1 and lhs2\n                            for lhs, rhs, prob in self.grammar.rhs_to_rules.get((lhs1, lhs2), []):\n                                new_prob = math.log(prob) + probs[(i, k)][lhs1] + probs[(k+1 , j)][lhs2] #k+1\n                                if lhs not in table[(i, j)] or new_prob &gt; probs[(i, j)][lhs]:\n                                    table[(i, j)][lhs] = ((lhs1, i, k), (lhs2, k+1 , j)) #k+1\n                                    probs[(i, j)][lhs] = new_prob\n        \n        return table, probs\n\ndef get_tree(chart, i,j, nt): \n    \"\"\"\n    Return the parse-tree rooted in non-terminal nt and covering span i,j.\n    \"\"\"\n    # Check if the span is valid and if the non-terminal exists in the chart\n    if (i, j) not in chart:\n        print(f\"Chart at ({i}, {j}) does not exist.\")\n        return None\n\n    if nt not in chart[(i, j)]:\n        print(f\"Chart at ({i}, {j}) does not contain non-terminal {nt}\")\n        return None\n\n    # Retrieve the backpointer for the non-terminal\n    backpointer = chart[(i, j - 1)][nt]\n\n    # If the backpointer is a string (terminal), return it as a terminal node\n    if isinstance(backpointer, str):\n        return (nt, backpointer)\n\n    # If the backpointer is a pair of backpointers, recursively construct the tree\n    left_child, right_child = backpointer\n    left_tree = get_tree(chart, left_child[1], left_child[2], left_child[0])\n    right_tree = get_tree(chart, right_child[1], right_child[2], right_child[0])\n\n    # Return a tuple representing the parse tree\n    return (nt, left_tree, right_tree)\n \n \n for the backpointers function, I'm getting \n \n True for both check table and check probs \n \n output: \n ({(0, 0): {'NP': 'miami'},\n  (0, 1): {'NP': (('NP', 0, 0), ('FLIGHTS', 1, 1)),\n   'FRAG': (('NP', 0, 0), ('NP', 1, 1)),\n   'FRAGBAR': (('NP', 0, 0), ('NP', 1, 1)),\n   'SQBAR': (('NP', 0, 0), ('NP', 1, 1)),\n   'VPBAR': (('NP', 0, 0), ('NP', 1, 1))},\n  (0, 2): {'FRAG': (('NP', 0, 0), ('FRAGBAR', 1, 2)),\n   'FRAGBAR': (('NP', 0, 0), ('FRAGBAR', 1, 2)),\n   'NP': (('NP', 0, 1), ('NP', 2, 2)),\n   'SQBAR': (('NP', 0, 0), ('SQBAR', 1, 2)),\n   'VPBAR': (('NP', 0, 0), ('VPBAR', 1, 2))},\n  (0, 3): {'FRAG': (('NP', 0, 0), ('FRAGBAR', 1, 3)),\n   'FRAGBAR': (('NP', 0, 0), ('FRAGBAR', 1, 3)),\n   'NP': (('NP', 0, 1), ('NP', 2, 3)),\n   'SQBAR': (('NP', 0, 0), ('SQBAR', 1, 3)),\n   'VPBAR': (('NP', 0, 0), ('VPBAR', 1, 3))},\n  (0, 4): {},\n  (0, 5): {},\n  (1, 1): {'FLIGHTS': 'flights', 'NP': 'flights'},\n  (1, 2): {'FRAG': (('NP', 1, 1), ('NP', 2, 2)),\n   'FRAGBAR': (('NP', 1, 1), ('NP', 2, 2)),\n   'NP': (('NP', 1, 1), ('NP', 2, 2)),\n   'SQBAR': (('NP', 1, 1), ('NP', 2, 2)),\n   'VPBAR': (('NP', 1, 1), ('NP', 2, 2))},\n  (1, 3): {'FRAG': (('NP', 1, 1), ('NP', 2, 3)),\n   'FRAGBAR': (('NP', 1, 1), ('NP', 2, 3)),\n   'NP': (('NP', 1, 1), ('NP', 2, 3)),\n   'SQBAR': (('NP', 1, 1), ('SQBAR', 2, 3)),\n   'VPBAR': (('NP', 1, 1), ('NP', 2, 3))},\n  (1, 4): {},\n  (1, 5): {},\n  (2, 2): {'CLEVELAND': 'cleveland', 'NP': 'cleveland'},\n  (2, 3): {'FRAG': (('NP', 2, 2), ('PP', 3, 3)),\n   'NP': (('NP', 2, 2), ('PP', 3, 3)),\n   'SQBAR': (('NP', 2, 2), ('PP', 3, 3)),\n   'VPBAR': (('NP', 2, 2), ('PP', 3, 3))},\n  (2, 4): {},\n  (2, 5): {},\n  (3, 3): {'FROM': 'from', 'PP': 'from'},\n  (3, 4): {},\n  (3, 5): {},\n  (4, 4): {'TO': 'to', 'X': 'to'},\n  (4, 5): {},\n  (5, 5): {'PUN': '.'}},\n {(0, 0): {'NP': -4.706522680248739},\n  (0, 1): {'NP': -12.121095561599688,\n   'FRAG': -10.17027139774193,\n   'FRAGBAR': -10.466537213884134,\n   'SQBAR': -10.240986922539397,\n   'VPBAR': -8.54766752612518},\n  (0, 2): {'FRAG': -16.02265925150763,\n   'FRAGBAR': -16.318925067651136,\n   'NP': -20.683040775760592,\n   'SQBAR': -15.549370753841544,\n   'VPBAR': -16.030803078912392},\n  (0, 3): {'FRAG': -26.066987069809162,\n   'FRAGBAR': -26.36325288595267,\n   'NP': -30.727368594062128,\n   'SQBAR': -24.882202252912503,\n   'VPBAR': -26.075130897213928},\n  (0, 4): {},\n  (0, 5): {},\n  (1, 1): {'FLIGHTS': 0.0, 'NP': -3.195065176174159},\n  (1, 2): {'FRAG': -9.4443343943576,\n   'FRAGBAR': -9.740600210499807,\n   'NP': -11.757010390335061,\n   'SQBAR': -9.515049919155068,\n   'VPBAR': -7.82173052274085},\n  (1, 3): {'FRAG': -19.488662212659136,\n   'FRAGBAR': -19.78492802880134,\n   'NP': -21.801338208636594,\n   'SQBAR': -18.847881418226027,\n   'VPBAR': -17.866058341042386},\n  (1, 4): {},\n  (1, 5): {},\n  (2, 2): {'CLEVELAND': 0.0, 'NP': -3.9805856768644103},\n  (2, 3): {'FRAG': -12.020710341309226,\n   'NP': -14.024913495165945,\n   'SQBAR': -14.325018087614133,\n   'VPBAR': -13.631870907056262},\n  (2, 4): {},\n  (2, 5): {},\n  (3, 3): {'FROM': 0.0, 'PP': -6.618738983514369},\n  (3, 4): {},\n  (3, 5): {},\n  (4, 4): {'TO': 0.0, 'X': -1.2527629684963681},\n  (4, 5): {},\n  (5, 5): {'PUN': 0.0}})\n  \n  But for get trees, I keep getting: \n  print(get_tree(table, 0, len(toks) - 1, grammar.startsymbol))\n  Chart at (0, 5) does not contain non-terminal TOP\nNone</pre><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>We discussed this during my office hours. I\u2019d be interested to hear what the issue was eventually.\u00a0</paragraph></document>"
    },
    {
        "source": "Late Policy. <document version=\"2.0\"><paragraph>Hi, I just wanted to clarify what the late policy for this class is? On the syllabus it says we can submit a maximum of 4 days late with a 20 point deduction, does that mean 5 points are deducted each day, or the deduction happens as soon as it becomes late?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>There is a one-time deduction of 20 points for any late submission. </paragraph><paragraph>Please reach out to me if you think your circumstances merit an extension. </paragraph></document>"
    },
    {
        "source": "HW 2 Part 3. <document version=\"2.0\"><paragraph>I was wondering if someone could clarify why table[(2,3)][\"FLIGHTS\"] should be flights? Using my understanding, using the hw example with tokens of ['flights', 'from', 'miami', 'to', 'cleveland', '.']\u00a0, table[0][1][\"FLIGHTS\"] would be flights but table[(2,3)] should only have the rules that have miami  on the right side? Additionally, is there any particular way we should map the \"parent\" indices for terminals? Ex: for the case of table[(0,1)'][\"Flights\"], what should we set this equal to? Thanks so much!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>So, some of the examples in the assignment description exist only to illustrate the format of the table, not the specific values in a specific example. </paragraph><paragraph>I don't quite understand what you mean by \"parent indices\". table[(0,1)'][\"Flights\"] should be \"flights\". </paragraph><paragraph/></document>"
    },
    {
        "source": "Will there be ungraded/ practice assignment for lecture 11?. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>I am wondering if there will be ungraded/ practice assignment for lecture 11.</paragraph><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I\u2019ll try to get one up, but it doesn\u2019t currently exist.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 P1: Just want to double-check that helper function is allowed. <document version=\"2.0\"><paragraph>For checking the lhs_to_rules and rhs_to_rules, I use the same helper function to reduce the redundancy in code. I just want to double make sure that helper function is allowed for this assignment :)</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s fine.\u00a0</paragraph></document>"
    },
    {
        "source": "What are the acceptable ranges for scores in HW2?. <document version=\"2.0\"><paragraph>What are the acceptable ranges for Coverage, Average F-score (parsed sentences), Average F-score (all sentences) in part 5 of hw2? Is there any way to improve the Average F-score (parsed sentences)?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>What is it now? There shouldn\u2019t really be too much variation on these.\u00a0</paragraph></document>"
    },
    {
        "source": "Part 5 F-Score. <document version=\"2.0\"><paragraph>Hi, I wonder if anyone got:</paragraph><paragraph>Coverage: 67.24%, Average F-score (parsed sentences): 0.9526771952649747, Average F-score (all sentences): 0.6405932864712761</paragraph><paragraph>Is it considered as a acceptable result? Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Got the same, pretty sure this matches the performance of Dr. Bauer\u2019s solution\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 P3:. <document version=\"2.0\"><paragraph>For assigning the inner dictionary values of span length 1, since it still needs to be a pair would I make the two instances in the pair the same since it is only based off the one word. For example, when assigning the inner dictionary values for the first word 'flights' in a sentence, the inner dictionary would look like {'flights': ((0,1,'NP'), (0,1,'NP'))}.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>\u2018flights\u2019 should never be a key in the dictionary.\u00a0<break/>Instead, the dictionary might look something like this {(0,1): {\u2018NP\u2019: \u2018flights\u2019}}</paragraph></document>"
    },
    {
        "source": "Not Recognizing grammar.py File. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>While working on part 3, my visual studio code stopped processing any changes I made. When I closed and reopened the window, I got the below error. What should I do to fix this? Thanks!</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/7QBPUpAAjjUdThCKDOe1F8fN\" width=\"658\" height=\"70.72380952380952\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>Make sure grammar.py is in the same directory. \u00a0You probably done any to edit this code in your Downloads directory because that\u2019s prone to being deleted.\u00a0</paragraph></document>"
    },
    {
        "source": "Ungraded n-Gram Language Models exercise. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>Will something similar to the inductive proof in n-Gram Language Models be tested in midterm or final exam?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No there will not be any proofs.\u00a0</paragraph></document>"
    },
    {
        "source": "CYK Algorithm Question. <document version=\"2.0\"><paragraph>Hello, apologies if this was already answered somewhere. Let us consider the following situation:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1Z8PzyIPD8OA7gAxec0dtNt7\" width=\"658\" height=\"611.7230769230769\"/></figure><paragraph>If there was such a rule in the form \"S --&gt; NP\", would an S be added to pi[0, 3]? Or would nothing be added, since the Cartesian product of a set with the empty set is always the empty set?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>S --&gt; NP would not be a valid rule according to chomsky normal form.</paragraph></document>"
    },
    {
        "source": "F-score for part 5. <document version=\"2.0\"><paragraph>Upon running evaluate_parser, I'm getting values that are somewhat lower than expected:</paragraph><paragraph>Coverage: 62.07%, Average F-score (parsed sentences): 0.876237979991289, Average F-score (all sentences): 0.5438718496497656</paragraph><paragraph>Does anyone know where I should look to correct this?  Will I need to trace out the parses to see what's going wrong?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I suspect that you only keep the highest scoring nonterminal for each span, non the highest-scoring split for <underline>each</underline> nonterminal.  </paragraph><paragraph/></document>"
    },
    {
        "source": "HW2 Q1 Start Symbol. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>In HW2 Q1, suppose S is the start symbol, and A, B are nonliterals, are rules such as A -&gt; SB or A -&gt; BS allowed? These rules do not seem to make sense, but they do not seem to contradict the rules of CFG.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>These rules would be permitted. The start symbol is just another nonterminal. In fact, it's okay for the start symbol to appear on the right hand side of a rule. </paragraph></document>"
    },
    {
        "source": "HW2 print all results. <document version=\"2.0\"><paragraph>Hi, just want to confirm whether should we print all results of functions in HW2, like shown in the assignment example code chunks (also for part 5), or just complete the functions and essential output?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>The intermediate results on the assignment description are just shown for illustration purposes. They do not have to be part of the output of your submission. </paragraph></document>"
    },
    {
        "source": "Midterm Preparation Tips. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I was wondering if the teaching team (or anyone else) had any good tips to prepare for the midterm next week? Would it be most helpful to review previous lectures or HW assignments? Are there any other resources you recommend we look at?</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph> I think ungraded exercises are helpful to practice for the exam. Also, seek advice/helpful resources.</paragraph><paragraph/></document>"
    },
    {
        "source": "Verify Grammar for HW 2. <document version=\"2.0\"><paragraph>Hello!</paragraph><paragraph/><paragraph>I hope you are all doing well. I had a question regarding the first part on the homework. Would zeros and punctuation be counted as terminals? For example, if I had PP-&gt; '0' or NP-&gt; '.', would this be considered valid CNF Form?</paragraph><paragraph/><paragraph>Thank you so much for your help!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, these would be counted as terminals. Anything that never appears on the LHS of any rule is a terminal. </paragraph></document>"
    },
    {
        "source": "HW2 P1: Difference from 1.0 Question. <document version=\"2.0\"><paragraph>According to the HW spec:</paragraph><paragraph>\"Because of numeric inaccuracies the sum may not be exactly 1.0.\"</paragraph><paragraph>When using the isclose method, can we leave the maximum allowed difference as default, or what should be the accepted difference from 1.0?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>You can use the default tolerance. </paragraph></document>"
    },
    {
        "source": "Ungraded Exercises Solutions. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>May I ask when will the solutions for ungraded exercises in dependency parsing and linear model &amp; feature functions be released?</paragraph><paragraph>Thank you :)</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Here is the solution for the dependency parsing problem:<break/>https://courseworks2.columbia.edu/courses/179333/pages/ungraded-exercise-solution-transition-based-dependency-parsing</paragraph><paragraph>I will post the feature function solution early this week. Sorry for the delay.\u00a0</paragraph></document>"
    },
    {
        "source": "CKY for PCFG parsing. <document version=\"2.0\"><paragraph>In the main loop of the algorithm for PCFG parsing, we have to iterate through all X that belongs to N and all X-&gt;BC that belongs to R. Why can't we do something similar to the original CKY algorithm where we only calculate the probabilities where span(i,k) can form B and span(k,j) can form C? Since for the ones that do not meet this criteria, the probability will be 0 anyways. </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>I\u2019m not sure I fully understand the question. There is a difference between how the algorithm is conceptually defined in pseudocode and how it is implemented.\u00a0</paragraph><paragraph>You could implement the rule lookup by taking all nonterminals with &gt;0 probability in (i,k), and and all nonterminals with &gt;0 probability in (k,j), \u00a0then go through all combinations B C and check it there is a matching A -&gt; B C rule.\u00a0</paragraph></document>"
    },
    {
        "source": "Part 4 (get tree) where nt is not in the table. <document version=\"1.0\"><paragraph>If get_tree is called with a non-terminal that\u2019s not in the tree (for example, calling get_tree(0, len(toks), grammar.startsymbol) on a table of a sentence that is not in the language (and therefore does not contain \u201cTOP\u201d), what should be returned? Is it an empty tuple?\u00a0</paragraph><paragraph>Thanks in advance!\u00a0</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Correct :) the evaluation script catches the KeyError and counts the sentence as not parseable.\u00a0</paragraph></document>"
    },
    {
        "source": "Midterm coverage. <document version=\"1.0\"><paragraph>Hi, I am wondering what is the coverage for our upcoming midterm? Which materials would be included and tested? Thank you.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>#184</paragraph></document>"
    },
    {
        "source": "Mulipying probabilities. <document version=\"2.0\"><paragraph>When calculating probabilities in part 3, Lecture 7 seems to suggest multiplication of the probabilities. However, should we add the log probs instead of multiplying because they are log probs? </paragraph><paragraph>Also, is it enough to multiply by the backpointers and assume that that will propagate all the multiplying across the probabilities?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hello! Not a TA but based on my understanding, yes and yes. Because of the base case initialization, you can think of the probabilities getting multiplied (aka log probs getting added) recursively on each step.</paragraph></document>"
    },
    {
        "source": "Part 3, Stucture of Dictionaries for probablitities and backpointers: What does it mean to only keep the most probable expansion?. <document version=\"2.0\"><paragraph>I'm a little confused by this. In question 3 we are asked to make a data structure that has a dictionary in a dictionary in order to store back pointers and probabilities. My question is about whether we can store all the back pointer possibilities or just the one best backpointer. In other words, should each entry in the dictionary be a dictionary with only one key?</paragraph><paragraph>In the problem it is states \"This value represents the log probability of the <italic>best parse tree (according to the grammar) </italic>for the span 0,3 that results in an NP.\" This seems to suggest as long as we store the best tree for NP mapped to NP we can still have other mappings. For instance</paragraph><paragraph>{Np : {NP : (npsource, num, num), P: ((source, i, k), (source2, k, j)})}</paragraph><paragraph>Or would one need to REMOVE either NP or P from this list in order to only have the biggest one? This seems to be what is suggested <link href=\"https://edstem.org/us/courses/46417/discussion/3618477\">here</link>, but because of the structure of the dictionary is very unintuitive to me. Why store objects in a dictionary when we would only every have one key in every dictionary? Plus most parses have 2 things they point to.<break/></paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I think I understand the wording a bit more now. Am I correct in understanding we ONLY replace those with a better probability that line up the same coordinates AND values. For example if there is already a an NP in (0,2) and we find an NP with a larger probability then we switch them out? But if there is an NP in (0,2) and we found a P with a smaller probability than the NP we could keep both because they are different keys?</paragraph></document>"
    },
    {
        "source": "HW2 P1: Result. <document version=\"2.0\"><paragraph>Curious if the answer we should be getting is a single output stating that the grammar is not valid? Are we expected to return additional outputs?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The main method in part 1 simply needs to print if the grammar is valid or not. The verify_grammar method simply needs to return True or False. No other output is needed.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 Part 5: Coverage 0.00%. <document version=\"2.0\"><paragraph>Not a question, but here's an issue I ran into in Part 5 that will hopefully help some other people as well. For context, I ran the command provided in the instructions <code>python evaluate_parser.py atis3.pcfg atis3_test.ptb</code> , but was getting the following output:</paragraph><pre> Coverage: 0.00%, Average F-score (parsed sentences): 0.950447540861, Average F-score (all sentences): 0.639094036096\n</pre><paragraph>The 0.00% coverage was confusing, especially since the F-scores matched those in the instructions. After looking into the code for <code>evaluate_parser.py</code>, I realized that the code for computing coverage <code>coverage = (parsed / total) *100</code> was operating on two ints (parsed and total), which defaulted to integer division (since my <code>python</code> command still points Python 2). Long story short, use <code>python3</code> for testing <code>evaluate_parser.py</code> , though the regular <code>python</code> command might've been updated to point to Python 3 in newer systems.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Thanks. Good point in general, we assume Python 3 for all assignments.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "HW 2 Part 3 Log Probabilities. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>For Part 3 of HW2, when it says to use \"log probabilities\", are we expected to just use the natural log? Or should we use log to a specific base?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hello! On the previous hw we used <code>math.log2</code> </paragraph></document>"
        },
        {
            "source": "HW 2 Part 3 Log Probabilities. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>For Part 3 of HW2, when it says to use \"log probabilities\", are we expected to just use the natural log? Or should we use log to a specific base?</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>It doesn\u2019t really matter. In my implementation i just used math.log which is the natural logarithm (base e).\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "Is \"TOP\" the equivalence for sentence in HW2?. <document version=\"2.0\"><paragraph>Hello! I'm doing the assignment and I am trying to understand what 'TOP' means. In this case, is 'TOP' what we want to find in our table[0, len(tokens)] at the end of the CKY algorithm? Meaning, is 'TOP' the non-terminal that originates a sentence within the language? I'm confused because we also have 'S' as a non-terminal in the rules.<break/><break/>Thank you!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>TOP is the startsymbol in this particular grammar. So unless you find TOP in the span for the entire input, the sentence cannot be parsed.\u00a0</paragraph><paragraph>You should probably use grammar.startsymbol instead of hardcoding a specific startsymbol.\u00a0</paragraph></document>"
    },
    {
        "source": "HW3 release. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Would HW3 be release this week or will be release during the week of Midterm 1? Thank you!</paragraph><paragraph>Best, </paragraph><paragraph>Adrian Harischand </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I\u2019m planning to release homework 3 toward the end of this week, after the due date for homework 2.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2: Question about the example in Part 3. <document version=\"2.0\"><paragraph>In the description of part 3, it is given that \"<italic>the value of table[(0,3)]['NP'] could be ((\"NP\",0,2),(\"FLIGHTS\",2,3))\",</italic> and \"<italic>the table entry for table[(2,3)][\"FLIGHTS\"] should be \"flights.\"</italic> Are these table entries for the tokens ['flights', 'from','miami', 'to', 'cleveland','.']? If we run <italic>parse_with_backpointers</italic> on the above list of tokens, should we get the same entries in the table?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>No the table entries do not match that example sentence. It\u2019s just an illustration of what the table format should look like.\u00a0</paragraph></document>"
    },
    {
        "source": "HW Part 5 Evaluate Parser score check. <document version=\"2.0\"><paragraph>Dear Professor,</paragraph><paragraph>I am getting the following scores. On rounding them the scores are roughly matching.</paragraph><paragraph>Coverage: 67.24%, Average F-score (parsed sentences): 0.9504475408614075, Average F-score (all sentences): 0.6390940360964636</paragraph><paragraph>Just wanted to confirm if these are fine.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, we have the same scores, refer to here: https://edstem.org/us/courses/46417/discussion/3606779?answer=8303893</paragraph></document>"
    },
    {
        "source": "HW2 part 1 question. <document version=\"2.0\"><paragraph>For the verify grammar problem, will there ever be a scenario where the inputted lhs has multiple words?  If so, do we just consider it as one word (since it is one string) and it is valid, or is it considered as multiple words and it is invalid?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No that can\u2019t happen. There will always be a single LHS symbol.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Verify Grammar Questions. <document version=\"2.0\"><paragraph>1. is it okay if there are many for loops in verify grammar and running it on a bit dataset takes some time? I'm talking like 10 seconds</paragraph><paragraph>2. In order for the grammar to be verified, all these following the only things we need to check?<break/></paragraph><paragraph>A) from the slides lecture 7 page 4, the node goes to either non-terminals or a terminal<break/>   (also does the amount of terminals/non-terminals matter? Should it be going to ONLY 2 non-terminals or 1 terminal? Or  could it go to 4 non-terminals or 1 non-terminal or 3 terminals? The 2 and 1 rules seems to be true for the grammar we are getting and the rules on the slides I just want to check)</paragraph><paragraph>B) all of the probabilities when going to non-terminal nodes add up to 1</paragraph><paragraph/><paragraph>if A) and B) are true can we assume this is a good grammar? Thanks</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Hello! Not a TA but my understanding is we're checking if it's in Chomsky Normal Form, meaning every rule maps from a non-terminal to a single terminal, or from a non-terminal to two nonterminals.</paragraph><paragraph>As far as I know, if every rule is in the form <code>A -&gt; B C</code> or <code>A -&gt; b</code> and the probabilities add up it should be good by the homework specs.</paragraph><paragraph>Also, if all we're checking is the above conditions I'm pretty sure <code>verify_grammar()</code> can be O(n) for n production rules. I ran my implementation on my pretty old laptop on atis3.pcfg and it was quite a bit less than a second, maybe make sure you're using the provided dictionaries correctly?</paragraph><paragraph/></document>"
        },
        {
            "source": "Verify Grammar Questions. <document version=\"2.0\"><paragraph>1. is it okay if there are many for loops in verify grammar and running it on a bit dataset takes some time? I'm talking like 10 seconds</paragraph><paragraph>2. In order for the grammar to be verified, all these following the only things we need to check?<break/></paragraph><paragraph>A) from the slides lecture 7 page 4, the node goes to either non-terminals or a terminal<break/>   (also does the amount of terminals/non-terminals matter? Should it be going to ONLY 2 non-terminals or 1 terminal? Or  could it go to 4 non-terminals or 1 non-terminal or 3 terminals? The 2 and 1 rules seems to be true for the grammar we are getting and the rules on the slides I just want to check)</paragraph><paragraph>B) all of the probabilities when going to non-terminal nodes add up to 1</paragraph><paragraph/><paragraph>if A) and B) are true can we assume this is a good grammar? Thanks</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>I\u2019m not a TA, but here\u2019s my understanding.\u00a0</paragraph><paragraph>Chomsky Normal Form allows production rules only of the form where there are two non-terminals or one terminal on the right hand side. The left hand side is always a Non-terminal.\u00a0</paragraph><paragraph>The probabilities for each of the non-terminals should add up to 1. Sometimes they might not add up exactly to 1, so the assignment suggests using math.isclose() for verification.\u00a0<break/></paragraph></document>"
        }
    ],
    [
        {
            "source": "Getting 0.0 for all.. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm getting all zeros.</paragraph><paragraph>Coverage: 0.00%, Average F-score (parsed sentences): 0.0, Average F-score (all sentences): 0.0</paragraph><paragraph>I'm trying to find what's wrong with my setting but have no clue.</paragraph><paragraph>Does anyone have the same problem and can give a hint about which part might have gone wrong?</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I'd look into evaluate_parser.py in the actual <code>evaluate_parser()</code>, and maybe add some logging? There's a chance you're throwing a keyerror or something similar every time</paragraph></document>"
        },
        {
            "source": "Getting 0.0 for all.. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm getting all zeros.</paragraph><paragraph>Coverage: 0.00%, Average F-score (parsed sentences): 0.0, Average F-score (all sentences): 0.0</paragraph><paragraph>I'm trying to find what's wrong with my setting but have no clue.</paragraph><paragraph>Does anyone have the same problem and can give a hint about which part might have gone wrong?</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>It\u2019s possible to at the output format is correct. Make sure each tree returned is a nested tuple.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "question about #175. <document version=\"2.0\"><paragraph>Is a TA able to endorse the answer to #175 please? I have the same question of can you assume all lhs are in the right format of only one nonterminal since you assume lhs make up inventory of nonterminals </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>I just responded to that thread. The student response is correct. \u00a0</paragraph></document>"
    },
    {
        "source": "HW2 F-scores for parsed and all sentences are the same. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>As title, I'm getting ~0.63 as the both F-scores, and they're exactly the same. The scores for every sentence are the same too. Any idea why? Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I suspect that the evaluation script is not registering when a sentence cannot be parsed. It expects that in such cases, the get_tree method throws a KeyError (because the start symbol will not be in the table for the full span). It likely also accepts None for unparseable sentences, but I'm not quite sure I remember correctly. </paragraph></document>"
    },
    {
        "source": "len(bps) != 2 table check. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Correct me if I misunderstood, but according to #173, as long as our get_tree consistently works does it matter if we continue to receive the len(bps) != 2 error? My get_tree continues to work despite me receiving this error</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>The problem is likely in the way you represent terminal symbols in the table. The scripts expects that these are plain strings -- not, for example, single element tuples with a string in it, such as ('flights',).</paragraph><paragraph>I would recommend you try to fix this before submitting, but as I mentioned in #173 it doesn't really matter. If you choose a different representation for terminals, I recommend you adapt the checker function, to accept the new format. </paragraph><paragraph/></document>"
    },
    {
        "source": "Follow up questions regarding HW1 solution. <document version=\"2.0\"><paragraph>I have 3 questions regarding our hw1 solution.<break/><break/>1. In part 3) raw_bigram_probability function, Is the expected return value for input bigram <italic>('START','START')</italic> 0?<break/></paragraph><paragraph>2. In part 3) raw_bigram_probability function, why are we returning bigramcounts/total#ofwords like below? Aren't we supposed to return unigram probability for v (which in this example is bigram[1]) so P(v | u,w) = P(v)?<break/><break/><italic>if self.unigramcounts[(bigram[0],)] == 0: return self.bigramcounts[bigram] / float(self.total_num_words)</italic><break/><break/>3. In part 3)  raw_trigram_probability function, why are we handling unseen token by replacing its probability with uniform distribution if we have already replaced all the unseen tokens with UNK? I feel like its a redundant step.<break/><break/><italic>if trigram_count == 0: <break/># We don't know anything about the distribution of the trigram[2]<break/># in the context of trigram[0], trigram[1] -- assume<break/># uniform distribution<break/>return 1 / self.total_num_words</italic></paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>1. That's correct!</paragraph><paragraph>2. I'm not sure I understand your question. When <italic>self.unigramcounts[(bigram[0],)] == 0,</italic> for instance when we have ('START', .. ), we want to ensure the denominator isn't 0 and thus we use <italic>self.bigramcounts[bigram] / float(self.total_num_words)</italic> (just like we use <italic>self.trigramcounts[trigram]</italic> for trigrams or <italic>self.unigramcounts[unigram]</italic> for unigrams in the numerator)</paragraph><paragraph>3. There's a difference between unseen tokens and unseen context. You have to handle that edge case by returning  <italic>1 / self.total_num_words</italic>. Please see <link href=\"https://edstem.org/us/courses/46417/discussion/3456167\">#15</link> as the difference was explained well there!</paragraph><paragraph>Hope that helps!</paragraph></document>"
    },
    [
        {
            "source": "HW2 Part 5: Reasonable F-score values. <document version=\"2.0\"><paragraph>Hello! I'm currently running into slightly-lower-than-expected F-scores, does anyone have any tips for areas to debug? I'm going to check things like &gt; vs &gt;= but other than that I'm not sure what to look for. </paragraph><paragraph>Coverage: 67.24%, Average F-score (parsed sentences): 0.9248795500760255, Average F-score (all sentences): 0.6219017664304309</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>I managed to match my score with the assignment by going through those tokens where didn\u2019t match 100% running evaluate_parser.py.</paragraph><paragraph>Taking those non 100% matches tokens, using a debugger and check the backtracking and prob tables generated to see why your get_parse_tree is not returning the expected tree.\u00a0</paragraph><paragraph>Check to see is the divergence caused by the data or the code.\u00a0</paragraph></document>"
        },
        {
            "source": "HW2 Part 5: Reasonable F-score values. <document version=\"2.0\"><paragraph>Hello! I'm currently running into slightly-lower-than-expected F-scores, does anyone have any tips for areas to debug? I'm going to check things like &gt; vs &gt;= but other than that I'm not sure what to look for. </paragraph><paragraph>Coverage: 67.24%, Average F-score (parsed sentences): 0.9248795500760255, Average F-score (all sentences): 0.6219017664304309</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>If anyone ends up with similar numbers to mine here, make sure you're checking the probabilities from the left and right symbols when picking the new parse, not just the probability for the new parsed nonterminal generating the left and right symbols. I missed that somehow and was just checking the probability of <code>new parse -&gt; left_symbol right_symbol</code>.</paragraph></document>"
        }
    ],
    {
        "source": "Part 4. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Are we suppose to utilize the probabilities in building our parse tree for part 4 or should  the table from part 3  contain the most probable parse for the given sentence which means that all we have to do in part 4 is just build that parse tree without having to worry about checking the probability of the subtrees? </paragraph><paragraph>Thank you!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>The table in part 3 will only contain the single most-probably expansion for each span and nonterminal. So in part 4 you do not have to consider the probabilities. You can simply traverse the table and assemble the tree, which is unique. </paragraph></document>"
    },
    {
        "source": "Midterm Material Cutoff. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs - </paragraph><paragraph>As exam 1 come close, I am wondering what is the material cutoff for review? Thank you so much</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Good question. I will upload a topic overview sheet and we will go through this in class next week. </paragraph><paragraph>We will cover everything up to and including the material of <underline>this</underline> week (the last topic word2vec). The material from next week will not be on the midterm. </paragraph></document>"
    },
    {
        "source": "Question on cky.py main comment. <document version=\"2.0\"><paragraph>Hi NLP teaching staffs -</paragraph><paragraph>I am running cky.py main and ran into an error: \"name chart is not definied\". Upon inspecting the pre-populated code for cky.py, I noticed that chart is not defined anywhere. I updated the given code to <bold>check_table_format(table)</bold> and it works. I am wondering if this is the correct fix or if there is anything wrong with my code?</paragraph><paragraph>Thank you so much!</paragraph><paragraph>cky.py main code:</paragraph><paragraph>#print(parser.is_in_language(toks))</paragraph><paragraph>#table,probs = parser.parse_with_backpointers(toks)</paragraph><paragraph>#assert check_table_format(chart) - <bold>should this be assert check_table_format(table) instead?</bold></paragraph><paragraph>#assert check_probs_format(probs)</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hu, odd this wasn't noticed before -- looks like just a typo in the template. Please change \"chart\" to \"table\". </paragraph></document>"
    },
    {
        "source": "HW1 regrade request. <document version=\"2.0\"><paragraph>I know I am supposed to email the TA who graded my assignment for a regrade request, but on courseworks it does not say a TA. It says graded by Professor Bauer. </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That's because I uploaded the grades. But if you look at the bottom of the comment field, it will say \"Grader: Shreyas\" </paragraph></document>"
    },
    [
        {
            "source": "Test part 1. <document version=\"2.0\"><paragraph>Is there any way to test verify grammar other than running on atis3 and getting true in response? </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Sorry, not that I'm aware. </paragraph></document>"
        },
        {
            "source": "Test part 1. <document version=\"2.0\"><paragraph>Is there any way to test verify grammar other than running on atis3 and getting true in response? </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>I would say it could be helpful to try out other grammars: make a toy grammar like the one from lecture (someone posted one on Ed), then break it in different ways to make it not in Chomsky Normal Form and verify that your check fails (don't think we're supposed to log the reason, but you could add logging temporarily, make sure the reason it fails is correct, then delete it later).</paragraph></document>"
        },
        {
            "source": "Test part 1. <document version=\"2.0\"><paragraph>Is there any way to test verify grammar other than running on atis3 and getting true in response? </paragraph><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>You could try constructing a simple toy grammar, and then use grammary.py over it. Just remember that the response to whether your grammar is in Chomsky Normal Form or not depends on how you define the production rules of your grammar.</paragraph></document>"
        }
    ],
    {
        "source": "HW2 Part 4 Print out. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>After implemented part 2, 3, 4, my get_tree function printed out this value: </paragraph><paragraph>('TOP', ('NP', ('NP', ('NP', ('NP', 'flights'), ('PP', 'from')), ('NP', 'miami')), ('PP', ('TO', 'to'), ('NP', 'cleveland'))), ('PUN', '.'))</paragraph><paragraph>instead of the one in homework description like this:</paragraph><pre>('TOP', ('NP', ('NP', 'flights'), ('NPBAR', ('PP', ('FROM', 'from'), ('NP', 'miami')), ('PP', ('TO', 'to'), ('NP', 'cleveland')))), ('PUN', '.'))\n</pre><paragraph>I'm just wondering, is this an accepted format? Or, is there something wrong with my implementation of constructing the table of backpointers? I kind of have no clue about finding the bug. </paragraph><paragraph>Thanks very much for any hint!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Bug resolved! Just for reference, I saved the wrong parse tree because of wrong probabilities.</paragraph></document>"
    },
    {
        "source": "HW Grading. <document version=\"2.0\"><paragraph>I saw the email that the grade has been released, but I checked my grading, it said 0 out of 0 points and a few comments. I downloaded the file and pretty sure that I did every part of the homework. So I am a little confused. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/41yzhnFb5NUnU2ZlOFDdBbNh\" width=\"337\" height=\"349\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>That's surprising. Can you send email to May and cc me so we can look into that for you?</paragraph></document>"
    },
    {
        "source": "check_table_format & Part 5 get tree. <document version=\"2.0\"><paragraph>1. In the last if statement of check_table_format, I noticed that the first element of the tuple is a string, while the second and third elements are integers. This seems to be in line with the assignment's instructions.</paragraph><paragraph>However, the error message states: </paragraph><paragraph>\"It must be a pair ((i, k, A), (k, j, B)).\" </paragraph><paragraph>I believe it should be corrected to: </paragraph><paragraph>\"it must be a pair ((A, i, k), (B, k, j)).\"</paragraph><paragraph>to avoid confusion.<break/><break/>2. I tried to test my <code><bold>get_tree</bold></code> function by implementing Part 5, but there was an issue when I used the test sentence:</paragraph><paragraph>toks = ['with', 'the', 'least', 'expensive', 'fare', '.']</paragraph><paragraph>I realised that this sentence cannot produce a valid parse tree. How should I handle such exceptions? Initially, I had it return None, but the coverage rate was still 100%, which doesn't seem correct. Do we need to check if it can generate a parse tree before we get it?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>1. Thank you. I will make sure to correct this in future iterations if this assignment. \u00a0</paragraph><paragraph>2. I think the evaluation script is looking for an exception, such as a KeyError when the sentence cannot be parsed.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 rhs_to_rules. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>I am curious to see whether the rule ('NP', ('NP', 'VP'), xxx) is the same as ('NP', ('VP', 'NP'), xxx). Note, the order of non-terminal changed. In my implementation, recognizing them as different rules helped to increase the coverage by appx. 4% and the overall F-score by 2%, but the parsed F-score decreased by 2.5%. My original implementation was very close to Prof. Bauer's performance. </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>They are two different rules. The order of the nonterminal matters.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 part1 checklist. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For Part 1:</paragraph><list style=\"ordered\"><list-item><paragraph>We need to check if there are any terminals in <code><bold>lhs_to_rules</bold></code>, right? If so, when I reviewed the pcfg file, all lhs keys were uppercase strings. Can we assume an lhs key is a non-terminal if it is composed of uppercase strings?</paragraph></list-item><list-item><paragraph>Do we just need to print a statement indicating whether the pcfg file is valid (e.g., print \"Valid\" if the above method returns true, and \"Invalid\" if not)?<break/></paragraph></list-item></list><paragraph>Thanks for your help!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>I believe in the assignment, it has been stated that all the symbols present on the LHS are assumed as Non-terminals.\u00a0</paragraph></document>"
    },
    {
        "source": "Arc standard transition. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am not sure when I should right-arc instead of left-arc in arc standard transition.</paragraph><paragraph>Thank you</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Look at the first word on the buffer and the top word on the stack. If the target dependency tree has a left edge between these tokens (buffer[0] to stack[0]), do a left arc. If it has a right edge between the tokens, do a right arc \u2014 but only if all dependents of buffer[0] that appear in the target have already been constructed.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 part 3 table format. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I had problems meeting the table checking where error comes from \"len(bps) != 2\". However, there is also \"Terminal symbols in the table could just be represented as strings. For example the table entry for table[(2,3)][\"FLIGHTS\"] should be \"flights\".\" The statement from the homework handout is different from the checking criteria, what should we do?</paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Hmm odd. Implement it according to the checking criteria when in doubt. But you can really do it either way as long as you are consistent and your get_tree function works.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 Part 3 -- parse_with_backpointers. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For Part 3, should we be calling is_in_language within parse_with_backpointers? Currently, my probs table comes out right but the backpointer table does not. The get_tree functions also outputs the correct solution despite this. Was just wondering if I was missing something. Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No is_in_language uses a different table format, so you do not need to call it in parse_with_backpointers.\u00a0</paragraph><paragraph>Is the problem that the check_table method reports and error but your get_tree method works? What\u2019s the specific error in that case?</paragraph></document>"
    },
    {
        "source": "HW2 part 3 best parse tree. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>In part 3, I am not sure how does the algorithm work to obtain the most probable parse tree. For example when I am focusing on the cell x03, do I check all combinations from x01,x13 and x02,x23 and just pick the combination that results in the highest probability to put into x03, discarding the other options? Therefore, when I move on to cell x04, the previously chosen combination stored in x03 will be used?<break/><break/>Alternatively, is it that for example when I am focusing on the cell x03, for every type of nonterminals, I check all combinations from x01,x13 and x02,x23 and pick the combination that results in the highest probability to put into x03, discarding the other options of the same type of terminals? Such that I result in a set of unique non-terminals that have the highest probability from its own respective types. With this second approach, I am concerned that the table will discard potential combinations that may become more optimal later in building the table.</paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Piggy back onto this, as I am dealing with the same exact question. Alternative to the greedy approach you mentioned would be to build for all possible parse trees and find the single max before returning from part 3 function. But the assignment explicitly mentioned that we should build the backtrack, and prob tables for the best parse tree during the parsing process.\u00a0</paragraph><paragraph>I still couldn\u2019t figure out how can this be done on the fly greedily.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 Part 1 CNF. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>In HW2 part one, the structure of the dataset we received doesn't strictly follow CNF. For example we have  ('PP', ('ABOUT', 'NP'), 0.00133511348465) seems to have a terminal ('ABOUT') and a nonterminal ('NP') on the right-hand side, which doesn't match the CNF format. Is this expected and I am not sure what we need to actually check for in part 1 to ensure match to CNF.<break/></paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That's incorrect. ABOUT is a nonterminal because it appears on the LHS of a rule. Note that there is also a terminal 'about'.  </paragraph></document>"
    },
    {
        "source": "HW2 F-scores and Part 2. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>As of now, my parsed average F-score is ~82% and overall at 55%, which are lower than Prof. Bauer's scores. As I am reviewing the function evaluate_parser, it seems like there should be some checks for language attribution in the function parser_with_backpointers. However, I am a bit unsure about how we treat the return value in this case. </paragraph><paragraph>Also, in the last paragraph of Part 2, you mentioned that the \"this method works for grammar with different start symbols.\" Would you mind elaborate on what will be the start symbols? I am using the grammar.startsymbol attribute in my implementation. I am not sure whether this is what you mean and whether changing this value assignment would increase F-score. My implementation of functions is_in_language and parse_with_backpointers are very similar. </paragraph><paragraph>Looking forward to hearing back. </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Can you clarify what you mean by \u201clanguage attribution\u201d?</paragraph><paragraph>Your choice of startsymbol is correct. This is essentially just a warning to not hardcode the startsymbol and instead use the one provided by the grammar.\u00a0</paragraph></document>"
    },
    {
        "source": "Exam format. <document version=\"2.0\"><paragraph>Hi! </paragraph><paragraph>I was wondering what the exam format will be, is a pen and paper and theory kind or more coding based. </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>There won\u2019t really be any coding on the midterm, except for possibly a \u201cmodify this pseudocode\u201d question, or \u201cthis is broken, can you find the mistake and fix it\u201d question. \u00a0</paragraph><paragraph>Most of the midterm will be \u201cconstructive\u201d problems, working with the algorithms we discussed and performing the steps of some algorithm on paper, similar to the ungraded exercises.\u00a0</paragraph><paragraph>There may be some factual short answer questions as well.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 Solutions. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For HW2, are there specific answers we should consistently be getting or is it like HW1 where answers vary?  I saw the HW said its graded on functionality, but I just wanted to receive some clarification. Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>There is less flexibility in homework 2, except there may be some small variation based on how you break ties when two splits have the same probability (updating on &gt; or on &gt;=).\u00a0</paragraph><paragraph>I don\u2019t have the exact numbers ready right now, unfortunately, but you are welcome to post your result so you can compare with other students. It should be more or less consistent.\u00a0</paragraph></document>"
    },
    {
        "source": "HW2 Part2 Question. <document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>For HW2 Part2, the instruction said \"The ATIS grammar actually overgenerates a lot, so many unintuitive sentences can be parsed\", and from what I observed in our atis grammar file, there're more than one non-terminals generate the same terminal. I wonder how do we take care of that cases - for example, should we check all combinations of possible non-terminals existed in the rule during the parsing?</paragraph><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The CKY algorithm already takes care of this. During initialization, if there are two rules for the same nonterminal, both nonterminals are included in the table. \u00a0For example if the input token s[1,2] is \u2018flights\u2019, and there are rules NP -&gt; flights and FLIGHTS -&gt; flights, both NP and FLIGHTS cover the span [1,2] and should be added to the table entry for this span.\u00a0</paragraph></document>"
    },
    {
        "source": "CKY parsing. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>If there are multiple nonterminals in one box(e.g. the NP,N box), do we try all combinations available (e.g. P NP and P N)? If multiple combinations work, do we list out all possible nonterminals (e.g. if NP -&gt; P N is valid, will the red box become PP, NP?)</paragraph><paragraph>Thank you</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/qVcxZmpBnUdegAcrskLROxSl\" width=\"658\" height=\"336.20033670033666\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>yes, you want to consider all feasible combinations</paragraph><paragraph/></document>"
    },
    {
        "source": "ungraded exercise. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am wondering if there is a typo in the solution to ungraded exercise. Why the two terms look exactly the same but have different calculation results?</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/Mr5wkAiLEn4SzwYoDbvY46OZ\" width=\"658\" height=\"384.1448863636364\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes, you are right. The second entry should be\u00a0</paragraph><paragraph>pi[2, V] = max( pi[1,N] * P(VIN) * P(flies | V), pi[1,V] * P(V|V) * P(flies|V)) :<break/>max (1/4 * 2/4 * 2/6),\u00a0(1/12 * 0 * 2/6)) = 1/24</paragraph></document>"
    },
    [
        {
            "source": "HW2 grammar.py. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Can we add \"import math\" to the grammar.py file?</paragraph><paragraph>Thank you</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>I\u2019m not a TA, but I believe we probably can. Because the assignment even suggests using math.isclose(), which is basically a function present in the math library of python.</paragraph></document>"
        },
        {
            "source": "HW2 grammar.py. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Can we add \"import math\" to the grammar.py file?</paragraph><paragraph>Thank you</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Yes, importing math is okay.\u00a0</paragraph></document>"
        }
    ],
    {
        "source": "HW2 Questions. Backtracking Table Structure Does Not Support Multiple Subtrees for Same Non-Terminal. <document version=\"2.0\"><paragraph>grammar.py<break/><break/>1. do we need to explicitly check if a word exist in the non-terminal set ? </paragraph><paragraph>     a.or can we assume that any word that is not part of the terminal set a valid non-terminal for the CNF ?</paragraph><paragraph/><paragraph>cky.pt<break/><break/>1. can we assume all words in the given input string are in the non-terminal set of the grammar ?<break/>2. Part 3, the desire structure based on what is required by the <bold>check_table_format</bold> function for the backtrack table does not allow for multiple paths for the same constituency.<break/>    a. for example, what happen if cell (i, j) has two different k-splits that resulted in VP like the toy cat with glasses example.<break/>    b. the structure below will have the latter split overrides the earlier VP's tuple.<break/>    c. so, shouldn't the the bps be a LIST instead of a TUPLE of exactly length 2 or  str ?<break/>    d. ```<italic>json</italic><break/>{<break/>      (i,j): {<break/>          VP -&gt; ((i, k_0, V), (k_0, j, VP),<break/>          ### what happen when k_1 is also another VP ???<break/>          ### VP -&gt; ((i, k_1, VP), (k_1, j, PP),<break/>          ### like above<break/>          NP -&gt; ((i, k_n, D), (k_n, j, N)),<break/>          ...<break/>      }<break/>}<break/>```</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>1. For each symbol on the right hand side of a production, you can assume it\u2019s a non terminal if it is ever used on the left hand side of any production. Otherwise, assume it\u2019s a terminal.\u00a0</paragraph><paragraph>2. That\u2019s correct. The table only keeps track of one split per nonterminal. That\u2019s okay because we are only interested in membership checking and then later obtaining the single highest probability parse tree.\u00a0</paragraph></document>"
    },
    {
        "source": "Viterbi Algorithm HW. <document version=\"2.0\"><paragraph>In the HW including Viterbi, I am confused about why we do not include \u03c0[2,Prep] in our second step? How do we know to include it in the 3rd with \u03c0[3,Prep]? Computing the DP value makes sense but I don't fully understand how the choice is being made.</paragraph><paragraph/><paragraph>\u03c0[2,N] = max( <bold>\u03c0[1,N] \u00b7 P(N|N) \u00b7 P(flies|N)</bold>, \u03c0[1,V] \u00b7 P(N|V) \u00b7 P(flies|N)) = max( (1/4 \u00b71/4 \u00b7 1/4), (1/12 \u00b72/3 \u00b7 1/4)) = 1/64<break/>\u03c0[2,V] = max( <bold>\u03c0[1,N] \u00b7 P(V|N) \u00b7 P(flies |V)</bold>, \u03c0[1,V] \u00b7 P(N|V) \u00b7 P(flies|N)) = max( (1/4 \u00b72/4 \u00b7 2/6), (1/12 \u00b7 <bold>0</bold> \u00b7 2/6)) = 1/24</paragraph><paragraph>\u03c0[3,V] = max( <bold>\u03c0[2,N] \u00b7P(V|N) \u00b7 P(like|V)</bold>, \u03c0[2,V] \u00b7 P(V|V) \u00b7 P(like|V)) = max( (1/64 \u00b7 2/4 \u00b7 2/6), (1/24 \u00b7 <bold>0</bold> \u00b7 2/6)) = 1/384<break/>\u03c0[3,Prep] = max( \u03c0[2,N] \u00b7P(Prep|N) \u00b7 P(like|Prep), <bold>\u03c0[2,V] \u00b7 P(Prep|V) \u00b7 P(like|Prep)</bold>) = max( (1/64 \u00b7 1/4 \u00b7 1), (1/24 \u00b7 1/3 \u00b7 1)) = 1/72</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The emission probability P(flies| Prep) = 0. \u00a0<break/>In other words, we already know that \u201cflies\u201d cannot be a preposition, according to the lexicon.\u00a0</paragraph></document>"
    },
    {
        "source": "ungraded exercise: n-gram. <document version=\"2.0\"><paragraph>Follow up #51, (not sure if anyone else asked before) so for questions in later exam, we also follow this rule: to add END as a word type but not count START, so |V| = #. of different word +1.</paragraph><paragraph>Or will we be given other specific rules in exam?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>No, that\u2019s the general rule. Any other choice will give you incorrect/inaccurate distributions.\u00a0</paragraph></document>"
    },
    {
        "source": "Submission instructions for HW 2. <document version=\"2.0\"><paragraph>courseworks mentions -</paragraph><paragraph>Pack these files together in a zip or tgz file as described on top of this page. <bold>Please follow the submission instructions precisely!</bold> </paragraph><paragraph>I can't seem to find the submission instructions referenced. thanks! </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Ah. Just zip the files and then upload the zip file as you did for homework 1.\u00a0</paragraph><paragraph>What you need to submit</paragraph><paragraph>cky.py<break/>evaluate_parser.py<break/>grammar.py</paragraph><paragraph>Do not submit the data files.\u00a0</paragraph></document>"
    },
    {
        "source": "get_tree function indexing. <document version=\"2.0\"><pre>get_tree(table, 0, len(toks), grammar.startsymbol)</pre><paragraph>is given in the written example in hw, and is used in the evaluator. </paragraph><paragraph>I'm assuming that the ranges indicated in the CKY algo are inclusive [i,j], which means my final parse chart would have a entry at (0, len(toks)-1), 'TOP', but not at (0, len(toks)), since len(toks) is one outside the range of the string.</paragraph><paragraph>Is it acceptable for evaluation purposes for the get_tree method to work with inclusive indexing for ease of recursion? </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>The indices specify the left and right boundaries of the tokens. So [0,1] would be the single token at index 0, and [0, N] would be the entire sentence of length N.\u00a0<break/>Your code should follow this convention.\u00a0</paragraph></document>"
    },
    {
        "source": "Question about backpointers. <document version=\"2.0\"><paragraph>Hi I hope you are doing great!</paragraph><paragraph/><paragraph>I worked through  the ungraded cky</paragraph><paragraph/><paragraph>and I was a liitle unsure of something  in part b.  </paragraph><paragraph>so  am i right about thinking that backpointers essentially tell us if there was multiple ways to build up to so block in the sentence.</paragraph><paragraph/><paragraph>so if there was no point where there were different ways to get to the same box, then there would only be one parse tree?</paragraph><paragraph>am i right in think that parse trees only come down to the last box we fill?</paragraph><paragraph/><paragraph>If there is more than one parse tree does that only show up in the last square we fill where we derive the final S?  Like if there were 3 options for that final S then there would be 3 parse trees?</paragraph><paragraph/><paragraph>thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>In the example we saw in class, there is only a single S entry in [0,6] (with backpointers to NP in [0,1] and VP in [1,6]). However, there are ultimately two different parse trees because there are two backpointers for VP in [1,6]. So it doesn\u2019t just come down to the last cell in the table.</paragraph><paragraph>Each time you find a new way of constructing a non terminal, you are creating a new backpointer.\u00a0</paragraph></document>"
    },
    {
        "source": "CFG Start Symbol 'S'. <document version=\"2.0\"><paragraph>from the textbook (17.2.1), the start symbol 'S' is considered a member of the non-terminal symbol set.</paragraph><paragraph>1. Given that, does that mean we can expect to see 'S' in right-hand-side of CKY's constituency ruie. </paragraph><paragraph>Is something below legitimate:</paragraph><paragraph> NP -&gt; S VP</paragraph><paragraph>the form above look weird as it doesn't make much sense. A NP can be a constituency describing start -&gt; verb phrase. Basically implying NP -&gt; VP ?</paragraph><paragraph>2. If we can't do what above, then how are we suppose to treat 'S' symbols that appear in any other cells not (0, n) memoization matrix during  the CKY algorithm.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The startsymbol is a nonterminal and can appear on the right hand side of productions.\u00a0<break/>An example for this are sentences like \u201che said that S\u201d\u00a0<break/></paragraph></document>"
    },
    {
        "source": "OH for Vedangi Kishor Wagh in CS TA room. <document version=\"2.0\"><paragraph>Hi curious if Vedangi Kishor Wagh still holding OH in room 122 Mudd today? Not seeing her. </paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi Vedangi sent out a message earlier today to move her office hours this week to Thursday from 12:00 noon to 1:30 pm on Zoom.\u00a0<break/>https://columbiauniversity.zoom.us/j/6826910890?pwd=RVJLeURDemRYSDloZDdSNzQxakhGdz09</paragraph><paragraph>Make sure you receive notifications sent through courseworks. We will generally use courseworks announcement for communication.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Toy Grammar for HW2. <document version=\"2.0\"><paragraph>Hi everyone.</paragraph><paragraph>In case anyone is interested in the \"toy grammar\" for HW2, here is the one I made (based on lecture 7) and used for myself. Please let me know if I need to correct anything.</paragraph><paragraph>Best.</paragraph><file url=\"https://static.us.edusercontent.com/files/eWaYNlriheodr3wpn9FZhtCn\" filename=\"toy.pcfg\"/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>Do you not need a set of rules that define what the TOP rule can create, or to redefine the start symbol to one or more of the non-terminals? Otherwise, there is no way to get from TOP to anywhere else when parsing.</paragraph></document>"
        },
        {
            "source": "Toy Grammar for HW2. <document version=\"2.0\"><paragraph>Hi everyone.</paragraph><paragraph>In case anyone is interested in the \"toy grammar\" for HW2, here is the one I made (based on lecture 7) and used for myself. Please let me know if I need to correct anything.</paragraph><paragraph>Best.</paragraph><file url=\"https://static.us.edusercontent.com/files/eWaYNlriheodr3wpn9FZhtCn\" filename=\"toy.pcfg\"/><paragraph/></document>",
            "target": "<document version=\"2.0\"><paragraph>minor typo in the provided pcfg file.<break/><break/>The start symbol need to match up with <italic>S -&gt; NP VP</italic> in order for CKY to able to work when checking the [0, n] cell. <break/><break/>Or change all instance <bold>S</bold> to <bold>TOP</bold> to match the given atis3 test file.</paragraph><paragraph/><pre>S ; 1.0\n#TOP ; 1.0\n\nS -&gt; NP VP ; 1.0\nVP -&gt; V NP ; 0.6\nVP -&gt; VP PP ; 0.4\nPP -&gt; P NP ; 1.0\nNP -&gt; D N ; 0.7\nNP -&gt; NP PP ; 0.2\n\nNP -&gt; she ; 0.05\nNP -&gt; glasses ; 0.05\nD -&gt; the ; 1.0\nN -&gt; cat ; 0.3\nN -&gt; glasses ; 0.7\nV -&gt; saw ; 1.0\nP -&gt; with ; 1.0\n</pre><paragraph/></document>"
        }
    ],
    [
        {
            "source": "Participation. <document version=\"2.0\"><paragraph>Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I'm not a TA so take this with a grain of salt but I recall the professor mentioned asking or participating on Edstem + OH could count towards your participation grade!</paragraph></document>"
        },
        {
            "source": "Participation. <document version=\"2.0\"><paragraph>Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? </paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Correct, participation in Ed courts for participation.\u00a0<break/>Note that this is technically an in person class, so the expectation is that you attend most lectures (although I\u2019m not taking attendance).\u00a0</paragraph></document>"
        },
        {
            "source": "Participation. <document version=\"2.0\"><paragraph>Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Not a TA, but I recall that participating in the Ed discussion should count toward the participation grade.</paragraph></document>"
        },
        {
            "source": "Participation. <document version=\"2.0\"><paragraph>Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Can we have a more clear expectation for participation on Ed + OH? Like is it posting/replying how many times vs. attending OH how often, etc?</paragraph></document>"
        }
    ],
    {
        "source": "Ungraded Exercise Solution. <document version=\"2.0\"><paragraph>Hi, when do the solution will be released for ungraded exercises for HMM? </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Sorry I thought I had uploaded this already but must have forgotten to release it :) I\u2019ll post it later today.\u00a0</paragraph></document>"
    },
    {
        "source": "log probability math domain error. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>When calculating my log probabilities, I keep getting a math domain error. Stack overflow points to errors with numbers close to 0 triggering this exception. I am not sure if I'm the only running into this error but a suggested fix according to the internet is to calculate the log probability of the probability + epsilon. I wanted to confirm if this is acceptable or if there are better alternatives. Thank you!</paragraph><paragraph>Best,</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/waZR5rXrndjyKPpGIskhCQro\" width=\"658\" height=\"489.37604456824516\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>This is because you have some trigram_prob = 0. You might want to double check your smoothed probability to avoid this case</paragraph></document>"
    },
    {
        "source": "Regarding commenting out tests in main. <document version=\"1.0\"><paragraph>I just went through all the unread discussions on ed and realized that somewhere it was mentioned that we need to comment out the tests we conducted in main. I did not do that and I am sorry for any inconvenience caused. I cannot resubmit now as well. Should I do something about it? I apologize again and will make sure to do so for the next assignment.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Don't worry about it. We will not actually run the program itself, but rather import your class and then call the individual methods. </paragraph></document>"
    },
    {
        "source": "Request for Extension Due to Serious Illness. <document version=\"2.0\"><paragraph>Dear Professor Bauer,</paragraph><paragraph>I hope this message finds you well. My name is Yolanda Zhu, and I am in your Natural Language Processing class. I regret to inform you that I tested positive for COVID-19 earlier this week and was feeling extremely unwell. While I initially tried to push through and complete the homework due tonight, my symptoms made it particularly challenging.</paragraph><paragraph>Therefore, I\u2019m wondering if it\u2019s possible for me to request a 1-2 day extension for HW1. I understand the importance of meeting deadlines, but under these circumstances, I hope you can consider my request.</paragraph><paragraph>I truly apologize for the inconvenience and appreciate your understanding during this challenging time.</paragraph><paragraph>Thank you for considering my situation. I look forward to your response.</paragraph><paragraph>Warm regards,</paragraph><paragraph>Yolanda Zhu</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Responded by email. Closing this thread. </paragraph></document>"
    },
    {
        "source": "Accuracy Too High?. <document version=\"2.0\"><paragraph>Going off of some previous posts regarding accuracy, I was wondering at what point does the accuracy start to become suspiciously high? Would accuracy of 0.90 or 0.925 or even 0.95 seem off? Thanks!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, that's too high. Sorry for the late response. </paragraph><paragraph>I suspect you didn't set up the experiment in part 7 correctly (you are probably testing one of the models against itself). </paragraph></document>"
    },
    {
        "source": "Comments in Code. <document version=\"2.0\"><paragraph>Not sure if this was already addressed, but are comments in the code read and taken into consideration, or should we just not bother writing them? Thank you!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>The way we grade assignments in general is to test the individual methods first using an autograding script. If they don\u2019t work as expected, the TAs look at your code in more detail. So comments would be read and taken into account at that stage. Comments are not required however.\u00a0</paragraph><paragraph>More generally, think of comments as improving readability for your own sake. It\u2019s good to get into the habit of writing meaningful comments.\u00a0</paragraph></document>"
    },
    {
        "source": "Perplexity for test data. <document version=\"2.0\"><paragraph>Sorry these questions have been asked a bunch, but does a perplexity score of ~85 for the test data tell me something is not right? For the training data I get a perplexity score of ~11  and my accuracy score comes out to about 87%</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm, the perplexity scores seem somewhat low, did you maybe include the start symbol in the denominator somewhere?</paragraph></document>"
    },
    {
        "source": "Laplace Smoothing for Unigram Models. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>In lecture 4 slides, I am a bit unclear as to exactly what we should use for N in the unigram probability calculation below. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/lnX0qVHU9Vr9CXeC9KjAQQFW\" width=\"658\" height=\"191.13043478260872\"/></figure><paragraph>Take the example of ungraded exercise 2 - part 1. </paragraph><paragraph><italic><bold>START dog bites human END<break/>START dog bites duck END<break/>START dog eats duck END<break/>START duck bites human END<break/>START human eats duck END<break/>START human eats salad END</bold></italic></paragraph><paragraph>What value should we use for N? </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>N in this case would be 24 (the sum of length of all the sentences, including END, but excluding START). </paragraph><paragraph>V would be {dog, bites, human, duck, eats, salad, END}  so |V| = 7</paragraph></document>"
    },
    {
        "source": "hw1 part 7 100% accuracy. <document version=\"2.0\"><paragraph>For part 7 is it normal to get 100% accuracy?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No, there is likely a bug in your experimental setup, I\u2019m afraid.\u00a0</paragraph></document>"
    },
    {
        "source": "Edge Case for Bigram. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>According to post #55 , we need to handle bigram edge cases of [\"START\", x]. What are we supposed to do in this case exactly? Thanks.</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>The reason why you need to handle this case separately is that count(START) is 0. My suggestion would be to keep track of the number of sentences in the training data in a separate instance variable, and then use that value for the denominator when computing P(x |start).\u00a0</paragraph></document>"
    },
    {
        "source": "What should the probabilites be for these two?. <document version=\"2.0\"><paragraph>raw_bigram_probability('START', 'START') </paragraph><paragraph>raw_trigram_probability ('START', 'START', 'START')</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>These should be 0.\u00a0</paragraph></document>"
    },
    {
        "source": "Q6 Question. <document version=\"2.0\"><pre><bold>For Q6 I'm getting:\n(base) omernauer@MacBook-Pro-689 HW1 % python -i trigram_model.py hw1_data/brown_train.txt hw1_data/brown_test.txt\nM: 98203\nl -4.488916578819041\npp = 22.454249203523293\n&gt;&gt;&gt; \n(base) omernauer@MacBook-Pro-689 HW1 % python -i trigram_model.py hw1_data/brown_train.txt hw1_data/brown_train.txt\nM: 1042565\nl -2.1932556839359254\npp = 4.573363791549528</bold>\n<bold>How can I understand where my mistake is?</bold>\n\n\nimport sys\nfrom collections import defaultdict\nimport math\nimport random\nimport os\nimport os.path\n\"\"\"\nCOMS W4705 - Natural Language Processing - Fall 2023 \nProgramming Homework 1 - Trigram Language Models\nDaniel Bauer\n\"\"\"\n\n\ndef corpus_reader(corpusfile, lexicon=None): \n    with open(corpusfile, 'r') as corpus:\n        for line in corpus: \n            if line.strip():\n                sequence = line.lower().strip().split()\n                if lexicon: \n                    yield [word if word in lexicon else \"UNK\" for word in sequence]\n                else: \n                    yield sequence\n\n\ndef get_lexicon(corpus):\n    word_counts = defaultdict(int)\n    for sentence in corpus:\n        for word in sentence: \n            word_counts[word] += 1\n    return set(word for word in word_counts if word_counts[word] &gt; 1)  \n\n\ndef get_ngrams(sequence, n):\n    <italic>\"\"\"<break/>    COMPLETE THIS FUNCTION (PART 1)<break/>    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.<break/>    This should work for arbitrary values of n &gt;= 1 <break/>    \"\"\"<break/><break/></italic>    ngrams = []\n\n    if n == 1:\n        init_tuple = 'START',\n        ngrams.append(init_tuple)\n        for word in sequence:\n            ngrams.append(tuple([word]))\n        ngrams.append(tuple(['STOP']))\n    elif n == 2:\n        bituple = ['START'] * 2\n        for word in sequence:\n            bituple.pop(0)\n            bituple.append(word)\n            ngrams.append(tuple(bituple))\n        bituple.pop(0)\n        bituple.append('STOP')\n        ngrams.append(tuple(bituple))\n    elif n == 3:\n        trituple = ['START'] * 3\n        for word in sequence:\n            trituple.pop(0)\n            trituple.append(word)\n            ngrams.append(tuple(trituple))\n        trituple.pop(0)\n        trituple.append('STOP')\n        ngrams.append(tuple(trituple))\n    return ngrams\n\n\nclass TrigramModel(object):\n    \n    def __init__(self, corpusfile):\n    \n        # Iterate through the corpus once to build a lexicon \n        generator = corpus_reader(corpusfile)\n        self.lexicon = get_lexicon(generator)\n        self.lexicon.add(\"UNK\")\n        self.lexicon.add(\"START\")\n        self.lexicon.add(\"STOP\")\n    \n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self.lexicon)\n        self.count_ngrams(generator)\n\n\n        # number of words = wordcount\n        generator = corpus_reader(corpusfile)\n        unigramcount = defaultdict(int)\n        uc = []\n        for sentence in generator:\n            uc.extend(get_ngrams(sentence, 1))\n        for word in uc:\n            unigramcount[word] += 1\n        self.wordcount = len(unigramcount)\n\n\n    def cut_trigram(self, trigram):  # returns a bigram\n        a = list(trigram)\n        a.pop(-1)\n        b = tuple(a)\n        return b\n\n    def cut_bigram(self, bigram):  # returns a unigram\n        a = list(bigram)\n        a.pop(-1)\n        b = tuple(a)\n        return b\n\n    def count_ngrams(self, corpus):\n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (PART 2)<break/>        Given a corpus iterator, populate dictionaries of unigram, bigram,<break/>        and trigram counts. <break/>        \"\"\"<break/></italic>        self.unigramcounts = defaultdict(int)  # might want to use defaultdict or Counter instead\n        self.bigramcounts = defaultdict(int)\n        self.trigramcounts = defaultdict(int)\n\n        unigram_corpus = []\n        bigram_corpus = []\n        trigram_corpus = []\n\n        for sentence in corpus:\n            unigram_corpus.extend(get_ngrams(sentence, 1))\n            bigram_corpus.extend(get_ngrams(sentence, 2))\n            trigram_corpus.extend(get_ngrams(sentence, 3))\n\n        for word in unigram_corpus:\n            self.unigramcounts[word] += 1\n\n        for word in bigram_corpus:\n            self.bigramcounts[word] += 1\n\n        for word in trigram_corpus:\n            self.trigramcounts[word] += 1\n\n        #print(\"unigram count:\", self.unigramcounts)\n        #print(\"bigram count:\", self.bigramcounts)\n        #print(\"trigram count:\", self.trigramcounts)\n        return\n\n    def raw_trigram_probability(self, trigram):\n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (PART 3)<break/>        Returns the raw (unsmoothed) trigram probability<break/>        \"\"\"<break/><break/></italic>        bigram = self.cut_trigram(trigram)\n\n        if self.bigramcounts[bigram] == 0:  # If we haven't seen the preceeding bigram\n            # Technique A\n            #return 1/(len(self.lexicon)-1)  # Taking out start\n\n            # Technique B\n            unigram = self.cut_bigram(bigram)\n            return self.raw_unigram_probability(unigram)\n        else:\n            return self.trigramcounts[trigram]/self.bigramcounts[bigram]\n\n\n\n    def raw_bigram_probability(self, bigram):\n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (PART 3)<break/>        Returns the raw (unsmoothed) bigram probability<break/>        \"\"\"<break/><break/></italic>        unigram = self.cut_bigram(bigram)\n        if self.unigramcounts[unigram] != 0:\n            return self.bigramcounts[bigram]/self.unigramcounts[unigram]\n        else:\n            return 0\n    \n    def raw_unigram_probability(self, unigram):\n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (PART 3)<break/>        Returns the raw (unsmoothed) unigram probability.<break/>        \"\"\"<break/><break/></italic>        return self.unigramcounts[unigram]/self.wordcount\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.\n\n\n    def generate_sentence(self,t=20): \n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (OPTIONAL)<break/>        Generate a random sentence from the trigram model. t specifies the<break/>        max length, but the sentence may be shorter if STOP is reached.<break/>        \"\"\"<break/></italic>        return result            \n\n    def smoothed_trigram_probability(self, trigram):\n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (PART 4)<break/>        Returns the smoothed trigram probability (using linear interpolation). <break/>        \"\"\"<break/><break/></italic>        lambda1 = 1/3.0\n        lambda2 = 1/3.0\n        lambda3 = 1/3.0\n        bigram = self.cut_trigram(trigram)\n        unigram = self.cut_bigram(bigram)\n        return (lambda1*self.raw_trigram_probability(trigram) +\n                lambda2*self.raw_bigram_probability(bigram) +\n                lambda3*self.raw_unigram_probability(unigram))\n\n\n        \n    def sentence_logprob(self, sentence):\n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (PART 5)<break/>        Returns the log probability of an entire sequence.<break/>        \"\"\"<break/><break/></italic>        probability = 0\n        trigrams = get_ngrams(sentence, 3)\n        for trigram in trigrams:\n            probability += math.log2(self.smoothed_trigram_probability(trigram))\n        return probability\n\n    def perplexity(self, corpus):\n        <italic>\"\"\"<break/>        COMPLETE THIS METHOD (PART 6) <break/>        Returns the log probability of an entire sequence.<break/>        \"\"\"<break/><break/></italic>        # Count the amount of words = M\n        counter = 0\n        uni = []\n        m_sum = 0\n        for s in corpus:\n            uni.extend(get_ngrams(s, 1))\n            m_sum += self.sentence_logprob(s)\n\n        for word in uni:\n            if word != (\"START\",):\n                counter += 1\n        M = counter\n        print(\"M:\", M)\n\n\n\n        l = m_sum/M\n        print(\"l\", l)\n        return math.pow(2, -l)\n\n\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n\n        model1 = TrigramModel(training_file1)\n        model2 = TrigramModel(training_file2)\n\n        total = 0\n        correct = 0       \n \n        for f in os.listdir(testdir1):\n            pp = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))\n            pp2 = model2.perplexity(corpus_reader(os.path.join(testdir1, f), model2.lexicon))\n            if pp &lt; pp2:\n                correct += 1\n            total += 1\n\n\n            # ..\n    \n        for f in os.listdir(testdir2):\n            pp2 = model2.perplexity(corpus_reader(os.path.join(testdir2, f), model2.lexicon))\n            pp = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))\n            if pp &gt; pp2:\n                correct += 1\n            total += 1\n        print(\"hi\")\n        return correct/total\n\nif __name__ == \"__main__\":\n\n    model = TrigramModel(sys.argv[1])\n\n    # put test code here...\n    # or run the script from the command line with \n    # $ python -i trigram_model.py [corpus_file]\n    # &gt;&gt;&gt; \n    #\n    # you can then call methods on the model instance in the interactive \n    # Python prompt. \n\n    \n    # Testing perplexity:\n    dev_corpus = corpus_reader(sys.argv[2], model.lexicon)\n\n    pp = model.perplexity(dev_corpus)\n    print(pp)\n\n\n    # Essay scoring experiment: \n    acc = essay_scoring_experiment('train_high.txt', '/train_low.txt', \"test_high\", \"test_low\")\n    print(acc)\n\n</pre></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi, one obvious bug is that in your code, you have </paragraph><pre> self.wordcount = len(unigramcount)-1 \n</pre><paragraph>This should be</paragraph><pre>self.wordcount = sum(unigramcount.values()) \n</pre><paragraph>By modifying this I was able to get:</paragraph><pre>M: 98203\nl -6.8231568321629466\n113.23348441315729\n</pre><paragraph>You might also want to consider modifying your calculations for raw probabilities to better handle edge cases. For ex, I modified your smoothed_trigram_probability and raw_unigram_probability and obtained:</paragraph><pre>M: 98203\nl -7.697321492396698\n207.5509165052973\n</pre><paragraph>Let me know if you have any other questions.</paragraph></document>"
    },
    {
        "source": "Part 7 Error. <document version=\"2.0\"><paragraph>I am having the same issue as <link href=\"https://edstem.org/us/courses/46417/discussion/3509974\">#93</link> but I re-downloaded the zip file twice and still get the following error:</paragraph><paragraph>FileNotFoundError: [Errno 2] No such file or directory: 'hw1_data/ets_toefl_data/test_low/1443872.txt'</paragraph><paragraph>when I run: </paragraph><paragraph>python3 trigram_model.py brown_train.txt brown_test.txt<break/><break/>I have checked that I am in the correct directory.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>I ran into the same issue actually about 10 minutes ago, and I ended up finding what my error was. Check your model1 and model2 testdir's in essay_scoring_experiment! That solved my error on my end! And also, part 7 doesn't take arguments, rather it uses the last few lines of code in the main method!</paragraph><paragraph/></document>"
    },
    {
        "source": "Math Error when Testing Sentence logprob. <document version=\"2.0\"><paragraph>While I am able to attain adequate perplexity and accuracy scores through my methods, when I try to run the following test on sentence_logprob, I am getting a math error due the smoothed probability of one of the trigrams being zero. Is this to be expected? <break/><break/>test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]</paragraph><paragraph>log_prob = model.sentence_logprob(test_sentence)</paragraph><paragraph>print(\"Log Probability:\", log_prob)</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes -- the tokens int he corpus are all lower-cased. </paragraph></document>"
    },
    {
        "source": "Question About Part 7. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm a bit confused on what essay_scoring_experiment() is supposed to do. I followed what Dr. Bauer said in post #58 but my code isn't working. This is what I have right now:</paragraph><pre>def essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n    model1 = TrigramModel(training_file1)\n    model2 = TrigramModel(training_file2)\n\n    total = 0\n    correct = 0\n\n    for f in os.listdir(testdir1):\n        pp_high = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))\n        pp_low = model2.perplexity(corpus_reader(os.path.join(testdir1, f), model2.lexicon))\n        if pp_high &lt; pp_low:\n            correct += 1\n        total += 1\n\n    for f in os.listdir(testdir2):\n        pp_low = model2.perplexity(corpus_reader(os.path.join(testdir2, f), model2.lexicon))\n        pp_high = model1.perplexity(corpus_reader(os.path.join(testdir2, f), model1.lexicon))\n        if pp_low &lt; pp_high:\n            correct += 1\n        total += 1\n\n    return correct/total</pre><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>That looks okay at first glance. Are you getting an error or incorrect results?</paragraph><paragraph/></document>"
    },
    {
        "source": "Perplexity Question. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>When I run my code I get the following output. Is my perplexity supposed to be this low or is this not actually the perplexity value that I'm returning or something?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/fxHXQU3v86LzPzRzhoCWgQTp\" width=\"658\" height=\"210.08020833333333\"/></figure><paragraph>Edit: Here's my perplexity func</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/fpUqTf2L6oQWP1wZ53T3xWdf\" width=\"644\" height=\"343.8357211079274\"/></figure><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>The corpus passed into perplexity() is not necessarily the train corpus and you cannot simply divide by self.total_words. In your case, the input corpus is brown_test.txt. Thus you need to calculate M in the perplexity formula by summing the total number of word tokens in the input corpus. </paragraph><paragraph/></document>"
    },
    {
        "source": "Debugging Perplexity and Accuracy. <document version=\"2.0\"><paragraph>Hello TA's! I'm having some trouble getting my model to achieve an acceptable perplexity/accuracy score. Right now I am getting Perplexity = 343 and Accuracy = 72%. I implemented error handling for edge cases and I've also checked that my bigram and trigram probabilities add up to 1, so I'm a bit unsure as to where my error is. If anyone could take a quick look and provide some guidance I would be forever grateful! (': </paragraph><pre>def get_ngrams(sequence, n):\r\n \r\n    new = sequence.copy()\r\n    for i in range(n-1):\r\n        new.insert(0, \"START\")\r\n    new.append(\"STOP\")\r\n\r\n    ngrams = []\r\n    for i in range(len(new)-n+1):\r\n        gram = []\r\n        for j in range(n):\r\n            gram.append(new[i+j])\r\n        ngrams.append(tuple(gram))\r\n\r\n    return ngrams\r\n\r\n\r\nclass TrigramModel(object):\r\n    \r\n    def __init__(self, corpusfile):\r\n    \r\n        # Iterate through the corpus once to build a lexicon \r\n        self.total_sentences = None\r\n        generator = corpus_reader(corpusfile)\r\n        self.lexicon = get_lexicon(generator)\r\n        self.lexicon.add(\"UNK\")\r\n        self.lexicon.add(\"START\")\r\n        self.lexicon.add(\"STOP\")\r\n    \r\n        # Now iterate through the corpus again and count ngrams\r\n        generator = corpus_reader(corpusfile, self.lexicon)\r\n        self.count_ngrams(generator)\r\n\r\n        self.total_words = 0\r\n        for key in self.unigramcounts:\r\n            self.total_words += self.unigramcounts[key]\r\n\r\n\r\n\r\n    def count_ngrams(self, corpus):\r\n     \r\n        self.unigramcounts = {} # might want to use defaultdict or Counter instead\r\n        self.bigramcounts = {} \r\n        self.trigramcounts = {}\r\n        corpus_list = []\r\n        unigrams = []\r\n        bigrams = []\r\n        trigrams = []\r\n\r\n        for sentence in corpus:\r\n            unigrams.append(get_ngrams(sentence, 1))\r\n            bigrams.append(get_ngrams(sentence, 2))\r\n            trigrams.append(get_ngrams(sentence, 3))\r\n\r\n        self.total_sentences = len(unigrams)\r\n\r\n        for sentence in range(len(unigrams)):\r\n            for unigram in unigrams[sentence]:\r\n                if unigram in self.unigramcounts:\r\n                    self.unigramcounts[unigram] += 1\r\n                else:\r\n                    self.unigramcounts[unigram] = 1\r\n\r\n            for bigram in bigrams[sentence]:\r\n                if bigram in self.bigramcounts:\r\n                    self.bigramcounts[bigram] += 1\r\n                else:\r\n                    self.bigramcounts[bigram] = 1\r\n\r\n            for trigram in trigrams[sentence]:\r\n                if trigram in self.trigramcounts:\r\n                    self.trigramcounts[trigram] += 1\r\n                else:\r\n                    self.trigramcounts[trigram] = 1\r\n\r\n\r\n\r\n    def raw_trigram_probability(self,trigram):\r\n       \r\n        uvw = 0\r\n        uv = 0\r\n\r\n        if trigram in self.trigramcounts:\r\n            uvw = self.trigramcounts[trigram]\r\n        if trigram[0] == \"START\" and trigram[1] == \"START\":\r\n            uv = self.total_sentences\r\n        elif (trigram[0], trigram[1]) in self.bigramcounts:\r\n            uv = self.bigramcounts[(trigram[0], trigram[1])]\r\n\r\n        if uv == 0:\r\n            return 1 / (len(self.lexicon) - 1)\r\n\r\n        return uvw / uv\r\n\r\n    def raw_bigram_probability(self, bigram):\r\n       \r\n        uv = 0\r\n\r\n        if bigram in self.bigramcounts:\r\n            uv = self.bigramcounts[bigram]\r\n        if bigram[0] == \"START\":\r\n            u = self.total_sentences\r\n        else:\r\n            u = self.unigramcounts[(bigram[0],)]\r\n        return uv / u\r\n    \r\n    def raw_unigram_probability(self, unigram):\r\n    \r\n        if unigram == (\"START\",):\r\n            return 0\r\n\r\n        return 1 / self.total_words\r\n\r\n    def smoothed_trigram_probability(self, trigram):\r\n       \r\n        lambda1 = 1/3.0\r\n        lambda2 = 1/3.0\r\n        lambda3 = 1/3.0\r\n\r\n        return lambda1 * self.raw_trigram_probability(trigram) + \\\r\n            lambda2 * self.raw_bigram_probability((trigram[0], trigram[1])) + \\\r\n            lambda3 * self.raw_unigram_probability((trigram[1]))\r\n        \r\n    def sentence_logprob(self, sentence):\r\n       \r\n        ngrams = get_ngrams(sentence, 3)\r\n        prob = 0\r\n        for gram in ngrams:\r\n            prob += math.log2(self.smoothed_trigram_probability(gram))\r\n\r\n        return prob\r\n\r\n    def perplexity(self, corpus):\r\n       \r\n        perplexity = 0\r\n        tokens = 0\r\n        for sentence in corpus:\r\n            tokens += self.count_words(sentence) + 1\r\n            perplexity += self.sentence_logprob(sentence)\r\n        perplexity = perplexity / tokens\r\n        perplexity = 1 / math.pow(2, perplexity)\r\n        return perplexity\r\n\r\n    def count_words(self, sentence):\r\n        count = 0\r\n        for word in sentence:\r\n            count += 1\r\n\r\n        return count\r\n\r\n\r\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\r\n\r\n        model1 = TrigramModel(training_file1)\r\n        model2 = TrigramModel(training_file2)\r\n\r\n        total = 0\r\n        correct = 0       \r\n \r\n        for f in os.listdir(testdir1):\r\n            pp1 = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))\r\n            pp2 = model2.perplexity(corpus_reader(os.path.join(testdir1, f), model2.lexicon))\r\n            total += 1\r\n            if 'high' in training_file1:\r\n                if pp1 &lt; pp2:\r\n                    correct += 1\r\n            else:\r\n                if pp2 &lt; pp1:\r\n                    correct += 1\r\n\r\n    \r\n        for f in os.listdir(testdir2):\r\n            pp1 = model2.perplexity(corpus_reader(os.path.join(testdir2, f), model2.lexicon))\r\n            pp2 = model2.perplexity(corpus_reader(os.path.join(testdir2, f), model2.lexicon))\r\n            total += 1\r\n            if 'high' in training_file1:\r\n                if pp1 &lt; pp2:\r\n                    correct += 1\r\n            else:\r\n                if pp2 &lt; pp1:\r\n                    correct += 1\r\n        \r\n        return correct / total\n</pre></document>",
        "target": "<document version=\"1.0\"><paragraph>Your raw unigram probability is incorrect. You are always returning 1/self.total_words rather than count(unigram)/self.total_words.\u00a0</paragraph></document>"
    },
    {
        "source": "Issues w/ Part 6 & Part 7. <document version=\"2.0\"><paragraph>When I test my part 6 &amp; 7, I am getting value &gt;400 and &lt;80% (specifically 457 and 76%). I tried testing parts 3-5 using:<break/>print(model.raw_trigram_probability((\"START\", \"START\", \"the\"))) # Part 3 print(model.raw_bigram_probability((\"START\", \"the\"))) # Part 3 print(model.raw_unigram_probability((\"the\",))) # Part 3 print(model.smoothed_trigram_probability((\"START\", \"START\", \"the\"))) # Part 4 print(model.sentence_logprob([\"natural\", \"language\", \"processing\"])) # Part 5</paragraph><paragraph>The output makes sense to me:<break/>4.0436716538617066e-05<break/>4.0436716538617066e-05<break/>0.058920067333931216<break/>0.019666980255669483<break/>-49.039356515775864</paragraph><paragraph/><paragraph>I have attended a few OHs but still no luck. I've included my current code in this post. Thank you in advance for any advice/feedback!</paragraph><snippet language=\"py3\" runnable=\"true\" line-numbers=\"true\"><snippet-file id=\"code\">import sys\nfrom collections import defaultdict\nimport math\nimport random\nimport os\nimport os.path\n\n\"\"\"\nCOMS W4705 - Natural Language Processing - Fall 2023 \nProgramming Homework 1 - Trigram Language Models\nDaniel Bauer\n\"\"\"\n\n\ndef corpus_reader(corpusfile, lexicon=None):\n    with open(corpusfile, \"r\") as corpus:\n        for line in corpus:\n            if line.strip():\n                sequence = line.lower().strip().split()\n                if lexicon:\n                    yield [word if word in lexicon else \"UNK\" for word in sequence]\n                else:\n                    yield sequence\n\n\ndef get_lexicon(corpus):\n    word_counts = defaultdict(int)\n    for sentence in corpus:\n        for word in sentence:\n            word_counts[word] += 1\n    return set(word for word in word_counts if word_counts[word] &gt; 1)\n\n\ndef get_ngrams(sequence, n):\n    \"\"\"\n    COMPLETE THIS FUNCTION (PART 1)\n    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n    This should work for arbitrary values of n &gt;= 1\n    \"\"\"\n    if n &lt; 1:\n        raise ValueError(\"n must be greater than or equal to 1\")\n\n    ngrams = []\n\n    # Pad the list of words with 'START' and 'STOP' tokens\n    padded_words = [\"START\"] * (n - 1) + sequence + [\"STOP\"]\n\n    for i in range(len(padded_words) - n + 1):\n        ngram = tuple(padded_words[i : i + n])\n        ngrams.append(ngram)\n\n    return ngrams\n\n\nclass TrigramModel(object):\n    def __init__(self, corpusfile):\n        # Iterate through the corpus once to build a lexicon\n        generator = corpus_reader(corpusfile)\n        self.lexicon = get_lexicon(generator)\n        self.lexicon.add(\"UNK\")\n        # self.lexicon.add(\"START\")\n        self.lexicon.add(\"STOP\")\n\n        self.total_words = 0\n\n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self.lexicon)\n        self.count_ngrams(generator)\n\n    def count_ngrams(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts.\n        \"\"\"\n\n        self.unigramcounts = defaultdict(int)  # Using defaultdict to initialize counts\n        self.bigramcounts = defaultdict(int)\n        self.trigramcounts = defaultdict(int)\n\n        for item in corpus:\n            # Count the number of word tokens in the corpus\n            self.total_words += len(item) + 1\n\n            unigrams = get_ngrams(item, 1)\n            bigrams = get_ngrams(item, 2)\n            trigrams = get_ngrams(item, 3)\n\n            for unigram in unigrams:\n                self.unigramcounts[unigram] += 1\n\n            for bigram in bigrams:\n                self.bigramcounts[bigram] += 1\n\n            for trigram in trigrams:\n                self.trigramcounts[trigram] += 1\n\n        return\n\n    def raw_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n        u, w, v = trigram\n\n        # Check if the trigram includes two \"START\" tokens at the beginning\n        if u == \"START\" and w == \"START\":\n            bigram = (w, v)\n            return self.raw_bigram_probability(bigram)\n        # Check if count(u, w) is 0\n        elif self.bigramcounts[(u, w)] &gt; 0:\n            return self.trigramcounts.get(trigram, 0) / self.bigramcounts[(u, w)]\n        else:\n            # Use uniform distribution over vocabulary size if unseen\n            return 1 / len(self.lexicon)\n            # return self.raw_unigram_probability((v,))\n\n    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        u, w = bigram\n\n        # Check if count(u) is 0\n        if self.unigramcounts[u] &gt; 0:\n            return self.bigramcounts.get(bigram, 0) / self.unigramcounts[u]\n        else:\n            # Use uniform distribution over vocabulary size if unseen\n            return 1 / len(self.lexicon)\n\n    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n\n        # hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once,\n        # store in the TrigramModel instance, and then re-use it.\n        w = unigram[0]\n\n        return self.unigramcounts.get(unigram, 0) / self.total_words\n\n    def generate_sentence(self, t=20):\n        \"\"\"\n        COMPLETE THIS METHOD (OPTIONAL)\n        Generate a random sentence from the trigram model. t specifies the\n        max length, but the sentence may be shorter if STOP is reached.\n        \"\"\"\n        return result\n\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation).\n        \"\"\"\n        u, w, v = trigram\n\n        lambda1 = 1 / 3.0\n        lambda2 = 1 / 3.0\n        lambda3 = 1 / 3.0\n\n        # Calculate the individual probabilities using raw methods\n        P_trigram = self.raw_trigram_probability(trigram)\n        P_bigram = self.raw_bigram_probability((w, v))\n        P_unigram = self.raw_unigram_probability((v,))\n\n        # Compute the smoothed probability using linear interpolation\n        smoothed_prob = lambda1 * P_trigram + lambda2 * P_bigram + lambda3 * P_unigram\n\n        return smoothed_prob\n\n    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n\n        # Initialize the log probability to 0\n        log_prob = 0.0\n\n        # Calculate the log probability of each trigram in the sentence\n        trigrams = get_ngrams(sentence, 3)\n        for trigram in trigrams:\n            log_prob_trigram = math.log2(self.smoothed_trigram_probability(trigram))\n            log_prob += log_prob_trigram\n\n        return log_prob\n\n    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        log_prob_total = 0.0\n        total_tokens = 0\n\n        for sentence in corpus:\n            # Calculate the log probability of the sentence\n            log_prob_total += self.sentence_logprob(sentence)\n\n            # Count the number of word tokens in the sentence\n            total_tokens += len(sentence) + 1\n\n        # Calculate the perplexity using the formula\n        perplexity = math.pow(2, -log_prob_total / total_tokens)\n\n        return perplexity\n\n\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n    model1 = TrigramModel(training_file1)\n    model2 = TrigramModel(training_file2)\n\n    total = 0\n    correct = 0\n\n    for f in os.listdir(testdir1):\n        pp = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))\n        pp2 = model2.perplexity(\n            corpus_reader(os.path.join(testdir1, f), model2.lexicon)\n        )\n\n        # If model1 has a lower (better) perplexity, then it's a correct prediction for testdir1\n        if pp &lt; pp2:\n            correct += 1\n        total += 1\n\n    for f in os.listdir(testdir2):\n        pp1 = model1.perplexity(\n            corpus_reader(os.path.join(testdir2, f), model1.lexicon)\n        )\n        pp2 = model2.perplexity(\n            corpus_reader(os.path.join(testdir2, f), model2.lexicon)\n        )\n\n        # If model2 has a lower (better) perplexity, then it's a correct prediction for testdir2\n        if pp2 &lt; pp1:\n            correct += 1\n        total += 1\n\n    return correct / total\n\n\nif __name__ == \"__main__\":\n    model = TrigramModel(sys.argv[1])\n\n    print(model.raw_trigram_probability((\"START\", \"START\", \"the\")))  # Part 3\n    print(model.raw_bigram_probability((\"START\", \"the\")))  # Part 3\n    print(model.raw_unigram_probability((\"the\",)))  # Part 3\n    print(model.smoothed_trigram_probability((\"START\", \"START\", \"the\")))  # Part 4\n    print(model.sentence_logprob([\"natural\", \"language\", \"processing\"]))  # Part 5\n\n    # put test code here...\n    # or run the script from the command line with\n    # $ python -i trigram_model.py [corpus_file]\n    # &gt;&gt;&gt;\n    #\n    # you can then call methods on the model instance in the interactive\n    # Python prompt.\n\n    # Testing perplexity:\n    # dev_corpus = corpus_reader(sys.argv[2], model.lexicon)\n    # pp = model.perplexity(dev_corpus)\n    # print(pp)\n\n    # Essay scoring experiment:\n    # acc = essay_scoring_experiment(\n    #     \"hw1_data/ets_toefl_data/train_high.txt\",\n    #     \"hw1_data/ets_toefl_data/train_low.txt\",\n    #     \"hw1_data/ets_toefl_data/test_high\",\n    #     \"hw1_data/ets_toefl_data/test_low\",\n    # )\n    # print(acc)\n</snippet-file></snippet></document>",
        "target": "<document version=\"1.0\"><paragraph>Just an idea: In line 127, you check self.unigramcounts[u], but I think the keys in that dictionary are in a tuple format, so self.unigramcount[(u,)].\u00a0</paragraph></document>"
    },
    {
        "source": "division by 0 error. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I'm running into an issue with my sentence_logprob() function where if I run this test code: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/PZ1ZoNbt0deosD9rv05EChyV\" width=\"658\" height=\"57.86970684039088\"/></figure><paragraph>where 'sample.txt' is a very small corpus, it would give me a 'math domain error' due to the fact that I'm taking the log of 0 because none of the words in \"natural language processing\" are in the corpus. For example, the first trigram created is ('START', 'START', 'natural) which I believe should return 0 for all three unigram, bigram, and trigram probabilities because the model has not seen \"natural.\" How should I take this into account?  When I try to implement a check to see if the smoothed_probability() return 0, I run into a division by 0 error with my bigram probability because it will divide by the unigram probability of \"natural\" --&gt; which will be 0. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/9eEzNeAcu6sSXenh5SF3yAq3\" width=\"658\" height=\"235\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>This is not something you need to account for. The smoothed trigram probability will eventually just use the unigram probability. There are no unseen tokens in the test data, because the corpus reader replaces them ass with UNK.\u00a0</paragraph><paragraph>So, in a sense, your test case is invalid. The token \u201cnatural\u201d would never be encountered during testing.\u00a0</paragraph></document>"
    },
    {
        "source": "Unseen Bigram and Trigram. <document version=\"2.0\"><paragraph>If the bigram or trigram is not in the count dictionary, should we simply return 0 or should we turn it into UNK and calculate the probability of it?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>in the raw probability methods you should just return 0 for unseen bigrams and trigrams. </paragraph></document>"
    },
    {
        "source": "HW1 - Perplexity Scores in train.txt/test.txt. <document version=\"2.0\"><paragraph>Hi all,<break/><break/> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ZSw5dWhN5mNTdUyCEIQ4BDDF\" width=\"658\" height=\"65.48441247002398\"/></figure><paragraph>When I'm testing perplexity against the training data, I get a score of 19.xxx (which seems okay?) but ~4.xxx-5.xxx when running the test data. I saw <link href=\"https://edstem.org/us/courses/46417/discussion/3493980\">#57</link> but I checked in OH that I'm getting the correct values for token/sentence counts so I don't think that is the issue. Anyone else having this problem?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>This seems very low. are you maybe using the tokens in the training data for M, instead of the test data?</paragraph></document>"
    },
    {
        "source": "perplexity: M vs m. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm wondering what $M$ and $m$ are in the perplexity equation:</paragraph><paragraph>$$\\begin{align*}<break/>2^{-l}&amp; &amp;\\text{where} \\ \\ \\ \\ \\ \\ \\ \\ &amp;l = \\frac{1}{M}\\sum_{i=1}^{m}\\log_2p(s_{i})<break/>\\end{align*}$$</paragraph><paragraph>Right now, I'm taking $M$ to be the sum of the frequency of all unigrams (not including the <code>(START,)</code> unigram). Then I multiply $M$ by the sum of the log probability for each sentence in the corpus. My perplexity is too low (less than 10) so I'm wondering if this is the source.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Update: I found the issue! M is the number of tokens in the corpus (test file). Perplexity is normal now.</paragraph><paragraph/></document>"
    },
    {
        "source": "Regarding where to submit HW. <document version=\"2.0\"><paragraph>Hi Daniel and TAs,</paragraph><paragraph>I would like to resubmit my assignment but I could not find where to upload it. I submitted my HW1 at Canvas earlier using the upload box at the end of the <link href=\"https://courseworks2.columbia.edu/courses/179333/assignments/1126409\">instruction page</link> but it disappeared after the first submission. I also could not find the course at Gradescope if we are using that. </paragraph><paragraph>Thank you very much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I can see your submission on Courseworks. You should be able to resubmit a new version. Did the entire submission box disappear? </paragraph><paragraph>We will only use Gradescope for providing feedback/grades on the midterm and final. A few students asked about the Gradescope link though -- did I accidentally specify Gradescope as the submission method for assignments somewhere? Thanks .</paragraph></document>"
    },
    {
        "source": "Raw Probability vs Maximum Likelihood Estimate. <document version=\"2.0\"><paragraph>Hello,<break/>I just wanted to clarify that I understand the difference between the regular probability equation, and the MLE equation for getting the probability of n-grams.<break/><break/>E.g. for a bigram:<break/><break/>raw probability:</paragraph><math>P\\left(w\\ \\ \\left|u\\right|\\right)=\\frac{count\\left(u,w\\right)}{count\\left(u\\right)}</math><paragraph>mle probability:</paragraph><math>\\left(P\\ \\left(w\\right|u\\right)=\\frac{count\\left(u,w\\right)}{\\sum_{u'\\ \\in u}^{ }count\\left(u',w\\right)}</math><paragraph>Can you explain what u' means here? </paragraph><paragraph>Is the count(u',w) the count of the total number of bigrams that contain a bigram?<break/>I am confused how the denominator of the MLE probability works, how can you sum of the counts if there is only one count of the bigram (u, w)?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm, the \"raw\" probabilities <underline>are</underline> the maximum likelihood estimates for the training data. </paragraph><paragraph>Where did you get the second formula from? Did you mean? </paragraph><math>P(w | u ) = \\frac{count(u,w)}{\\sum_{w' \\in V} count(u, w')}</math><paragraph>That's equivalent to your first formula because </paragraph><math>\\sum_{_{w'\\ \\in V\\ }}^{ }count\\left(u,w'\\right)  = count(u)</math></document>"
    },
    {
        "source": "HW1 - Interlude. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I have 2 questions regarding the interlude:</paragraph><paragraph>1. When handling the edge case of UNK being the only possible next word, are we allowed to use any methods other than beam search? For instance, if we're stuck on UNK after multiple resamples, can we prematurely end the sentence?</paragraph><paragraph>2. The two given examples all end in periods. Is this a requirement at the end of the generated sentence? </paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Since the interlude is ungraded you can solve the UNK edge case either way. The sentence is not required to end in a period. </paragraph></document>"
    },
    [
        {
            "source": "HW1 - Change in Perplexity but not in Accuracy. <document version=\"2.0\"><paragraph>Dear teaching team,<break/><break/>Just out of curiosity: I made a little change to my code to account for STOP words when calculating perplexity, and it reduced my overall perplexity (from ~290 to ~216 when running for the train file). However, my accuracy remained exactly the same. I wonder if there is any expected theoretical justification for this or if it should be just a random occurrence (I did some sensitivity analysis and didn't seem to be a bug at least). <break/><break/>My guess is that it changes perplexity equally for both models when evaluating the essays, and therefore does not change how they are split into the two categories, but I'm not so confident about it.<break/><break/>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>So since you are comparing the perplexity scores between the two models for the same data, the perplexity simply scales (and normalizes) the log probability for each test document. It won't change the relative order of the scores at all. </paragraph></document>"
        },
        {
            "source": "HW1 - Change in Perplexity but not in Accuracy. <document version=\"2.0\"><paragraph>Dear teaching team,<break/><break/>Just out of curiosity: I made a little change to my code to account for STOP words when calculating perplexity, and it reduced my overall perplexity (from ~290 to ~216 when running for the train file). However, my accuracy remained exactly the same. I wonder if there is any expected theoretical justification for this or if it should be just a random occurrence (I did some sensitivity analysis and didn't seem to be a bug at least). <break/><break/>My guess is that it changes perplexity equally for both models when evaluating the essays, and therefore does not change how they are split into the two categories, but I'm not so confident about it.<break/><break/>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Adding on to the above with some anecdotal evidence: I noticed a small increase in accuracy when i got perplexity down from 400s to 200s, but tweaks beyond that didn't change anything.</paragraph></document>"
        }
    ],
    {
        "source": "Comment out main function body?. <document version=\"2.0\"><paragraph>Hello, quick question: do we have to comment out everything under the main function? Should we be leaving anything within the body of main?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>(not a TA) I believe they said they would just be testing individual methods, nothing in main </paragraph><paragraph/></document>"
    },
    {
        "source": "Handling end-of-sentence indicators. <document version=\"2.0\"><paragraph>Hi Teaching Staff,</paragraph><paragraph><bold>1) Can you elaborate on how periods/exclamation points/question marks (as end-of-sentence indicators) are supposed to be handled when we count_ngrams?</bold><break/><break/>Take for instance this processed sentence from brown_train.txt:</paragraph><pre>['the', 'fulton', ... , 'took', 'place', '.']\n</pre><paragraph>and accounting for all the edge cases mentioned on Ed:</paragraph><blockquote>INCLUSIONS: <code>[('STOP'),]</code> in unigramcounts |  <code>[('word', 'STOP')]</code> and <code>[('START', 'word')]</code> in bigramcounts | <code>[('word', 'word', 'STOP')]</code> and <code>[('START', 'word', 'word')</code> and <code>[('START', 'START', 'word')]</code>in trigramcounts</blockquote><blockquote>EXCLUSIONS: <code>[('START')]</code> in unigramcounts | <code>[('START', 'START')]</code> in bigramcounts</blockquote><paragraph><bold>2) Would <code>[('.'), ('STOP')]</code> be a valid bigram?</bold> </paragraph><paragraph><bold>3) How about <code>[('place'), ('.'), ('STOP')]</code> as a valid trigram?</bold> </paragraph><paragraph><bold>4) If valid, why are these ngrams important to account for in our model? If not valid, do we implement code to ignore {.?!}?</bold></paragraph><paragraph>Thank You.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Punctuation symbols should be handled like any other token. So [('.'), ('STOP')] and [('place'), ('.'), ('STOP')] would be a correct bigram / trigram. </paragraph><paragraph>We want the model to learn that a sentence typically ends in a period. </paragraph></document>"
    },
    {
        "source": "Question about HW1 Output. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph><break/>This is what I got for output:</paragraph><paragraph>train: 16.091318233161303</paragraph><paragraph>test: 208.49720370974467</paragraph><paragraph>accuracy: 0.8446215139442231<break/><break/>I wonder if this perplexity is in the acceptable range?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, that seems okay to me. </paragraph></document>"
    },
    {
        "source": "Assignment Submission. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>I just wanted to confirm that we need to submit the assignment only on Courseworks.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Correct.\u00a0</paragraph></document>"
    },
    {
        "source": "Division by zero error. <document version=\"2.0\"><paragraph>Hi, I am getting this super weird division by zero error where my unigram count for a specific unigram for some reason is zero when running Part 7. I thought that if it was zero, it should be replaced by \"UNK\".  Would love any insight into why this could be happening my below code! It works for parts 1-6.<break/></paragraph><paragraph>For some reason, this happens with a lot of words such as: ('discovers',). Any help is appreciated!</paragraph><pre>Traceback (most recent call last):\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model.py\", line 273, in &lt;module&gt;\n    acc = essay_scoring_experiment('ets_toefl_data/train_high.txt', \"ets_toefl_data/train_low.txt\", \"ets_toefl_data/test_high\", \"ets_toefl_data/test_low\")\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model.py\", line 228, in essay_scoring_experiment\n    pp_2 = model2.perplexity(corpus_reader(os.path.join(testdir1,f), model2.lexicon))\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model.py\", line 210, in perplexity\n    log_prob.append(self.sentence_logprob(sentence))\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model.py\", line 193, in sentence_logprob\n    nonlog_trigram_prob = self.smoothed_trigram_probability(trigram)\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model.py\", line 172, in smoothed_trigram_probability\n    bigram_probs = self.raw_bigram_probability(bigram)\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model.py\", line 138, in raw_bigram_probability\n    return self.bigramcounts.get(bigram, 0) / unigram_count\nZeroDivisionError: division by zero\n</pre><paragraph>raw_bigram_probability():</paragraph><pre>def raw_bigram_probability(self, bigram):\n    <italic>\"\"\"<break/>    COMPLETE THIS METHOD (PART 3)<break/>    Returns the raw (unsmoothed) bigram probability<break/>    \"\"\"<break/></italic>    unigram = (bigram[0],)\n    unigram_count = self.unigramcounts[unigram]\n\n    if bigram[0] == \"START\":\n        unigram_count = self.sentence_count\n    if unigram_count == 0:\n        print(unigram)\n        return self.bigramcounts[bigram] / 1\n    return self.bigramcounts[bigram] / unigram_count\n</pre><paragraph>essay_scoring_experiment</paragraph><pre>def essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n\n        model1 = TrigramModel(training_file1)\n        model2 = TrigramModel(training_file2)\n\n        total = 0\n        correct = 0       \n \n        for f in os.listdir(testdir1):\n            pp = model1.perplexity(corpus_reader(os.path.join(testdir1, f), model1.lexicon))\n            pp_2 = model2.perplexity(corpus_reader(os.path.join(testdir1,f), model2.lexicon))\n\n            if pp &lt; pp_2:\n                correct += 1\n            total += 1\n        for f in os.listdir(testdir2):\n            pp = model2.perplexity(corpus_reader(os.path.join(testdir2, f), model2.lexicon))\n            pp_2 = model1.perplexity(corpus_reader(os.path.join(testdir2, f), model1.lexicon))\n\n            if pp_2 &gt; pp:\n                correct += 1\n            total += 1\n        \n        return correct / total\n</pre><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm your version of raw_bigram_probability does not appear to match the error message. Did you maybe change it? </paragraph></document>"
    },
    {
        "source": "HW1 debug. <document version=\"2.0\"><paragraph>Is there any suggestions on how to debug the code, Like any part need to pay attention on? I already go through my code several times but got the followings which I think are not that correct...</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/bcA3KwejAajAXaKnu1fFcgVW\" width=\"466\" height=\"86\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>That looks pretty good to me. </paragraph></document>"
    },
    {
        "source": "Hw1 part6. <document version=\"1.0\"><paragraph>Do we need to count the word 1 more than each sentence when computing perplexity? Because I think there will be a hiding \u201cSTOP\u201d for each sentence.<break/>So the total word tokens is the words in document plus number of sentences. Am I understanding this correctly?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, you need to include the STOP occurrences -- there will be one per sentence. </paragraph></document>"
    },
    {
        "source": "No access to gradescope. <document version=\"2.0\"><paragraph>I found out that I am enrolled in the class but not the gradescope. Is there an entry code that I should use to enroll myself? Clicking the link on canvas didn't seem to help. </paragraph><paragraph>My netid: zw2958</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>We won\u2019t be using Gradescope until the midterm. At that point we will enroll you automatically.\u00a0</paragraph><paragraph>For homework assignments we use Courseworks.\u00a0</paragraph></document>"
    },
    {
        "source": "Count clarification. <document version=\"2.0\"><paragraph>Hello! I have a couple questions regarding the count_ngrams method.</paragraph><paragraph>1. For some reason, the counts return 0 for words starting with an uppercase letter (i.e. model.unigramcounts[('<bold>State</bold>',)] = 0) while returning some number for the same words starting with a lowercase letter ( i.e. model.unigramcounts[('<bold>state</bold>',)] = 830). Is this phenomenon supposed to happen?</paragraph><paragraph>2. To double check what previous people have asked regarding how to count the 'START' and 'STOP' tokens, we do <bold>not</bold> count the unigram 'START' but we do count the unigram 'STOP'.  (meaning model.unigramcounts[('START',)] = 0).</paragraph><paragraph>Meanwhile, bigramcounts should count bigrams (START, w_1) and (w_n, STOP) and trigramcounts should include trigrams (START, START, w1), (START, w1, w2) and ( w_n-1, w_n, STOP). The bigram (START, START) should be ignored however in favor of finding the number of sentences.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>1. The `corpus_reader` function provided automatically lowercase all incoming words. So all ngrams will be lowercase by default. Which explain why the vocab will not contain any uppercase words.</paragraph><paragraph>2. I believe you can either choose to not include it in the Counters, or another approach is to just ignore START and return 0 when raw_unigram_probability is given START as an argument.</paragraph><paragraph>3. bigram (START, START) should be ignored as it does not provide any context to the model similar to Unigram (START)</paragraph><paragraph/></document>"
    },
    {
        "source": "Emergency Extension \u2014 Assignment 1. <document version=\"1.0\"><paragraph>Dear Professor and TAs,</paragraph><paragraph>Hello, my name is Charles Yoon and I am currently at the hospital after being transported to the ER at around 4 AM. I began having intense back pain Tuesday that immobilized me completely, rendering me unable to walk, stand, or really do anything except lay down and use my phone. I thought it would get better if I rested in bed after taking some Tylenol, but it kept getting worse and once my legs started to lose feeling, I was immediately taken to the ER by CUEMS, where I was diagnosed with a herniated disc.</paragraph><paragraph>The doctors want me to stay in the hospital for the time being to receive proper care, and unfortunately I will not be able to type or submit a coding assignment by Thursday.</paragraph><paragraph>Would it be possible to be granted an emergency extension? Thank you. I am willing to provide any documentation that might be necessary.</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Sorry to hear; that doesn\u2019t sound good.\u00a0</paragraph><paragraph>You can submit homework 1 until Monday October 2nd.\u00a0</paragraph><paragraph>Beyond that, it may be better to drop homework 1 from grading and reweigh the remaining deliverables.\u00a0</paragraph><paragraph>I hope you feel better soon.\u00a0</paragraph></document>"
    },
    {
        "source": "Where to start in reducing perplexity (Part 6). <document version=\"2.0\"><paragraph>Hi! I currently have a low-300s perplexity score and was wondering where to start looking to get this score down? Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hello! One thing that helped me was making sure to account for the edge cases mentioned in other discussions here related to 'START' tokens and word counts. For unigram ('START') and bigram ('START', 'START'), making sure that the count and probability of those was handled properly. </paragraph><paragraph>I.e., for counts, how many sentences would kind of implicitly begin with ('START') and ('START', 'START'), and for probabilities, what should the chances of our model generating a 'START' token be?</paragraph></document>"
    },
    {
        "source": "Part 7 Error Running. <document version=\"2.0\"><paragraph>Hi! </paragraph><paragraph>When running part 7, I keep stumbling on an error: FileNotFoundError: [Errno 2] No such file or directory: 'hw1_data/ets_toefl_data/test_low/1443872.txt'</paragraph><paragraph>I have set the file paths properly: Acc =</paragraph><paragraph>essay_scoring_experiment(\"hw1_data/ets_toefl_data/train_high.txt\",\"hw1_data/ets_toefl_data/train_low.txt\", \"hw1_data/ets_toefl_data/test_high\", \"hw1_data/ets_toefl_data/test_low\") </paragraph><paragraph>I was thinking maybe it had something to do with the command I was running so could you please provide me with clarification on what to follow with here \"python trigram_model.py ____\"</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>For part 7 the command line parameters shouldn\u2019t matter because the path names are all hardcoded. I wonder if you possible use the wrong directory name for one of the test batches in essay_scoring_experiment.\u00a0</paragraph></document>"
    },
    {
        "source": "Is 95% accuracy too high?. <document version=\"2.0\"><paragraph>My accuracy is surprisingly high (0.958).</paragraph><paragraph>My perplexity for Q6 was 196.5. I've checked my previous methods so many times; not sure which step could've gone wrong. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XhS6lZdiRjw7lPo8zjipVzdK\" width=\"658\" height=\"436.72566371681415\"/></figure><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>You seem to be using model 2 twice when computing perplexity for the \"low\" test data. So essentially all of the \"low\" documents are classified as correct :)</paragraph><paragraph/></document>"
    },
    {
        "source": "Really low perplexity/high accuracy for question 6&7. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Not sure if I can ask this here therefore put it on private but for question 6 &amp; 7, at first for raw_trigram_probability I returned 1/word_count (where word_count is total number of words in training data) for when the count(u,w) would be 0 and got a perplexity of around 250 and accuracy of 73% for Q6 &amp; Q7. </paragraph><paragraph>After I changed  the raw_trigram_probability for when the count(u,w) would be 0 to be 1/lexicon_size (number of unique unigrams except for START), I got a perplexity of around 28 and accuracy 99%. Is this possible or is this too good to be true and I accidentally made something wrong here ?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Accuracy of 99% is definitely not right -- I suspect there is something wrong with your experimental setup. </paragraph></document>"
    },
    {
        "source": "HW1 Part 3 Start. <document version=\"2.0\"><paragraph>Regarding the probability of the unigram ('START',), would this be a special case where we need to divide the number of sentences in the corpus by the total unigram count?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The unigram ('START') should never be predicted by the language model and therefore it should have a probability of 0. </paragraph></document>"
    },
    {
        "source": "Resubmitting File New Name. <document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I just submitted my lab and I want to double check that it's okay that the file is now called trigram_model-1.py</paragraph><paragraph/><paragraph>Thanks! </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>This is fine -- Courseworks automatically renames resubmissions this way. </paragraph></document>"
    },
    [
        {
            "source": "debugging perplexity. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am getting perplexity scores which are very high. </paragraph><paragraph>3.6668123503602652e+50 </paragraph><paragraph>Is there something I should check that maybe be throwing this number way off. </paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I recalled having the same problem yesterday, several things to check:</paragraph><paragraph>1. ensure your count_ngram is correct(as the example in the homework description page)</paragraph><paragraph>2. special cases in <code>raw_trigram_probability, raw_bigram_probability, raw_unigram_probability</code> , I recommend checking #55, and that you are assigning zero to those bigrams/trigrams that didn't appear(thus numerator being zero)</paragraph><paragraph>3. check the perplexity denominator of l(should be number of tokens appeared in testing)</paragraph><paragraph>Hopefully these will help you debug this. I recalled not having any problem with <code>sentence_logprob</code> ,but having problem mostly with the 4 functions mentioned above</paragraph></document>"
        },
        {
            "source": "debugging perplexity. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am getting perplexity scores which are very high. </paragraph><paragraph>3.6668123503602652e+50 </paragraph><paragraph>Is there something I should check that maybe be throwing this number way off. </paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I also had this issue because I didn't realize there was a difference between m and M. m is the total number of sentences while M is the total number of word tokens</paragraph></document>"
        }
    ],
    {
        "source": "HW1 Part 3 Clarification. <document version=\"2.0\"><paragraph>Just to ensure my understanding of Part 3 is correct, is calculating the unigram probabilities just P(v) = count(v) / |V|? Meanwhile, is finding the bigram probability just P(w|u) = count(u and w)/count(u) while finding the trigram probability just P(w|u,v) = count(u, v, w)/count(u,v)? (not taking into account the edge cases). </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>The unigram probability should be count(u) / N (where N is the total number of <underline>tokens</underline> in the corpus). </paragraph></document>"
    },
    {
        "source": "Will we penalized if part 6 and 7 take a min or two to run?. <document version=\"2.0\"><paragraph>Hello! For clarification purposes, if, for parts 6 &amp;7, our program takes a minute or two to display an output, will that negatively impact our grade? Or as long as an output is rendered, are we fine? Thank you in advance! </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Chances are that, in that case, you are not using dictionaries efficiently. We may take a small deduction if the program is inefficient. That being said, iterating through the training corpora may take a minute or so during training. </paragraph></document>"
    },
    {
        "source": "in continution to previous post (not sure if approach was allowed to be shared). <document version=\"2.0\"><paragraph>when I run the \"generate_sentence\" function, I keep getting sentences that have no meaning at all such as the following, and the probability of the \"STOP\" token is very small and almost constant (of the order of e-5) everytime, does this make sense and is it the same for anyone else ?</paragraph><paragraph>Example outputs with a t of 10:</paragraph><paragraph>['distinguish', 'proteases', 'sell', 'neil', 'whichever', 'corner', 'dogmatically', 'hilum', 'talking', '%']</paragraph><paragraph>['yearned', 'dominant', 'timeless', 'townsmen', 'mountains', 'interpreter', 'prize', 'assume', 'depressed', 'agreeing']</paragraph><paragraph>['instantly', 'covering', 'enforcing', 'disrespect', 'peeled', 'blasphemous', 'analyzing', 'charming', 'wrath', \"o'dwyers\"]</paragraph><paragraph>Not sure where I'm going wrong.</paragraph><paragraph>Approach:</paragraph><paragraph>1) for each word in the lexicon, generate a trigram probability</paragraph><paragraph>2) Draw a random number r between 0 and 1. <break/>3) Iterate through every word in the lexicon, keeping a sum of event probabilities. <break/>4) Once the sum exceeds r, return the current event. </paragraph><paragraph>is my step 1 correct? am I approaching it the wrong way?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Any clarification on this would be very much appreciated! Thank you!</paragraph><paragraph/></document>"
    },
    {
        "source": "Re: \"generate_sentence\" function. <document version=\"2.0\"><paragraph>when I run the \"generate_sentence\" function, I keep getting sentences that have no meaning at all such as the following, and the probability of the \"STOP\" token is very small and almost constant (of the order of e-5) everytime, does this make sense and is it the same for anyone else ?</paragraph><paragraph>Example outputs with a t of 10:</paragraph><paragraph>['distinguish', 'proteases', 'sell', 'neil', 'whichever', 'corner', 'dogmatically', 'hilum', 'talking', '%']</paragraph><paragraph>['yearned', 'dominant', 'timeless', 'townsmen', 'mountains', 'interpreter', 'prize', 'assume', 'depressed', 'agreeing']</paragraph><paragraph>['instantly', 'covering', 'enforcing', 'disrespect', 'peeled', 'blasphemous', 'analyzing', 'charming', 'wrath', \"o'dwyers\"]</paragraph><paragraph>Not sure where I'm going wrong.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Also getting this problem with responses like:<break/>['lilly', 'preferred', 'banks', 'the', 'banks', 'preferred', 'the', 'butcher', 'shop', 'shop', 'the', 'took', 'f or', 'acting', 'preferred', 'family', 'butcher', 'butcher', 'preferred', ',', 'shop', ',', 'but', 'will', 'took' , 'be', 'took', 'butcher', 'where', 'for', 'the', 'butcher', 'butcher', 'the', 'the', 'took', 'shop', 'chronicle s', 'students', 'help', 'of', 'family', 'leveling', 'relationships', 'took', 'their', 'shop', 'preferred', 'the', 'the', 'reports', 'students', 'privacy', 'her', '.', 'loneliness', 'continue', 'STOP'] <break/>If you increase t you see STOP more frequently but I'm getting a lot of repeated words and nonsensical sentences. </paragraph><paragraph/></document>"
    },
    {
        "source": "High Perplexity Score. <document version=\"2.0\"><paragraph>Hi TAs,</paragraph><paragraph>I believe I have done parts 1-5 correctly; however, my perplexity score is still very high. I have made sure to not count ('START',) in my unigramcounts, but I still achieve a 325.7 as my score. </paragraph><paragraph>I would appreciate any guidance on how I can reduce this number.</paragraph><paragraph>Thanks!</paragraph><paragraph>Here is my code for reference:</paragraph><pre>def get_ngrams(sequence, n):\n    \"\"\"\n    COMPLETE THIS FUNCTION (PART 1)\n    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n    This should work for arbitrary values of n &gt;= 1 \n    \"\"\"\n    temp = ((['START']*(n-1)) + sequence + ['STOP']) if n &gt;= 2 else (['START'] + sequence + ['STOP'])\n\n    res = []\n    for i in range(len(temp) - n + 1):\n        res.append(tuple(temp[i:i+n]))\n\n    return res\n</pre><pre>    def count_ngrams(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts. \n        \"\"\"\n\n        self.unigramcounts = {} # might want to use defaultdict or Counter instead\n        self.bigramcounts = {} \n        self.trigramcounts = {} \n\n        ##Your code here\n\n        for sentence in corpus:\n            for gram in get_ngrams(sentence, 1):\n                if gram != ('START',):\n                    self.unigramcounts[tuple(gram)] = 1 + self.unigramcounts.get(tuple(gram), 0)\n\n            for gram in get_ngrams(sentence, 2):\n                self.bigramcounts[tuple(gram)] = 1 + self.bigramcounts.get(tuple(gram), 0)\n            \n            for gram in get_ngrams(sentence, 3):\n                self.trigramcounts[tuple(gram)] = 1 + self.trigramcounts.get(tuple(gram), 0)\n\n        self.total_words = sum(self.unigramcounts.values())\n        print(self.total_words)\n\n        return\n</pre><pre>    def raw_trigram_probability(self,trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n\n        numerator = self.trigramcounts.get(trigram, 0)\n        denominator = self.bigramcounts.get(trigram[0:2], 0)\n\n        if denominator == 0:\n            return 1/len(self.lexicon)\n        \n        return numerator/denominator\n</pre><pre>    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        numerator = self.bigramcounts.get(bigram, 0)\n        denominator = self.unigramcounts.get(bigram[0:1], 0)\n\n        if denominator == 0:\n            return 1/len(self.lexicon)\n        \n        return numerator/denominator\n</pre><pre>    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.\n        \n        return self.unigramcounts.get(tuple(unigram), 0)/self.total_words\n</pre><pre>    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation). \n        \"\"\"\n        lambda1 = 1/3.0\n        lambda2 = 1/3.0\n        lambda3 = 1/3.0\n\n        return lambda1 * self.raw_trigram_probability(trigram) + lambda2 * self.raw_bigram_probability(trigram[1:3]) + lambda3 * self.raw_unigram_probability(trigram[2:3])\n</pre><pre>    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        trigrams = get_ngrams(sentence, 3)\n        res = 0\n\n        for i in sentence:\n            self.total_test += 1\n            \n        for gram in trigrams:\n            res += math.log2(self.smoothed_trigram_probability(gram))\n\n        return res\n</pre><pre>    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6) \n        Returns the log probability of an entire sequence.\n        \"\"\"\n        res = 0\n\n        for sentence in corpus:\n            res += self.sentence_logprob(sentence)\n\n        res /= self.total_test\n\n        return 2**(-1 * res)\n</pre></document>",
        "target": "<document version=\"2.0\"><paragraph>My question has been answered!</paragraph></document>"
    },
    {
        "source": "Hw 1 Part 6 Perplexity Score. <document version=\"2.0\"><paragraph>I'm getting a really, really high perplexity and am struggling to figure out why. I went step by step through 1-5 which all seemed to work smoothly, but for perplexity, I am getting around a 6000 score. Did anyone else get a score in this range and figure out why?</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>If your 1-5 worked smoothly, I would suggest you looking into the part where you calculate the log probabilities.  If you still have the problem, you can make a private post so the TA can take a look at your code.</paragraph><paragraph/></document>"
    },
    {
        "source": "hw1 pt7. <document version=\"2.0\"><paragraph>Is it acceptable if I get an accuracy of 73.7% for \"essay_scoring_experiment\"?</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That seems a little low, unfortunately. People are reporting accuracies around 84% or 85%.\u00a0</paragraph></document>"
    },
    {
        "source": "Testing hw 1 part 6. <document version=\"2.0\"><paragraph>Hello</paragraph><paragraph>How what command do I use to test part 6? When I run <code>python trigram_model.py hw1_data/brown_test.txt</code> I get an IndexError. </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>You also need to provide the test corpus as a second command line parameter.\u00a0</paragraph></document>"
    },
    {
        "source": "Start Clarifications. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am confused on how to implement start. </paragraph><paragraph>I currently have: </paragraph><paragraph>In get_ngrams I add START as a unigram. </paragraph><paragraph>I do not count start as one of the words in the lexicon. </paragraph><paragraph>In raw_bigram_prob, if it is START, WORD i calculate probability by doing bigramcounts[bigram]/sentencecount. </paragraph><paragraph>In raw_trigram. If I have START, START, WORD, then i get prob by saying trigramcounts[trigram]/sentencecount. </paragraph><paragraph>(I did not include any special cases for START, WORD, WORD as that did not seem to require one)</paragraph><paragraph>Are these correct assumptions. I am getting errors in my code and trying to debug. </paragraph><paragraph>Thanks </paragraph><paragraph/><paragraph/><paragraph> </paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That sounds all correct to me.\u00a0</paragraph></document>"
    },
    {
        "source": "On Suggestion for \"P(v | u,w) not clear\", Part3 HW1. <document version=\"2.0\"><paragraph>Dear teaching team,</paragraph><paragraph>I was revising the homework before submitting and one detail made me curious. In Part3, we read the following: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/f6MP69rOsbYn6r1g3O8Vv9sg\" width=\"658\" height=\"73.67567567567568\"/></figure><paragraph>My intuition says that the second alternative, using P(v) as a proxy, would be better since assuming a uniform distribution across all words could introduce bias. Is there any reason why the first alternative is the one recommended?<break/><break/>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>The first version empirically works better on this data. I suspect it\u2019s because the context is a very strong indicator for what the next word should be, so using the unigram probability will assign too much probability mass to some words.\u00a0</paragraph></document>"
    },
    {
        "source": "Strange Numbers for Part 6. <document version=\"2.0\"><paragraph>Hi! </paragraph><paragraph>I am getting way too big numbers (about 10/10^2 off) from what has been mentioned on Ed. I'm not sure what is wrong with my code as each step seems to work/make sense on its own. Any tips as to what might be causing the issue?</paragraph><paragraph/><paragraph>Thanks!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/nLttrgzTSESJQjKiCJi8Qk8b\" width=\"643\" height=\"441.82966396292\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/zl7RT3GxxagczvAMPZzqgCgy\" width=\"658\" height=\"568.0088235294118\"/></figure></document>",
        "target": "<document version=\"1.0\"><paragraph>One thing I\u2019m noticing is that your smoothed_trigram_probability function isn\u2019t quite right.\u00a0<break/>For a trigram (u,v,w) you are using u as the unigram and (u,v) as the bigram, rather than w for the unigram and (v,w) as the trigram.\u00a0</paragraph><paragraph>Additionaly, i don\u2019t think you need to worry about the case (START, w) and (START, START, w) in the smoothed probability if you are handling it correctly in the raw probability functions.\u00a0</paragraph></document>"
    },
    {
        "source": "H1 Clarification. <document version=\"2.0\"><paragraph>Hi, I have a few questions needs to be clarified: </paragraph><list style=\"number\"><list-item><paragraph>In method <bold>count_ngrams</bold>, should we count the number of <bold>START</bold> tokens as 0? i.e. <code>model.unigramcounts[('START', )] = 0</code> or regular counts number? </paragraph></list-item><list-item><paragraph>Related to the question 1, if we <bold>DO NOT</bold> count the <bold>START</bold> tokens, then case <bold>('START', x)</bold> would be a special case (division by zero). If so, should we treat <bold>('START', x) = (x)?</bold> If we <bold>COUNT</bold> the START tokens, then case <bold>('START', x)</bold> would be a regular case. </paragraph></list-item><list-item><paragraph>Related to the question 2, should we assign <bold>0 possibility</bold> to START in <bold>raw_unigram_probability</bold>? If we do so, then <bold>('START', x)</bold> would be a division by zero case again. </paragraph></list-item><list-item><paragraph>In method <bold>raw_trigram_probability</bold>, if count(u, v) = 0, then we use 1 / |V|. For |V|, should we minus the <bold>START</bold> token from the size of the lexicon? i.e. should we use <code>1 / (len(self.lexicon) - 1)</code> or <code>1 / len(self.lexicon)</code>?</paragraph></list-item><list-item><paragraph>In method <bold>raw_trigram_probability</bold>, should we treat <bold>('START', 'START', x) = ('START', x)?</bold> Or should we treat it as a regular case, and use <bold>number of sentences in corpus</bold> for ('START', 'START')?</paragraph></list-item><list-item><paragraph>For essay score, I got 83.6. Is this score acceptable or does it still indicate that there are some minor errors in my code? </paragraph></list-item></list><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>1. I think the START counts should be all 0. At least the probability for the start unigram should be 0. </paragraph><paragraph>2. You will have to treat (START, x) as a special case -- i recommend storing the number of sentences in a new instance variable, and then using this for the denominator. The same applies to (START, START, x).</paragraph><paragraph>3. Yes P(START) should be 0. </paragraph><paragraph>4. START should not be included in the lexicon. So either remove it from the lexicon or use <code>1 / (len(self.lexicon) - 1).</code></paragraph><paragraph>5. These two approaches should be equivalent since count(START,START,x) = count(START, x).</paragraph><paragraph>6. That seems just a tad low. Most implementations I have seen get 84-85%. </paragraph></document>"
    },
    {
        "source": "HW 1 Part 6 -- Training vs Testing Data. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Just making sure, but, in part 6, m and M refer to sentences and word tokens within the <bold>test</bold> data, correct? Does this also mean that before part 6, any mention of word count or sentence count refers only to the <bold>train</bold> data? </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Hm, I'm not sure I understand. When computing the probabilities that make up the mode, you can only rely on the training data. </paragraph><paragraph>For the perplexity calculation, m and M refer to the sentence and token count in the test data.</paragraph></document>"
    },
    [
        {
            "source": "Interlude hint. <document version=\"2.0\"><paragraph>Write the method <code>generate_sentence</code>, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\",\"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each.<break/>Draw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated.</paragraph><paragraph>Wanted to get the hint on the above as mentioned in courseworks.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>In Python, I believe you can use random.choices while supplying the optional weights variable. </paragraph></document>"
        },
        {
            "source": "Interlude hint. <document version=\"2.0\"><paragraph>Write the method <code>generate_sentence</code>, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\",\"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each.<break/>Draw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated.</paragraph><paragraph>Wanted to get the hint on the above as mentioned in courseworks.</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>There are several approaches here. </paragraph><paragraph>The simplest one for our use case:</paragraph><paragraph>1. Draw a random number r between 0 and 1. <break/>2. Iterate through the different event, keep a sum of event probabilities. <break/>3. Once the sum exceeds r, return the current event. </paragraph><paragraph>Another easy approach is to use the random.choice function: </paragraph><paragraph>&gt;&gt;&gt; numpy.random.choice(['A','B','C'], p = [0.7, 0.2, 0.1]) </paragraph><paragraph>You should also skip UNK tokens. An easy way is to simply sample the next token again if you end up with UNK. </paragraph></document>"
        }
    ],
    {
        "source": "Expressions for computing raw probabilities of the following. <document version=\"2.0\"><paragraph>Just wanted to ask the exact expressions to compute the Raw Probabilities of (START, START) in case of Bigram, (START, X) in case of Bigram, (START, START, X) for trigram, and (START, X, X) for Trigram, as we would not be computing the unigram probabilities for START.</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hello! One thing that helped me was to distinguish between when we use probability versus count and which formulas use which. Some formulas will want the count of how many times a given ngram (such as START, START) occur in our corpus, and some will want the probability of that bigram, aka, what should the chances be of our model generating that bigram be. </paragraph></document>"
    },
    {
        "source": "(\"START\",) unigram. <document version=\"2.0\"><paragraph>Per previous discussions, I understand that it does not make statistical sense to include the <code>(\"START\",)</code> unigram when counting the total number of tokens in the corpus and each unigram's counts. </paragraph><paragraph>Since the <code>get_ngrams</code> is only called when 1) calculating the counts, and 2) calculating sentence log-prob (with n=3), why do we even return <code>(\"START\",)</code>  as part of the unigrams list? Isn't this the equivalent of returning <code>(\"START\", \"START\")</code> when <code>n=2</code> and <code>(\"START\", \"START\", \"START\")</code> when <code>n=3</code> both which we do not do. </paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph> In the comments of this <link href=\"https://edstem.org/us/courses/46417/discussion/3467312\">https://edstem.org/us/courses/46417/discussion/3467312</link> post, I saw Prof. Daniel reply:</paragraph><paragraph>\"It makes sense to not generate it at all. Feel free to implement accordingly (even if this contradicts the example).\" to the same question. </paragraph><paragraph>Keeping the post up for more visibility if anyone else is curious. </paragraph></document>"
    },
    {
        "source": "What is an acceptable perplexity score?. <document version=\"2.0\"><paragraph>Hello, Is 287 an acceptable perplexity score?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That sounds reasonable. </paragraph></document>"
    },
    {
        "source": "Interlude - Generating text. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am curious if we get additional marks for implementing Interlude - Generating text.</paragraph><paragraph>Kind regards,</paragraph><paragraph>Junbang</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>No, this part is entirely optional. </paragraph></document>"
    },
    {
        "source": "Perplexity Score. <document version=\"2.0\"><paragraph>So the instruction says that for step 6, the perplexity should be less than 400, is 308 acceptable? cuz i saw in other posts saying 304 is too high</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, I think most are in the range of 150 or so. There is some variation here in how you handle some corner cases. So 300 may be okay. </paragraph><paragraph/></document>"
    },
    {
        "source": "Creating \"model\" for Part 2. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am not sure if this is a naive question but I was a bit confused about how to set up model to test count_ngrams in Part 2. The instructions say \"Where<italic> model </italic>is an instance of TrigramModel that has been trained on a corpus.\" but I am not totally sure what this means. Thank you in advance for any help!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Oh, we just mean that the variable \"model\" refers to a TrigramModel after it has been trained, for example</paragraph><paragraph>model = TrigramModel(\"brown_train.txt\")</paragraph></document>"
    },
    {
        "source": "running test for part 7. <document version=\"2.0\"><paragraph>Hi I hope you are doing great</paragraph><paragraph/><paragraph> when I am running the test for part 7, i commented out the two lines of code in the main</paragraph><paragraph/><paragraph>when i run it on the command line i am running</paragraph><paragraph/><paragraph>python trigram_model.py</paragraph><paragraph/><paragraph>I am getting an EOL error so I think I am missing argument.  Is there something else we should be including on the command line for testing part 7</paragraph><paragraph/><paragraph>thank you so much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>this is resolved, thank you so much!</paragraph></document>"
    },
    {
        "source": "Query regarding ngrams when n=4. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>For the sequence [\"natural\", \"language\", \"processing\"] we know that the first trigram is ('START','START','natural')</paragraph><paragraph/><paragraph>I wanted to confirm that for the sequence [\"natural\", \"language\", \"processing\", \"class\"] the ngrams when n=4 would be</paragraph><paragraph/><paragraph>('START','START','START','natural'), ('START','START','natural','language'),('START','natural','language','processing'), ('natural','language','processing','class'), ('language','processing','class','END')</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, that's correct. </paragraph></document>"
    },
    {
        "source": "Question about Part 7. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.  I have been wrestling with part 7 for a while and I think I am having trouble with the intuition of what its happening in the code of this part.</paragraph><paragraph/><paragraph>there is one loop going through all the test files of group 1 and for each file is determining a perplexity</paragraph><paragraph/><paragraph>this is follwed by</paragraph><paragraph/><paragraph>a second loop going through all the test files of group 2 and for each file is determining a perplexity</paragraph><paragraph/><paragraph>but group1 - is all test_high</paragraph><paragraph>and group 2 - is all test_low</paragraph><paragraph>so how do we actually compare the perplexities to determine if something is correct?</paragraph><paragraph/><paragraph>thank you so much!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>For each document in group 1, run both models. If the \"high\" model has lower perplexity than the \"low\" model, you are classifying the document correctly, so you count 1 \"correct\" point. Otherwise, you are classifying it incorrectly. </paragraph><paragraph>Then you repeat the same for each document in group 2, except that now the \"low\" model should have lower perplexity than the \"high\" model. </paragraph></document>"
    },
    {
        "source": "sanity check on # of tokens of brown_train. <document version=\"2.0\"><paragraph>I'm on part 6 of the hw and I'm getting really low perplexity values--like ~5 and ~3 </paragraph><paragraph>When I find the summation of tokens in the corpus of brown_train.txt I get 98203 tokens, does this seem right? </paragraph><paragraph>I figure I'm doing the smoothing or above incorrectly or I'm counting tokens incorrectly. </paragraph><paragraph>Thanks in advance!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>Make sure you use the number of tokens in the test data for M when you compute perplexity.</paragraph></document>"
    },
    {
        "source": "Clarification for M and N. <document version=\"2.0\"><paragraph><code>M</code> in the equation for calculating perplexity means the total number of word tokens = number of unigram tokens, so does that mean <code>M = (sum(self.unigramcounts.values()) - 1)</code> The <code>-1</code> here to exclude the <code>START</code>? Or does that mean <code>M = sum(len(sentence) for sentence in corpus)</code>?</paragraph><paragraph>Same question for when counting the <code>raw_unigram_probability</code>, do we use<code>(sum(self.unigramcounts.values()) - 1)</code> or <code>sum(len(sentence) for sentence in corpus)</code> for <code>N</code>? </paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>M is the number of tokens in the <underline>test</underline> data, so your second expression. </paragraph><paragraph>Similarly, N is the number of sentences in the test data. </paragraph><paragraph/></document>"
    },
    {
        "source": "Homework1 Part 3 Trigram ('START','START','the'). <document version=\"2.0\"><paragraph>I tested the </paragraph><paragraph>model.raw_trigram_probability(('START'<bold>,</bold>'START'<bold>,</bold>'the'))</paragraph><paragraph>but</paragraph><paragraph>self.bigramcounts[('START'<bold>,</bold>'START')]  </paragraph><paragraph>does not exist. (KeyError: ('START', 'START'))</paragraph><paragraph>Should we ignore this case?</paragraph><paragraph>Thank you.</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>No, this is a special case you need to account for. My suggestion would be to keep track of the total number of sentences in the training data in a new instance variable, then use that value as the denominator when you compute P(the | START, START). </paragraph></document>"
    },
    {
        "source": "HW 1 q3. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I wanted to make clarify:</paragraph><paragraph>The raw uni/bi/trigram probability methods returns the probability for a specific ngram and not for the whole set, correct?</paragraph><paragraph>Best,</paragraph><paragraph>Omer</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Right this method returns a single P(w|u,v) probability.\u00a0</paragraph></document>"
    },
    {
        "source": "Questions regarding Assignment 1. <document version=\"2.0\"><paragraph>HI! I hope you have a great day!</paragraph><paragraph>There are a couple of questions in HW1:</paragraph><list style=\"ordered\"><list-item><paragraph>I know someone already asked this, but I'm unsure if I fully grasped the concept. Is the reason why we don't encounter cases where <bold>unigram(prior) = 0</bold> in bigrams (given (prior, word)) because we treat those values as <bold>UNK</bold>?</paragraph></list-item><list-item><paragraph>We don't count the <bold>START</bold> token, but why do we include <bold>START</bold> in the lexicon?</paragraph></list-item><list-item><paragraph>In <bold>raw_unigram_probability</bold>, one of the questions that was already asked was if we should return 0 when encountering the 'START' or 'STOP' token, and the answer indicated 'yes'. I understand for <bold>START</bold> since it's not included, but shouldn't the count for <bold>STOP</bold> be non-zero since it's included?</paragraph></list-item><list-item><paragraph>In <bold>raw_bigram_probability</bold> and <bold>raw_trigram_probability</bold>, we didn't include (<bold>START</bold>) in unigrams and (<bold>START</bold>, <bold>START</bold>) in bigrams. Should we set the value to the number of sentences in the corpus when we need to count these cases to get the denominator?</paragraph></list-item><list-item><paragraph>Similarly, for the bigram (<bold>START</bold>, <bold>START</bold>), is this different from the case where <bold>count(u, v) = 0</bold>?</paragraph></list-item><list-item><paragraph>For computing perplexity, is <bold>M</bold> the number of words before being processed by <bold>get_ngrams</bold>? Or is it the count after adding the <bold>STOP</bold> token?</paragraph></list-item></list><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>1. correct -- all unseen tokens have been replaced with UNK by the corpus reader. There should not be any cases where you encounter a unigram in the test data that has not been seen during training. </paragraph><paragraph>2. It's not really necessary to include START in the lexicon. In fact, when you use the size of the lexicon make sure START is not included. </paragraph><paragraph>3. It's debatable if you should consider STOP as part of the unigram distribution, but I think it makes sense to include it (thus not set it to 0). Imagine generating a document by randomly sampling from this distribution repeatedly until you encounter STOP. </paragraph><paragraph>4. Yes, you can use the number of sentences in the training corpus here. </paragraph><paragraph>5. Yes, that's a different scenario. You DO know the distribution of words at the beginning of the sentence. </paragraph><paragraph>6. When computing perplexity, M is the number of tokens in the test data, including STOP (i.e., the number of predictions the language model makes on the data. </paragraph><paragraph/><paragraph/></document>"
    },
    {
        "source": "Ungraded n-gram Language Model. <document version=\"2.0\"><paragraph>In the solution \"There are 6 words, plus the END symbol, so |V|=7\". I understand that there are 6 words, but why do we not count \"START + END\" and only count \"END\"? </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>When you think of the model as generating sentences by drawing the next token from the conditional distribution, given the context, it will at some point select the STOP token. But it will never sample the START token because there is no probability for P(START | x).\u00a0<break/></paragraph></document>"
    },
    {
        "source": "Acceptable perplexity score. <document version=\"2.0\"><paragraph>Hello, what is an acceptable perplexity score? Is 304 acceptable?</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>That would be a little bit too high, but we will let you guys know what would be the baseline for this part</paragraph></document>"
    },
    {
        "source": "HW1 Part 3 Question. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I was wondering if the case that we encounter when \"count(u,w) is also 0\" in the trigram probability model will also apply to the bigram probability model. Will we ever see a time where we haven't seen the first bigram word in a sequence and we should do the same uniform distribution as with the trigram case?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>You mean the case where you haven\u2019t seen the context u for a bigram (u,v)? That cannot happen because there are no unseen unigrams. The corpus reader automatically replaces them with UNK.\u00a0</paragraph></document>"
    },
    {
        "source": "Acceptable Essay Scoring Accuracy Score. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>What's an acceptable accuracy score for the essay_scoring_experiment() function. Is an 84% accuracy acceptable or should I try and increase it?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That seems okay to me.\u00a0</paragraph></document>"
    },
    {
        "source": "Cannot install msilib. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I apologize if this is a very simple fix but I am <bold>very, very</bold> new to programming in Python and keep getting the following error message: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Ucu2QrbtIPX4nfOBpZSYBI6I\" width=\"658\" height=\"320.7826086956522\"/></figure><paragraph/><paragraph>My understanding is that msilib is a Windows specific package (?) but despite trying to manually install it I keep getting this message. I appreciate any help! Thanks!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hm, according to the traceback this is an import in the trigram_model.py file? This is definitely not in the template :) see what happens if you delete that line.\u00a0</paragraph></document>"
    },
    {
        "source": "HW 1 clarifications. <document version=\"2.0\"><paragraph>I had a few questions:</paragraph><list style=\"number\"><list-item><paragraph>While computing the raw probability for bigrams, if we come across a word that is not seen before (i.e, unigram count is zero), should we treat it as UNK, and instead use the count for ('UNK', )?</paragraph></list-item><list-item><paragraph>While finding the number of words for the denominator in calculating the raw unigram probabilities, if there are repeated words, they should be counted multiple times right? That is, we are not finding the number of unique words but instead finding the total number of words?</paragraph></list-item><list-item><paragraph>Similarly, while calculating perplexity, does the denominator (number of word tokens) count repeated words multiple times? Also, while counting the number of word tokens, should we count 'STOP' multiple times, or only once?</paragraph></list-item><list-item><paragraph>For the generate_sentence() function, to generate the next word, my understanding is that we should we randomly choose from all words that appears in the given context (last two words). Is this correct?</paragraph></list-item></list><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>1. \u00a0No the corpses reader does this automatically. If you look at the __init__ method, it takes a lexicon as parameter, which allows the corpus reader to identify unseen tokens.\u00a0</paragraph><paragraph>2. Right. The denominator in the raw unigram probability is the number of tokens (word instances including duplicates) in the training data.\u00a0</paragraph><paragraph>3. Yes. M in the perplexity formula is the number of tokens in the training data.\u00a0</paragraph><paragraph>4. Yes, you would choose the word from the conditional distribution for the context.\u00a0</paragraph><paragraph/></document>"
    },
    {
        "source": "Saturday OH. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Is the OH on Saturday still happening or is there a different link that is being used?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>Hi, May had to move her office hours to Thursday for this week only. Sorry for the inconvenience.\u00a0</paragraph></document>"
    },
    {
        "source": "Assignment 1 format. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am wondering if we need to keep the directory in the original code when submitting. In order to test, I have changed it to my own directory and is it necessary to change it back to the original?</paragraph><paragraph/><paragraph>Best,</paragraph><paragraph>Xiaoyu</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Please change any pathnames in the code back to defaults. I actually don\u2019t think we use this for testing because we call the methods individually, but better to be safe.\u00a0</paragraph></document>"
    },
    {
        "source": "Raw unigram probabilities in HW1. <document version=\"2.0\"><paragraph>For the denominator of the raw unigram probabilities, should we include \"STOP\" as part of the total word count, but not \"START\"? </paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s \u00a0correct. Include STOP but not START.\u00a0</paragraph></document>"
    },
    {
        "source": "More clarification for hw1. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I have following questions for hw1,</paragraph><paragraph>1. Since we count ('STOP'), (?, 'STOP'), (?, ?, 'STOP') in unigramcounts, bigramcounts, trigramcounts respectively. How do we deal with ('START') in unigramcounts, (start, ?) in bigramcounts, (start, start, ?) and (start, ?, ?) in trigramcounts.</paragraph><paragraph>2. If we do not store ('START') in unigram dictionary, (start, ?) in bigram dictionary, (start, start, ?) and (start, ?, ?) in trigram dictionary. I am not sure how can we deal with P( (u, v) | 'START'), P(u, v, w | ('START', v) ) and P(u, v, w | ('START', 'START') ) as a special case.</paragraph><paragraph>3. Can we use basic numpy functions such as np.sum in our scripts?</paragraph><paragraph>4.  Shall I assume that the input sentence has already been tokenized in sentence_logprob(sentence)?</paragraph><paragraph>5. Shall I return zero as probability for raw_unigram_probability() when I get tokens 'START' or 'STOP'?</paragraph><paragraph>6. Is |V| in raw_bigram_probability() or raw_trigram_probability() the total number of distinct unigrams excluding 'START'?</paragraph><paragraph>Sorry for the long questions.</paragraph><paragraph/><paragraph>Kind regards,</paragraph><paragraph>Junbang</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>1 and 2: you would count bigrams (START, x) and trigrams (START, x, x). I recommend not counting the bigram (START, START) or the unigram (START,). Instead, keep track of the number of sentences in a separate instance variable. \u00a0</paragraph><paragraph>3: I would prefer you didn\u2019t use anything but in the standard library because it slows down grading. The regular sum function or math.fsum should do the job here.\u00a0</paragraph><paragraph>4: yes, the corpus reader tokenizes the sentences.\u00a0</paragraph><paragraph>5: yes returning 0 makes sense.\u00a0</paragraph><paragraph>6: yes. But you should only need |V|for the raw trigram probability when dealing with unseen contexts. I don\u2019t think it\u2019s used for the bigram probabilities.\u00a0</paragraph></document>"
    },
    {
        "source": "Assignment 1 last question. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>In the last question in assignment 1, I found that it is common to encounter unseen words from training when calculating perplexity of testing dataset. This results in a probability of zero returned by the smoothed_trigram_probability() and calculating the log probability with zero results in math error. Is this expected and how can we resolve this problem.</paragraph><paragraph>Kind regards,</paragraph><paragraph>Junbang</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Hmm something is wrong with your setup. There should be no unseen unigrams.\u00a0<break/>The idea is that the model collects the lexicon during training and replaces all words appearing only once with UNK. During testing, the lexicon is passed to the corpus iterator, so that unseen words in the test data are replaced with UNK as well.\u00a0<break/></paragraph></document>"
    },
    {
        "source": "meeting id not valid zoom. <document version=\"2.0\"><paragraph>Hi for the office hours today i am getting the message that the meeting id is not valid.  is there something else i can try?  thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hi can you try it again? Not sure if its my problem or zoom's problem</paragraph></document>"
    },
    {
        "source": "Perplexity formula. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Does \"<italic>total number of words tokens in the corpus\",</italic> which is referring to M, include bigrams and trigrams?</paragraph><paragraph>Thank you</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>No, just the unigram tokens, including STOP (but not START).\u00a0</paragraph></document>"
    },
    [
        {
            "source": "negative perplexity when tested. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.</paragraph><paragraph/><paragraph>I just ran the test for my perplexity with argv[1]  being brown_train and argv[2] being brown_test</paragraph><paragraph/><paragraph>The value returned is:</paragraph><paragraph>-1.1662575414840142</paragraph><paragraph/><paragraph>Does this make any sense?</paragraph><paragraph/><paragraph>Thank you so much!</paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>Something is off, that shouldn\u2019t happen.\u00a0</paragraph></document>"
        },
        {
            "source": "negative perplexity when tested. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.</paragraph><paragraph/><paragraph>I just ran the test for my perplexity with argv[1]  being brown_train and argv[2] being brown_test</paragraph><paragraph/><paragraph>The value returned is:</paragraph><paragraph>-1.1662575414840142</paragraph><paragraph/><paragraph>Does this make any sense?</paragraph><paragraph/><paragraph>Thank you so much!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>So several things could happen. <break/><break/>Perplexity is computed using the equation 2^\u2212<italic>l</italic>, where <italic>l</italic> represents the average of the log probabilities of sentences. The value of <italic>l</italic> should be negative, ensuring that 2^\u2212<italic>l</italic> is positive. If <italic>l</italic> is used directly as perplexity instead of 2^\u2212<italic>l</italic>, the result would be incorrect.</paragraph><paragraph>Given your provided numbers, assuming that is you value of  <italic>l</italic>, calculating 2^\u2212<italic>l</italic> yields an implausibly small perplexity. A potential issue might be with the calculation of <italic>M</italic>. It's important to note that <italic>M</italic> should not be the total number of words in the train corpus (self.total_words) but rather the total number of words in the testing corpus. When iterating over the log probabilities of sentences, ensure that you count the number of words in the test set. Additionally, remember to count the \"STOP\" token for each sentence.</paragraph></document>"
        }
    ],
    {
        "source": "testing raw probabilities for Q3. <document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Could we possibly get the raw probabilities for some test ngrams just so I can check my understanding of Q3? Using the 1/|V| rule for where the conditioned on ngram = 0 if possible...</paragraph><paragraph>For example:</paragraph><pre>&gt;&gt; model.raw_trigram_probability[('START','START','the')]\n\n&gt;&gt; model.raw_trigram_probability[('fake,'example','here')]\n\n&gt;&gt;&gt; model.raw_bigram_probability[('START','the')]\n\n&gt;&gt;&gt; model.raw_bigram_probability[('fake','example')]\n\n&gt;&gt;&gt; model.raw_unigram_probability[('the',)]\n</pre><paragraph>Thank you so much!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>You could approximate this by doing some calculations on the corpus, especially for the start token:<break/><break/>If you iterate through the corpus and check the first word of every sentence and build a distribution table of that, then you'll find approximately (barring differences stemming from tokenization) how common each word is as a starter word. That should roughly match the result of:<break/></paragraph><paragraph>&gt;&gt; model.raw_trigram_probability[('START','START','the')] <break/></paragraph><paragraph>At least. Because all the bigrams's it's checked against with are START START word and the denominator is just the number of words in the sentence. You should get a decently high value for picking a very common word like The, since lots of sentences start with it.</paragraph></document>"
    },
    {
        "source": "Assignment 1 part 3. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Part 3 wants us to compute the raw probability, then are we still supposed to make P(v | u,w) = 1 / |V| (where |V| is the size of the lexicon), if count(u,w) is 0?</paragraph><paragraph>Thank you</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Yes, you will need a workaround for the situation where count(u,w) is 0 -- otherwise you will end up with a division by 0 error. My suggestion is to set P(v | u,w) = 1 / |V|. </paragraph></document>"
    },
    {
        "source": "Size of lexicon. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Just for clarification, is the size of lexicon basically the same as number of unigrams?</paragraph><paragraph>Thank you</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>It's the size of the set of all unigrams. So if a word appears twice, it's only counted once.</paragraph></document>"
    },
    {
        "source": "Is this allowed to count the total tokens in the text?. <document version=\"2.0\"><paragraph>Hello!<break/><break/>Could I declare a total_tokens class variable, and set it to be the return value of count_ngrams? This way I wouldn't have to traverse the text again. I'm asking since  count_ngrams doesn't originally have any return value.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1hs4pVZmpJdepMEAqnfYrtMG\" width=\"658\" height=\"411.25\"/></figure><paragraph>Thanks for your help!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s fine. But why not just populate the total_token count inside of count_ngrams?\u00a0</paragraph></document>"
    },
    {
        "source": "self in python. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.</paragraph><paragraph/><paragraph>I would like to double check my understanding of self  in python if that is okay</paragraph><paragraph/><paragraph>i know that self refers to  an instance of a class, so if I have an instance variable inst</paragraph><paragraph>i can use it by tying self.inst</paragraph><paragraph> I think that is right but</paragraph><paragraph>I am less sure  using methods that have self as arguments</paragraph><paragraph>for example in the definition of this function:</paragraph><paragraph/><paragraph>def raw_trigram_probability(self,trigram):</paragraph><paragraph/><paragraph>self is in an argument, so does that mean</paragraph><paragraph/><paragraph>that if I later use this method in a different method I can use it like: raw_trigram_probability(self, some_trigram)</paragraph><paragraph/><paragraph>or do i have to say:</paragraph><paragraph> self.raw_trigram_probability(self, some_trigram)</paragraph><paragraph/><paragraph>thank you so much!</paragraph><paragraph/></document>",
        "target": "<document version=\"2.0\"><pre>class Dog:\n    def __init__(self, name):\n        self.name = name\n    def bark(self):\n        print(\"Woof my name is\", self.name)\n\nrover = Dog(\"Rover\")\nrover.bark()\n</pre><paragraph>The \".\" passes the variable on the left as the first argument to the function. So to have Rover bark, you can either do Dog.bark(rover) or rover.bark().</paragraph></document>"
    },
    {
        "source": "Homework Submission. <document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Do we have to comment out all test code before submit?</paragraph><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes please.\u00a0</paragraph></document>"
    },
    {
        "source": "\"UNK\" wrod when calculating perplexity and generating sentences. <document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>Do we need to include the count of the \"UNK\" token when calculating the perplexity and do we need to skip the \"UNK\" word if the next generated word is \"UNK\" and generate a new word instead?</paragraph><paragraph>Thanks a lot!</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>You should include it. From the perspective of the language model UNK is just another token (like any other).\u00a0</paragraph></document>"
    },
    {
        "source": "Clarification Questions. <document version=\"2.0\"><paragraph>Hi, a couple clarification questions:</paragraph><list style=\"number\"><list-item><paragraph>What's the newest version of Python we can use for this assignment?</paragraph></list-item><list-item><paragraph>May we import anything in the standard library to work on this?</paragraph></list-item><list-item><paragraph>May we define custom helper functions?</paragraph></list-item><list-item><paragraph>May we add type hints to all the function signatures?</paragraph></list-item><list-item><paragraph>May we save the total token count in TrigramModel.<bold>init</bold>? If not, where?</paragraph></list-item><list-item><paragraph>In TrigramModel.count_ngrams, are we obligated to use get_ngrams, or may we use any function that yields the correct answer?</paragraph></list-item><list-item><paragraph>Should unigramcounts include \"START\"?</paragraph></list-item><list-item><paragraph>Should unigramcounts include \"STOP\"?</paragraph></list-item><list-item><paragraph>Should the \"if <bold>name</bold> == \"<bold>main</bold>\" clause do anything in particular when we finish? Can we set it to whatever we want?</paragraph></list-item><list-item><paragraph>In TrigramModel.generate_sentence, should the model be capable of producing the \"UKN\" token?</paragraph></list-item><list-item><paragraph>Do raw_{uni,bi,tri}gram_count_total accept tuples of one, two, and three strings, respectively?</paragraph></list-item></list><paragraph>Thank you, Daniel Indictor</paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>That's quite a few questions. Thanks for being efficient. </paragraph><paragraph>1. I would be careful using anything past 3.9</paragraph><paragraph>2. Yes, the standard library is fair game. Third party libraries (including spacy, nltk, and even numpy) should not be used, unless otherwise specified. Generally, the assignments are designed to be solvable with only the information provided. That is, if you need something from the standard library (such as collections.Counter or collections.defaultdict) that will be mentioned in the assignment. </paragraph><paragraph>3. Sure</paragraph><paragraph>4. Yes, because type annotations won't affect functionality. </paragraph><paragraph>5. Yes, that's a good idea. </paragraph><paragraph>6. I would recommend you use get_ngrams. We will test the function by functionality, so if it works correctly without calling get_ngrams that should be fine. But that suggests you are repeating code. </paragraph><paragraph>7. No. </paragraph><paragraph>8. Yes. </paragraph><paragraph>The way to think about counting START and STOP is this: STOP is predicted when the model \"generates\" a sentence, but \"START\" is not. So the unigram event STOP should have some probability assigned to it, but START should not. Of course, then you won't be able to use count(START) when computing P(word | START. I suggest keeping track of the number of sentences instead and treating P(word | START) as a special case. </paragraph><paragraph>9. We won't test the \"main\" part, so it can be anything you want. </paragraph><paragraph>10. This part is optional, so you implement this anyway you like. But I would suggest avoiding the UNK token in the generated output. </paragraph><paragraph>11. Yes </paragraph><paragraph/></document>"
    },
    {
        "source": "raw unigram probability. <document version=\"2.0\"><paragraph>Hi I hope you are doing great! I am working on raw unigram probability method.  Taking the hint in the code:</paragraph><paragraph>#hint: recomputing the denominator every time the method is called # can be slow! You might want to compute the total number of words once, # store in the TrigramModel instance, and then re-use it.</paragraph><paragraph/><paragraph>My idea is to store the length of the corpus_file as an instance variable.  To calculate this it seems I would have to add to the  corpus_reader method that was given to us.  I don't think we are supposed to do that\u2019s  though.</paragraph><paragraph/><paragraph>Is this what the hint meant, or am I misunderstanding something?</paragraph><paragraph/><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>I would count the total number of tokens (and maybe the total number of sentences) in the count_ngrams method. </paragraph><paragraph>The problem with modifying the corpus_reader is that this function is separate from the TrigramModel class, so you wouldn't be able to directly store the token count as an instance variable. </paragraph><paragraph/></document>"
    },
    {
        "source": "Perplexity denominator. <document version=\"2.0\"><paragraph>For the total number of word tokens here, should we count any padding we've added? Or just the actual words?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/PL16cqD0mE2DyDNtFkXeF06y\" width=\"322\" height=\"89\"/></figure><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>You should count the END marker, but not START. The idea is that the number or predictions the model makes does include END tokens, but START is never predicted.\u00a0</paragraph></document>"
    },
    {
        "source": "Question about Katz' Backoff Trigrams. <document version=\"2.0\"><paragraph>I'm confused about how Katz' Backoff for Trigrams works. I wonder if there is a more specific example about it, thank you so much!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ceA2vvo2ZAcoH9eMWG0FpNyj\" width=\"658\" height=\"518.3369803063457\"/></figure></document>",
        "target": "<document version=\"2.0\"><paragraph>The essential idea is: if you count for trigram is not 0, use trigram probability; if it's 0, then backoff to bigram. The idea is that the occurrence of trigram is self contained in bigram, but not the other way around. Then you continue the process from bigram to unigram and etc. <break/><break/>The alpha term is a normalizing factor representing residual probability for backoff to ensure a normalized final probability distribution (summing up to 1). </paragraph></document>"
    },
    {
        "source": "testing count ngrams. <document version=\"2.0\"><paragraph>Hi I hope you are doing great!</paragraph><paragraph>I have a question about these instructions:</paragraph><paragraph><bold>Counting n-grams</bold></paragraph><paragraph>Now it's your turn again. In this step, you will implement the method <code>count_ngrams</code>that should count the occurrence frequencies for ngrams in the corpus. The method already creates three instance variables of TrigramModel, which store the unigram, bigram, and trigram counts in the corpus. Each variable is a dictionary (a hash map) that maps the n-gram to its count in the corpus. <break/>For example, after populating these dictionaries, we want to be able to query</paragraph><pre>&gt;&gt;&gt; model.trigramcounts[('START','START','the')]\n5478\n&gt;&gt;&gt; model.bigramcounts[('START','the')]\n5478\n&gt;&gt;&gt; model.unigramcounts[('the',)]\n61428\n</pre><paragraph>Where<italic> model </italic>is an instance of TrigramModel that has been trained on a corpus. Note that the unigrams are represented as one-element tuples (indicated by the , in the end). Note that the actual numbers might be slightly different depending on how you set things up. </paragraph><paragraph/><paragraph>I am at this step and have been trying to test my count_ngrams, but I dont think I am doing it correctly.</paragraph><paragraph/><paragraph>should i be able to just run this line:</paragraph><paragraph/><pre>model.trigramcounts[('START','START','the')]</pre><paragraph/><paragraph>in the main, or is there some other steps I should be taking?</paragraph><paragraph/><paragraph>I tried putting the brown data in txt in the same folder, but i think I have gotten myself mixed up.</paragraph><paragraph/><paragraph>Thank you so much!</paragraph><paragraph/><paragraph/></document>",
        "target": "<document version=\"2.0\"><paragraph>you can run the program using command python trigram_model.py hw1_data/brown_train.txt. </paragraph><paragraph>sys.argv[1] is the parameter name for the input file</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/P8DxDwsBa6vSQYhfBIpHgQpJ\" width=\"726\" height=\"131.16806722689074\"/></figure><paragraph>if you run the program with brown_train.txt instead of brown_test.txt, you should get the same number as the example. that's what I have at least... </paragraph><paragraph>and of course, you need to print the counts out using print statements under main</paragraph></document>"
    },
    [
        {
            "source": "testing incrementally. <document version=\"2.0\"><paragraph>Hi I hope you are having a great day!</paragraph><paragraph/><paragraph>Im working through part 2, but i am unsure of how to test this part.  Do you have any advice on how to most effectively test the code part by part.  </paragraph><paragraph/><paragraph>Thank you so much! </paragraph></document>",
            "target": "<document version=\"1.0\"><paragraph>In general, I would try to run the various parts on a small toy training corpus so that you can check counts and probabilities by hand.\u00a0</paragraph></document>"
        },
        {
            "source": "testing incrementally. <document version=\"2.0\"><paragraph>Hi I hope you are having a great day!</paragraph><paragraph/><paragraph>Im working through part 2, but i am unsure of how to test this part.  Do you have any advice on how to most effectively test the code part by part.  </paragraph><paragraph/><paragraph>Thank you so much! </paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>i set up a unittest to do it; it helps a lot for automating testcases for edge cases. It also helps for not flooding stdout as only failed test cases are printed.</paragraph></document>"
        }
    ],
    {
        "source": "part 2 default dict or collection. <document version=\"2.0\"><paragraph>Hi I hope you are doing great! In Part 2s code it has a comment:</paragraph><paragraph/><pre>        self.unigramcounts = {} # might want to use defaultdict or Counter instead\n        self.bigramcounts = {}\n        self.trigramcounts = {}</pre><paragraph/><paragraph>so if i want use a default dict or Counter, is it okay if I just change the definitions above?<break/><break/>Also there is an empty return at the end of the method.  If I'm not returning anything should I leave the return there or can i remove it?</paragraph><paragraph/><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>Yes you can simply replace the initialization. \u00a0</paragraph><paragraph>You can remove the return statement at the end.\u00a0</paragraph></document>"
    },
    {
        "source": "Part 2 Clarification. <document version=\"2.0\"><paragraph>Hi I hope you are doing great.  Even though we are not changing this part of the given code, I'm a little unclear on this part : \"The idea is that words that appear only once are so rare that they are a good stand-in for words that have not been seen at all in unseen text. You do not have to modify this function.\"  </paragraph><paragraph/><paragraph>What does this actually mean? How are they a stand- in?</paragraph><paragraph/><paragraph>Thank you so much!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>This is because the words that appears only once are so rare so that you can treat them as any other OOV words that you might encounter in the future. The main intuition behind this approach is to (1) it allows you to cuts off the long tail of the distribution (2) create smooth probability for new OOV words.</paragraph><paragraph/></document>"
    },
    {
        "source": "Answers to Ungraded Exercise. <document version=\"2.0\"><paragraph>Where can we find the answers to the ungraded exercises, if there is any?</paragraph><paragraph/></document>",
        "target": "<document version=\"1.0\"><paragraph>I will post these usually with a delay. So if it isn\u2019t up yet, check back the next day.\u00a0</paragraph></document>"
    },
    {
        "source": "Clarification on unseen context & unseen token. <document version=\"2.0\"><paragraph>As covered during today's lectures I'm a bit confused between the difference of unseen tokens, unseen contexts and unknown tokens. </paragraph><paragraph>Is unseen tokens merely the process of replacing words with unknown tokens and unseen contexts the process of assigning values to the the unknown tokens to mellow out the spikes? </paragraph><paragraph>Thanks!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Hello! This is how I understand it, TA's please correct me if I'm wrong:</paragraph><paragraph>The idea of something being \"seen\" has to do with whether or not the model has encountered it (the token, the ngram) after going through all the training data. </paragraph><paragraph>For individual <bold>unseen tokens</bold>, the question is: how do we have our model handle tokens in the test data that we never encountered in the training data? The approach we discussed in lecture was: we treat all tokens we haven't seen as the same token, which we label as unknown (<code>UNK</code>). </paragraph><paragraph>Now the question is, how do we prepare our model to handle the token <code>UNK</code>? The strategy we discussed was to pick a number <code>k</code>, and say that for any token in our training data, if the token shows up fewer than <code>k</code> times, we replace it with <code>UNK</code>. Then we treat all instances of <code>UNK</code>  in our training data the same as any other token, calculating unigram and ngram probabilities or whatever approach we're choosing.</paragraph><paragraph>Now that we've made sure that our model thinks it's seen every individual token it encounters in the test data, the question for <bold>unseen contexts</bold> was: how do we calculate the probability of some ngram, e.g. the trigram (w | u, v), when it's possible that  <code>count(u, v)</code> is 0, meaning, we've never seen <code>v</code> in the context of <code>u</code>. And that part of lecture discussed different strategies for figuring out how to calculate probabilities like (w | u, v) given that possibility. </paragraph><paragraph>I hope that helps!</paragraph></document>"
    },
    [
        {
            "source": "HW1 Part 1. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>In part 1 example, </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/vN8TlnNobHPKnPzi6gBgxNz5\" width=\"658\" height=\"48.073059360730596\"/></figure><paragraph>The first return element has two 'START's, whereas the previous n=1 and n=2 examples have only 1 'START' in the first return element. </paragraph><paragraph>I am confused on the padding rules here...</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I have the same question. Is it an arbitrary pattern? </paragraph></document>"
        },
        {
            "source": "HW1 Part 1. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>In part 1 example, </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/vN8TlnNobHPKnPzi6gBgxNz5\" width=\"658\" height=\"48.073059360730596\"/></figure><paragraph>The first return element has two 'START's, whereas the previous n=1 and n=2 examples have only 1 'START' in the first return element. </paragraph><paragraph>I am confused on the padding rules here...</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>From my understanding, we pad n-grams with \"START\" tokens to provide the appropriate context (i.e., indicate the start of a sentence).</paragraph><paragraph>In the case of bigrams, we pad the first bigram with one \"START\" token to generate the bigram that only includes the first word. In the case of trigrams, to generate the trigram that only includes the first word, we need to pad it with two \"START\" tokens, and to generate the trigram that only includes the first two words, we need to pad it with one \"START\" token. And so on... for quadgrams, we would need three \"START\" tokens in the first quadgram, etc.</paragraph><paragraph>For instance, in the case of trigrams, if we only had one \"START\" token, the first trigram generated would be (\"START\", \"natural\", \"language\") (i.e., a sentence that begins with \"natural language\"). But this precludes something else we might be interested in: the sentence that starts with just \"natural\"! So we would also include the trigram (\"START\", \"START\", \"natural\").</paragraph><paragraph>In some implementations, we would actually directly add the (n-1) \"START\" tokens to the beginning of the sentence and generate the n-grams that way, but the gist is that the number of \"START\" tokens depends on the number of empty/incomplete \"spots\" at the beginning of the n-gram that we need to fill in. </paragraph></document>"
        },
        {
            "source": "HW1 Part 1. <document version=\"2.0\"><paragraph>Hi Prof. Bauer and TAs,</paragraph><paragraph>In part 1 example, </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/vN8TlnNobHPKnPzi6gBgxNz5\" width=\"658\" height=\"48.073059360730596\"/></figure><paragraph>The first return element has two 'START's, whereas the previous n=1 and n=2 examples have only 1 'START' in the first return element. </paragraph><paragraph>I am confused on the padding rules here...</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>In my opinion, after today's class, I understand that we need two w_0 word vector to create Markov chain to predict the future probability of the next word, so there should be two 'START' to create a beginning. Hope this can help you!</paragraph></document>"
        }
    ],
    {
        "source": "Extra Practice - Naive Bayes. <document version=\"2.0\"><paragraph>Hi there!<break/><break/>I'd like to know if the teaching team has any recommendations as to what sources we should use to get extra experience with topics covered in ungraded assignments such as Naive Bayes. It would be great to train and check solutions to verify our understanding, even if the assignments themselves don't affect our grades. <break/><break/>Thank you!</paragraph></document>",
        "target": "<document version=\"1.0\"><paragraph>That\u2019s what the ungraded exercises are supposed to be for. You could check the Jurafsky and Martin textbook to see if there are any additional exercises.\u00a0</paragraph></document>"
    },
    [
        {
            "source": "Conditional Independence Question. <document version=\"2.0\"><paragraph>Hi I hope you are doing great,  Sorry for all the questions.<break/><break/>On a slide from today:</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/fs7Zz39d54Q9xlxpkejcM749\" width=\"563\" height=\"152.72183908045977\"/></figure><paragraph>I get the first line that B and C are becoming independent of each other.<break/><break/>But I dont get the equivalently part.  Like for each part below is it only true given A, or shouldn't we also meet the criteria of  =  P(B| C), and =P(C| B)  (for each part respectively wouldn't that also have to be true)</paragraph><paragraph/><paragraph/><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>I believe that if we were to add the criteria of = P(B| C), and =P(C| B) then we would no longer have events B, C solely dependent on A but rather dependent on each other and on A. This would mean that they would no longer be conditionally independent. </paragraph></document>"
        },
        {
            "source": "Conditional Independence Question. <document version=\"2.0\"><paragraph>Hi I hope you are doing great,  Sorry for all the questions.<break/><break/>On a slide from today:</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/fs7Zz39d54Q9xlxpkejcM749\" width=\"563\" height=\"152.72183908045977\"/></figure><paragraph>I get the first line that B and C are becoming independent of each other.<break/><break/>But I dont get the equivalently part.  Like for each part below is it only true given A, or shouldn't we also meet the criteria of  =  P(B| C), and =P(C| B)  (for each part respectively wouldn't that also have to be true)</paragraph><paragraph/><paragraph/><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>So B and C are conditionally independent given A. Here is the full chain of equations that might be helpful. Derived using the chain rule of probability (line 1) and rule of conditional independence (line 2), </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Y262WhX8ClxhH12ya6XFVjvJ\" width=\"725\" height=\"145\"/></figure></document>"
        },
        {
            "source": "Conditional Independence Question. <document version=\"2.0\"><paragraph>Hi I hope you are doing great,  Sorry for all the questions.<break/><break/>On a slide from today:</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/fs7Zz39d54Q9xlxpkejcM749\" width=\"563\" height=\"152.72183908045977\"/></figure><paragraph>I get the first line that B and C are becoming independent of each other.<break/><break/>But I dont get the equivalently part.  Like for each part below is it only true given A, or shouldn't we also meet the criteria of  =  P(B| C), and =P(C| B)  (for each part respectively wouldn't that also have to be true)</paragraph><paragraph/><paragraph/><paragraph>Thank you!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>Both the equations make sense. The equation on the top makes sense because both B and C are independent to each other, such that A has already occurred. </paragraph><paragraph>The second equation simply suggests that the knowledge of C occurring does. Ot provide any knowledge of B occurring and vice-versa. The left equation suggests that even if both A and C were to occur together jointly, only the occurrence of A alone would suffice to suggest about the occurrence of B. The equation on the right suggests that even if A and B were to occur together, only the occurrence of A alone would suffice to suggest about the occurrence. Both together imply that the occurrence of B in no way affects occurrence of C and vice-versa, even if these variables were to occur in the presence of A (jointly), whose occurrence does provide information of the occurrence of both B and C, as these two are dependent on A. </paragraph></document>"
        }
    ],
    [
        {
            "source": "denominator of bayes rules. <document version=\"2.0\"><paragraph>Hi I hope you are doing great!</paragraph><paragraph>In class when we were doing examples  and using bayes rule, we were able to just ignore the denominator.  Is this just because we can drop the denominator anytime we are calculating the probability of a particular class,  or is there some other rule that decides when we can do the calculation without the denominator.</paragraph><paragraph/><paragraph>Thnak you so much!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>The denominator can take on the value of some constant alpha (as defined in class). This is because for each calculation the denominator value remains the same. When looking for a label y that can maximize the probability we can choose based on a reduced simpler version that has the alpha.</paragraph></document>"
        },
        {
            "source": "denominator of bayes rules. <document version=\"2.0\"><paragraph>Hi I hope you are doing great!</paragraph><paragraph>In class when we were doing examples  and using bayes rule, we were able to just ignore the denominator.  Is this just because we can drop the denominator anytime we are calculating the probability of a particular class,  or is there some other rule that decides when we can do the calculation without the denominator.</paragraph><paragraph/><paragraph>Thnak you so much!</paragraph></document>",
            "target": "<document version=\"2.0\"><paragraph>In other words, we are always comparing two Probabilities, P(y=1 | x) vs P(y=0 | x). When you write them out using Bayes rules, you will notice that on both sides of the equation, the same denominator p(x) is present. Algebraically we can cancel this from our equation, so when we are formalizing what the Naive Bayes Classifier <bold>score</bold> is, we can just ignore the denominator completely. </paragraph></document>"
        }
    ],
    {
        "source": "symbol question. <document version=\"2.0\"><paragraph>Hi I hope you  are doing great.</paragraph><paragraph>What does this symbol stand for:</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/827Yp02GNOgFONKJRx1344NG\" width=\"64\" height=\"118\"/></figure><paragraph>Thank you!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>The capital pi is the product operator, and you can think of it as the multiplicative analog of the summation operator (capital sigma).</paragraph><paragraph>For instance:</paragraph><math>\\sum_{n=1}^4n=1+2+3+4=10</math><math>\\prod_{n=1}^4n=1\\cdot2\\cdot3\\cdot4=24</math></document>"
    },
    {
        "source": "TA's OHs & HW Due Dates. <document version=\"2.0\"><paragraph>Greetings to all the teaching staffs,</paragraph><paragraph>Does anyone have an idea when the TAs' OHs and the DUE dates for all assignments will be up for this course?</paragraph><paragraph>Thank you for your time and attention!</paragraph></document>",
        "target": "<document version=\"2.0\"><paragraph>Dear all,</paragraph><paragraph>We've posted our TA Office Hours on the Canvas syllabus. Although due dates vary from Mon-Fri, we'll try our best to ensure there's an Office Hour on each due date. Check Canvas for details.</paragraph><paragraph/></document>"
    }
]