[
    {
        "source": "#827: Solution to final exam&HW5. Can we get solution to final exam as well as HW5?",
        "target": "Itll take a while for me to get these up, but I can post them eventually. "
    },
    {
        "source": "#826: Canvas Average. Hi I hope you are doing great.  Is the average currently reflected in Canvas, the accurate final grade?  Or do I need to any other calculation on my own? It looks like everything is updated, but I just want to make sure if that is okay. Thank you!",
        "target": "Courseworks does not support dropping and assignment, so you need to calculate the final score yourself using the formula on the syllabus. "
    },
    {
        "source": "#825: Lowest Programming Assignment Dropped? . Hi NLP Teaching Staff, Hope the new year is off to a good start for all of you. I just wanted to make sure that the lowest programming assignment was dropped in my final grade. I calculated my final grade with my lowest assignment dropped and it seems to be different from the one appearing on courseworks. I did not submit hw3 because I took it as my dropped assignment. Thank you for your hard work this semester! ! Hanna Martin",
        "target": "The lowest assignment was dropped in my final grade calculation. The score on coursework does not reflect the drop because coursework doesnt support elaborate grading logic like this. "
    },
    {
        "source": "#824: No regrade request option for final exam marks. I am not able to find any regrade request option in the gradescope for any of the questions in the final exam marks. How do we put them?",
        "target": "First, make sure that you absolutely need a regrade request. A few points will likely not affect your letter grade at all. Please limit any such requests to major issues (for example, if the grader completely missed your answer because it was on a different page). You can email me requests directly, but I may not be able to respond to them before I release final grades. If a request results in a change of letter grade, I will have to submit a change-grade request later. "
    },
    {
        "source": "#822: HW4 Grade. Hi NLP teaching staffs - I am wondering whether hw4 grade has been released as I only saw the grade for hw5 on my Canvas. Thank you so much!",
        "target": "Unfortunately we are missing some part of hw4 and I cant release them before everything is in. But the final exam is fully graded and I should be able to push those grades out tomorrow. "
    },
    {
        "source": "#820: last minute error converting to pdf. Hi I had a little situation when submitting hw5 a few minutes ago my jupter notebook stoped responding when I try to generate the pdf, I didn't submit the pdf successfully but I did submit the notebook (ipynb file), and I included the pdf file in this post. what should I do? do I resubmit?",
        "target": "Please resubmit, its much easier for grading to have everything in one place. Obviously, we will count this as on-time. "
    },
    {
        "source": "#819: HW 5 Jupyter Notebook Input. Hello! I was running into an issue earlier where my Jupyter Notebook would become unresponsive, so I would restart the kernel (https://edstem. org/us/courses/46417/discussion/4078889? answer=9381425). However, whenever I'm attempting to input something into my Jupyter Notebook (i. e. when it asks me if I want to replace a file and to answer with [y/n]) I'm unable to find a space to input a response. So far, I've just been working around this by deleting the files in ontonotes_srl. zip. For convenience's sake, is there a way to access an input box for this assignment?",
        "target": "i believe you could just skip this box if you already get to the later steps in ur previous attempts!"
    },
    {
        "source": "#818: HW5 Submission. Should we submit notebook and pdf in one zip file? Or should we upload two files separately into CourseWorks?",
        "target": "I just click \"add another file\" and loaded them both. Pretty sure if you submit a ZIP it will extract it out anyways."
    },
    {
        "source": "#818: HW5 Submission. Should we submit notebook and pdf in one zip file? Or should we upload two files separately into CourseWorks?",
        "target": "Two separate files please. "
    },
    {
        "source": "#817: HW 5 Part 3 length-related questions. For part 3, I kept getting the length errors. RuntimeError: stack expects each tensor to be equal size, but got [128] at entry 0 and [161] at entry 13\nI tried to look through many posts in the past and added codes to truncate (1) and skip long sentences (2). I even try to skip allllllllll. kinda stuck hereCan anyone tell me how to truncate the data / skip the too-long sentence correctly? ?",
        "target": "I truncated in the getitem method in 1. 2. 3 and that fixed that error for me"
    },
    {
        "source": "#817: HW 5 Part 3 length-related questions. For part 3, I kept getting the length errors. RuntimeError: stack expects each tensor to be equal size, but got [128] at entry 0 and [161] at entry 13\nI tried to look through many posts in the past and added codes to truncate (1) and skip long sentences (2). I even try to skip allllllllll. kinda stuck hereCan anyone tell me how to truncate the data / skip the too-long sentence correctly? ?",
        "target": "This should be handled on the level of the dataset, not the training loop. "
    },
    {
        "source": "#816: HW5 Part3 Training Loss, Accuracy, and Epoch. My model reaches very low training loss(less than 0. 19) and high accuracy(more than 0. 94) after the first epoch. Is it normal to get this behavior?",
        "target": "I believe most people get a training accuracy in the 80s after the first epoch."
    },
    {
        "source": "#814: General Question About Google Cloud. I am stopping my Google Cloud instance when not in use in order to save credits, but when I try to restart the VM, it gives me this error again: A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-west1-b zone. Is there any way to move all these resources to another region, or is the only way forward to create a new VM entirely with identical hardware and re-clone everything again? Just wondering what best practice is here. I never run into the instance unavailable error with other cloud services. Thanks in advance.",
        "target": "https://edstem. org/us/courses/46417/discussion/4078868"
    },
    {
        "source": "#813: CUDA out of memory. I am wondering if there are any quick solution for this? ?? I tried to restarted the kernel and it sometimes work but still getting a lot :(((",
        "target": "Something that worked for me was changing the batch_size to 16. "
    },
    {
        "source": "#813: CUDA out of memory. I am wondering if there are any quick solution for this? ?? I tried to restarted the kernel and it sometimes work but still getting a lot :(((",
        "target": "Same issue, any solution for this?"
    },
    {
        "source": "#811: Part 6 Score. I'm getting an F score of about 0. 845 Is that reasonable?",
        "target": "Yes, that seems fine!"
    },
    {
        "source": "#809: HW5-PDF submission. I printed many debug logs during the data loading and the training process. Then I realized that these cell outputs for debugging purposes makes the pdf saving process extremely long. I was wondering if it is okay to clear the output of some of these cells, including the train() cell, but mainly leave the evaluation result cells in output for pdf submission?",
        "target": "Okay thats fine. Maybe keep a comment that you removed the output because there were too many debugging messages. "
    },
    {
        "source": "#808: Jupyter Notebook Won't Load. Hi, I was working on Part 5 when my Jupyter Notebook started being unresponsive. I've tried restarting the VM multiple times, but I can't get my notebook to load. It just shows a blank screen for a bit, and then it eventually crashes. Does anyone have any tips for how to fix this? ",
        "target": "here are a couple of ideas:1. restart jupyter kernel:If the Jupyter Notebook itself is unresponsive, you might try restarting the Jupyter kernel. You can do this from the Jupyter interface or use the following command in a Jupyter cell:pythonCopy code\nimport os\nos. _exit(00)2. clear output cells:Sometimes, large outputs in Jupyter cells can cause performance issues, so if you have large outputs, try clearing the output cells and running the notebook again. 3. check disk space:Check if there's enough disk space on your VM instance. You can use the following command in a Jupyter cell to check disk space:pythonCopy code\n! df -h"
    },
    {
        "source": "#807: HW5 - VM instance won't start. I'm trying to reopen the VM instance I have but got this error. I already have work done for this assignment on the current VM instance I have. Would opening a new instance make the current work I have disappear?",
        "target": "Yes  although maybe you can disconnect the GPU, start the instance, save the notebook, then start a new GPU instance in a different availability zone and upload the work. "
    },
    {
        "source": "#806: Weird error for Hw 5 part 3. Hi! I am trying to understand part 3's code so I am moving through the batches and trying to print the ids, but for some reason I encounter this error. &lt;__array_function__ internals&gt; in reshape(*args, **kwargs)\n\n/opt/conda/lib/python3. 7/site-packages/numpy/core/fromnumeric. py in reshape(a, newshape, order)\n    296            [5, 6]])\n    297     \"\"\"\n--&gt; 298     return _wrapfunc(a, 'reshape', newshape, order=order)\n    299 \n    300 \n\n/opt/conda/lib/python3. 7/site-packages/numpy/core/fromnumeric. py in _wrapfunc(obj, method, *args, **kwds)\n     55 \n     56     try:\n---&gt; 57         return bound(*args, **kwds)\n     58     except TypeError:\n     59         # A TypeError occurs if the object does have such a method in its\n\nValueError: cannot reshape array of size 146 into shape (1, 128)\nMy part 1 works fine, so I don't really understand what's throwing this error. Why is my token tensor longer than 128 even though I reshaped the array? ",
        "target": "I was running into what I think is the same issue, after some debugging it seems that there are simply just some sentences, once tokenized, have over 128 tokens. I've attached an example below. I'm not sure how to handle these cases, or if somehow I'm not preprocessing everything correctly in Part 1. The only idea I have right now to handle this is to simply not store the sentence in self. items if it has a token length &gt; 126 (to account for [CLS] and [SEP] tokens)"
    },
    {
        "source": "#806: Weird error for Hw 5 part 3. Hi! I am trying to understand part 3's code so I am moving through the batches and trying to print the ids, but for some reason I encounter this error. &lt;__array_function__ internals&gt; in reshape(*args, **kwargs)\n\n/opt/conda/lib/python3. 7/site-packages/numpy/core/fromnumeric. py in reshape(a, newshape, order)\n    296            [5, 6]])\n    297     \"\"\"\n--&gt; 298     return _wrapfunc(a, 'reshape', newshape, order=order)\n    299 \n    300 \n\n/opt/conda/lib/python3. 7/site-packages/numpy/core/fromnumeric. py in _wrapfunc(obj, method, *args, **kwds)\n     55 \n     56     try:\n---&gt; 57         return bound(*args, **kwds)\n     58     except TypeError:\n     59         # A TypeError occurs if the object does have such a method in its\n\nValueError: cannot reshape array of size 146 into shape (1, 128)\nMy part 1 works fine, so I don't really understand what's throwing this error. Why is my token tensor longer than 128 even though I reshaped the array? ",
        "target": "I think you can truncate to 128 tokens based on #757 "
    },
    {
        "source": "#805: attention mask. Hi, Should the attention mask in part 1 have a 1 in the positions for the CLS and SEP token? ",
        "target": "Yes"
    },
    {
        "source": "#804: BERT outputting nonsense after [SEP] token. . When I trained my model and evaluated it via the decoding exercise and the token accuracy, my model seemed fine. However, my precision, and F1 scores are really poor. I output the labels my model is making to see what the problem is. What they are supposed to be:['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']What my model is outputting:['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'I-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O']It seems that where the model is supposed to be outputting \"[PAD]\" tokens, it is instead outputting random nonsense. Since this area should be masked to 0 by the attention mask, I am confused as to what is happening here. Does anyone have any ideas/suggestions for what to look for or try? Thanks!",
        "target": "My model did the same thing, I don't think this is an issue as long as your SEP token is in the right spot."
    },
    {
        "source": "#804: BERT outputting nonsense after [SEP] token. . When I trained my model and evaluated it via the decoding exercise and the token accuracy, my model seemed fine. However, my precision, and F1 scores are really poor. I output the labels my model is making to see what the problem is. What they are supposed to be:['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']What my model is outputting:['[CLS]', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'B-ARG1', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'I-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O']It seems that where the model is supposed to be outputting \"[PAD]\" tokens, it is instead outputting random nonsense. Since this area should be masked to 0 by the attention mask, I am confused as to what is happening here. Does anyone have any ideas/suggestions for what to look for or try? Thanks!",
        "target": "Yes multiplying the prediction matrix with the attention mask is a great approach. "
    },
    {
        "source": "#803: Outputs seem to be shifted incorrectly. . When I get to the decoding problem, I notice that my output labels seem correct; they just seem to be shifted incorrectly by 1. Did anyone else run into this? What was the reason behind it?",
        "target": "I figured this out. I didn't remove the CLS token, so they weren't aligned!"
    },
    {
        "source": "#802: HW 5 Part 3 Training Loss and Accuracy. Hello Professor, My model was finally able to train properly! I got the following loss and accuracy after 2 epochs, and I just wanted to make sure my numbers are reasonable, especially since the second training epoch loss of 0. 148 is a little lower than the expected value of 0. 19.",
        "target": "That looks good. "
    },
    {
        "source": "#801: Going back to original tokens. Hi! For part 6, I am trying to find a way to get the original list of tokens prior to tokenization. Is there an official way of doing so? I have tried using tokenizer. decode, but found that it is not accurate.",
        "target": "Yes its not necessary to detokenize. "
    },
    {
        "source": "#799: HW 5 Incredibly high batch loss values. Hi! I'm getting super high batch loss values when I train the model in Part 3. I've spent hours trying to find the issue and have changed a few things, but still getting these kinds of values. Is there anything you recommend that I check in my code?",
        "target": "Hard to tell. Maybe there is something wrong with the target labels?  They could be mismatched. "
    },
    {
        "source": "#798: Moving dataset to GPU? . Maybe I'm misreading this code, but is there a reason we don't move the entire dataset into the GPU? Given its footprint is small, I think it would eliminate what would normally be lots of expensive GPU &lt;&gt; CPU communication.",
        "target": "You could try this. Let me know if it works? "
    },
    {
        "source": "#797: HW 5 Negative probabilities(? ). The model's output is a tensor of shape (1, 128, 53), where 128 is presumably the sentence length and 53 is a probability distribution over the labels per slot. When I print this, some of those 53 probabilities are negative. Am I misunderstanding the output of the model, or is this normal?",
        "target": "The last layer is a linear layer, which means it has not been fed through a softmax layer. This is not supposed to be directly considered a probability distribution."
    },
    {
        "source": "#796: HW 5 [prd] in tag output. When I print out my decoded tokens in part 2, some of them are \"[prd]\". Is anyone else getting that? I'm assuming there's an issue here. ",
        "target": "[prd] is part of the role dictionary, and so without training its one of the possible outputs. The tag was likely left over from me experimenting. It should never appear in the training data though and should not be predicted once the model is trained. "
    },
    {
        "source": "#795: HW 5 training. While calculating accuracy, should we use weighted average accuracy or just average accuracy across iterations? ",
        "target": "Just averaging across iterations will be fine. "
    },
    {
        "source": "#793: HW 5 Part 4 Model predicts 'O' tag. I am working on part 4 and I noticed that the highest recommended tag is often 'O' Here is my result with the example given in 4. The 'B-V' tag is accurate but it doesn't seem like the other tags are predicted. It is not because the other predicted tags are not in the id_to_role dictionary, but literally the 'O' tag is getting predicted. Does anyone know why this might be happening? The model inputs are input_ids and attn_mask processed similar to when we loaded in the dataset (part 1).  The input pred_indicator is also similar to the part 1 implementation, I just set pred_indicator[pred_index] = 1",
        "target": "Because the O tag is by far the majority tag, it seems reasonable that the model would learn to default to it. It may mean you need to train longer. What does the training loss and accuracy look like? "
    },
    {
        "source": "#793: HW 5 Part 4 Model predicts 'O' tag. I am working on part 4 and I noticed that the highest recommended tag is often 'O' Here is my result with the example given in 4. The 'B-V' tag is accurate but it doesn't seem like the other tags are predicted. It is not because the other predicted tags are not in the id_to_role dictionary, but literally the 'O' tag is getting predicted. Does anyone know why this might be happening? The model inputs are input_ids and attn_mask processed similar to when we loaded in the dataset (part 1).  The input pred_indicator is also similar to the part 1 implementation, I just set pred_indicator[pred_index] = 1",
        "target": "Hi. I had a similar issue as well. Not sure if we made the same mistake, but my mistake was that I called tokenizer. tokenize(tokens) which messed up the tokens since they were already tokenized. Once I deleted that line, everything worked as intended."
    },
    {
        "source": "#793: HW 5 Part 4 Model predicts 'O' tag. I am working on part 4 and I noticed that the highest recommended tag is often 'O' Here is my result with the example given in 4. The 'B-V' tag is accurate but it doesn't seem like the other tags are predicted. It is not because the other predicted tags are not in the id_to_role dictionary, but literally the 'O' tag is getting predicted. Does anyone know why this might be happening? The model inputs are input_ids and attn_mask processed similar to when we loaded in the dataset (part 1).  The input pred_indicator is also similar to the part 1 implementation, I just set pred_indicator[pred_index] = 1",
        "target": "i am having this same issue (#779) but my loss and accuracy are the same as professor bauer after 2 epochs"
    },
    {
        "source": "#792: GCP VM Instance Stops When Training. Hello, I am currently training my model on part 3, and everything is working well. My issue is after about an hour and a half, the VM shuts down and times out, so all of my training progress gets lost. Is there anything I can do to prevent this from happening? I saw someone on EdStem said to use the \"screen\" command, but the issue stemmed from the VM instance itself timing out.",
        "target": "What do you mean the VM instance timed out? My problem was with my ssh session timing out so using screen to set the process to be in the background worked for me. "
    },
    {
        "source": "#791: HW5. Hello, I have been working primarily with Google Cloud for HW 5. I have been experiencing on and off issues. I am just checking to make sure Google Colab works and we don't need to do anything else but change to TCP4. Thanks!",
        "target": "So the assignment is meant to be solved on GCP and doesnt have to run anywhere else. But it should work on colab just fine, provided you get a GPU runtime. No changes to the actual code should be required. "
    },
    {
        "source": "#790: high loss and low accuracy values for part 3. Hi, I'm getting these values for part 3 after 2 epoch. Is this ok or are these off by too much? I'm not sure what could be wrong because everything up to this point, I've been getting the expected output. (similar loss value in part 2 for example). Some additional weird behavior I'm seeing. 1. The two epochs only take about 20-30 minutes to train for me, not several hours like is expected. The only thing I changed in the given train() code was computing the accuracy part. 2. this is epoch 2. The loss jumps up from 0. 237 to 0. 41 and stays around there until it's done. 3. I saved this model and used it for part 4 and 5. And I got 0. 92227 accuracy for token-based accuracy, which doesn't seem terrible.",
        "target": "Hm the training loss seems a bit high, but if you get reasonable results during testing it may not be an issue. Its normal for the loss to jump around a little bit. "
    },
    {
        "source": "#789: HW5 part 1. 2. 3. Hi, I'm unclear as to what the parameter k is supposed to be. Can anyone clarify?",
        "target": "I'm not entirely sure, but I interpreted k as the index used to retrieve data from self. items."
    },
    {
        "source": "#788: k in 1. 2. 3 __getitem__(self, k). is k in this section the actual item we need to process or the index of an item from our list of items?",
        "target": "k is the index of the item that should be returned. Presumably, the data was just loaded into a list. Its just the index in this list. "
    },
    {
        "source": "#787: Inconsistency in data size. Hi, In part 1. 2. 3, it is required that Each sequence must be a pytorch tensor of shape (1, 128). However, in part 3, each batch are of the shape (32, 1, 128). To make sure that the shape is maintained correctly as (32, 128), do we change part 1. 2. 3 into a tensor of shape 128, or modify relevant codes in the dataloader in part 3?",
        "target": "I had the same question! Did you find a fix for this? Nvm, looks like it's covered in #773!"
    },
    {
        "source": "#787: Inconsistency in data size. Hi, In part 1. 2. 3, it is required that Each sequence must be a pytorch tensor of shape (1, 128). However, in part 3, each batch are of the shape (32, 1, 128). To make sure that the shape is maintained correctly as (32, 128), do we change part 1. 2. 3 into a tensor of shape 128, or modify relevant codes in the dataloader in part 3?",
        "target": "I would change part 1. 2. 3. "
    },
    {
        "source": "#786: Key error. Hi, I'm on the last part of part 2. I'm trying to take the argmax over every position. This is the part of the instructions I'm on (listed below).  However, when I do this my result looks like this. For some reason, some of my predictions are 0, and then when I try to use 0 with the id_to_role dictionary to decode the actual tokens, I get a key error because 0 is not a key in the id_to_role dictionary. Does anyone know what may be causing this? The loss I got from part 2 (3. 89) was  close to the expected loss (3. 97) so I think my parts 1 and 2 are correct so far.",
        "target": "Not a TA, but the ids of the roles are 1-indexed, so I added 1 to the argmax's. Not sure if this is correct though"
    },
    {
        "source": "#786: Key error. Hi, I'm on the last part of part 2. I'm trying to take the argmax over every position. This is the part of the instructions I'm on (listed below).  However, when I do this my result looks like this. For some reason, some of my predictions are 0, and then when I try to use 0 with the id_to_role dictionary to decode the actual tokens, I get a key error because 0 is not a key in the id_to_role dictionary. Does anyone know what may be causing this? The loss I got from part 2 (3. 89) was  close to the expected loss (3. 97) so I think my parts 1 and 2 are correct so far.",
        "target": "I think that this is happening because this is before training . One of the posts says to return 'O' for values not in the dictionary "
    },
    {
        "source": "#785: GCP Keeps on Crashing. Hi, when I am trying to train my model on GCP, it keeps on crashing it would run for some time but then would throw an error, how can I fix this?",
        "target": "What is the error?"
    },
    {
        "source": "#784: HW5 Q2 loss unstable despite averaging. Hi, For HW2 Q2 I have tried the averaging different instantiations of the model trick mentioned in other posts. When averaging 5 results, the average loss ranges from about 3. 84 to 4. 06. When averaging 10 results, the error still goes around 3. 91 to 4. 01. Is this considered acceptable result?",
        "target": "That looks good to me. There is really no guarantee it will be exactly the expected value, but with more samples you should get closer. "
    },
    {
        "source": "#783: Regrade Request HW3. Hello, I recently saw that I received a 25/30 on Part 3 for having too many dense layers. While I agree that this representation is incorrect according to the specifications of Part 3, I ended up submitting an experiment for changing the model (as was encouraged in Part 4 and performs better than the model specified in Part 3). In doing so, I changed the structure of what was asked from us in Part 3. While I understand, it is my fault for deviating from the instructions and I understand if I do not get the points back, I thought submitting a better version of the model asked from us in part 3 would suffice in its place. ",
        "target": "Please send an email so I have it in my inbox. Im processing these slowly. "
    },
    {
        "source": "#782: Hw 5 padding the sentence and tokens. I'm a bit confused on how to pad the tokens in part 1. 2. 3. The instructions say, \"We need to process the sentence by adding \"[CLS]\" as the first token and \"[SEP]\" as the last token. The need to pad the token sequence to 128 tokens using the \"[PAD]\" symbol. This needs to happen both for the inputs (sentence token sequence) and outputs (BIO tag sequence). \"Does that mean we need to add \"[CLS]\" , \"[SEP]\", and \"[PAD]\"  to the BIO tag sequence too?",
        "target": "Thats correct. Both the input and target need to be processed that way. "
    },
    {
        "source": "#781: HW 5 Part 2 Loss function runtime error. I made the changes suggested in  #624 and it seemed like my model was created successfully (previously was getting the error described in #624). However, I'm still getting the same error again when computing the loss for one item. I am making sure to input the correct dimensions for output and targets, and I'm not sure how to fix this as I already made sure to edit the model code so it looks like this now. Does anyone know if there is any other ways to resolve the error?",
        "target": "Update in case others have the same issue: I manually pushed the target tensor to GPU with . cuda() so that both target and outputs were on the GPU and it works now!"
    },
    {
        "source": "#781: HW 5 Part 2 Loss function runtime error. I made the changes suggested in  #624 and it seemed like my model was created successfully (previously was getting the error described in #624). However, I'm still getting the same error again when computing the loss for one item. I am making sure to input the correct dimensions for output and targets, and I'm not sure how to fix this as I already made sure to edit the model code so it looks like this now. Does anyone know if there is any other ways to resolve the error?",
        "target": "I believe targets is not saved to cuda. maybe do targets. to('cuda') before loss(). .. I did this step in SrlData getitem"
    },
    {
        "source": "#780: HW5 Accuracy Calculation. For the accuracy calculation, I just noticed that there is already some code in train() calculating the \"predictions\" and \"matching\" however I'm a bit confused at why the \"matching\" calculation does not seem to ignore the cases where [PAD] is predicted correctly. In my code I also stripped those matches out, are we not supposed to do that? ",
        "target": "The code provided is incomplete. You will have to exclude the padded tokens before computing the accuracy. "
    },
    {
        "source": "#779: Decoding. My epoch accuracy is 0. 939 after training but my decoding output is the following. I checked all of my model inputs and outputs, so i think my model is just not predicting the right things. I'm not sure why this is happening when I have a pretty good training accuracy. I also tried using my model after 1 epoch (accuracy = 0. 888) and have the same result. Any suggestions on debugging? ('A', '[CLS]')\n('U. ', 'O')\n('N. ', 'O')\n('team', 'O')\n('spent', 'O')\n('an', 'O')\n('hour', 'O')\n('inside', 'O')\n('the', 'O')\n('hospital', 'O')\n(', ', 'O')\n('where', 'O')\n('it', 'O')\n('found', 'B-V')\n('evident', 'O')\n('signs', 'O')\n('of', 'O')\n('shelling', 'O')\n('and', 'O')\n('gunfire', 'O')\n('. ', 'O')",
        "target": "Oh my god. Ok, so I was correct that our models were trained properly. The pred_idx  that we're passing into label_sentence doesn't account for the fact that all indices are shifted right one to account for adding the [CLS] token. So to get the correct index for the predicate it's actually pred_idx + 1. "
    },
    {
        "source": "#777: HW5 Part 4 - (a). Hi, For decoding in label_sentence, are we supposed to tokenize each token and send it to Bert for testing? Asking this because the word \"shelling\" is not tokenized in this expected output. Do we have to re-combine the tokenized words before outputting them? Thanks, Samhit",
        "target": "I remember in a post professor told that Bert accepts tokenized text as input, so I think we need to send tokenized text to input and later convert the tokenized output labels back to labels for individual words and return the output."
    },
    {
        "source": "#776: HW5 - Part 3: Average accuracy. Hi, For the average accuracy for each epoch calculation, can we just sum up all the accuracies for each batch (matching divided by predictions for that batch) and divide by the total number of steps, similar to average loss. (Or) should we sum up all the matchings, sum up all the predictions and divide the total_matchings by the total_predictions ?",
        "target": "I did the former using np. mean. "
    },
    {
        "source": "#775: HW5 Part 6 - Same Span with different Tags. Hello, I'm not sure if in the case where the prediction and target set both have the same span but with different tags, should we categorize it as FN or FP. Or both? Thank you!",
        "target": "That situation would cause both a false negative and false positive. "
    },
    {
        "source": "#774: HW 5 Memory Issues + Part 3 training error. Hello, I'm running into some memory issues when trying to run train() and I don't know how to proceed. Before this error appeared, I also ran into the same issues as #773 and #766 with the not enough values to unpack.  Any help would be appreciated. ",
        "target": "Im not quite sure. What batch_size are you using? Did you make sure the BERT model is instantiated with a token length of 128?"
    },
    {
        "source": "#773: HW5 Part3 Training Error. I'm receiving this error and I'm not sure how to debug (I also don't have the unsqueeze calling in my getitem(). If anybody could help that would be great! Been stuck on this for a while:",
        "target": "What is the actual input_shape (input_ids. shape). You could print it. "
    },
    {
        "source": "#773: HW5 Part3 Training Error. I'm receiving this error and I'm not sure how to debug (I also don't have the unsqueeze calling in my getitem(). If anybody could help that would be great! Been stuck on this for a while:",
        "target": "I believe that the output tensors from __ getitem __ function is supposed to be in the shape of 128 (no need to reshape or unsqueeze). I only applied the reshape or unsqueeze to make it (1, 128) for the individual tests before the \"train\" function since \"1\" refers to the batch size. In train(), the shape should be (32, 128) since the batch size is 32."
    },
    {
        "source": "#771: HW 5 Submission. are we supposed to submit the PDF from Print-&gt;Save as PDF as well as the FIle-&gt;Download As-&gt; Notebook?",
        "target": "Thats right. "
    },
    {
        "source": "#770: HW5 Part 6: Viable score. Would it be possible to achieve an F1 score of 0. 87 for evaluate_spans? Is this too high? Should we be including CLS and/or SEP?",
        "target": "In the span based eval you should not include CLS or SEP (these are not part of any span). This seems high. But not impossible. What was your per token accuracy and loss? "
    },
    {
        "source": "#769: HW 5 Part 3 Training Error. Hello, For Part 3, when running Train(), my program is able to run for a few batches, but keeps crashing and outputting different errors, saying that it doesn't recognize a key. Is there an issue with the way in which I am invoking my get_item code? My original loss from the end of part 2 is around 3. 8-4. 1 too. I truncated the token sequences before making them tensors, but I am still getting the follow errors:KeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_2827/3364925475. py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_2827/2922944801. py in train()\n     19     epoch_acc=0\n     20 \n---&gt; 21     for idx, batch in enumerate(loader):\n     22         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in __next__(self)\n    519             if self. _sampler_iter is None:\n    520                 self. _reset()\n--&gt; 521             data = self. _next_data()\n    522             self. _num_yielded += 1\n    523             if self. _dataset_kind == _DatasetKind. Iterable and \\\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self. _next_index()  # may raise StopIteration\n--&gt; 561         data = self. _dataset_fetcher. fetch(index)  # may raise StopIteration\n    562         if self. _pin_memory:\n    563             data = _utils. pin_memory. pin_memory(data)\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in &lt;listcomp&gt;(. 0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/tmp/ipykernel_2827/2744729194. py in __getitem__(self, k)\n    120         tag_ids= []\n    121         for tag in padded[1]:\n--&gt; 122             tag_ids. append(role_to_id[tag])\n    123 \n    124 \n\nKeyError: 'B-ARGM-REC'\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_2827/3364925475. py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_2827/2922944801. py in train()\n     19     epoch_acc=0\n     20 \n---&gt; 21     for idx, batch in enumerate(loader):\n     22         print(\"in da loop\")\n     23         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in __next__(self)\n    519             if self. _sampler_iter is None:\n    520                 self. _reset()\n--&gt; 521             data = self. _next_data()\n    522             self. _num_yielded += 1\n    523             if self. _dataset_kind == _DatasetKind. Iterable and \\\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self. _next_index()  # may raise StopIteration\n--&gt; 561         data = self. _dataset_fetcher. fetch(index)  # may raise StopIteration\n    562         if self. _pin_memory:\n    563             data = _utils. pin_memory. pin_memory(data)\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in &lt;listcomp&gt;(. 0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/tmp/ipykernel_2827/2763776596. py in __getitem__(self, k)\n    119         tag_ids= []\n    120         for tag in padded[1]:\n--&gt; 121             tag_ids. append(role_to_id[tag])\n    122 \n    123 \n\nKeyError: 'B-ARG5'\n",
        "target": "So I think the professor said that if you have a label that is not in the role_to_id dictionary (i. e. key errors like these), you replace it with 'O', so you would key into the 'O' when tag is not in the dictionary. I used a ternary operator for this, so I recommend using that! (formatted as role_to_id[tag if tag in role_to_id else 'O'], something like that)"
    },
    {
        "source": "#768: HW-5 Model Accuracy and F1-score Validity. Hi, Just to confirm if they are valid scores,  I am getting the following after training for 2 epochs:Training loss epoch: 0. 053995941260483904Training accuracy epoch: 0. 983526443835309Overall P: 0. 7871418357806742 Overall R: 0. 7957545229120103 Overall F1: 0. 7914247481282417",
        "target": "That looks pretty good. "
    },
    {
        "source": "#767: Port Not Connecting for Jupiter. Hi, I hope you are doing wellI'm trying to run the jupiter notebook on my browser, but I keep getting this error message even though my virtual machine successfully runs the command. I'm not sure how to proceed in this case, and I would love some guidance on this. Thank you!",
        "target": "Double check the Jupyter notebook config file to check that the notebook is running on the external IP, not just 127. 0. 0. 1. Also check the firewall config of the VM in the GCP console. "
    },
    {
        "source": "#766: hw 5 part 2 not enough values to unpack. I'm running just the first part of part 2 where I just pick an item from my dataset and use it in the model. However, I'm running to this error: \"not enough values to unpack, expected 2 not 1\"I double checked that each input to the model (ids, mask, and pred) are each tensors of length 128 -- does anyone know what might be the issue? ",
        "target": "Hi, I believe that the input tensors have to be of shape (1, 128), where 1 is your batch size."
    },
    {
        "source": "#765: HW 5 Part 2 Prediction Example. Hello, For Part 2, the example on the argmax prediction has '[CLS]' as the first token predicted. However, since we have not done any training by this point, it is expected that our example would not have '[CLS]' as its first token, correct? I just want to make sure I understand this part.",
        "target": "So the model has not been fine tuned, but it has been pretrained in the masked LM objective. So I think its likely that it would predict [CLS] for the first token. "
    },
    {
        "source": "#763: VM Instance not creating. Hey everyone, I got approved for 2 GPU quota, and its been over 15 minutes, but instance is still not finishing creating. Anyone else having this issue? ",
        "target": "You can try hovering over the red exclamation point to see exactly what's going wrong, but I think that there might not be any GPUs available in the region that you're trying to create your VM instance. Prof Bauer talks about it in this question: #577 "
    },
    {
        "source": "#762: HW5 Connection to Jupyter problem. Hi everyone, I'm having a problem with accessing Jupyter notebook page. I followed the instructions and setup the firewall rules for jupyter notebooks, however when I tried to open the page [my vm ip]:8888 and it shows error message 'The site can't be reached' and 'refused to connect'. Is there anything I could do to fix this? Thank you so much for the help! !",
        "target": "Hard to tell without a closer look. I suspect the Jupyter config may not be correct and the notebook server is only listening on the 127. 0. 0. 1 IP, not the external ip of the VM. Make sure you copied the notebook config file correctly.  "
    },
    {
        "source": "#761: can't reached the site. I followed all the steps in the instructions, and cannot open the site. Can anyone tell me which step I did wrong? ",
        "target": "It looks like Jupyter is only running on the local IP. Make sure you copied the Jupyter configuration correctly.  "
    },
    {
        "source": "#760: Cannot open Jupyter notebook using VM. Hi, I've tried for a couple of hours to figure out how to open the Jupyter notebook from Google cloud VM. But it still does not work. Is it possible to solely use Colab to finish homework 5? ",
        "target": "Hey can you rejoin the zoom? Thanks!"
    },
    {
        "source": "#759: HW 5 Unzipping Ontonotes. Hello Professor Bauer and Teaching Staff, I was wondering how to address this given that we can not respond  is the second screenshot a possible correct approach? ",
        "target": "Seems good. But it looks like you already had a version.  "
    },
    {
        "source": "#758: model training time. How long does \"a few hours\" mean for part 3?",
        "target": "Hi, For the T4 GPU, I used tqdm and I am seeing as per the SS below, around 2 hrs per epoch"
    },
    {
        "source": "#757: sentences with > 128 tokens. how should we handle this case?",
        "target": "In my implementation I just truncated these. But it may be better to just ignore the entire sentence. "
    },
    {
        "source": "#756: HW5 part 1. 1. Hi, In HW5 part 1. 1, is it that the \"tokenize_with_labels\" function is already completed and we don't need to do anything about it? When I run \"tokenize_with_labels(\"the fancyful penguin devoured yummy fish . \". split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\". split(), tokenizer)\", I am already getting \"(['the', 'fancy', '##ful', 'penguin', 'dev', '##oured', 'yu', '##mmy', 'fish', '. '], ['B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'B-V', 'I-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'O'])\" as output. Thanks!",
        "target": "Yes, Professor Bauer left it completed for us. #606"
    },
    {
        "source": "#753: blank notebook. Has anyone had an issue with their notebook appearing blank? Its been working fine the last few days but now when i open the notebook its completely blank. ssh into my instance is fine, its showing an active kernel, correct files are showing, it's showing my last save point. ..",
        "target": "have you try restarting the kernel?"
    },
    {
        "source": "#753: blank notebook. Has anyone had an issue with their notebook appearing blank? Its been working fine the last few days but now when i open the notebook its completely blank. ssh into my instance is fine, its showing an active kernel, correct files are showing, it's showing my last save point. ..",
        "target": "Does your ssh terminal say anything? I had this issue once, and i found that my jupyternotebook got untrusted. I used this jupyter trust 4705-f23-hw5/4705_f23_hw5. ipynb  to solve :)[W 21:11:19. 919 NotebookApp] Notebook 4705-f23-hw5/4705_f23_hw5. ipynb is not trusted^Ctl3189@instance-2:~$ jupyter trust 4705-f23-hw5/4705_f23_hw5. ipynb"
    },
    {
        "source": "#752: hw5 part 2 loss. similar question to #616: My average loss is 3. 99 but i've seen values as low as 3. 73 and high as 4. 14 after instantiating my model 10. Is this normal? I'm not really sure what I would change to get my loss consistently closer to the approximation. Is anyone able to explain why the loss should be close to the approximation and what would be affecting my values?",
        "target": "Yeah I was getting similar results. Eventually I got a result, after reinstantiating the model many times, that was 0. 03 off, and then I somehow managed to lose the result through restarting the kernel so that was frustrating. The professor said to just keep instantiating the model until you get something close to the desired result. It's not a very efficient way to handle this but I think it's the only way. Could be wrong though, so if anyone can confirm that would be helpful too."
    },
    {
        "source": "#751: How to fix this error in jupyter notebooks. . Hi, i was trying to follow the  setup instructions in jupyter notebook by running the code in each of the blocks. How do i fix the error in part 1 data preparation.  It says transformers module not found. I assume i have to install the transformers module somewhere, but I'm unfamiliar with Jupyter notebooks so not sure how to do fix this.",
        "target": "in Jupiter terminal \"! pip install transformers datasets\"https://huggingface. co/docs/transformers/quicktour"
    },
    {
        "source": "#750: HW 5 Create firewall rule - Targets? . What should I select? If 'Specified target tags\", what should I input:And why?",
        "target": "I picked all instances in the network and it worked. Not sure if this is correct though"
    },
    {
        "source": "#750: HW 5 Create firewall rule - Targets? . What should I select? If 'Specified target tags\", what should I input:And why?",
        "target": "All instances. "
    },
    {
        "source": "#749: Jupyter token authentication is required. . Hi, I was following the steps for setting up HW5. as seen in this screenshotAfter accessing from my local browser by running http://[your ip]:8888. I get this page. How should I proceed? I didn't see any instructions for how to handle this page in the HW directions.",
        "target": "Try adding this to the Jupyter config:c. NotebookApp. token = ''Alternatively, just copy/paste the token thats being displayed on the terminal when you run Jupyter notebook. "
    },
    {
        "source": "#748: hw5 set up. hi! in the future, I think a video tutorial for setting up this assignment (from gcp all the way to opening the notebook) would be extremely helpful. just wanted to suggest!",
        "target": "Thanks. I had an old tutorial from past semesters that was outdated, so I didnt distribute it this time. We can produce a new one for the upcoming spring iteration of the course. "
    },
    {
        "source": "#745: HW5 Part 3 Training Error - \"too many values to unpack\". Hello all, When trying to initially train the model in part 3, before modifying the train method at all, I get the below error (I also get it after modifying it).  The issue seems to be in the forward method of class SrlModel(). Does anyone know what might be going wrong here?",
        "target": "Can you print out the actual input shape? Seems like the shape tuple has more than two entries. "
    },
    {
        "source": "#745: HW5 Part 3 Training Error - \"too many values to unpack\". Hello all, When trying to initially train the model in part 3, before modifying the train method at all, I get the below error (I also get it after modifying it).  The issue seems to be in the forward method of class SrlModel(). Does anyone know what might be going wrong here?",
        "target": "I also observed this error, if I do torch. tensor(token_ids, dtype=torch. long). unsqueeze(0) , but removing \"unsqueeze\", makes it work fine. If you have the unsqueeze, try removing and retry "
    },
    {
        "source": "#743: Remaining OH for HW5? . Hi, Wondering if there's any TA OH left for HW5? highly appreciate it! ",
        "target": "I will be holding one tmr"
    },
    {
        "source": "#740: HW 5 - CUDA error. RuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_17324/2697911999. py in &lt;module&gt;\r\n----&gt; 1 model = SrlModel(). to('cuda') # create new model and store weights in GPU memory\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in to(self, *args, **kwargs)\r\n    897             return t. to(device, dtype if t. is_floating_point() or t. is_complex() else None, non_blocking)\r\n    898 \r\n--&gt; 899         return self. _apply(convert)\r\n    900 \r\n    901     def register_backward_hook(\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self. children():\r\n--&gt; 570             module. _apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self. children():\r\n--&gt; 570             module. _apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self. children():\r\n--&gt; 570             module. _apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    591             # `with torch. no_grad():`\r\n    592             with torch. no_grad():\r\n--&gt; 593                 param_applied = fn(param)\r\n    594             should_use_set_data = compute_should_use_set_data(param, param_applied)\r\n    595             if should_use_set_data:\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in convert(t)\r\n    895                 return t. to(device, dtype if t. is_floating_point() or t. is_complex() else None,\r\n    896                             non_blocking, memory_format=convert_to_format)\r\n--&gt; 897             return t. to(device, dtype if t. is_floating_point() or t. is_complex() else None, non_blocking)\r\n    898 \r\n    899         return self. _apply(convert)\r\n\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1. I keep getting this error when I try to instantiate SRLModel, I'm unsure what to do. Here is the code block I'm running:model = SrlModel(). to('cuda') # create new model and store weights in GPU memory",
        "target": "Nevermind I just restarted the notebook server and it worked"
    },
    {
        "source": "#739: Incomplete Grade. I recently sent an email to Professor Bauer to request an incomplete grade for this class. I was hoping to receive his approval given that the deadline is tonight. Thank you!",
        "target": "Responded by email. "
    },
    {
        "source": "#738: HW5 - Span-based Evaluation:. Hi TAs and Prof. , Wanted to understand if my understanding of TP, FP, FN is correct because I'm getting very weird results for the span-based evaluation: True Positives (TP): Count a prediction as TP if the predicted span exactly matches a target span, including both the span and the label. False Positives (FP): Count a prediction as FP if the predicted span does not match any span in the target. This includes:Predicted spans that don't exist in the target. Predicted spans where the span is correct, but the label is incorrect. False Negatives (FN): Count as FN if a span in the target:Does not exist in the predicted spans. Exists in the predicted spans but with a different label. Span-based valuation results: Overall P: 0. 16827258962528463  Overall R: 0. 7939763509332494  Overall F1: 0. 27769208370387277Token-based valuation results: Accuracy: 0. 9246983648015192 (Looks decent)Latest training epoch (after 2 epochs): Training loss epoch: 0. 2138111023245621, Training accuracy epoch: 90. 54548921959775% (Similar to what was given)Not sure why I'm getting such a low F-1 given other metrics seems to be correct? Thanks in advance!        # Calculate TP, FP, FN\n        for span, label in predicted_spans. items():\n            if span in target_spans:\n                if target_spans[span] == label:\n                    total_tp += 1  # Correct span and label\n                else:\n                    total_fp += 1  # Correct span but incorrect label\n            else:\n                total_fp += 1  # Span not in target\n\n        for span, label in target_spans. items():\n            if span not in predicted_spans:\n                total_fn += 1  # Span missing in predictions\n            elif predicted_spans[span] ! = label:\n                total_fn += 1  # Span present but labeled incorrectly in predictions",
        "target": "Hi, I think you might forget to only consider the span within [CLS_idx+1 : SEP_idx]. Otherwise, there would be too many FP leading to super low Precision and hence low F1. In the case where the span exists in both prediction and target but with different tags, I'm not sure how to categorize this case (either FN or FN or both lol). We might need further clarifications."
    },
    {
        "source": "#737: HW5 Part 3 Implementation. Hello, Part 3 asks us to \"modify the training loop. .. so that it also computes the accuracy for each batch and reports the average accuracy after the epoch. \" I just wanted clarification as to where exactly we implement this. Should this be under the part that says, \"# Compute accuracy for this batch\"? ",
        "target": "Yes I would add it there. "
    },
    {
        "source": "#736: HW5 - 8888 in use when opening jupyter from browser. Hello, I have been trying to open jupyter from my browser, but it's not working. My config should be correct. I've attached a screenshot just in case. I've tried a few different IPs. http://127. 0. 0. 1:8923, http://10. 208. 0. 2:8923, and http://34. 165. 248. 255:8923. I know it's not 8888, but according to the screenshot below, it seems to be in use. I have a feeling this is the issue, as the firewall rule is configured for 8888. The latter two IPs just load forever, while the first immediately fails (as expected as it is the loopback). This is what I currently see. ",
        "target": "I seem to be having the same issue. There are no error messages in the terminal, but Jupyter won't open."
    },
    {
        "source": "#736: HW5 - 8888 in use when opening jupyter from browser. Hello, I have been trying to open jupyter from my browser, but it's not working. My config should be correct. I've attached a screenshot just in case. I've tried a few different IPs. http://127. 0. 0. 1:8923, http://10. 208. 0. 2:8923, and http://34. 165. 248. 255:8923. I know it's not 8888, but according to the screenshot below, it seems to be in use. I have a feeling this is the issue, as the firewall rule is configured for 8888. The latter two IPs just load forever, while the first immediately fails (as expected as it is the loopback). This is what I currently see. ",
        "target": "Odd. I assume restarting the vm instance would fix this. But you could also try to find which process is using that port and then kill it. To find the process try:sudo netstat -nlp | grep :8888"
    },
    {
        "source": "#736: HW5 - 8888 in use when opening jupyter from browser. Hello, I have been trying to open jupyter from my browser, but it's not working. My config should be correct. I've attached a screenshot just in case. I've tried a few different IPs. http://127. 0. 0. 1:8923, http://10. 208. 0. 2:8923, and http://34. 165. 248. 255:8923. I know it's not 8888, but according to the screenshot below, it seems to be in use. I have a feeling this is the issue, as the firewall rule is configured for 8888. The latter two IPs just load forever, while the first immediately fails (as expected as it is the loopback). This is what I currently see. ",
        "target": "I'm also running into the same issue! 8888 is available and I also followed the same instructions, but am unable to access the notebook. Has anyone been able to fix it? "
    },
    {
        "source": "#735: HW 3 Regrade Requests. I was just wondering, what is the procedure to request regrades for HW 3? The instructions on the home page say to email the TA who graded the assignment, but I can't see the name of the TA who graded my HW 3. Is there any other way to submit a regrade request?",
        "target": "Please email me directly. I forgot to include the TA names."
    },
    {
        "source": "#734: HW5- Dealing with label outside of the role_to_id dictionary. When I'm training, I found cases where the label (e. g. 'B-ARGM-COM') is not a part of the dictionary created by the role_list. txt The assignment didn't specify how to handle these specifically. Would treating them like ['PAD'] works? What would be the preferred approach? Thanks in advance! Training output: Batch loss:  4. 183680534362793Current average loss: 4. 183680534362793, Current average accuracy: 0. 45558086560364464%Batch loss:  4. 087785243988037Batch loss:  4. 029718399047852Batch loss:  4. 006991863250732Error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_11409/3364925475. py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_11409/2737204925. py in train()\n     18     model. train()\n     19 \n---&gt; 20     for idx, batch in enumerate(loader):\n     21 \n     22         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in __next__(self)\n    519             if self. _sampler_iter is None:\n    520                 self. _reset()\n--&gt; 521             data = self. _next_data()\n    522             self. _num_yielded += 1\n    523             if self. _dataset_kind == _DatasetKind. Iterable and \\\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self. _next_index()  # may raise StopIteration\n--&gt; 561         data = self. _dataset_fetcher. fetch(index)  # may raise StopIteration\n    562         if self. _pin_memory:\n    563             data = _utils. pin_memory. pin_memory(data)\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in &lt;listcomp&gt;(. 0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/tmp/ipykernel_11409/835064567. py in __getitem__(self, k)\n     48 \n     49         #labels to ids\n---&gt; 50         label_ids = [role_to_id[label] for label in label_sentence]\n     51 \n     52         # Create attention mask\n\n/tmp/ipykernel_11409/835064567. py in &lt;listcomp&gt;(. 0)\n     48 \n     49         #labels to ids\n---&gt; 50         label_ids = [role_to_id[label] for label in label_sentence]\n     51 \n     52         # Create attention mask\n\nKeyError: 'B-ARGM-COM'\n",
        "target": "If a label doesnt appear on the role dictionary it should be replaced with O. "
    },
    {
        "source": "#733: HW5: Kernel death after Part2. After running model here In this step or the prior step, I keep getting Kernal Death or Kernal Restart automatically I tried starting my Notebook, restarting my SSH, and laptop, none of them works Are we allocated the right amount of memory for this task?",
        "target": "What's the specific error message you encounter (if any)? Are using a T4 GPU? How much RAM did you set up for the VM?"
    },
    {
        "source": "#732: HW5 Setup. I encountered the following error when creating the instance as required. Create VM instance \"instance-1\" and its boot disk \"instance-1\"A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-central1-a zone. Alternatively, you can try your request again with a different VM hardware configuration or at a later time. For more information, see the troubleshooting documentation. What should i do? how to fix it? I have changed and tried several zones. All unavailable.",
        "target": "Yeah it can be tricky to find an instance. Keep trying (its often easier during evenings/weekends). "
    },
    {
        "source": "#732: HW5 Setup. I encountered the following error when creating the instance as required. Create VM instance \"instance-1\" and its boot disk \"instance-1\"A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-central1-a zone. Alternatively, you can try your request again with a different VM hardware configuration or at a later time. For more information, see the troubleshooting documentation. What should i do? how to fix it? I have changed and tried several zones. All unavailable.",
        "target": "I know this might not be particularly helpful but I just kept trying different zones until it worked!"
    },
    {
        "source": "#731: Exam Q3. Hi, On the exam today, For Q3, I used capital L instead of lowercase l because i didn't want to confuse it for 1. I forgot to write that clarification down, is that ok? Thanks",
        "target": "Thats fine. "
    },
    {
        "source": "#729: AMR graph mandatory or optional? . For any English-&gt;AMR question on the final, can we skip the graph and go straight to the PENMAN representation or do we have to draw the graph first?",
        "target": "Either output representation would be fine. "
    },
    {
        "source": "#728: Ungraded exercise solution: AMR. For the problem \"He admired Bob's love of math\", the solution is (a/ admire\n    :ARG0 (h / he)\n         :ARG1 (l / love\n                    :ARG0 (b / Bob)\n                    :ARG1 (m / math))) It seems that \"love\" is arg1 of he, rather than arg1 of admire. Is this true? I feel like for \"He admires love\", love should be arg1 of admire.",
        "target": "I believe this is just a weird formatting issue. \"love\" is ARG1 of admire here. ChatGPT confirms this:"
    },
    {
        "source": "#728: Ungraded exercise solution: AMR. For the problem \"He admired Bob's love of math\", the solution is (a/ admire\n    :ARG0 (h / he)\n         :ARG1 (l / love\n                    :ARG0 (b / Bob)\n                    :ARG1 (m / math))) It seems that \"love\" is arg1 of he, rather than arg1 of admire. Is this true? I feel like for \"He admires love\", love should be arg1 of admire.",
        "target": "I think you can think in such way:He admired (Bob's) love (of math) -&gt; He (ARG0) admired love (ARG1). Bob's love of math -&gt; Bob(ARG0)'s love of math(ARG1)."
    },
    {
        "source": "#727: AMR format. (y / yield-03 :ARG0 (f / fund :mod (t / top) :mod (m / money)) :ARG1 (o / over :op1 (p / percentage-entity :value 9) :degree (w / well)) :time (c / current))\" do we need to know the exact number like '03' behind 'yield' in the exam2",
        "target": "Its been confirmed that you do not need to know how to do AMR to this level of detail."
    },
    {
        "source": "#726: coverage for the final. There was another thread saying how semantic parsing wasn't going to be on the final, but are there other sections that will not be covered on the final? Or in other words which topics will be covered on the final?",
        "target": "There's been a Page on CourseWorks with the entire topic list: https://courseworks2. columbia. edu/courses/179333/pages/exam-2-topic-overview"
    },
    {
        "source": "#725: Ungraded Exercise IBM Model. Is there a typo in part 3 of the IBM Model ungraded exercise? q(5 | 1, 5, 5) appears twice in the equation but I think of them is supposed to be q(1 | 5, 5, 5)?",
        "target": "Yes, it's a typo, according to professor's comments in #678 "
    },
    {
        "source": "#724: Ungraded Exercise: IBM Model 2. I don't understand the answer to #670. How can I know that the alignment is inconsistent and crosses between noun phrases when \"e\" in the foreign sentence 2 to map to position 4 in the English sentence? In addition, is there any general strategy to find q probability? This is confusing since It looks like there are many possible cases.",
        "target": "If the alignments for the nouns were different, you would get phrases that are not well nested. Of course this _could_ be the case, but seems highly unlikely given the general patterns implied by the other examples in the data. "
    },
    {
        "source": "#723: Generative vs. Discriminative. Hi, I'm recapping for Exam 2 and I've been struggling to understand the difference between Generative and Discriminative models. Are BERT and Naive Bayes discriminative or generative models? ",
        "target": "I'm pretty sure that the fundamental difference between generative and discriminative models is that  generative models capture the joint probability P(x, y) while discriminative models capture the conditional probability P(y|x). Where y is a class and x is some input (what x is depends on the model we are working with I believe). What they actually do with the probability they capture varies between implementation of specific models. Generally though generative model is modelling the probability of a input and class occurring as a pair, this allows them to be better at modelling patterns within the data making them well suited to tasks needing the generation of new of data. Discriminative models instead capture the probability of class given some input, which makes them better at modelling the difference between classes and thus are often used for classification tasks. Also BERT is a discriminative model and Naive Bayes, despite being commonly used as a classifier, is a generative model."
    },
    {
        "source": "#723: Generative vs. Discriminative. Hi, I'm recapping for Exam 2 and I've been struggling to understand the difference between Generative and Discriminative models. Are BERT and Naive Bayes discriminative or generative models? ",
        "target": "In a generative model, you model the distribution of the data, given some hidden variable, i. e. P(x|y). You then use Bayes rule to find the most likely y to explain your observation (input data) x. In a discriminative model you directly model the distribution P(y | x). "
    },
    {
        "source": "#722: Do ELMo embeddings use attention? . While reviewing the ELMo slides, I realized that the \"s\" task-specific parameter looks the same as the \"\" parameter mentioned in the slides for the attention mechanism. Does this mean that the ELMo representations also use the attention mechanism to generate the task-specific, contextualized embeddings? Below are the relevant slides: Attention: ",
        "target": "I believe that in the context of ELMo the j's correspond to the layers in a stacked bidirectional RNN. For some tasks, researchers found that more heavily weighing the layers deeper into the model (where the embeddings have become more abstract) gives better results than just summing them all together equally, and sometimes they only use the top layer (layer L). This is different from the alpha parameter in attention, which corresponds to how relevant the key embedding of a token is to the query embedding of another token in a given layer. The attention mechanism involves a sum over N (the number of tokens), while the sum in the provided ELMo equation is over L (the number of bidirectional layers in the model)."
    },
    {
        "source": "#721: Question on AMR. Hi, I was wondering if there were any more resources to practice translating from AMR-to-English  in addition to the ungraded excercise.",
        "target": "I found this video to be really helpful. Especially the last 20 minutes. Start at 16:32 and watch in 2x speed. It goes through examples similar to the ungraded exercises. https://www. youtube. com/watch? v=uCZ9nAe76Ss"
    },
    {
        "source": "#720: Representing task as GPT input problem. Dumb quesiton but what does transformer return here for classification task ? Processed input representation with context ? Next word ? How does that translate to classificaiton ? Any help is appreciated, thank you! ",
        "target": "If you take a look at a GPT classifier (like GPT2ForSequenceClassification in https://github. com/huggingface/transformers/blob/v4. 24. 0/src/transformers/models/gpt2/modeling_gpt2. py#L1328), you will see the original head of the pre-trained model (which was used to train the model) is not used. Instead, a new head which is a singular Linear layer is used to convert the transformer outputs to logits. These then get converted to class scores via either MSELoss (regression), CrossEntropyLoss (binary classification), or BCEWithLogitsLoss (multiclass classification)."
    },
    {
        "source": "#720: Representing task as GPT input problem. Dumb quesiton but what does transformer return here for classification task ? Processed input representation with context ? Next word ? How does that translate to classificaiton ? Any help is appreciated, thank you! ",
        "target": "The idea is that you augment the input with an Extract token, then you use the representation that GPT computes for this token (prior to the classification head that was used for pre training) and feed it into a classifier. "
    },
    {
        "source": "#719: Attention similarity score. Hi, In the slides, it says that the way we compute the similarity score for attention is by doing matrix multiplication and scaling. I was wondering if there are other ways of doing this. Thanks,",
        "target": "There are other ways to do this, but GPUs are really good at doing matrix multiplication as it is a highly parallelizable task. The alternative will dramatically increase the runtime needed to do attention. "
    },
    {
        "source": "#718: [Important] Exam 1 Question. Hello! I think the regrade window is closed, and that's honestly my bad, but I knew I got this question right. I assumed the grade I got was the grade I deserved. However, the misgrade on Question 1 cost me a large number of points. Although it's technically my fault for missing the regrade deadline, as I am reviewing for the final exam, this -10 points on my midterm is just too large of a number for me to not try to ask for a regrade request even at this time of the semester. As is apparent from the screenshots below, I did not use a table to show my work for the problem. Instead, I did it the other allowable way with equations. I did note to go to page 12 for my work on this problem, as this problem did not have an extra scratch page next to it on the exam. This regrade request is not a petty few points. Getting these points back would solidly put my midterm grade in average zone. I would really appreciate if staff could make this change. Thank you so much for the consideration.",
        "target": "Hi please send this to my by email. There is a good chance it will get lost on Ed. "
    },
    {
        "source": "#717: Inputs and outputs of transformer component. Just to check my understanding =&gt; let's say we put 1 input token in, process and get a positional-encoded representation of the tokenAfter that it's fed to attention block, which spits out a context vector of every ( is it every or is it every previous tokens ? ) in the input, with respect to the token that we are trying to guess. We do this by taking the weighted sum of those tokens and return that weighted sum (which is a context vector that contains similarity scores of each token)This context vector (I assume the dimension is #of inputs *1) is then fed into a feed forward network. =&gt; My question is: is the process above correct ? What does the feed forward network do ? Finally, I remember Dr. Bauer mentioned that we can have multiple context vector for the same token but I can't imagine how this is done, if anyone can give an example it would be amazing. Thanks",
        "target": "I think one point of confusion is that there is no designated target token. All tokens in the input are passed into the encoder at the same time. The encoder produces a contextualized representation for each of the input tokens. The output will be a (batch_size, input_length, embedding_dim) tensor. "
    },
    {
        "source": "#716: Meaning of f, e in MT setup, could be helpful for internalizing. Hey everyone, I was pretty confused about this naming convention for e, f (and the associated terms p(f|e), p(e) etc. ) Kept thinking \"e\" stood for evidence in my attempts to internalize the content. Could not come up with a good reason why the Spanish text was called \"f\". I had almost given up in deciphering the naming method used here. Turns out, historically, the setup/explanation of the MT task is usually made in the scenario where we are attempting to translate French (f) to English (e). [source1, source2] It helped me to reason about the \"f\" as \"s\" in the lecture slides for Spanish, sharing in case it is helpful or if someone else was curious why they were called \"f\" and \"e\". ",
        "target": "f is for french and e is for english in the examples"
    },
    {
        "source": "#716: Meaning of f, e in MT setup, could be helpful for internalizing. Hey everyone, I was pretty confused about this naming convention for e, f (and the associated terms p(f|e), p(e) etc. ) Kept thinking \"e\" stood for evidence in my attempts to internalize the content. Could not come up with a good reason why the Spanish text was called \"f\". I had almost given up in deciphering the naming method used here. Turns out, historically, the setup/explanation of the MT task is usually made in the scenario where we are attempting to translate French (f) to English (e). [source1, source2] It helped me to reason about the \"f\" as \"s\" in the lecture slides for Spanish, sharing in case it is helpful or if someone else was curious why they were called \"f\" and \"e\". ",
        "target": "i think it means \"foreign\" and \"english\"?"
    },
    {
        "source": "#715: Semantic Parsing in Final. Just to make sure: Semantic Parsing this whole part will not be tested in the final right? Thank you.",
        "target": "I just checked the final review recording, it will not be tested."
    },
    {
        "source": "#715: Semantic Parsing in Final. Just to make sure: Semantic Parsing this whole part will not be tested in the final right? Thank you.",
        "target": "It won't be tested. "
    },
    {
        "source": "#711: Will the final review session slides be posted? . Will the final review session slides be posted?",
        "target": "Hey Jennifer, the slides should be here #630 ! Hope this helps-SN"
    },
    {
        "source": "#710: Attention Matrices Q, K, and V. I think the concept of attention as a soft lookup makes a lot of sense. You use the dot product as a sort of \"similarity\" score, and then use these as weights to fine-tune an aggregate of vectors. This basically is storing information about what values to pay \"attention\" to in the context vector. However, when this translates to attention matrices in transformers, I have a hard time understanding the concept. We're still taking the similarity score of Q and K (this time as a matrix multiplication so we don't have to do it for every step), but what is V? What is the Value? In the above example, after calculating the score, we just multiply back the keys with high score (values and keys seem to be the same). What exactly is the V matrix here? How do we calculate it? Is something different about Q and K? When I googled this, it made it sound like Q, K, and V are all matrices that result from a matrix multiplication of the word embeddings with some weight matrix that is trained in the transformer. But that just made me more confused. Any help would be appreciated.",
        "target": "I believe V is a matrix where each column is an input vector (following the positional encoding). The output is a matrix of vectors that have been adjusted for self-attention."
    },
    {
        "source": "#710: Attention Matrices Q, K, and V. I think the concept of attention as a soft lookup makes a lot of sense. You use the dot product as a sort of \"similarity\" score, and then use these as weights to fine-tune an aggregate of vectors. This basically is storing information about what values to pay \"attention\" to in the context vector. However, when this translates to attention matrices in transformers, I have a hard time understanding the concept. We're still taking the similarity score of Q and K (this time as a matrix multiplication so we don't have to do it for every step), but what is V? What is the Value? In the above example, after calculating the score, we just multiply back the keys with high score (values and keys seem to be the same). What exactly is the V matrix here? How do we calculate it? Is something different about Q and K? When I googled this, it made it sound like Q, K, and V are all matrices that result from a matrix multiplication of the word embeddings with some weight matrix that is trained in the transformer. But that just made me more confused. Any help would be appreciated.",
        "target": "V is a transformation matrix acting on the embeddings. It is is something different than Q, K. Maybe this, somewhat lengthy response in a different thread might help. https://edstem. org/us/courses/46417/discussion/4035163? answer=9287537"
    },
    {
        "source": "#707: Exam synchronously on Zoom. Hi, I just wanted to know when we get some information on how the test is going to take place on zoom and general instructions for the exam. Thanks",
        "target": "For those of you signed up for this in advance, I will distribute the link tomorrow morning. You will get access to the exam on Gradescope at 1:10pm. You can then either print it, complete, and scan the final version back in -- or just use separate sheets of paper for your answers. There will be extra time after the exam to finish scanning and uploading. "
    },
    {
        "source": "#706: WordPiece and Bye-Pair Encoding. Hi! In class we went through two types of tokenization that BERT uses (bye-pair encoding and WordPiece) are these things we should be able to replicate or is understanding the concept alone sufficient? Thank you! ",
        "target": "Understanding the concept is sufficient. "
    },
    {
        "source": "#705: amr structure. Hi! I am just wondering is it acceptable  if we output sentence with a different structure than the solution provided during AMR translation (still have the same meaning)thank you",
        "target": "found answered in #702 thanks"
    },
    {
        "source": "#704: BERT vs GPT? . Hi, I'm confused on the difference between BERT and GPT and Transformer Models. Is the transformer model itself a combination of both BERT and GPT? If you are to choose between using BERT vs GPT is it just GPT if the task falls into one of the 4 categories (MCQ, Classification, Similarity, or Entailment) and BERT otherwise?",
        "target": "not a TA so correct me if I am wrong:BERT is implemented with a [encoder-only] transformer architecture, that's why it's called Bidirectional Encoder Representations from Transformers. GPT is implemented with [decoder-only] transformer architecture, that's why it's called Generative Pre-trained Transformer. So, in short, both of them are transformer models, I'd say it's a model architecture that's implemented by the above two models. I am unsure of what exact model you want to choose between BERT and GPT. BERT is a masked model, which means that you can \"mask\" a word in a sentence and let BERT predict what the word would be(thus doesn't have to be a next word prediction). GPT is a generative model and thus can only predict the next word to appear. I don't think there are tasks that reply on specifically BERT or GPT. "
    },
    {
        "source": "#704: BERT vs GPT? . Hi, I'm confused on the difference between BERT and GPT and Transformer Models. Is the transformer model itself a combination of both BERT and GPT? If you are to choose between using BERT vs GPT is it just GPT if the task falls into one of the 4 categories (MCQ, Classification, Similarity, or Entailment) and BERT otherwise?",
        "target": "I think BERT is good for classification task and GPT is mainly used on generation task, generally speaking."
    },
    {
        "source": "#704: BERT vs GPT? . Hi, I'm confused on the difference between BERT and GPT and Transformer Models. Is the transformer model itself a combination of both BERT and GPT? If you are to choose between using BERT vs GPT is it just GPT if the task falls into one of the 4 categories (MCQ, Classification, Similarity, or Entailment) and BERT otherwise?",
        "target": "I think BERT is better for understanding context because it considers left and right context, while GPT is more suitable for generative tasks. The inputs are also quite different for the two models, as the start/sep tokens are different for BERT and GPT(different input tokens depending on MC, similarity, entailment). "
    },
    {
        "source": "#703: ELMo Output. Hello, What exactly is the output of running ELMo? My understanding was that it returned a word embeddings vector (but with context), similar to Word2Vec. But I got confused reviewing the nearest neighbors slide in Lecture 13; is the implication that ELMo is actually a sentence embeddings vector?",
        "target": "I think it might be the contextual embedding. For the \"nearest neighbors\", I think that refers to when you're trying to map the contextual embedding back into the lexeme during running time, you need to find the nearest contextual embedding to be its output lexeme."
    },
    {
        "source": "#702: AMR Translation. Hi, While performing AMR translation is it okay if the sentence we make is structured differently than the original solution if they have the same meaning?",
        "target": "Yes, AMR generalized away from the surface form of the sentence, so many different sentences can have the same AMR representation. "
    },
    {
        "source": "#701: Context Size Query. Hello! I am a little confused about whether context size is uni- or bi-directional in the scope of this class. By which I mean, for the sentence: \"The Quick Brown Fox\" with a context of 2, would the context for the word Quick be {\"Quick, The\", \"Quick, Brown\", \"Quick, Fox\"} or only {\"Quick, Brown\", \"Quick, Fox\"}.",
        "target": "I guess you could define this either way. Typically I would say something like \"a context window of +-2\". But when it just says \"2-word window\" I would read this as +-1"
    },
    {
        "source": "#700: AMR Translation to English. For AMR translation to English such as in the ungraded exercise, does the verb tense of our answer matter as long as the overall semantic roles and meanings are captured? For example, would \"Currently, the top money fund yields well over 9%\" be accepted versus the answer in the solution \"Currently, the top money fund is yielding well over 9%? \" Thanks! ",
        "target": "No, tense is one of the generalizations that AMR makes. It's not reflected in the AMR and therefore the English sentence could have any tense. "
    },
    {
        "source": "#699: IBM arg max Question. In lecture 18, the slides show that a_i = argmax q(j|i, l, m) * t (fi, ej)I'm wondering is it equivalent to a_i = argmax q(j|i, l, m) * t (fi | ej)In IBM exercise problem, we are also calculating the conditional probability instead of joint probability.",
        "target": "Ah, there's a typo in the slides. The t parameters are conditional probabilities, conditioned on e. So it should always be t (fi | ej)."
    },
    {
        "source": "#698: exam 6 b. hi i hope you are great, I dont understand the solution for 6 BIn part A we calculated 3ed +dv parametersIn B we move from being giving embedding to, to do doing one hot. so we add layer for one hotso this would add: 3V * 3E parametersi thinkso wouldnt the total be:(3V * 3E) + 3ed +dv parametersi dont see how the actual answer is calculatedthank you so much!",
        "target": "We use the same shared VxE embedding matrix for each of the tokens. We do compute the embedding matrix multiplication three times, but the matrix we multiply the one-hot encoding by is the same for each of these three multiplications, so there are only V*E new learnable parameters."
    },
    {
        "source": "#696: ARM Question. Hi, I wonder how could we differentiate between ARG2 and ARG3 in AMR parsing? Thank you!",
        "target": "You would need to look up the definition for ARG2 and ARG3 in the propbank framefile as these differ per predicate. Clearly not something you can do on the exam. "
    },
    {
        "source": "#694: Exam2 location and Time. Just to confirm that the exam2 will start at  1:10pm Dec 11th in 309 Havemeyer ?",
        "target": "Yes, see #464"
    },
    {
        "source": "#694: Exam2 location and Time. Just to confirm that the exam2 will start at  1:10pm Dec 11th in 309 Havemeyer ?",
        "target": "Yes, that's correct -- the usual time and place. "
    },
    {
        "source": "#693: Question about IBM Model2 q. Hi, I'm a bit confused about IBM Model 2 after looking at the ungraded exercise. I saw that, for t(skwk|fly) and q(5 |1, 5, 5), the \"5\" in q corresponds to \"skwk\" and the \"1\" corresponds to \"fly\". If we have a problem like this on the final, should the foreign or the english word come first in both t and q? In the ungraded exercise, the foreign word comes first in t but the english word comes first in q.",
        "target": "For IBM Model 2, I believe it's:$t(f|e)$ for the probability that the output word $f$ is generated from the input word $e$. $q(j|i, l, m)$ for the probability that the $i$-th alignment variable (i. e. , $a_i$) is $j$ for input sentence length $l$ and output sentence length $m$. In other words, the probability that the $i$-th output word (i. e. , $f_i$) is aligned to the $j$-th input word (i. e. , $e_j$). The probability of the whole sentence is $$\\prod_{i=1}^m q(a_i | i, l, m) t(f_i | e_{a_i}). $$"
    },
    {
        "source": "#692: AMR Practice Question - why is the concept marked as -01, -03. For \"(y / yield-03 :ARG0 (f / fund :mod (t / top) :mod (m / money)) :ARG1 (o / over :op1 (p / percentage-entity :value 9) :degree (w / well)) :time (c / current))\" I didn't get what does the \"-03\" mean in this case",
        "target": "The -03 is just there to note that its the third sense/subsense of \"yield\" as defined in proppbank"
    },
    {
        "source": "#691: AMR: Expected to know NER format. For the exam, are we expected to know the format for NERs like time, person, date, etc? (I mean this: time: (t/ time-12 :value \"4:00pm\"))Or will we be given this formatting information on the exam? Just want to know if I should be placing it on my cheatsheet.",
        "target": "You don't have to know the exact format. "
    },
    {
        "source": "#690: Interpretation of Query, Key, Value Representations. I understand that in the transformer model, query, key, and value vector representations are calculated using trainable weight matrices and the input embeddings. I'm curious how these different vector representaions can be interpreted. I'm wondering if there these embedding-like vectors carry different meanings of the same word? Like does the key form of a token supposedly represent a \"contextual meaning\" of a word whereas the query form captures a more central meaning of the word as a target (whatever that means)?",
        "target": "This is a cool question. reminds me of a talk I had listened to either with Andrej Karpathy or Ilya Sutskever (can't remember which) discussing something similar. This is how I internalized some of that discussion. At each attention layer, we have embeddings for each position. As we get deeper in the model, 2 things happen. 1. The embeddings get more complex, due to additional non-linearities. 2. They also get more contextualized. The attention mechanism, with Q, K, V matrices (which are shared for each position's embedding) facilitate 2. And a nice analogy to how it does this is \"message passing\" or an \"online quorum\".   When an embedding e_i is multiplied by Q(uery), q_i is generated. q_i, in my understanding, can be thought of as: \"As embedding e_i, here is a representation of the things I would like to know\"So it's sort of a question that is asked by e_i, in the same way you might ask an online forum about what you'd like to know. The k_i generated from the K(ey) is the natural analog to the above. It represents something along the lines:\"As embedding e_i, I can answers these kinds of questions\" The score we calculate is then, kind of a matchmaking between askers and answerers. Note that what the questions an embedding might be able to answer (it's k_i (key) value) and the actual answers to those questions might be different. And that's where the V(alue) comes in. When two indices agree (via the score) that one can answer the other's question, it seeks the answer from the value vector. Thus v_i denotes something like:\"As embedding e_i, here is the knowledge I possess that would be helpful to others\" It happens that most of the time, the best answerer to a question an embedding might ask is itself. (i. e. the softmax is highest between its own query and key values) But attention basically presents an opportunity to see if there are other positions that possess something that might be helpful, which turns out to be super effective. (and the residual connections ensure that we don't lose track of what e_i was originally in this chatter) So I think yes, the training does push these q, k, v vectors to capture different meaning, perhaps not semantically but contextually. I know this about the world e_i (i. e. my semantics are set), What can I do with this? 1. Ask questions2. List questions I can answer3. Answer questionsSorry for the long answer, I really like your question! "
    },
    {
        "source": "#689: Important Conceptual Q. Bayesian approaches: Model P(x|y) and P(y) using Bayes Rule. Assume observed data generated by some hidden class label. Naive BayesGaussian Mixture ModelsHMMApplications: N-gram Language ModelCKY Parsing, PCFGsTransition-based Dependency ParsingIBM Model 2JAMRDiscriminative approaches: Predicting P(y|x) directly with inductive learning: given a bunch of input/output pairs, find h(x) that best approximates f(x) that maps inputs to outputs Linear and Log-linear modelSVMDecision trees, random forrestApplications: Neural NetworksRNNTransformersSummarizationLanguage and VisionHi TAs and Prof. Bauer, want to understand if my understanding of the different approaches and Applications are correct for all the models we covered. Please point out if I missed or misunderstood anything as well. Thanks in advance!",
        "target": "Not sure about JAMR being a discriminative model. I think they are using a log-linear model in the original implementation. You also seem to be mixing together models and applications. For example, summarization and language and vision would be an application, while RNN or transformer would be a model. "
    },
    {
        "source": "#688: Ibm-Model 2. Hi, In this equation, what exactly are all the variables? (If translating from english to French, which is the english word and french (for both j, i, and l, m). q(j | i, l, m) Also, are l and m necessarily equal. Can we also compare things which are 3 words and 5 words and find patterns? Thanks!",
        "target": "i is a token position in the source sentence (called \"foreign\" in the slides) and j is a token position in the target sentence (called \"English\" in the slides). l is the number of tokens in the target, m the number of tokens in the source. l and m are not necessarily equal. You can have several source tokens aligned to the same target token, or even to nothing at all. You can also have unaligned tokens in the target language.  "
    },
    {
        "source": "#687: Difference between k q and v for attention in transformers? . Confused about the difference between Keys values and queries without the context  of transformers. if they are all input representations what makes them different? and do they change how  different they are for instance when calculating  cross attention (vs self attention) in the  decoder?",
        "target": "In self attention, k, v, and q are all the token representations from the previous transformer layer. In cross attention k and v are the token representations computed by the encoder, while q are the token representation from the previous decoder layer. "
    },
    {
        "source": "#685: Do Stacked RNNs need to be balanced? . Hello! For a stacked RNN model since we are encoding different vectors for each token, does it matter that the number of layers corresponding to each word in the text might not be the same or am I perhaps misunderstanding something? Thank you!",
        "target": "Hm yeah Im not sure I understand the question. The number of layers is a hyper parameter of the model. Its the same for any input. "
    },
    {
        "source": "#684: Unfolded RNN vs traditional feed forward NN. What purpose does encoding hidden layers as an input parameter serve? . Hello! I am still a little confused on what exactly the difference is between the unfolded RNN and the traditional feed forward RNN is? I understand that we are now accounting for the vector encoded by the hidden layer (W*x) as a representation of the context to the left of the target word. Is the objective here that instead of each word having the semantic rep using 2 vectors (the word as a target and as a context word), we are combining into one vector? And if yes, what advantage does that hold over the regular NN model? Thank you!",
        "target": "Are you asking about unfolding for BPTT? The idea is that this provides a way to train the model and propagate later errors back to earlier input positions. There is no benefit in terms of the prediction itself and the model does not use more information than the not-unfolded RNN. "
    },
    {
        "source": "#683: Regarding Regualr Grammar. According to definition in slides, A-&gt;aB or A-&gt;a is considered Regular Grammar. Is A-&gt;Ba also considered part of Regular Grammar?",
        "target": "No"
    },
    {
        "source": "#682: Question AMR Ungraded Assignment. Hello, I'm confused about the AMR solution to the sentence \"We are expected to meet him at 4:00pm in the courtyard. \" Why are ARG1 and ARG0 both (w / we)? I appreciate your help! !",
        "target": "(e / expect-01\n          :ARG1 (w / we)\n          :ARG2 (m / meet\n                    :ARG0 (w / we)\n                    :ARG1 (h / him)\n                    :time (t / time-12 :value 4:00pm)\n                    :location (c / courtyard)))\"We are expected to . .. .\", thus, with respect to the verb expect, \"we\" are being affected and thus become the patient/theme, hence tag ARG1. What about the word \"meet\" ? Who is the person that takes this action ? The answer is \"we\", hence the tag ARG0 under meet."
    },
    {
        "source": "#680: vertbi ungraded. hi i hope you are wellI dont quite understand why we say we take the max at every step for vertibi.  This is from the ungraded question solution:[0, Start] = 1[1, N] = [0, Start]  P(N|start)  P(time |N) = 1  1/2  2/4 = 1/4[1, V] = [0, Start]  P(V|start)  P(time |V) = 1  1/2  1/6 = 1/12[2, N] = max( [1, N]  P(N|N)  P(flies|N), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 1/4  1/4), (1/12 2/3  1/4)) = 1/64[2, V] = max( [1, N]  P(V|N)  P(flies |V), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 2/4  2/6), (1/12  0  2/6)) = 1/24[3, V] = max( [2, N] P(V|N)  P(like|V), [2, V]  P(V|V)  P(like|V)) = max( (1/64  2/4  2/6), (1/24  0  2/6)) = 1/384[3, Prep] = max( [2, N] P(Prep|N)  P(like|Prep), [2, V]  P(Prep|V)  P(like|Prep)) = max( (1/64  1/4  1), (1/24  1/3  1)) = 1/72[4, N] = max( [3, V] P(N|V)  P(leaves|N), [3, Prep]  P(N|Prep)  P(leaves|N)) = max( (1/384  2/3  1/4), (1/72  1  1/4 )) = 1/288[4, V] = max( [3, V] P(V|V)  P(leaves|V), [3, Prep]  P(V|Prep)  P(leaves|V)) = max( (1/384  0  1/6), (1/72  0  1/6)) = 0The most likely state sequence is N V Prep N. I dont understand why we saying we take the max at each step.  It seems like we are work out each sequence from start to end and picking the one with the highest probability.  thank you!",
        "target": "I think the reason of taking the max at each step is to avoid the need to explicitly calculate and store probabilities for all possible sequences since that would be computationally expensive. "
    },
    {
        "source": "#678: Question about IBM Model2. Hi all, When finishing ungraded assignment, I got confused about this:How can we get this distribution? Why all the probabilities are 1? And how can we get this: Thank you!",
        "target": "And in this assignment, it seems that there is something wrong with the answer: or I have a misunderstanding about this part? This is my answer:"
    },
    {
        "source": "#678: Question about IBM Model2. Hi all, When finishing ungraded assignment, I got confused about this:How can we get this distribution? Why all the probabilities are 1? And how can we get this: Thank you!",
        "target": "I think there are some typos in the solution. The last one should be q(1|5, 5, 5). "
    },
    {
        "source": "#678: Question about IBM Model2. Hi all, When finishing ungraded assignment, I got confused about this:How can we get this distribution? Why all the probabilities are 1? And how can we get this: Thank you!",
        "target": "All alignment possibilities are 1 because there is only one possible alignment per sentence pair, and for the same length these alignments are always the same. In the data, for each sentence pair of length 1, token 1 is aligned to 5, token 2 is aligned to 4,  etc"
    },
    {
        "source": "#677: MT alignment probability understanding. In the ungraded HW problem, we see an example of lexical divergence, resulting in non-1 translation probabilities. I was wondering in what scenario will there be a non-1 alignment probability and wanted to check if my understanding is correct. If this is the example translation: He drinks coffee daily &lt;--&gt; on p'et kofe kazhdyy denAnd this is the dictionary: He : on, drinks : p'et, coffee : kofe, daily : kazhdyy denWill the alignment probabilities look something like this? q(1 | 1, 4, 5) = 1, q(2 | 2, 4, 5) = 1, q(3 | 3, 4, 5) = 1, q(4 | 4, 4, 5) = 0. 5, q(5 | 4, 4, 5) = 0. 5. Thank you in advance.",
        "target": "Yes, if that one sentence was your only training data, you would get q(4 | 4, 4, 5) = 0. 5, q(5 | 4, 4, 5) = 0. 5. More typically, you will have some inconsistent alignment  between multiple sentences, i. e. in some sentence pairs token 4 is aligned to position 4 and sometimes to position 5. "
    },
    {
        "source": "#676: ELMo Understanding. Hello, I am looking over the lecture slides again and wanted to make sure my understanding of ELMo was correct. Would it be that for a given sentence such as \"the cat is happy\" where you are analyzing the word \"cat\" ELMo would go forward for the context before the word \"cat\" (while attempting to predict \"cat\") and backwards for the words \"is happy\" (while attempting to predict \"cat\")? Then finally both of these predictions would be combined to form an output?",
        "target": "That sounds almost correct. There are initially two different embedding for each token, one using the left context and one using the right context. Then, these are just concatenation. "
    },
    {
        "source": "#675: Exam1 Q6 b). For Exam1 Question6 b), when using one-hot representation for input words and computing its embeddings, why does the embedding layer only add V*e parameters instead of 3V*e since there are 3 embeddings? How do we decide the embedding matrix is shared or not?",
        "target": "I think the embedding matrix weights are shared across each context word because we want to generalize a common matrix which encodes all the input word representations together."
    },
    {
        "source": "#674: Review session recording. Hello! I was wondering where we can find the recording of the review session from today (Sat. Dec 9th). I understand it just concluded, but wondering if it is already posted. Thanks!",
        "target": "i think its already posted :))"
    },
    {
        "source": "#672: ungraded exercise - graph based dependency pasring. Hi there, In the lecture notes, for the graph-based dependency parsing, we start from a completely connected graph and then find the maximum spanning tree from the fully connected graph as the dependency tree, so why the answer to the first question is \" The approach might create graphs that are disconnected\" given the generated tree is MST. Thanks a lot!",
        "target": "The approach described on the ungraded exercise does NOT use an MST algorithm. It simply greedily selects the highest scoring head for each token. "
    },
    {
        "source": "#671: Attention as lookup? . Hi, I understand attention as a concept generally (weighting hidden states depending on the decoding timestep) but I don't understand the \"attention as lookup\" idea. If the context vector is a aggregate of the hidden states, how does \"lookup\" fit in here? In the diagram below that I found, the query maps to all keys, so isn't it better to just describe this as a weighted sum? Why do we talk about attention as lookup? I feel like I'm missing something.",
        "target": "You are right that it feels like a weighted sum, at the end of the day there is a weighted sum that happens. Why this \"weighted sum\" is often though as a ~fuzzy~ lookup is because the mixing ratios in your weighted sum (for each embedding index) are determined/\"personalized\" by each position's query. So people call this \"queried\"/\"personalized\" weighted sum that happens in attention a sort of lookup table by virtue of it being \"queryable\" like a lookup table. "
    },
    {
        "source": "#671: Attention as lookup? . Hi, I understand attention as a concept generally (weighting hidden states depending on the decoding timestep) but I don't understand the \"attention as lookup\" idea. If the context vector is a aggregate of the hidden states, how does \"lookup\" fit in here? In the diagram below that I found, the query maps to all keys, so isn't it better to just describe this as a weighted sum? Why do we talk about attention as lookup? I feel like I'm missing something.",
        "target": "Very nice description, thanks.  "
    },
    {
        "source": "#670: IBM Model 2: how to identify position when two of the same words in one sentence. For ungraded assignment IBM Model 2 in example #4, isn't it possible for the \"e\" in the foreign sentence position 2 to map to position 4 in the English sentence?",
        "target": "Sure, but that would mean that the alignment is inconsistent and crosses between noun phrases. So while possible, it's not the most intuitive alignment. "
    },
    {
        "source": "#669: RNN Downstream Task. Suppose we use RNN to some downstream task on a whole sentence (with input tokens x1, x2, .. .. ,xk). Do we use the final hidden state h_k or use the final output y_k (or both) to input in a linear model?",
        "target": "I think it depends on what you are trying to do with the sentence. Using hidden state can help understand the whole sentence. Final output can help understand what the sentence might llead to or its conclusion."
    },
    {
        "source": "#669: RNN Downstream Task. Suppose we use RNN to some downstream task on a whole sentence (with input tokens x1, x2, .. .. ,xk). Do we use the final hidden state h_k or use the final output y_k (or both) to input in a linear model?",
        "target": "Typically you would feed the hidden representations into the downstream classifier. Note that there is a difference between token classification tasks and sentence classification tasks. In sentence classification you would use the hidden state computed after the last time step. "
    },
    {
        "source": "#667: IBM Model 2 General Questions. Hi, I am a little confused about the IBM Model 2, and I have some questions:1. When calculating the probability q(ji), is the index i representing the position in the source language or the target language? 2. How does this choice impact the alignment and translation probabilities, and what considerations should be taken into account for optimal model performance?",
        "target": "To answer your first question, it would make sense that i represents the position in the source language since q(j|i, l, m) represents the probability that the alignment variable i takes value j (from the slides). In other words, what is the probability that the value of variable i actually takes the value of j?  For instance, if \"gato\" is at position i in some Spanish sentence and \"cat\" is at position j in the English sentence, then q(j | i, l, m) would represent the probability that the alignment variable i (representing the position of \"gato\" in the source language Spanish) takes the value j (the position of \"cat\" in the target language). https://web. stanford. edu/class/archive/cs/cs224n/cs224n. 1162/handouts/Collins_annotated. pdf"
    },
    {
        "source": "#666: AMR assignment. I'm trying to better understand the usage of ARG0 and ARG1 in AMR. Could you explain when to use ARG0 as opposed to ARG1? (a/ admire\n    :ARG0 (h / he)\nHere we use ARG0 following the verb admire. (e / expect-01\n          :ARG1 (w / we)Here we use ARG1.",
        "target": "Here's my interpretation (which may or may not be right): ARG0 is typically used to represent the entity or entities that perform the action denoted by the main verb. In \"(a/ admire :ARG0 (h / he)), \" ARG0 is used to represent the agent, which is \"he\" (referring to someone) who is performing the action of admiring. On the other hand, ARG1 is generally used to represent the entity or entities that undergo the action or are affected by it. In \"(e / expect-01 :ARG1 (w / we)), \" now ARG1 is used to represent the patient or theme, which is \"we\" which refers to a group of people who are being expected. Overall, use ARG0 to represent the agent or entities performing the action and use ARG1 to represent the entities undergoing or affected by the action."
    },
    {
        "source": "#666: AMR assignment. I'm trying to better understand the usage of ARG0 and ARG1 in AMR. Could you explain when to use ARG0 as opposed to ARG1? (a/ admire\n    :ARG0 (h / he)\nHere we use ARG0 following the verb admire. (e / expect-01\n          :ARG1 (w / we)Here we use ARG1.",
        "target": "I find that this example do a good job of understanding AMR for me. Notice that for line 2 and 3; we are tending to predicate b: beg -01. With respect to the word beg, we could see that \"I beg you\" would make \"I' the action-taker (hence ARG-0) and you the affected (hence ARG1). However, when we apply this recursive structure to the predicate \"excuse\", the roles are reveresed and so are ARG0 and ARG1 labels."
    },
    {
        "source": "#665: Can distillation work for generative NLP models like GPT? . If so, why hasn't a distilled version of say GPT4 been developed? I would think a smaller, more portable version of GPT would be immensely useful for potential applications like on-device ChatGPT. I found this Microsoft paper (https://arxiv. org/pdf/2108. 13487. pdf) that seems to indicate that at least on GPT3 similar training methodologies can lead to the new model outperforming the original model:\"Brown et al. (2020) propose to directly use GPT-3 for downstream tasks, with the n given labeled instances and no fine-tuning. We refer to this strategy as raw GPT-3. We note that raw GPT-3 is expensive, as its cost goes linearly with the number of instances during inference. Also, it has a relatively high latency when deployed for real applications. However, even in terms of accuracy, we observe in the experiments from section 3. 3 that the in-house models trained with GPT-3 labels can often outperform raw GPT-3. We argue that by using data labeled by GPT-3, we are essentially performing self-training: the predictions on unlabeled samples act as regularization on induced models and help improve the performance. In particular, for classification problems, we can theoretically upper-bound the error rate of the best in-house model using the labels generated by GPT-3. \"I am just curious why if this is the case has it not been used or deployed in the real world.",
        "target": "Correct me if I misunderstood or if I am wrong. If you are asking about why we are not using labels from GPT4 to train models, I believe this is a (maybe widely) used approach for finetuning many LLM now: to use GPT4 feedback in finetuning as there has been evidence of positive correlation between human preference and GPT4 selection and always collecting human response is quite expensive. It's then collectively passed to a reinforcement strategy called DPO(direct policy optimization). I suggest checking out Prof. Rush's video here explaining how they did this on Zephyr(finetuned Mistral) https://www. youtube. com/watch? v=cuObPxCOBCw&amp;t=34s (paper here: https://arxiv. org/abs/2305. 18290) "
    },
    {
        "source": "#664: AMR Ungraded Assignment. What's the purpose of ARG1 (w/we) after the line (e / expect -01)? My understanding was that \"we are expected to meet him\" could be covered by:(e / expect-01)     :ARG1 (m/meet)           :ARG0 (w/we)            :ARG1 (h/him)    ",
        "target": "I suppose this is a bit up to discussion, based on the propbank frameset for expect-01. Does this sense just cover ARG0 expects an event ARG1, or would it instead be ARG0 has an expectation of ARG1 to do ARG2. I actually think your interpretation is better (and matches the one we came up with in class iirc). "
    },
    {
        "source": "#662: CVN Final Logistics. Hello, I hope you are doing well. I was wondering when we will get the CVN instructions for those of us taking the exam asynchronously. (Mostly just want to know the time frame in which we need to take the exam. )Thanks in advance!",
        "target": "Hi, The time window will open at 1:10pm on Monday and extend until Tuesday night, 11:59pm. "
    },
    {
        "source": "#661: Question about cheat sheet. Dear Prof, I'm wondering what kind of cheat sheet is accepted. Can we use printed cheat sheet? Thanks!",
        "target": "#384"
    },
    {
        "source": "#660: IBM model2. 1) for the probability q(j|i) in IBM model, is the dimension i standing for the dimension of source language or target language? 2) In order to make a one to many translation, does it mean the token number in source language (foreign language) is always more than the token number of target language (ie. English)? 3) If source language has 7 words and target language has 9 words, Can we use IBM Model?",
        "target": "1) i is in the source language, j is in the target language. 2) Not necessarily, some words in the target language may simply not be aligned to anything in the source language. 3) Yes, see 2. "
    },
    {
        "source": "#659: GPT API fine-tuning notes. Hi, Want to ask for any helpful notes about using GPT 3. 5-Turbo, GPT 4 API for fine-tuning? Any source available at Columbia? Thanks.",
        "target": "Hm, I'm not aware of a specific resource maintained at Columbia -- are you wondering about best practices etc. ? I think most researchers are a little skeptical about using the API for fine-tuning since that doesn't give them full control of the process and transparency. So, in most recent work I have seen people tend to use open source models (like Llama). "
    },
    {
        "source": "#658: Questions on Participation. Hi, I want to know if there are any requirements for participation on Ed in order to get participation points, such as a certain number of times or should not be anonymous? Thanks!",
        "target": "No require except for occasional posting. I can see you in the statistics tab, even when you post anonymously. "
    },
    {
        "source": "#657: how to count trainable parameters of model. Hi, last exam we had a problem about how to count the trainable parameters of a model. I am still a bit confused about this concept. Can I ask the relationship between trainable parameters with input embedding, hidden layer, possible output words. And how this concept apply to like RNN or some other models.",
        "target": "Since feed-forward layers are fully connected, the weights between two layers can be expressed as a (e, d) matrix, where e is the dimensionality of the previous layer and d is the dimensionality of the successor vector. For a simple Elman RNN model in the slides, the parameters consist of the weight matrices U and W, and a weight matrix to compute the output from the input. For some of the more advanced models (like LSTMs and transformers etc. ) things are more complicated because the internal architecture of the different components is more complex (think of the LSTM gates or the attention mechanism in the transformer). "
    },
    {
        "source": "#656: GLUE and SuperGLUE. Is there a reading material to understand in detail about how GLUE and SuperGLUE are computed? ",
        "target": "You could take a look at the glue and superglue websites:https://gluebenchmark. comhttps://super. gluebenchmark. comThis will not be in the exam (though it helps to have an idea how some of these problems can be solved with pretrained language models) "
    },
    {
        "source": "#655: Regarding BERT Pre-training. There are two modeling objectives while training BERT. MLM and Next Sentence Prediction. Are these two tasks trained together? Specifically, I want to know how the loss function would look like.",
        "target": "Hi, Based on what I read online, I believe that these two tasks are trained together. If you look at the diagram below from the research paper introducing BERT, it seems like when they are feeding in the pre-training examples that they are masking two sentences and then concatenating them together with the [SEP] token. Then, the special classification token at the start is used to predict the NSP. Based on this, I believe that the loss function would probably be the combined MLM loss and NSP loss. Based off this medium article and the BERT paper:https://arxiv. org/abs/1810. 04805https://medium. com/@Suraj_Yadav/what-is-bert-how-it-is-trained-a-high-level-overview-1207a910aaedBest Regards, @fr. baebae"
    },
    {
        "source": "#655: Regarding BERT Pre-training. There are two modeling objectives while training BERT. MLM and Next Sentence Prediction. Are these two tasks trained together? Specifically, I want to know how the loss function would look like.",
        "target": "Yes thats correct. Both objectives are used simultaneously and the losses are combined. "
    },
    {
        "source": "#654: Ungraded Ex Graph-Based Dependency Parsing. Hi, In the objective function proposed in question c, I don't see how this object optimize for labeling the type of the edge. Do we need to think about this? Thanks!",
        "target": "The problem assumes that dependencies are unlabeled  but its probably a good idea to think about how you could modify the approach to deal with labeled edges. "
    },
    {
        "source": "#653: Incorrect file name submitted for HW4. Hi Professor and TAs, I am just now realizing that the filename path for W2VMODEL_FILENAME in my lexsub_main. py submission is my local file path ('/Users/manasisoman/Downloads/GoogleNews-vectors-negative300. bin. gz') instead of ('GoogleNews-vectors-negative300. bin. gz'). I'm assuming this affects the autograder, so I just wanted to let you know that my Word2VecSubst class might not work as intended due to the file name error. This is completely my bad, and I'm really sorry if this adds additional work to the TAs when grading. Let me know if there's anything I should do / if you think it's best if I resubmit the same code but with the different file name. ",
        "target": "This will be fine. The TAs will be able to change the pathname during grading.  "
    },
    {
        "source": "#652: lec17 predicate and function. Hi, I feel not clear about the def of predicate and function in FOL models, why \"Student\" is a predicate not a function(on slide 12). In my understanding, predicate is a verb, argument represents each single word( not sure if I misunderstand these ). Thanks in advance!",
        "target": "A predicate makes an assertion Student(Mary) asserts that the individual designated by constant Mary is a student. The result is a truth value, either true or false. A function maps one constant to another one. Brother(Mary) returns another constant who is the brother of Mary. Its not asserting that Mary is a Brother. "
    },
    {
        "source": "#651: accidentally included code from external source in hw4. Hi. Recently when I was doing hw4 part 6 I tried to solve the question by trying out a few methods including extracting word embedding from bert and using cosine similarity. when I was trying to use pytorch to manipulate the tensors since I don't have pervious experience with large language models and pytorch I consulted chatgpt. I worked on it for hours but in the end it didn't work, I did not include part6. predict in my files because I know my method didn't work but I did included whatever I had at the last minute in the code because I was hoping to get some advice from the TAs about why it didn't work and I leaved a comment stating that my code for part 6 does not work. yesterday while I was going through my code for part 6 again I realized I included some code from chat gpt. I am not sure about the policy about chatgpt regarding this class but I think I should report this because I am not sure if this is academic dishonest.  I am sorry for any trouble I have caused you and I will definitely learn from this in the future.",
        "target": "Thanks for coming forward and letting us know. We probably wouldn't have noticed, in all honesty. We will just grade your code as submitted without any issues. Note though that the policy on the syllabus states \"AI-Tools/LLMs: The use of language model based AI to complete programming assignments is not permitted in this course, unless specified as part of an assignment. \"So if you are working on homework 5, please do not use ChatGPT. "
    },
    {
        "source": "#649: reentrancy in AMR. What is reentrancy in the context of AMR and where in the slides is it explained?",
        "target": "Hi there, from my understanding reentrancy means that we are reusing the variables when something is referenced again in the sentence. I find this graph helps me a lot to understand the elements in AMR:"
    },
    {
        "source": "#649: reentrancy in AMR. What is reentrancy in the context of AMR and where in the slides is it explained?",
        "target": "\"reentrancy\" simply refers to the idea that a vertex may have multiple incoming edges in the AMR graph. An entity may play multiple roles in relation to different predicates. "
    },
    {
        "source": "#648: midterm regrade request. I think this is a long shot, but I thought it was worth asking. I am redoing the my midterm and I noticed for question 1, I got points off for my chart, but I got the right answer. I am realizing that because of the lack of space, i could not write my full calculations, so I just wrote down the answer and not all the fractions that went into it. I did make a mistake  when starting the viterbi algorithm, I did not multiple the starting word with the corresponding pos value. This is a mistake, but I carried the mistake throughout the problem, which demonstrates an understanding of the algorithm. Looking at the rubric, I think i deserve 2 or 4 points off rather than 6 because I still got the final answer, I just made one wrong calculation that I carried throughout the problem. I know there is a lot of grading going on at the moment, and it is way past the midterm, so I understand if my request is denied. ",
        "target": "Hi, sorry for the slow response. The expectation was that you demonstrate understanding of the algorithm. Unfortunately, even though the answer is correct, your chart does not suggest that you computed the intermediate probabilities correctly (as previous_probability * transition_probability * emission_probability). A agree with the TAs' grading here. Also note that the regrade period for exam 1 has passed. "
    },
    {
        "source": "#647: window size for semantic relatedness and semantic similarity. Hello! I am not sure I entirely follow the logic behind wanting to keep the window size as large as possible to capture semantic relatedness (this alone makes sense since we want to have broader context for the target word) but keep the window small for semantic similarity? Is this simply because we dont want too many synonyms of hypernyms used in wrong contexts because as we broaden the context window, with more words substituting for the target word, we could have mismatches? Thank you!",
        "target": "For small context windows, the idea is that other words that appear in the same context must be substitutes, so at least they need to have the correct part of speech. By capturing the immediate context, you will also likely capture some of the tokens context words have a direct semantic relation with the target (either arguments or the predicate of the target word, for example). So other words will share the same relation. If you make the context bigger it becomes less likely for similar words to be exact substitutes. But they will still co-occur with the same content words, so they should be about the same topic and thus generally related. "
    },
    {
        "source": "#646: blank midterm. Hi is there a copy of the blank midterm? I want to use it as practice without the solutions",
        "target": "I don't think there is a blank copy. The files section on Canvas only contains the solution copy. "
    },
    {
        "source": "#645: Cross attention. Hi, I wonder how does the cross attention work and its algorithm? From my understanding the cross attention is implemented in the decoder layer as the multi-head attention to the input representation. Is that correct?",
        "target": "Yes, the only difference is between self-attention and cross-attention is that in the former all of key, query, and value vectors are from the same encoder embeddings whereas in the latter key and value from encoder embeddings while the query is constructed from decoder-side. Check this neat illustration, especially the section \"Decoder Side\"http://jalammar. github. io/illustrated-transformer/"
    },
    {
        "source": "#644: Question Towards Ungraded Exercise. the bird ate a fly -----&gt; skwk e qta skwk etIt's unclear here whether the first skwk means bird or the second means bird. As a result, q(1|2, 5, 5) should not be zero because  the first skwk (at position 1) may correspond to bird (in the 2nd position), right? Similarly,  q(4|5, 5, 5) should not be zero because there's a possibility that in sentence 3, j =4 and i = 5. Is my analysis correct?",
        "target": "So since \"e\" must mean \"a\" and \"et\" must mean \"the\" (from example 1 and 2), in sentence 3 the nounphrase \"the bird\" must appear to the right of the verb while \"a fly\" must appear to the left of the verb. It would be very odd if the alignments were just crossing between noun phrases. "
    },
    {
        "source": "#643: Weight Vector in Linear Models. In question 5 of midterm, Specify a set of concrete features and a corresponding weight vector w that will produce the correct classification for the documents in the training data. In the solution the weight vector is given as (1, 2, 3). I did not understand how it has been calculated.",
        "target": "The weights were selected to break ties between sentence 1 and 2 and sentence 2 and 3. In sentence 2, both feature 1 and 2 would be active, so by assigning a higher weight to feature 2, the model selects the correct label. Similarly in sentence 3, both feature 2 and 3 would be active, so you need to select a higher weight for feature 3 in order to predict the correct label. There are many feature vectors that satisfy these conditions, it could have been (1, 10, 100)  or (1, 3, 9) just as long as weight 2 &gt; weight 1 and weight 3 &gt; weight 2. "
    },
    {
        "source": "#642: AMR Question. How to represent \"am/is/are\" in AMRFor instant, \"I am good at coding\". or \"I am so tired about the final exam\". How to use AMR to represent this?",
        "target": "Hi, I believe that you don't need to ever represent \"am/is/are\" in AMR. If you think about it AMR is to just convey the meaning away from the syntactical representation. Am/is/are do not add meaning to any sentence and they are more like linking some subject to its predicate. They are also just versions of \"be\", so you can just look here in the documentation to see: https://www. isi. edu/~ulf/amr/lib/popup/be. htmlalso this video was in that documentation: https://www. youtube. com/watch? v=h66NudhsJgM&amp;ab_channel=isinlpAs for your example I am not 100% but they way I would represent the first one would be(c / code-01\n           :ARG0 (i / I)\n           :manner (g / good))\nI am not sure if the top node could be \"excel\", \"proficient\" or something like that since the main concept is the skill level. "
    },
    {
        "source": "#641: Weight Vectors inn Linear Models. I am a little lost on 1) the importance and use of weight vectors in Linear Models where we are manually identifying features and 2) how we solve for these weight vectors. There are 2 problems I am reviewing - the ungraded and the Exam1 problem, but I can't find the approach or reasoning to solve these. Can someone please provide insight into how I go about solving this problem? Thank you.",
        "target": "1) What do you mean by importance? Without the weight vector you would not be able to predict anything. 2) There is, in general, not an analytical way to find the optimal weight vector. Instead, training is done incrementally (using perceptron training for the linear model or gradient ascent on a loglinear model). But on the specific problems we have seen, the weight vector was easy to find, mostly because there were only very few trainign examples. The trick is to look at which features will fire for each instance, and then select the weight vector so that the feature that is paired with the correct class has the highest score. I think there was an Ed post asking about this specific question a while ago (when the midterm solutions first came out), but I couldn't find it right now. "
    },
    {
        "source": "#640: GPT/BERT downstream task. Suppose in we are given a task similar to what we did in CFG Parsing, ie. finding the Non terminal for each word, but using a BERT/GPT to do so. We talk in class to decapitate the last layer of BERT/GPT, and add a classifier. 1) If we are asked about this kind of down stream task, do we need to specify the linear classifier (ie. it may be a multi-class logistic regression or a Feed forward network)? Do we need to specify the loss function or training objective of the task (ie. maximize the sum of log probability of each true Nonterminal label)? 2) Generally I remember we use CLS token or Extract token for downstream tasks. In this case, we need to classify each token's non terminal, do we need these CLS/Extract tokens to input them into linear classifier. I can think of several casesa) only input CLS/Extract Tokenb) forget about CLS/Extract, input BERT/Transformer embeddings for each wordc) Use a augmented input ie. [CLS, Vi] for each token where Vi is the embedding for each token. Which method should I select? Generally in any tasks, how should we select for (a), (b), (c)3) To do So, there are two methods, one is fixing the original BERT/GPT's parameter fixed, the other is changing BERT/GPT's parameter according to downstream task. Which should we choose if we are given these types of problem?",
        "target": "So for this kind of problem there are generally multiple approaches (all of which would give you full points on an exam). It sounds like your understanding here is sound. 1) Yes, you would have to specify in detail what the downstream classifier looks like. The question would ask for training objective and loss function specifically if that was expected. 2) It depends on the ask. For per-sentence or multi-sentence tasks, you would use the CLS or Extract token. For per-token tasks you would use the individual token embeddings. I think your approach c) is not super common -- I have seen this before, but the applications are limited. 3) It depends! On some tasks fine-tuning the model on the downstream task is necessary, but it's usually much more expensive. On other tasks, the pre-trained contextualized representations may be good enough (for example, because the task is somewhat similar to the pretraining objective). "
    },
    {
        "source": "#639: Differences between GPT models. So in lecture, the model we go over is called GPT (I assume this is GPT1). Are there major training/design differences between the different GPT models (GPT1 vs GPT2 vs GPT3 vs GPT3. 5 vs GPT4), or are larger number versions typically just larger models trained on larger corpuses?",
        "target": "As far as I understand, for the most part the models are identical just with exponentially more parameters and trained on larger and more varied data sets.  GPT3 adds instruction tuning. "
    },
    {
        "source": "#638: HW3 and HW4 Grades Release Date. Hi Teaching Team, By when can we expect hw3 and hw4 grades to be released? ",
        "target": "Hey! I recommend checking out #618! Hope this helps!"
    },
    {
        "source": "#637: OHs after Exam? . Will there be reduced office hours after the final exam? Or will they continue as normal for students doing HW 5? Thanks!",
        "target": "They will likely be reduced, but some office hours will continue throughout reading week. "
    },
    {
        "source": "#636: Question about final exam. Hi all, Will we take the final exam at 309 Havemyer? Thanks!",
        "target": "We will take the final at the lecture room on the day of if I recall correctly from what Professor Bauer said in class (same situation as midterm)."
    },
    {
        "source": "#635: Homework 3 and 4 Grades. Will homework 3 and 4 grades be released before homework 5 is due?",
        "target": "Hi! I recommend checking out #618! Hope this helps!"
    },
    {
        "source": "#634: Participation Grade. Hello! Was wondering if participation on EdStem would suffice for a 10/10 on the participation grade. Thanks!",
        "target": "Yes, the participation grade is all-or-nothing. Participation on Ed is sufficient to get this part of the grade. "
    },
    {
        "source": "#633: AMR translation. While we translate AMR into sentences, could we have multiple correct sentences derived from the same ARM? Thank you.",
        "target": "Not a TA, but since the AMR tree is just a complication of lemmas, there are many different ways to interpret the sentence while preserving the semantics of the sentence. As shown in the paper \"Abstract Meaning Representation for Sembanking \":AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences he described her as a genius, his description of her: genius, and she was a genius, according to his description are all as- signed the same AMR. "
    },
    {
        "source": "#633: AMR translation. While we translate AMR into sentences, could we have multiple correct sentences derived from the same ARM? Thank you.",
        "target": "Yes, thats a crucial idea behind AMR: many sentences can have the same annotation. AMR is abstracting from the specific surface form. "
    },
    {
        "source": "#632: AMR Solution. For the AMR problem, are we supposed to know the two approaches discussed in class to parse (JAMR and bauers method)? I was wondering if there is more than one correct solutions. In other words, does my answer have to be identical to the solution? I get the general idea of how to parse (main verb up top, recursively find arguments or concepts to further break down) but I am confused about the step by step 'formula' or details. Just a little concerned about how to solve this type of problem on a test.",
        "target": "There is no specific algorithm for doing these translations  and since you dont know the full annotation guidelines, chances are you wont be able to get English-&gt;AMR exactly correct. Note that even trained annotators only get about a 90% agreement. For AMR-&gt; English, there are clearly many different sentences (all with the same meaning). What we are after is simply who does what to whom, when, where, why. .. .  As long as the basic predicate/argument structure (including reentrancy) reflects the sentence, your solution would count as correct. "
    },
    {
        "source": "#629: Clarification on Semantic roles, theta roles, deep cases. Hi there, Just wondering if these terminologies are interchangeable? Thanks ",
        "target": "For the most part. They originate from a slightly different branch of the literature, but they essentially mean the same."
    },
    {
        "source": "#628: Question towards dependency parsing. My question is towards the Arc-standard system and Oracle algorithm. In Oracle algorithm we perform Right-Arc only if there's no relationship pointing to the right element. Is this rule giving us a unique sequence of actions? In Arc Standard rule, do we have this rule for right-arc? ",
        "target": "Im not quite sure I understand this question. In the arg standard oracle algorithm, you perform the right_arc transition if 1) there is a dependency edge between the first word on the stack and the first word on the buffer and 2) all dependents of the first word on the buffer (according to the gold tree) have already been connected in the partially derived tree up to this point. This oracle algorithm is deterministic, so it produces one specific sequence of actions. Its possible that there are multiple sequences that result in the same tree. However, the oracle algorithm will produce only one of them. "
    },
    {
        "source": "#627: AMR Ungraded Exercise Q. Q3: Why is arg0 h/he and arg1 l/love not in the same indentation? Q4: Clarify difference and definition of arg1 and arg2? Whys is we arg1 and meet arg2? Thanks in advance!",
        "target": "Yes, I had the same question on Q3. I assumed that 'he' and 'love' would both be arguments for admire. I also was hoping to get some clarity about using pronouns as arguments. AMR is meant to omit details, but do pronouns not count as details?"
    },
    {
        "source": "#627: AMR Ungraded Exercise Q. Q3: Why is arg0 h/he and arg1 l/love not in the same indentation? Q4: Clarify difference and definition of arg1 and arg2? Whys is we arg1 and meet arg2? Thanks in advance!",
        "target": "same here. .. some clarifications would be nice"
    },
    {
        "source": "#626: Logical Form and MT Questions. Logical form:What are unbound variables? Does vocabulary or signature always mean arity value? Machine translation:For back translation, why create synthetic back-translation from French to English instead of English to French directly? IBM: why P(f, a| e, m)? BLUE: what does it mean by recall is ignored? Using the example in class exp(log(0. 8)+log(0. 5)*(1/2)) =0. 78 and not 0. 6325",
        "target": "Logical form (note this is not part of the material for the final): What are unbound variables? Bound variables are bound to a quantifier \"forall x. student(x)-&gt;smart(x)\". Unbound variables appear without a quantifier. Vocabulary/Signature refers to the collection of all predicates. But each predicate has a specific arity (number of arguments) and that information is part of the signature. MT:The assumption is that you have more mono-lingual data for french. m is the number of tokens in the F sentence. The q parameters depend on m, so the entire distribution depends on m. BLUE: Blue score only measure n-gram precision, i. e. how many of the system output/predicted ngrams appear in any of the reference translations. It does not measure n-gram recall, i. e. how many of the ngrams in the references appear in the system output. Thanks, this may be a typo. "
    },
    {
        "source": "#625: HW 5 Part 2 Loss Function. I am having trouble getting the correction dimensions from the cross_entropy loss(input, target) function in part 2. For the input parameter, I tried to mimic the 'outputs. transpose(2, 1)' code in the given training cell for part 3, but I am not sure why this tensor must be transposed if the model output already returns the logit tensor anyway. For the target parameter, I am very confused as to what 'input_pos'  is referring to in the hint \"# complete this. Note that you still have to provide a (batch_size, input_pos)  tensor for each parameter, where batch_size =1\" . Is the 'input_pos' the index of the role our sentence is predicated on form role_ids? The pytorch documentation keeps referring to the target parameter in reference to the \"class indice\" but I was not sure what this meant in this context, or if \"input_pos\" was the class indice. ValueError: Expected input batch_size (1) to match target batch_size (2).\n",
        "target": "The input should be (1, 128) since there are 128 input positions. The output will be (1, 128, num_labels). "
    },
    {
        "source": "#623: What word embedding method is used for transformers? . Is word2vec used to create the embeddings that get fed into a transformer typically? Does it vary based on transformer architecture and downstream task?",
        "target": "Transformer models usually have a trainable embedding layer (as in the neural language model, for example), but this layer is initialized with static embedding obtained from word2vec or glove. "
    },
    {
        "source": "#622: Does it normal to have 0 id in my untrained model prediction result. Hi! For the single prediction result on the untrained model, after doing argmax, for some indexes, I got 0 id,  while 0 is not presented in the keys of \"id_to_role\", so I am wondering if it is normal for our model to predict 0 at this stage? Thanks!",
        "target": "Yes thats possible. "
    },
    {
        "source": "#622: Does it normal to have 0 id in my untrained model prediction result. Hi! For the single prediction result on the untrained model, after doing argmax, for some indexes, I got 0 id,  while 0 is not presented in the keys of \"id_to_role\", so I am wondering if it is normal for our model to predict 0 at this stage? Thanks!",
        "target": "gocha"
    },
    {
        "source": "#621: Structure of Conceptual Questions. Hello, was just wondering what is to be expected of the structure of the conceptual questions. Someone of Ed mentioned that they would be similar to the last question on the first exam. Is this accurate? Or would these questions be more theoretical?",
        "target": "Just saw the newly posted ungraded exercises, so that kind of answers my question"
    },
    {
        "source": "#621: Structure of Conceptual Questions. Hello, was just wondering what is to be expected of the structure of the conceptual questions. Someone of Ed mentioned that they would be similar to the last question on the first exam. Is this accurate? Or would these questions be more theoretical?",
        "target": "Id recommend watching the lecture from this Wednesday! Professor Bauer throws out some examples of what conceptual questions could look like (I. e. using RNNs/Transformers to compute WSD the same way as the Lesk algorithm)"
    },
    {
        "source": "#620: Homework 4 Solutions. Hi- I was wondering if it would be possible to have a solution set released for homework 4 before the final if the grades will not be released in time. Thanks!",
        "target": "Yes, I can likely get these up -- but note that there is significant variation in how things can be implemented. "
    },
    {
        "source": "#619: Missing B-ARGM-REC role. Hi there, In the training corpus, there is an example that contains an unknown role As    long    as    these    two    countries    stand    together    ,    then    it    seems    that    other    countries    could    not    do    anything    to    them    .\n O    O    O    B-ARG0    I-ARG0    I-ARG0    B-V    B-ARGM-REC    O    O    O    O    O    O    O    O    O    O    O    O    O    OMay I ask how to handle this one? Also, for part 1. 1, it looks like the function \"tokenize_with_labels\" is finished, do we need to add anything to the function? Do I miss anything here? Thanks! ",
        "target": "The completed tokenize_with_labels function was an oversight :) I forgot to remove it-- feel free to use my version. I filtered out roles that appeared very infrequently (iirc less than 1k occurences in the dev data). So if you find a label in the corpus that is not in the list, you can just ignore it. I removed both the B and I tags in those cases, so simply ignoring the individual tags will not lead to inconsistent annotations. "
    },
    {
        "source": "#618: HW Grading Timeline. Hi--I wanted to ask if the teaching staff had an approximate guideline of when HW 3 and 4 would be finished grading. I just want to figure out if we would know any more of our grades before the final and if I should start working on HW5. Thank you so much!",
        "target": "We should have homework 3 done very soon (missing for one block of grades from one TA). Homework 4 won't be done in time, unfortunately. "
    },
    {
        "source": "#616: HW5 Q2 single loss for untrained model. Hi there, For part 2, by running a single input on the untrained model, I got a loss value to be 4. 164388656616211. Given the desired initial loss is 3. 970291913552122, I am wondering if my initial loss value is acceptable. Thanks!",
        "target": "That seems a bit high. Try to instantiate the model multiple times, then compute the initial loss and average. "
    },
    {
        "source": "#615: LSTM for SRL. Is the following logic correct? If the input consists of 4 words and the task involves assigning BIO tags to each word indicating their semantic roles, the number of outputs generated by the model would typically be 4 times the number of possible BIO tags or semantic role categories. For instance, considering a simplified set of BIO tags:B-Agent: Beginning of an Agent role. I-Agent: Inside an Agent role. B-Patient: Beginning of a Patient role. I-Patient: Inside a Patient role. O: Outside any role. If each word in the input sequence is assigned one of these tags (B-Agent, I-Agent, B-Patient, I-Patient, or O), the number of possible outputs for each word would be 5 (including O). Therefore, for an input sequence of 4 words, the total number of outputs would be 4 multiplied by the number of possible tags for each word, which in this case would be 4 * 5 = 20 outputs. These outputs correspond to the predicted BIO tags for each word in the sequence, indicating their assigned semantic roles or lack thereof (as denoted by O). Therefore, in the following LSTM, there should be more than 4 output probabilities, correct?",
        "target": "Sorry for the slow response. I'm not sure I understand the question here. Yes, there will be number_of_possible_labels output units. So if you unroll the RNN, there would be number_of_tokens * number_of_possible_labels output units. During decoding, that isn't typically done though -- you only consider one time-step at a time/ Are you just asking why it only shows P(Arg0) for the first token, and not all of them? The figure is demonstrating the training process, so P(BArg0) is the output that matters for computing the loss on the training data. "
    },
    {
        "source": "#614: SRL Path From Argument to Target. What do we get out of finding the path from argument to target here?",
        "target": "The parse tree path is a relevant feature for determining if the candidate constituent is an argument and what its label may be. "
    },
    {
        "source": "#613: Helpful Resource for Transformers, GPT, and BERT. https://jalammar. github. io/illustrated-gpt2/",
        "target": "thank you so much! this seems really cool"
    },
    {
        "source": "#612: HW5 - Opening Jupyter from browser. Hi! I was setting up my instance and got to the last steps but I can't connect to the notebook in my browser :(The firewall rules are set per the spec, and the notebook looks like its up and running when I run:jupyter notebook &gt;&gt; jup_notebook. log 2&gt;&amp;1 &amp; tail -f jup_notebook. logBut I'm never able to connect, has anyone else had this issue?",
        "target": "Did you change the jupyter config file? "
    },
    {
        "source": "#611: 12/06 lecture exam review. Hi! i am wondering if tomorrow's lecture will be an exam 2 review session (as indicated in the syllabus)? Thanks! ",
        "target": "#609 "
    },
    {
        "source": "#610: Review Session Virtual Options? . Will the Saturday final review session have a Zoom option as well? Also, will it be recorded for us to review after the session ends? Thanks!",
        "target": "Yes, we will try to broadcast it on zoom and record. "
    },
    {
        "source": "#609: Review Session. Is there a final review session today and if there is when and where is it being held?",
        "target": "There will be a review session on Saturday (tentatively at 4pm). Room TBD. We will also have a review session in class tomorrow. "
    },
    {
        "source": "#608: HW 5 1. 2. 1. Hello, I am currently trying to populate the self. items array by reading in the entire tsv file into an array, and parsing every 4 lines into their tokens and bio_tags. Although this method makes sense to me logically, it is very inefficient as the file has over a million lines. Is there anything I can do to optimize this parsing? Is there a better approach I could take maybe?",
        "target": "So there is no (efficient) way around storing the data in memory at some point. Its probably better to only store the final dictionary rather than the initial lines from the file. I would read the file line-by-line rather than all at once and keep count of the lines. After every four lines, process the data you have read to create a new item. "
    },
    {
        "source": "#607: Suggested reading for linguistics. Hi, Thanks Prof. Bauer and the TA's for a wonderful semester! I was wondering if there are any books/articles on linguistics that can help us understand natural language processing better, especially for those without a linguistic background. Thank you very much!",
        "target": "Are you more interested in textbooks or in casual reading? One of the books that got me interested/started in linguistics was Stephen Pinker's \"The Language Instinct\". It's not completely up-to-date in terms of the consensus within cognitive/neuro science, but still a fascinating read. "
    },
    {
        "source": "#607: Suggested reading for linguistics. Hi, Thanks Prof. Bauer and the TA's for a wonderful semester! I was wondering if there are any books/articles on linguistics that can help us understand natural language processing better, especially for those without a linguistic background. Thank you very much!",
        "target": "not a TA but did my undergrad in linguistics major, I can suggest some textbooks we read for some courses in my ugrad:1. Intro to linguistics: Language Files: Materials for an Introduction to Language and Linguistics2. Intro to Phonology/Phonetics: Zsiga, Elizabeth, C. (2013) The Sounds of Language: An Introduction to Phonetics and Phonology. Malden: Wiley-Blackwell. [I actually found this book quite interesting]3. Intro to historical linguistics: Lyle Campbell, Historical Linguistics: An Introduction, 3rd ed. 4. Intro to syntax/semantics: syntax: Sportiche, Dominique, Hilda Koopman and Edward Stabler 2014. An Introduction to Syntactic Analysis and Theory. Wiley Blackwell ; semantics: Elbourne, Paul. 2011. Meaning: A Slim Guide to Semantics. Oxford University Press5. Pragmatics: https://plato. stanford. edu/entries/pragmatics/ while these maybe too specific and too academic, feel free to take a look at the linguistics branch that corresponds to your interest in nlp. Hope this helps :)"
    },
    {
        "source": "#606: HW 5 part 1. 1. Hello, I wanted to get a head start on hw 5 after finishing hw4. I noticed for part 1. 1, where we are to map the tokenized sentence to the proper BIO tags, that the code given produces the gold output shown in the prompt. Was this code left completed on purpose? def tokenize_with_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n    Word piece tokenization makes it difficult to match word labels\n    back up with individual word pieces. This function tokenizes each\n    word one at a time so that it is easier to preserve the correct\n    label for each subword. It is, of course, a bit slower in processing\n    time, but it will help our model achieve higher accuracy.\n    \"\"\"\n\n    tokenized_sentence = []\n    labels = []\n    \n    for word, label in zip(sentence, text_labels):\n\n        # Tokenize the word and count # of subwords the word is broken into\n        tokenized_word = tokenizer. tokenize(word)\n        n_subwords = len(tokenized_word)\n\n        # Add the tokenized word to the final tokenized word list\n        tokenized_sentence. extend(tokenized_word)\n\n        # Add the same label to the new list of labels `n_subwords` times\n                  \n        if label. startswith(\"B\"):\n            labels. append(label)\n            labels. extend([\"I-\"+label. split(\"-\", 1)[1]] * (n_subwords-1))\n        else: \n            labels. extend([label]* n_subwords)\n            \n\n    return tokenized_sentence, labels\n",
        "target": "Ha, yes it was supposed to be removed. Feel free to use this part, obviously. "
    },
    {
        "source": "#605: HW4 part2 &3 attempted number less than total. Hi Professor and TAs, When testing HW4 part 2 and part 3, I noticed that the number of attempts shown is slightly smaller than total, but this situation does not occur in part 4, 5 &amp; 6. I'm curious about what could lead to the failure of some attempts. ",
        "target": "So several people have made this observation and I havent been able to figure out why this is happening. I suspect some form of encoding issue that causes the Perl script to ignore some lines. "
    },
    {
        "source": "#604: HW 4 Submission Question. Hello! I submitted at 12 a. m. and not 11:59 p. m. Will my score by deducted by 20%? Also, I am not really sure how it is scored, but say there is an error in my syntax that results in a compile error. How is that graded?",
        "target": "No that falls within the gray area. "
    },
    {
        "source": "#602: hw3 grade. Hi, just want to get an idea about when will the grade of hw3 be released? thank you so much!",
        "target": "Im hoping tomorrow or Wednesday. Theres one part missing. "
    },
    {
        "source": "#600: P6) LLM API/HF API. For part 6, would I be allowed to call another HF models api/the openai api to test how it performs?",
        "target": "Yes. Thats a good idea. "
    },
    {
        "source": "#599: Output Tokens from BERT. Do we need to process any tokens outputted from the BERT model like . replace(\"_\", \" \") and . lower() or can we assume the tokens outputted by BERT are singular lower-case words? ",
        "target": "Technically the tokens outputted by BERT are wordpieces. But since the output is filtered by matching against the candidates you can expect the remaining predictions to be individual lower-case words. "
    },
    {
        "source": "#598: Part 6 Runtime. I call part 2, 3, 4, 5 in part 6 and it takes a really long time for my code to run (over an hour). Is that okay, or do I need to come up with a part six that has a quicker runtime to get the points?  Thanks!",
        "target": "I had this problem too! I just decided to modify my code in a slightly different way because I didn't want to risk it :)"
    },
    {
        "source": "#598: Part 6 Runtime. I call part 2, 3, 4, 5 in part 6 and it takes a really long time for my code to run (over an hour). Is that okay, or do I need to come up with a part six that has a quicker runtime to get the points?  Thanks!",
        "target": "Not a TA, but according to the assignment, \"Any small improvement (or conceptually interesting approach) will do to get credit for part 6. \" So I think you're totally fine even if the \"performance, \" however this is being defined, isn't better. See #594."
    },
    {
        "source": "#598: Part 6 Runtime. I call part 2, 3, 4, 5 in part 6 and it takes a really long time for my code to run (over an hour). Is that okay, or do I need to come up with a part six that has a quicker runtime to get the points?  Thanks!",
        "target": "A little late, but one thing that could cause this is if your code for part 6 (or main) were creating the model multiple times (if you're using BERT or Word2Vec as base). "
    },
    {
        "source": "#597: HW4 Test Warning. I got the warning like this:/Users/. .. /venv-metal/lib/python3. 9/site-packages/urllib3/__init__. py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1. 1. 1+, currently the 'ssl' module is compiled with 'LibreSSL 2. 8. 3'. See: https://github. com/urllib3/urllib3/issues/3020 warnings. warn(when I tries to run the code python lexsub_main. py lexsub_trial. xml  &gt; smurf. predict. Would this affect my implementation or setup? How to fix it?",
        "target": "Um since its just a warning I dont see why it would affect the output. But Im confused why urllib would be needed for anything. .."
    },
    {
        "source": "#596: HW 4 Part 2. would the result like this:Total = 298, attempted = 298precision = 0. 092, recall = 0. 092Total with mode 206 attempted 206precision = 0. 131, recall = 0. 131good enough for this part?",
        "target": "someone posted these exact stats in a previous post and Dr. Bauer approved it, I think you're all good."
    },
    {
        "source": "#595: HW4 Outputs. Are these acceptable outputs? Did you guys get similar?",
        "target": "See #585. They have similar scores, and Professor Bauer said they look normal. "
    },
    {
        "source": "#594: HW4 Part 6 Score. Hi everybody! I was wondering if it is okay if our part 6 score is significantly worse than some of the other predictors as long as it is conceptually interesting/unique. Thanks for the help!",
        "target": "Hi David! I'm not super sure, but the assignment says, \"Any small improvement (or conceptually interesting approach) will do to get credit for part 6\". I assumed we could have a good approach that performs only as well as one of the other methods. Hope this helps!"
    },
    {
        "source": "#594: HW4 Part 6 Score. Hi everybody! I was wondering if it is okay if our part 6 score is significantly worse than some of the other predictors as long as it is conceptually interesting/unique. Thanks for the help!",
        "target": "As long as the approach is interesting/different from the other parts, its okay if it doesnt outperform the other approaches. "
    },
    {
        "source": "#593: punctuation as tokens. Does the Lesk algorithm consider punctuation such as \"? \" and \", \" as tokens when calculating overlap?",
        "target": "No punctuation should be ignored for part 3.  "
    },
    {
        "source": "#591: ARM Ungraded exercises solution. Hi - I am wondering whether the ARM exercises solution has been posted because I was not able to find it. Tysm!",
        "target": "Posted in Pages now."
    },
    {
        "source": "#590: import collections? . Are we allowed to import collections?",
        "target": "Yes according to https://edstem. org/us/courses/46417/discussion/3974511"
    },
    {
        "source": "#590: import collections? . Are we allowed to import collections?",
        "target": "Thats fine. "
    },
    {
        "source": "#589: part 6. hi i hope you are great.  I have an idea for part 6 that involves importing a new library in hw.  is that okay? Thank you!",
        "target": "Yes for part 6 this is fine. Make sure you list any necessary dependencies in a comment. "
    },
    {
        "source": "#588: Hw 4 Part 6. Do we have to write explanation for the part 6 of HW 4 or is it sufficient to just include the code.",
        "target": "An explanation in a comment would be useful for the graders. "
    },
    {
        "source": "#587: Final exam weighting. Sorry if this was announced already, but I was wondering in there would be a weighting on the final between material covered before and after exam 1?",
        "target": "It will be approximately 60% new material, 40% old material. "
    },
    {
        "source": "#585: HW4: part3 result better than part2. For some reason my part3 result # Total = 298, attempted = 298\r\n# precision = 0. 112, recall = 0. 112\r\n# Total with mode 206 attempted 206\r\n# precision = 0. 155, recall = 0. 155\r\nis better than part2 result# Total = 298, attempted = 298\n# precision = 0. 074, recall = 0. 074\n# Total with mode 206 attempted 206\n# precision = 0. 107, recall = 0. 107Does this result looks normal?",
        "target": "Yes, that looks normal. "
    },
    {
        "source": "#584: HW4 Part 2 Scores Question. Hi, I am getting the following scores for part 2, but they are quite low compared to the other ones I have seen in Ed. Is this within the reasonable range? This is how I am implementing it. Is there something I'm overlooking? Thanks in advance!",
        "target": "You need to use the . count() method on the lexemes to obtain the frequency. "
    },
    {
        "source": "#583: HW5 Missing role_list. txt. Where should we download rolelist. txt? After unzipping ontonotes_srl. zip, I only got propbank_dev. tsv  propbank_test. tsv  propbank_train. tsv",
        "target": "I think you are the first person working on this. :) I added the rolelist. txt file to the zip file. Please download it again. "
    },
    {
        "source": "#582: HW4 Part 3 Details. Hi, I just want to confirm my understanding of calculating overlap in simple lesk is correct:1. Suppose a word appears in the definition of synset m times and in the context of the target word n times. Then when calculating the overlap, we count this as 1 overlap, instead of min(m, n)? 2. We need to discard the occurrences of the target word in the definition/contextWith the above understanding (and the score based approach in #477) I get the following result for part 3:which is significantly better than the result for part 2:Is this expected behavior, or did I do something wrong?",
        "target": "Yes, both of these points are correct. Point 1 is a bit up to interpretation, but in my implementation I simply used set intersection, so duplicate words were discarded. The results look good tome. "
    },
    {
        "source": "#581: Same score for part 4 and part 5. Hi, I got exactly the same score for part 4 and part 5 . Is that an issue or Ok? \"(base) qmn@qmndeMBP hw4_files % perl score. pl part5. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170(base) qmn@qmndeMBP hw4_files % perl score. pl part4. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170'Also, Am I run part 5 correctly? ",
        "target": "I got the same result, see #520 "
    },
    {
        "source": "#581: Same score for part 4 and part 5. Hi, I got exactly the same score for part 4 and part 5 . Is that an issue or Ok? \"(base) qmn@qmndeMBP hw4_files % perl score. pl part5. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170(base) qmn@qmndeMBP hw4_files % perl score. pl part4. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170'Also, Am I run part 5 correctly? ",
        "target": "Same to you. PART4:\nTotal = 298, attempted = 298\nprecision = 0. 115, recall = 0. 115\nTotal with mode 206 attempted 206\nprecision = 0. 170, recall = 0. 170\n\nPART5:\nTotal = 298, attempted = 298\nprecision = 0. 115, recall = 0. 115\nTotal with mode 206 attempted 206\nprecision = 0. 170, recall = 0. 170\n"
    },
    {
        "source": "#580: HW 4 Part 3 Score. Hello, I have written my code for part 3, and I followed the logic as recommended in the hw and EdStem posts, but I am getting a low performance as shown below:Any tips on how I can improve this performance? I checked, and my tokenization is correct. I dont include the target word in the full context to make it easier to parse later too.",
        "target": "Hi, this might not fix everything but I noticed \"use_up\" in your prediction - make sure to replace all \"_\" in the predictions with a space \" \". You can do this by using the replace() function, for example: word. replace(\"_\", \" \")"
    },
    {
        "source": "#580: HW 4 Part 3 Score. Hello, I have written my code for part 3, and I followed the logic as recommended in the hw and EdStem posts, but I am getting a low performance as shown below:Any tips on how I can improve this performance? I checked, and my tokenization is correct. I dont include the target word in the full context to make it easier to parse later too.",
        "target": "Im getting similarly low numbers for this part. Im combining left/right context, tokenizing the context, getting synsets for the target word with the appropriate POS. Then I strip stop words from the context, iterate through the synsets, obtain definition, examples, hypernyms for each, tokenize the definition, remove stop words from definition and each example.  Next, I look for overlap in the definition, incrementing counter each time a match is found. Then I check for overlap in each example (tokenizing each example first), obtain examples and definition for each hypernym, remove stop words and tokenize those, iterate through them and again search for overlap. This in the end gives me a dict of synsets with total overlap counts for definition, examples, hypernym examples and definition. If the dict is empty (no overlap), I iterate through the synsets, then through the lemmas in each synset and store the lemma count for each synset in another dict. I take the max from this dict to get the most frequent synset, then iterate through the lemmas in this synset and store their counts. The lemma with the maximum count I take as the synonym. If there is a tie, I do the same thing, only checking the synsets that have the highest (tied) score, going through the lemma counts for the most frequent synset, choosing the max as the synonym. I think there may be a problem with how Im handling no overlap/ties, but Im not sure. I didnt try to implement the professors suggested algorithm. Question:  If the scores for this part are low/incorrect, will we be graded on our code/algorithm process and be given partial credit?"
    },
    {
        "source": "#579: HW4 Part 2 (illegal division by zero). I have seen several posts about \"Illegal division by zero at score. pl line 109\" on Windows but I haven't seen many solutions. I tried to use the lines written in #476 but I still get the same error when I run perl score. pl wn_frequency_predictor gold. trial. I would appreciate any guidance from people who have been able to solve this issue!",
        "target": "I got this error when the file name was incorrect.  Did you mean wn_frequency_predictor. predict?"
    },
    {
        "source": "#579: HW4 Part 2 (illegal division by zero). I have seen several posts about \"Illegal division by zero at score. pl line 109\" on Windows but I haven't seen many solutions. I tried to use the lines written in #476 but I still get the same error when I run perl score. pl wn_frequency_predictor gold. trial. I would appreciate any guidance from people who have been able to solve this issue!",
        "target": "I think first check your file name and whether you change the main when you test for each part. And did you do \"python lexsub_main. py lexsub_trial. xml &gt; wn_frequency_predictor. predict \"  before you run score. pl? (I had the same error and it turns out to be this issue. )"
    },
    {
        "source": "#578: Part 3 Synonyms. In Part 3, we need to find the synset with the greatest overlap between the context and the definition. When we get this synset, how should I decide which word to pull out? Because a synset is a collection of lemmas, most of these synsets will have numerous words. Am I supposed to use the p2 results to return the most common lemma in that synset? Thank you!",
        "target": "Yes you would then choose the lemma with the highest . count() value. Take a look at #477 for an optional alternative approach to selecting a lemma and breaking ties. "
    },
    {
        "source": "#577: HW5 GCP GPU Quota. Hi Prof. Bauer and TAs, When getting quota for GPU, I am facing 2 problems:There is no link that takes me to the GPU Quota page when the initial VM creation failed. The \"Learn More\" takes me to a documentation page, not a place to request for GPU quota. I am able to find the page to request GPU quota but no quota can be selected (but can be seen)Any suggestions on how to set up the VM instance?",
        "target": "So that error message indicates that no GPUs are available in your availability zone. Please try a different one. One the Quota page, filter for the \"NVIDIA T4 GPUs\" quota. You should see a list of all availability zones and a Limit of 0 on each. Select the zone you need (I had luck with us-east4). Then select the checkbox and click \"Edit Quotas\" on top of the page. "
    },
    {
        "source": "#575: HW4 part 5 clarification. Hey Staff, I don't think I am understanding what I am supposed to do in part 5:The instructions are:Obtain a set of candidate synonyms (for example, by calling get_candidates). Convert the information in context into a suitable masked input representation for the DistilBERT model (see example above). Make sure you store the index of the masked target word (the position of the [MASK] token). Run the DistilBERT model on the input representation. Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target position (i. e. the position of the masked word). Return this word. So firstly, call get_candidates and store it in a list. Secondly, Take the context, find the index with the context word and swap the word with [mask]. Thirdly, run the model on the context and get back the 10 best words based on the model. Lastly, -here is where I don't fully understand- from the set of candidate synonyms in part 1 choose the highest scoring word for the masked word. But then where does the BERT model come in? Here is my code:def predict(self, context : Context) -&gt; str:\n\n    # 1. TODO Obtain a set of candidate synonyms (for example, by calling get_candidates).    cs_list = set(get_candidates(context. lemma, context. pos))\n\n    # 2. TODO Convert the information in context into a suitable masked input representation for the DistilBERT model (see example above).    #. .. TODO Make sure you store the index of the masked target word (the position of the [MASK] token).    index = 0\n    context_list = context\n    for i in range(0, len(context_list)):\n        if context_list[i] == context. lemma:\n            index = i\n            context_list[i] = '[MASK]'\n\n    input_toks = self. tokenizer. encode(context_list)\n    input_mat = np. array(input_toks). reshape((1, -1))\n\n    # 3. TODO Run the DistilBERT model on the input representation.    outputs = self. model. predict(input_mat)\n    predictions = outputs[0]\n    best_words = np. argsort(predictions[0][5])[::-1]  # Sort in increasing order\n    best_tokens = self. tokenizer. convert_ids_to_tokens(best_words[:10])\n\n    # 4. TODO Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target    #. .. TODO position (i. e. the position of the masked word). Return this word. What exactly am I supposed to do in the last part? Thank you!",
        "target": "I would iterate through the predicted tokens suggested by BERT (in decreasing order of their probability). For each, you check if it appears in the candidate list. If it does, return the candidate. If not continue to the next predicted token. "
    },
    {
        "source": "#574: Part 5 Time. Hi, I noticed another Ed Post that indicated Part 5 should not take a long time. However, it seems to be taking at least 5 to 6 minutes for me, and I'm not sure if this is normal? Thanks so much! ",
        "target": "That seems fine to me. There may be a bottle-neck in how you sort the predictions. "
    },
    {
        "source": "#573: Error with Word2Vec embeddings. When I try to test my genism download, I had the same issue as #502, so I edit my embeddings import statement to not include the . gz extension. When I do that, I don't get an error, but when I try to run v1 = model. wv['computer'], I get the below errorAttributeError: 'KeyedVectors' object has no attribute 'wv'Do you know why this could be happening?",
        "target": "I saw that the workaround for this is just below on the instructions, please disregard this"
    },
    {
        "source": "#572: HW 5: Google Cloud and GPU. Dear Teaching Team, Just out of curiosity, when we it's said that HW 5 \"requires a GPU with sufficient memory\", what magnitude are talking about? Is it completely unfeasible to run this sort of projects on a personal computer? I'm curious because I want to try to make some projects over winter break. Thank you!",
        "target": "You will need about 16GB of GPU memory to fine-tune BERT.  "
    },
    {
        "source": "#571: question about part 2 breaking ties. In part2, if there is a tie, I just choose the word appears first (I made a dictionary to record all lemmas in all sysnets and call item() ). I got 0. 074 and 0. 107. If I choose the word appears last, I got 0. 035 and 0. 01. Why it differs so much? Is this score acceptable?",
        "target": "I'm actually surprised it makes such a huge difference. But then again, there aren't that many test instances. "
    },
    {
        "source": "#570: Allowed imports? . Hi, Are we allowed to import extra packages? I imported string, re, and defaultdict",
        "target": "Yes, as long as they are part of the python standard library. "
    },
    {
        "source": "#568: HW4 Part5. In part5, do we need to consider the case where none of the prediction word is in the candidates(getting from part1)? Thanks!",
        "target": "Yes, I don't think this happens with this data, but it's a good idea to have a way of handling it. Maybe simply select the most likely BERT prediction :)? "
    },
    {
        "source": "#567: score for part 3. Hi, I'm getting \"Total = 298, attempted = 298precision = 0. 111, recall = 0. 111Total with mode 206 attempted 206precision = 0. 150, recall = 0. 150\"for part 3, which I believe is so much higher because simple lesk algorithm should not outperform the WordNet frequency baseline. . Is this expected? I post the output file just for reference.",
        "target": "Professor Bauer responded to another student about part 3 outperforming part 2! #510 #585 "
    },
    {
        "source": "#566: Bert Model weird output. Hi, When I run python lexsub_main. py lexsub_trial. xml  &gt; part5. predict i get this message printed over and over below. However, my code doesn't error out (is this just a warning? )Then when I do perl score. pl part5. predict gold. trialI get results that are similar to what other students are getting for part 5.",
        "target": "This is just a warning that you can safely ignore. "
    },
    {
        "source": "#565: HW4 Overlap Definition. I am little confused that, set and list, which data type we should use for calculate overlap score. For example, the context is [\"insitution\"] and gloss of a \"bank\" sense is depository financial institution, bank, banking company\n\na financial institution that accepts deposits and channels the money\nThere are two \"institution\" in the gloss. In this case, the overlap score is 1 or 2?",
        "target": "I recommend just using a set. The overlap score would then be 1. You could make an argument that counting duplicates accounts for some words being more important, but I think you would have to control for the varying lengths of the glosses. "
    },
    {
        "source": "#563: HW4 part5 output. Hi, For my part 5 output, it prints the process to the . predict file. Do I need to remove it from the . predict file? Thank you",
        "target": "For me, adding 'verbose=0' inside model. predict() helped remove these extra lines from the . predict file. So, the line of code would look something like:outputs = self. model. predict(input_mat, verbose=0)"
    },
    {
        "source": "#562: Part 4 Low Accuracy. Hello! At the moment for HW 4 part 4, I'm getting a prediction and accuracy of 0. 102. Does anyone have any suggestions on slightly increasing these values? Thank you!",
        "target": "#495"
    },
    {
        "source": "#561: HW 4 Part 5 tokenizer. encode. Hello, I wanted to sanity check some behavior I noticed. Should the tokenizer. encode add unnecessary #'s and split the sentence somewhat incorrectly in some cases such as the below? The third line starting with ['[CLS]', .. .. ] is what I get when I print the result of calling tokens. convert_ids_tokens on tokenizer. encode(first sentence shown below). I believe the extraneous #'s might be affecting how my model performs but I'm not sure if it's expected.",
        "target": "This is the subword wordpiece tokenization. Bert is trained on this type of tokenization, so the input is necessary for the encoder to work properly. "
    },
    {
        "source": "#560: HW4 Part 3. Hi, I am working on part 3 right now and I have a question. I went into office hours early which helped me get started and get a basic idea of what we are doing for this part, but I'm bit confused. I've pasted my code below. For the code under my part B comment, shouldn't it go under the \"for lemma in lemmas\" loop from part A? Or am I understanding something wrong. Also for the tie breaker stuff that was mentioned in #477, where would this fit into my code? Like should I be handling the tie breaker stuff after my part C? Thanks. def wn_simple_lesk_predictor(context : Context) -&gt; str:\n    stop_words = stopwords. words('english')\n    # For part a, tokenize the context and finding the overlap between the synset + synset hypernyms definitions\n    # and examples\n    tokens = set()\n    context_lr = context. left_context + context. right_context\n    context_string = \" \". join(context_lr)\n    for token in tokenize(context_string):\n        if token not in stop_words:\n            tokens. add(token)\n    lemmas = wn. lemmas(context. lemma, context. pos)\n    for lemma in lemmas:\n        syn = lemma. synset()\n        definition = syn. definition() + \" \" + \" \". join(syn. examples())\n        for hypernym in syn. hypernyms():\n            definition += hypernym. definition() + \" \" + \" \". join(syn. examples())\n        definition_set = set(word. lower() for word in tokenize(definition) if word not in stop_words and word. isalpha())\n        overlap = tokens. intersection(definition_set)\n\n    # For part b, loop through the synset get the count go synset. lemma. count\n    for synset in overlap:\n        for lemmas in synset. lemma:\n\n\n\n    # Part c loop through synset. lemma, get l. count for l in synset. lemma\n    for l in synset. lemma:\n        count = 0\n\n\n    return None #replace for part 3",
        "target": "Im not sure what the loop under your part b comment I supposed to do. Once you have the overlap (in the for lemma in lemmas loop) you can compute the score for the current lexeme (the lemma variable) as described in #477. Then store the lemma and the score in a dictionary. Finally (after the main for loop) select the candidate with the max score. "
    },
    {
        "source": "#559: python: can't open file 'perl': [Errno 2] No such file or directory. Hi I hope you are great. I have beeen testing my files non stop for days. I'm not sure what i did but i just got the errorpython: can't open file 'perl': [Errno 2] No such file or directorythe perl command has been working for days soi am not sure what is going onthank you",
        "target": "I suspect you are trying to run Perl from inside the Python interpreter. "
    },
    {
        "source": "#556: BertPredictor predict() outputting one more word. Hi Professor and TAs, I tested my algorithm for pt 5 with Professor's test codes (in main). However, there's always an extra word at the end of my outputs (for example, it's supposed to be['tight', 'close', 'loose', 'deep', 'big', 'tighter', 'tied', 'firm', 'hard']\ninstead of['tight', 'close', 'loose', 'deep', 'big', 'tighter', 'tied', 'firm', 'hard', 'down']\nI am also confused about why I have to manually add the add_special_tokens = True in my test code input_toks = tokenizer. encode(\"If your money is tight, don't cut corners\", add_special_tokens=True)\nTo make it print the above outputs, if I don't add it, then the output is [', ', '. ', '-', ';', 'and', '/', '. .. ', '! ', ':', 'to']. Any hint or direction would be greatly appreciated, thank you in advance.",
        "target": "Hm, my example code suggests that there should be 10 tokens predicted, not 9 (as shown in the example). So your output looks good. I think with the special tokens here is what's going on: With special tokens, the tokenizer will automatically insert [CLS] and [SEP]. Then the the target word \"tight\" would be at index 5. Without special tokens it would be at index 4, and then the \", \" would be at index 5. Daniel "
    },
    {
        "source": "#553: Still getting divison by zero error. Hi, I know other students also asked about this but I still struggle with it. I got \"(base) . .. .. .. hw4_files % perl score. pl smurf. predict gold. trialIllegal division by zero at score. pl line 109. \"even when trying with smurf. predict. The main method now is BTW I'm using Mac. I also tried import io but it still not workingThanks a lot!",
        "target": "hi did you do the piping step before creating smurf. predict, i think that might be the problem"
    },
    {
        "source": "#550: HW 4 Desired Results. For HW 4, is our goal to have the scores be as close to the expected values or do we want to have the scores for each part to be as high as possible on each individual task? I believe my scores have typically been around the estimates given in the HW4 documentation, but just want to understand whether we should prioritize increasing the score for each part or getting the \"right\" score",
        "target": "My advise is to not obsess about the score  -- use the provided scores as a reference to check if your code is working correctly in general. The evaluation for hw4 is relatively objective because we are using the \"official\" evaluation script provided for the lexical substitution task. So you can trust that better results are better :)"
    },
    {
        "source": "#545: Error in Part -5 evaluation. Hi, I ran the evaluation code on the part5. predict file. I got the following error.",
        "target": "Please take a look at the actual output file and see if anything looks off. If you are running this on windows, you may be encountering an issue with the line endings. #476 "
    },
    {
        "source": "#543: Hw4 - Part 6. If we create a conceptually different approach for Part 6, however, the performance is worse, should we include part 5 or Part 6 in the main part of our final submission? Thank you so much! !!",
        "target": "It doesn't really matter. Let's say you should run the part that gives you the best performance -- in your case part 5. "
    },
    {
        "source": "#542: HW 5: Part 5 output. Hi, This is my output when I run part 5. It prints some \"error in line\" from lines 1 - 600 before printing the scores. However, my precision seems to be slightly better than the word2vec approach as mentioned in the assignment. Can you please confirm if I should be worried about this \"error in line\" part or is it fine as long as my precision is reasonable. Thank you!",
        "target": "Same problem here. Did you solve it?"
    },
    {
        "source": "#542: HW 5: Part 5 output. Hi, This is my output when I run part 5. It prints some \"error in line\" from lines 1 - 600 before printing the scores. However, my precision seems to be slightly better than the word2vec approach as mentioned in the assignment. Can you please confirm if I should be worried about this \"error in line\" part or is it fine as long as my precision is reasonable. Thank you!",
        "target": "Hello! Looks like this can be prevented using what someone else mentioned above, but I'd also point out that it looks like these extra lines don't effect the actual effectiveness of the scoring script"
    },
    {
        "source": "#541: part 5 get candidates. Hi I hope you are great. For this part of part 5:Obtain a set of candidate synonyms (for example, by calling get_candidates). can i just copy over the logic, or do i have to actaully call the get_candidates function.  Thank you!",
        "target": "It's actually easier to copy the logic because you can then score the candidates on-line instead of having to store them all in a list. "
    },
    {
        "source": "#540: Error when installing gensim. Hello! I have been trying to \"pip install gensim\" into my Python environment, but I keep getting this error. Does anyone happen to have encountered this/resolved it? Thank you so much for any help!",
        "target": "Can you copy/paste the entire output? It looks cut-off to the right and top."
    },
    {
        "source": "#537: part5 example. Hi I hope you are great. I was wondering for the part 5 example, to go through the example, do I just create a random . py file and run the file? thank you!",
        "target": "Correct me if I'm misunderstanding your question, to do the introductory lines of code that introduce questions 5 you should be able to use your terminal (at least for mac). If you open a terminal and type python then hit enter, you should be able to type in the lines and get the expected returns. Anything with &gt;&gt;&gt; is something you type in, the next line is the return you should get in the terminal, if any. This would be the easier option, but yes you could also creat a . pay and run the file in your ide."
    },
    {
        "source": "#534: Part 3. Hi  I hope you are doing great.  My part 3 results are above.  I am not sure how to improve this.  Is there anything you can suggest?  Thank you!",
        "target": "I would double check that you are tokenizing the text groupings properly for the intersection check. I think it was left+right context vs definition+examples+hypernyms examples+hypernyms definitions"
    },
    {
        "source": "#533: HW4 Score. Does my following scores for part 3&amp;4&amp;5 looks reasonable? My Bert score is much lower than the word2vec model. So I am not sure is something that I did wrong. Thank you! perl score. pl wn_simple_lesk. predict gold. trialTotal = 298, attempted = 298 precision = 0. 039, recall = 0. 039 Total with mode 206 attempted 206 precision = 0. 068, recall = 0. 068perl score. pl word2vec. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170perl score. pl bert. predict gold. trialTotal = 298, attempted = 298precision = 0. 098, recall = 0. 098Total with mode 206 attempted 206precision = 0. 126, recall = 0. 126Then as a result, I tried to use Bert Large for my part 6, which also have a lower score. Is this fine? Do we need to actually have a model 6 that has the highest score over all models? perl score. pl bertlarge. predict gold. trialTotal = 298, attempted = 298precision = 0. 099, recall = 0. 099Total with mode 206 attempted 206precision = 0. 146, recall = 0. 146",
        "target": "My word2vec scores were better than my bert scores too. So for part 6 I decided to work off the word2vec method and make some small improvements to push the score up. I would imagine they expect your part 6 to be able to at least marginally beat your part 3, 4, and 5 scores since you have the flexibility to mix and match the techniques."
    },
    {
        "source": "#533: HW4 Score. Does my following scores for part 3&amp;4&amp;5 looks reasonable? My Bert score is much lower than the word2vec model. So I am not sure is something that I did wrong. Thank you! perl score. pl wn_simple_lesk. predict gold. trialTotal = 298, attempted = 298 precision = 0. 039, recall = 0. 039 Total with mode 206 attempted 206 precision = 0. 068, recall = 0. 068perl score. pl word2vec. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170perl score. pl bert. predict gold. trialTotal = 298, attempted = 298precision = 0. 098, recall = 0. 098Total with mode 206 attempted 206precision = 0. 126, recall = 0. 126Then as a result, I tried to use Bert Large for my part 6, which also have a lower score. Is this fine? Do we need to actually have a model 6 that has the highest score over all models? perl score. pl bertlarge. predict gold. trialTotal = 298, attempted = 298precision = 0. 099, recall = 0. 099Total with mode 206 attempted 206precision = 0. 146, recall = 0. 146",
        "target": "Even my Word2Vec scores were better than the BERT scores. Infact, the Word2Vec scores were higher than the scores produced by any of the other three techniques. "
    },
    {
        "source": "#533: HW4 Score. Does my following scores for part 3&amp;4&amp;5 looks reasonable? My Bert score is much lower than the word2vec model. So I am not sure is something that I did wrong. Thank you! perl score. pl wn_simple_lesk. predict gold. trialTotal = 298, attempted = 298 precision = 0. 039, recall = 0. 039 Total with mode 206 attempted 206 precision = 0. 068, recall = 0. 068perl score. pl word2vec. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170perl score. pl bert. predict gold. trialTotal = 298, attempted = 298precision = 0. 098, recall = 0. 098Total with mode 206 attempted 206precision = 0. 126, recall = 0. 126Then as a result, I tried to use Bert Large for my part 6, which also have a lower score. Is this fine? Do we need to actually have a model 6 that has the highest score over all models? perl score. pl bertlarge. predict gold. trialTotal = 298, attempted = 298precision = 0. 099, recall = 0. 099Total with mode 206 attempted 206precision = 0. 146, recall = 0. 146",
        "target": "Just adding on to the others aboveI had the same word2vec performance, and Bert exactly tied that. The simple lesk performance you have looks a bit low though, I'd make sure you try out Dr. Bauer's recommendation about breaking ties in #477 "
    },
    {
        "source": "#532: potential file issue. Hi,  I hope you are doing great. I accidentally exported and converted my main file to a txt.  I brought it back to the correct folder and changed the extension back to . py. It seems to be functioing normally, but when I look at the file in the hw4 folder it no longer shows, the . py extension, but it shows the exstension in my ide.  Is this okay?  Thank you!",
        "target": "The file you submit on Courseworks should have a . py extension. "
    },
    {
        "source": "#530: Problem 3 Question. Hi, when we are returning values in Q3 predictions, should they be strings or synsets? for example should I return the string \"streak\" or \"Synset('streak. n. 01')\"? Here is an example of it returning the string \"go\" and the synset \"streak. n. 01\".",
        "target": "I believe the return should just be a string, e. g. \"streak\". so when you check your . predict file it will look like:run. v 300 :: streak"
    },
    {
        "source": "#529: Tensorflow not Working. Hello! I hope you are doing well. I'm currently trying to work on the assignment, but I'm having issues with my tensorflow. I'm getting this reccuring message, and I'm not sure what commands I have to run to get my tensorflow working on my Mac. Any feedback would be greatly appreciated!",
        "target": "It looks like you are using the tensorflow-metal accelerated version. For this assignment you should be good with the CPU version. $ pip uninstall tensorflow-metal\n$ pip install tensorflow\n"
    },
    {
        "source": "#528: Hw4 part4 and part5 score. Hello Professor, Is it possible for Part 4 and Part 5 to have the same score, even if their predictions are not the same? Thank you for your help.",
        "target": "I got the same result, see #520 "
    },
    {
        "source": "#527: Question about final exam. Dear Prof. and TAs, Im wondering whether we can get sample paper of previous final exam or not? And Im curious about the question patterns, will it be the same as the midterm exam? Thanks!",
        "target": "Unfortunately I don't have a large enough trove of exam problems to be able to release a sample exam. I will try to share some individual example problems (as ungraded exercises)."
    },
    {
        "source": "#525: Can I submit my . predict files as output by my Windows pc? . I have completed my assignment 4 on Windows pc and my perl score. pl script outputs precision results that I am satisfied with. However, when I try to run the same script on my mac book the script is not working. I am aware that this difference is due to Unix file endings as explained in one of the previous posts. What machine will be grading our . predict files? Can I submit my . predict files as output by my Windows pc?",
        "target": "Yes that should be okay -- worst case we can convert the file endings. "
    },
    {
        "source": "#524: Results too high? . Hello, I was wondering if for part 4 these results were too high. Thanks! perl score. pl part4. predict gold. trialTotal = 298, attempted = 298precision = 0. 136, recall = 0. 136Total with mode 206 attempted 206precision = 0. 218, recall = 0. 218",
        "target": "Hu, the results with mode are really quite good. Unless you somehow modified the evaluation script, it's difficult to \"cheat\" on this problem, so I would trust the good results. "
    },
    {
        "source": "#522: Hw4 part2 and part3 score. Hello Professor and TA, It this score good for part 2 and part 3? ",
        "target": "These look reasonable. "
    },
    {
        "source": "#521: score. pl and files. In regards to #476, I implemented the solution suggested here in the main function and I am still met with the error of illegal division by 0 on line 109. I previously tried to include the io. open line within the functions and was met with the same error. Here is a snippet of my code. Am I doing this correctly? Thanks!",
        "target": "Nevermind, I got it"
    },
    {
        "source": "#520: Hw4 Part 4 & 5 Score. Hi NLP teaching staffs - Are these score okay for Part 4 and part 5? My bert model does not perform better than word2vec unfortunately, and I am not quite sure how I can improve it. Best,",
        "target": "That's expected. My theory here is that both w2v and Bert are really good at selecting a candidate, but that unfortunately the candidate set obtained from wordnet is too limited. "
    },
    {
        "source": "#519: Hw4 Part 5: Bert predictor only run in virtual environment. Hi NLP teaching staffs - For hw4 part 5, I have to create a virtual python environment and reinstall all the dependencies to run Bert Predictor (otherwise I would run into some Keras package/ dependency issues in my local main machine). When I run the code in the virtual environment, it works. I want to make sure that this is okay and it is not something with my code that cause the issue. Thank you so much! ",
        "target": "Sure, you can assume the TAs environment when grading the assignment is set up properly. "
    },
    {
        "source": "#518: Part 3 Score. Hi! I see on courseworks that part 3 isn't expected to outperform part 2. However, based on what I've seen from other students' posts on Ed, I'm worried my output is too low. Do these scores seem acceptable? Thank you!  ",
        "target": "It's difficult to say based on the scores themselves. They don't look suspiciously low to me. "
    },
    {
        "source": "#517: Part 4 Key Error. Hi, I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a KeyError: \"Key 'moving picture' not present. \"it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? Thanks!",
        "target": "There may be a way to fix this, but when I chose to skip those words with a space in between, I got the following results:Total = 298, attempted = 298; precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206; precision = 0. 170, recall = 0. 170"
    },
    {
        "source": "#517: Part 4 Key Error. Hi, I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a KeyError: \"Key 'moving picture' not present. \"it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? Thanks!",
        "target": "Note that in part 1 instruction, \"WordNet will represent such lemmas as \"turn_around\", so you need to remove the . \". Hence, from my understanding, you should update part 1 to convert word with space in between into _ instead. After that, the error should be fixed."
    },
    {
        "source": "#517: Part 4 Key Error. Hi, I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a KeyError: \"Key 'moving picture' not present. \"it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? Thanks!",
        "target": "I think you can try: if word not in self. model. key_to_index: proceed to next wordto skip the word that is not in the pre-trained model."
    },
    {
        "source": "#517: Part 4 Key Error. Hi, I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a KeyError: \"Key 'moving picture' not present. \"it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? Thanks!",
        "target": "For skipping, depending on the version I think the syntax I found worked was if word not in self. model. FWIW for part 6 I tried some more creative approaches for handling compound words that aren't in the model, but didn't find anything that improved (or even matched) the performance when just skipping them."
    },
    {
        "source": "#515: HW4 Part3. Hi, For the simple less predictor, when using the shortcut in #477 to break ties, should we still expect there to be some ties between lexemes? When I implemented the shortcut, there were about 41/300 instances where there were still ties. For example, when the target word was 'cross' in context 51, the lexemes (Synset('crisscross. n. 01'), Lemma('crisscross. n. 01. crisscross')) and (Synset('crisscross. n. 01'), Lemma('crisscross. n. 01. mark')) were tied. I randomly picked crisscross. I think this tie is occurring because when I used count() to calculate their frequencies, I got 0, and since they are in the same synset, they had the same aggregate score. These are my results for part 3:Total = 298, attempted = 298; precision = 0. 109, recall = 0. 109Total with mode 206 attempted 206; precision = 0. 150, recall = 0. 150Also, I was surprised to find that when I always used the tie breaking shortcut on all lexemes, it slightly outperformed my original results:Total = 298, attempted = 298; precision = 0. 112, recall = 0. 112Total with mode 206 attempted 206; precision = 0. 155, recall = 0. 155",
        "target": "I didn't actually check how many remaining ties there are. But yes, I would expect there are some. "
    },
    {
        "source": "#514: Lecture 19 Text Summarization Slides. Hello, I don't see the slides for today's lecture (Text Summarization) on courseworks. When can we expect to see them published? Thank you!",
        "target": "Theyre up now in the Files section on courseworks. "
    },
    {
        "source": "#513: Lecture Slides 18 pdf. Would it be possible to upload the lecture 18 slides as a PDF to Courseworks?",
        "target": "Theyre up now (in the files section. Ill link them from modules later) "
    },
    {
        "source": "#512: Homework Grading. Hi, I am wondering if hw4 will be our last coding assignment. The syllabus says the course grade will be based on 5 programming assignments and the lowest grade will be dropped. ",
        "target": "In lecture, I believe the professor mentioned that there will be a fifth coding assignment, but that this will be due after the final. Also, since the lowest grade will be dropped, I think this assignment will be optional."
    },
    {
        "source": "#511: HW4 Part5. How long should it take to run part 5 of hw4? My laptop is taking upwards of 20 minutes and I wanted to know if that was normal or if there are some issues within my code.",
        "target": "That seems rather slow. It probably has to do with how you sort the BERT predictions. "
    },
    {
        "source": "#511: HW4 Part5. How long should it take to run part 5 of hw4? My laptop is taking upwards of 20 minutes and I wanted to know if that was normal or if there are some issues within my code.",
        "target": "I think you may load the word2vec = Word2VecSubst('GoogleNews-vectors-negative300. bin')\n in your loop function. I did not notice this for the first time I ran this. Move it outside and just call the predict_nearest in your loop"
    },
    {
        "source": "#510: HW4 Part 2 & 3 Score. Hi NLP teaching staffs - Is this score good for Part 2 and part 3. My lesk model (part3) slightly outperformed wn_frequency model (part2), and I want to make sure it is okay. Thank you so much!",
        "target": "yes, that looks good ."
    },
    {
        "source": "#509: Hw4: Update tokenize function? . Hi NLP teaching staffs - For HW4, can I update tokenize function and create helper method for my code? Tysm!",
        "target": "Yes. "
    },
    {
        "source": "#508: Hw4 Part2 Instruction Clarification. Hi NLP teaching staffs -Can I get some clarification on how to find \"the possible synonym with the highest total occurence frequency (according to WordNet)\". Does highest total occurence frequency mean occurence frequency of this sense of 'synonym' in the SemCor corpus? Can I simply get this frequency by using the . count() function? Tysm! !! !Best",
        "target": "Yes, you can simply get this using . count()The only thing to watch out for is that a synonym may occur in two different synsets."
    },
    {
        "source": "#507: Part3 Breaking ties. Hi, I'm struggling to break the tie in part 3. I believe I have the correct overlap score, but I think I'm making errors in determining the values for 'b' and 'c' in @477. My logic is as follows:1. Create tokens of the contexts2. Find synsets using context3. For each synset in the synsets:     3. a. Calculate overlap_score for the synset     3. b. Retrieve synset lemmas.     3. c. For each lemma in the synset:          3. a. i. if this lemma is the same as the target(b case) -&gt; add it to b variable          3. a. 2. all the other lemmas will fall under the c case. -&gt; Increase c with each count     3. d. Calculate the total score and select the highest. However, this approach yields a low output (approximately 0. 06). Could you let me know where I might be making mistakes? ",
        "target": "Hi, from my understanding of @477, it seems that your logic is correct, but just want to clarify some things in your logic. In steps 3c-3d, are you calculating an aggregate score for each lexeme? or each synset? And in step 3. a. 2, when you calculate 'c' for \"all other lemmas, \" are you calculating a separate 'c' for each lemma?    "
    },
    {
        "source": "#506: hw4 part6 improvement. For part 6, the precision and recall improve in the total = 298, but in mode 206, they decrease. Can this result be considered an improvement?",
        "target": "I'd consider it an improvement. But more importantly, we will not grade part 6 according to absolute improvement as long as you tried a different approach (one that is qualitatively different than the other parts). "
    },
    {
        "source": "#505: hw4 part2 scores question. Hi this is a general question about the varying scores that people are posting for this section. For part2, aren't the predictions deterministic, since we are just outputting the most frequently occurring possible substitute? So why are people getting different scores for this section? Or am I misunderstanding the prompt. .. Much thanks!",
        "target": "There are a few implementation decisions that may lead to different scores:  Are you implementing the scoring method proposed in #477 or did you implement tie breaking differently? How did you preprocess the definitions and example sentences? "
    },
    {
        "source": "#503: Question about lecture16 PPT. Dear Prof. and TAs, I'm wondering how this graph was derived, I can't find any definition about the dependency structure or replacement rules on the PPT, could you please explain how the graph was derived to me? Thanks! (starting from this:)",
        "target": "The next step here would be to use Rule 5 (the full grammar is a few slides past this one) to convert the S2 edge into a VP2 edge. Then replace the VP2 edge using rule 6. "
    },
    {
        "source": "#502: File 'GoogleNews-vectors-negative300. bin' or 'bin. gz'. Hi, When I downloaded the file, it was named 'GoogleNews-vectors-negative300. bin', not 'GoogleNews-vectors-negative300. bin. gz'. I attempted to convert it to the . gz format but encountered an error: gzip. BadGzipFile: Not a gzipped file (b'30'). Is it possible to use the 'GoogleNews-vectors-negative300. bin' file directly in lexsub_main. py?",
        "target": "Just imported it as the '. bin' file, it is a trained model."
    },
    {
        "source": "#502: File 'GoogleNews-vectors-negative300. bin' or 'bin. gz'. Hi, When I downloaded the file, it was named 'GoogleNews-vectors-negative300. bin', not 'GoogleNews-vectors-negative300. bin. gz'. I attempted to convert it to the . gz format but encountered an error: gzip. BadGzipFile: Not a gzipped file (b'30'). Is it possible to use the 'GoogleNews-vectors-negative300. bin' file directly in lexsub_main. py?",
        "target": "Yes, I think on some systems / browsers the file is automatically unzipped. Using the . bin file is fine. "
    },
    {
        "source": "#498: hw4 part 2 & 3. May I ask if the results in the picture are reasonable results for frequency and simple leak predictor?",
        "target": "These look reasonable to me!"
    },
    {
        "source": "#497: Error Using the Tokenize Method. Hi, when trying to use the tokenize method, I get an error saying it doesn't recognize the item \"string\". Did anyone else encounter this problem? Would it be wrong to simply tokenize by using . split(\" \")?",
        "target": "feel free to import string. import string\nYou could simply split by whitespaces, but that would not handle punctuation at all, such as commas and periods. "
    },
    {
        "source": "#495: hw4 part 5. Hi there, I am doing the part5 and I am not sure about how to handle the case when the candidate from WordNet is a multi-word like \"at long last\", in this case, how can we compare the candidate with the output tokens of the DistillBert model, as I think the prediction result from the DistillBert model is just a single token and doesn't include the multi-word case. Thanks a lot in advance!",
        "target": "In my implementation I skipped these candidates. "
    },
    {
        "source": "#494: hw4 part1 Question. For part 1, shall we return a list or a set?",
        "target": "Return a set, as shown in the example. Sorry for the misleading description. "
    },
    {
        "source": "#493: HW4 part 2. Hi, I am also confused what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". In particular I am confused what it means by \"if the word and the target appear together in multiple synsets\" Is it simply counting frequency of synonym occurrence in the list from part 1? Thanks!",
        "target": "Lets say the target word is bright and both bright and light appear in synset s1 and s2. To get the score for the candidate light you need to sum the count for &lt;light, s1&gt; and &lt;light, s2&gt;. "
    },
    {
        "source": "#492: HW4 part2 result question. Hi dear prof. and TAs, After finishing part2, I got this result, it seemed that it was lower than 10%, is it OK?",
        "target": "Hm that looks a little low from what I remember. Maybe take a look at the predict file and see if you notice anything suspicious. "
    },
    {
        "source": "#491: HW4 Part 6 question with using BERT-Large. Hi, For part 6, I tried to use a larger version of the BERT model to increase the prediction \"accuracy\" of the replacement for [MASK] label. The version I used is BERT-large, also included in the transformer package. After making this replacement, the predictions for the [MASK] label all becomes labels like [unused818]. Why would this happen?",
        "target": "Are you also using the corresponding tokenizer? Its important to use the same tokenizer that the model was pretrained on. "
    },
    {
        "source": "#490: HW4 Part 2 Scoring. Hi! I'm currently working on part 2 and had a clarification question: am I meant to be looking at the word_form or the lemma of the given context? It seems to make more sense to me to use the lemma, but I'm getting the following results that don't seem quite right:My results when using the word_form are even further off, so I'm a bit confused about where I'm going wrong. Thank you!",
        "target": "You should be using the lemma. Look at the output file and see if you notice anything suspicious. "
    },
    {
        "source": "#489: Exact same performance for Word2VecSubst and BertPredictor. When I evaluate both implementations, I get the following results:(tf) sameerkhanna@Sameers-MacBook-Pro hw4_files % perl score. pl word_vec. predict gold. trial Total = 298, attempted = 298 precision = 0. 115, recall = 0. 115 Total with mode 206 attempted 206 precision = 0. 170, recall = 0. 170 (tf) sameerkhanna@Sameers-MacBook-Pro hw4_files % perl score. pl bert. predict gold. trial Total = 298, attempted = 298 precision = 0. 115, recall = 0. 115 Total with mode 206 attempted 206 precision = 0. 170, recall = 0. 170I checked and confirmed the prediction files are indeed predicting different things. Is this result ok, or are we supposed to get better performance using BERT? ",
        "target": "Yes this is expected."
    },
    {
        "source": "#488: hw 4 pt 2 results. I want to confirm that my results for hw 4 part 2 wn_frequency test look reasonable. The assignment says they should both be around 10% and mine is 9% and 13%. ",
        "target": "I think this is fine. "
    },
    {
        "source": "#487: Illegal Instruction 4. Hi, I'm getting an error \"Illegal Instruction: 4\" when I try to run the line python lexsub_main. py lexsub_trial. xml  &gt; smurf. predict. Everything else in the instructions has worked up until this point, so not entirely sure what could be happening. Thanks so much! ",
        "target": "Can you post the entire error message you are getting? "
    },
    {
        "source": "#486: Hw4 part 1. The documentation mentions removing the _ in some of the words. By removing, does that mean delete the underscore (which combines the words) or replace the underscore with a space? Ex: CanisFamiliaris or Canis Familiaris",
        "target": "In the test data, multiword expressions show up with white spaces \" \". But note that this won't help you for the gensim and Bert approach (because multiword expressions are not in the lexicon for these models). "
    },
    {
        "source": "#485: HW 4 Part 6 Question. As mentioned earlier in #476, Windows laptops have an issue with the newlines. To circumvent this, I made it so that my python file directly opens and writes a file. For the final submission, is it fine to keep my code written in that manner so it automatically creates a part6. predict file when we run this linepython lexsub_main. py lexsub_trial. xmlor does it need to be runnable in the following format (as in the HW documentation)? python lexsub_main. py lexsub_trial. xml  &gt; part6. predict\nI would prefer to have it so that my python file is able to open and write the file itself instead of storing the printed lines in a file, because the newlines in Windows do not behave appropriately the second way.",
        "target": "Its fine to leave it the way you changed it, with the output being written directly to a file. "
    },
    {
        "source": "#483: problem with python. Hi I was running the python file for assignment 4 and I got an unexpected error. I always ran python file using pycharm but this time I got an pop up window that says python quited unexpected. I retried a few times but this window appears every time I try to run. I am using python 3. 9. 7 and mac. I am not very familiar with python, can anyone give me some suggestion about how to solve this? any advice helps, Thank you so much.",
        "target": "Not sure what's going on -- the issue seems to be the pycharm IDE. Maybe try a different IDE or editor? You could also try to install a separate Python environment, rather than the one that comes preinstalled with macOS. I recommend looking into Anaconda (or miniconda). "
    },
    {
        "source": "#482: HW4 Part 3 Acceptable Score. Hello, For part 3, what is an acceptable precision/recall score? My implementation produces the following result: I was wondering whether this is acceptable or not. Thanks!",
        "target": "Yes its expected that the scores for part 4 and 5 produce very similar results. The score for part 3 also looks fine to me, but maybe post publicly for comparison? "
    },
    {
        "source": "#480: biLM representations of different senses. Hi, I was reviewing the slides and I have a question about biLM embeddings. From my understanding, biLM generates different embeddings for the same word if it has different senses. How does biLM differentiate the two senses? I would guess that even if two words have the same sense but appear in slightly different contexts, their embeddings would be slightly different, but not enough to consider them to have different senses. Does biLM store embeddings for each of the instances of a word and then classify them into groups depending on their similarities? Thanks in advance!",
        "target": "The representations computed by a bidirectional language model do not differentiate between discrete word senses. Rather, each token representation will be unique based on the actual sentence context that the word appears in. Tokens that appear in similar contexts will have similar senses. The premise is that there are no discrete senses and that contextualized embeddings capture fine-grained sense distinctions more accurately. After all, the sense annotations in WordNet (and other resources, like VerbNet, PropBank and FrameNet) are coarse and somewhat artificial -- they are created by human annotators. If you wanted to use contextualized word representations to obtain representations for discrete senses (as annotated in WordNet), you could do the following. For each lexeme, obtain contextualized embeddings for all annotations of the lexeme in SemCor. Then average these embeddings (which should, after all, be reasonably similar anyway). "
    },
    {
        "source": "#479: Are we allowed to use cheat sheets for Final Exam? . Are we allowed to use cheat sheets for Final Exam?",
        "target": "\"One double-sided letter-sized \"cheat sheet\" will be allowed\" (on Courseworks)"
    },
    {
        "source": "#478: HW4 Part 4. When working on part 4, I noticed that some candidate synonyms (obtained from the get_candidates method) does not exist in the word2vec model. For example, for the word film. n, I got the following error:KeyError: \"Key 'moving picture' not present\"\nI'm wondering that when processing multi-word expressions in part 1, by removing '_', should we change (for example) the word turn_around to turn around or turnaround? If it's the first case, when computing the embedding of this phrase, should the embedding be the average of the embedding of each word? If it's the second case, this word will still be shown as not present. How should we process this case? Also, I noticed that words like of cannot be found in the KeyedVector. Should we consider using the stopword to remove words like this?",
        "target": "Check out this thread: #517"
    },
    {
        "source": "#476: HW 4 part 2 Issue with score. pl. I was able to get the score. pl to work with our smurf. predict, and it correctly gives 0 for everything. However, in part 2, we implemented a version that should be better. I checked my output part2. predict file and it did look like the words seemed to line up occasionally with the gold. trial. However, I still receive 0 for both the precision and recall (even though my part2. predict file looks more accurate) when I run the following command:perl score. pl part2. predict gold. trialThe only parts that I updated were the part1 and part2 code so far, and I changed the prediction to use part 2's wn_frequency_predictor method to generate the part2. predict file. Can anyone help me understand where I am going wrong and how to move forward?",
        "target": "I suspect this is an issue of file endings. The Perl script expects Unix file endings, but you may be running your code on windows, in which case Python produces windows file endings by default.  You a modify the code to write to a file directly rather than printing the predictions to console. That would allow you to control the formatting and encoding of the file. Try something like this:import iof = io. open('file. presidict, 'w', newline='\\n')"
    },
    {
        "source": "#476: HW 4 part 2 Issue with score. pl. I was able to get the score. pl to work with our smurf. predict, and it correctly gives 0 for everything. However, in part 2, we implemented a version that should be better. I checked my output part2. predict file and it did look like the words seemed to line up occasionally with the gold. trial. However, I still receive 0 for both the precision and recall (even though my part2. predict file looks more accurate) when I run the following command:perl score. pl part2. predict gold. trialThe only parts that I updated were the part1 and part2 code so far, and I changed the prediction to use part 2's wn_frequency_predictor method to generate the part2. predict file. Can anyone help me understand where I am going wrong and how to move forward?",
        "target": "Thank you so much professor! !!"
    },
    {
        "source": "#474: HW4 Part 3. Hi, I am a bit confused about the following statements in the description for part 3If this is the case (or if there is a tie), you should select the most frequent synset (i. e. the Synset with which the target word forms the most frequent lexeme, according to WordNet). Then select the most frequent lexeme from that synset as the result. How do we define the most frequent lexeme and the most frequent synset? As far as I understand, WordNet does not contain any information on the frequency of each synset. Do we need to rely on any external corpus to get this frequency? I would really appreciate some explanations on this, preferably with an example. Thanks!",
        "target": "Wordnet contains frequency information about individual lexemes (in the SemCor corpus). So if your candidate lemma is bright. an and you  find three lexemes for synset 1, 2, and 3, lets call them &lt;bright-s1&gt; &lt;bright-s2&gt; &lt;bright-s3&gt;, then the most frequent synset is the one for the most frequent of these lexemes. There is a shortcut/trick for doing the tie-braking. I will post in another thread. "
    },
    {
        "source": "#473: Ungraded Exercise AMR. Hi! Hope everyone is having a good break, just wondering when the solutions to the ungraded AMR exercise will become available :)",
        "target": "I will post them early this week. "
    },
    {
        "source": "#472: 'keras. engine' when running Bert. Hi, I try to run the Bert model by using &gt;&gt;&gt; import transformers \n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; model = transformers. TFDistilBertForMaskedLM. from_pretrained('distilbert-base-uncased')and encounter this error:File \"/Users/. .. /anaconda3/lib/python3. 11/site-packages/transformers/modeling_tf_utils. py\", line 70, in  from keras. engine import data_adapter ModuleNotFoundError: No module named 'keras. engine'RuntimeError: Failed to import transformers. models. distilbert. modeling_tf_distilbert because of the following error (look up to see its traceback):No module named 'keras. engine'How to resolve this error? I tried to update my tensorflow and transformers, and keras is also installed in my computer, but none of them worked. Thanks!",
        "target": "Sorry for the slow response. This looks like a version incompatibility between tensorflow and transformers. Make sure you have the latest version of each. pip install --upgrade keraspip install --upgrade transformersIs anyone else encountering the same issue? "
    },
    {
        "source": "#471: HW4 Part 6. For Part 6, should we be creating another function separate from the functions from the first 5 parts? For instance, if I want to refine the model in Part 4, could I just define another function inside the Word2VecSubst class? Also, do we only need to refine one of the 5 models? Thanks!",
        "target": "Yes this should be a new function (though it can duplicate code or call other functions. The refinement soils be to one of the 5 models or a combination of multiple models. We are more interested in new/creative ways of attempting this task rather than just getting the best performance. "
    },
    {
        "source": "#470: Combination of Lecture-based Conceptual Qs. ML for Dependency Parsing: Max of 3 untyped choices: $|R| \\times 2 + 1$ ? I didnt understand what R is and why is it multiplied by 2 and added by 1? What does it mean by Left arc transition focus on relating head to adjacent, while right arc focusing on relating the head? So is buffer typically considered head? GPT/BERT: does auto-regressor and auto-encoder classification suggest how the models are originally trained or the type of tasks they can do? PropBank: how to differentiate between Arg2 and Arg3? Seems quite similar to each other. Span-graph based SRL predictions: what is p, a, l in this case? can you give a more detailed explaination of the Unary scores, label score, and combined score and how they relate and interact with each other? For the PropBank SRL Results: the last paper on swapping out ELMo, its still based on RNN network or Span-graph? If span-graph is it the Word &amp; character representation layer (x) that is being replaced? Thanks in advance!",
        "target": "Sorry for the slow response. Let me address these one-by-oneR is the set of relation labels. Each label can appear with two transitions, arc_left and arc_right. The +1 is for the shift transition which does not require a label. Not sure I understand the question. With arc_left the head is the first word on the buffer and the dependent is the first word on the stack. With arc_right, it's the other way around. Yes, the terms auto-regressor and auto-encoder refer to the pre-training objective. Arg2 - Arg4 have to be interpreted differently for each verb sense). They do not generalize across different predicates. The definition for what the Arg2-Arg4 roles mean is part of the propbank annotation (in the \"frame file\" for that verb). p, a, l stand for predicate, argument, and label. We didn't go too much into detail for this specific model -- if you are interested in the detail I suggest looking at the He et al. 2018 paper. https://aclanthology. org/P18-2058/The last row is the span-graph based approach, but using pre-trained ELMo to compute contextualized word representations. And yes, this would replace the \"Word &amp; Character representation layer\" in the figure in lecture 15 slide 48. "
    },
    {
        "source": "#469: midterm distribution. Hi, I'm just wondering where can i find the class distribution of midterm. Thank you very much",
        "target": "Hi this should be available on Gradescope, but was also included in the Announcement I sent out when the grades were released. "
    },
    {
        "source": "#468: Hw4 Part 2 Clarification. Hi, I'm a bit unsure of what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". Would you be able to give an example of this?",
        "target": "I am also struggling to understand what is meant here."
    },
    {
        "source": "#468: Hw4 Part 2 Clarification. Hi, I'm a bit unsure of what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". Would you be able to give an example of this?",
        "target": "Similar to the get_candidates method, as you iterate through all the lemmas, store the counts for each word encountered to get the most frequent synonym."
    },
    {
        "source": "#467: HW 4 Wordnet Installation. Really insignificant but just wanted to make sure I didn't set up something incorrectly but when I run :l1 = wn. lemmas('break', pos='n')[0]\ns1 = l1. synset()\ns1. hypernyms()I should get: \"[Synset('happening. v. 01')]\"but instead, get:\"[Synset('happening. n. 01')]\"Everything else looks okay but just wanted to make sure ",
        "target": "\"[Synset('happening. n. 01')]\" seems good to me actually because your initial search was for a noun. "
    },
    {
        "source": "#466: Final exam material. Will the final be cumulative or will it be the part covered only after the midterm.  ",
        "target": "(not a TA) They mentioned the final will be cumulative"
    },
    {
        "source": "#464: Final Exam Schedule. Hi - I noticed that the final exam schedule date published on SSOL is different from the one noted on Canvas. I want to confirm which one is the correct schedule:Canvas/ syllabus: 12/11 (class time)SSOL: 12/18/23 Best,",
        "target": "The syllabus is correct -- exam 2 will be on the last day of class (12/11, 1:10pm). The registrar automatically schedules a final exam slot, but we are not using it. "
    },
    {
        "source": "#462: Hw4 Release date. Hi NLP teaching staffs -I am wondering when will HW4 be released so I can plan my Thanksgiving plan accordingly? Thank you so much!",
        "target": "(Not a TA) I think in lecture he said it would be released on Friday "
    },
    {
        "source": "#462: Hw4 Release date. Hi NLP teaching staffs -I am wondering when will HW4 be released so I can plan my Thanksgiving plan accordingly? Thank you so much!",
        "target": "I'm still working on it, apologies. But it should come out today or maybe tomorrow. "
    },
    {
        "source": "#461: Lecture 16: Semantic Parsing - Abstract Meaning Representation Slides Courseworks. Hi, I was wondering when the slides for Lecture 16 (the slide deck for the second half of today's lecture) would be uploaded to courseworks? I don't see them currently under lecture notes. Thank you very much!",
        "target": "Lecture 16 is now available on Courseworks. "
    },
    {
        "source": "#458: Exam question. Dear TAs and Professor, I noticed that my exam grade on canvas is different from that is on the grade scope. Will the grade on canvas be changed later? Thanks!",
        "target": "Yes I will synchronize these once all regrades have been processed. "
    },
    {
        "source": "#457: Prof. Bauer's OH Today. Hi, A few of us are on the zoom link sent via the announcement, wondering if we are in the right place or is there a change in OH? Thanks",
        "target": "I was running late, but I think I got to talk to you eventually (as well as everyone else who was waiting). Sorry for that. "
    },
    {
        "source": "#456: final crib sheet. Hi i hope you are doing great.  I think I remember from class discussion that on the final we are allowed to have a cheat/crib sheet.  If so, how long can the sheet be and are there any rules about  what can and can't be on it, if it has to written or typed etc.  Thank you!",
        "target": "#384"
    },
    {
        "source": "#455: nlp internships. Hi I'm sorry if this is an inappropriate  use of ED, but I am trying to get into the NLP field and I was wondering if anyone knew of any NLP internships (particularly with mitigating bias) that are specifically  for undergrads?",
        "target": "Hi there! A little late, but I believe looking into an Ontology/Knowledge Engineering internship at Amazon could be helpful. I was an intern at Alexa AI last year and while not directly related to NLP, my work was somewhat adjacent to it. I believe one of the other interns in our team was working directly with NLP, so if you show interest in the topic maybe an NLP project can be assigned to you. In any case, it could be a nice way to get some exposure to it. Hope this helps"
    },
    {
        "source": "#454: Couseworks grade not updated after regrade. Hello Teaching staff, Just letting you know that my courseworks grade is not yet updated after the regrade is reflected on gradescope.",
        "target": "Yes, these don't automatically synchronize. I will update them after the end of the regrade period. "
    },
    {
        "source": "#453: HW3 - Finally debugged! . Hi TAs and Prof. Bauer, This is Millie and I know I asked for a lot of help on HW3. The code seems all correct but was just not running properly. Turns out, there may be problems with my machines. I transported everything onto Google Colab and finally got very reasonable results. Training Model Loss: Done loading data. Epoch 1/5 18996/18996 [==============================] - 80s 4ms/step - loss: 0. 4917 Epoch 2/5 18996/18996 [==============================] - 79s 4ms/step - loss: 0. 3149 Epoch 3/5 18996/18996 [==============================] - 78s 4ms/step - loss: 0. 2796 Epoch 4/5 18996/18996 [==============================] - 78s 4ms/step - loss: 0. 2597 Epoch 5/5 18996/18996 [==============================] - 80s 4ms/step - loss: 0. 2462Evaluation: 5039 sentence. Micro Avg. Labeled Attachment Score: 0. 7465165107845009 Micro Avg. Unlabeled Attachment Score: 0. 7942181887591316 Macro Avg. Labeled Attachment Score: 0. 7558937632762938 Macro Avg. Unlabeled Attachment Score: 0. 8047567682996984So happy to finally get the program to work, but perhaps suggest issues with my machine I may need to look into for future HWs. Overall, thank you again for all your patience and help throughout the process! ! ",
        "target": "How odd! No other change? Id be very curious to learn what the issue was in the end. "
    },
    {
        "source": "#452: Grading Question. Hi, I hope you are doing great. I was wondering if I could ask if students could be given the option to weigh the final more heavily in the grade calculations.  I was wondering if I could make the final worth 50% of my grade and drop my midterm grade. I  understand if this is not possible.  I am not trying to make excuses or to get special treatment.  I am not yet sure what I misunderstood with the midterm material, but I think I can get a lot better on the final. Thank you for your consideration!",
        "target": "Unfortunately this is not really an option, unless there is a significant reason for a lower midterm performance (which would typically mean health-related -- and even then I have entertained options like this one only in extreme cases). The final exam is curved, so everyone is affected by the midterm exam in the same way. "
    },
    {
        "source": "#450: Midterm solution. Hi, Will midterm solutions be posted? Would love to learn how to improve for the final. Thanks",
        "target": "See #446"
    },
    {
        "source": "#448: mid question 1. Hi there, Just a quick question, if the answer to Question 1 mid-exam is \"v-&gt;d-&gt;adj-&gt;n\"? Thanks a lot!",
        "target": "Yes, the POS sequence is V-&gt;D-&gt;Adj-&gt;N."
    },
    {
        "source": "#447: Midterm. Incredibly embarrassed that I have to ask, but what is the likelihood of failing the class if I got a 38 on the midterm :( ",
        "target": "My experience is that people only fail this class if they stop submitting work. The homework problems alone are worth 40% of the grade. A 38% is not great, but I would think of it as a diagnostic tool to see how you can improve for exam 2. I will release a sample solution for the exam today. Maybe take a look and compare to your exam. If there are any remaining questions, we can go through your exam together in office hours and analyze which concepts you should revise and how to prepare better for exam 2. "
    },
    {
        "source": "#446: exam question. Hi I hope your are doing great.  Are the correct answers for the exam available somewhere?  I see the deductions, but I am trying to figure out where I went wrong. Thank you",
        "target": "Yes, I will release a sample solution. I just didnt get a chance to type it up. "
    },
    {
        "source": "#445: Exam issue. Hello, I think my exam for question 4 was correct, but the text was blurred. It only scanned part of my answer on it. Is there any way to deal with this issue?",
        "target": "Yea please submit a regrade request through Gradescope so we can process these more efficiently. Worst case, I can try to chase down the physical copy. "
    },
    {
        "source": "#443: Submission file name. Hi! I totally missed the part of the assignment instructions for HW 3 where we were supposed to name our file &lt;uni&gt;_homework3 -- is this big enough of an issue to resubmit or is there any way to resubmit without losing credit?",
        "target": "No, not worth resubmitting. We will figure it out. "
    },
    {
        "source": "#442: HW3 resubmission. Hello! I just realized I submitted my code with a line I meant to delete that was leftover from testing. It was a single line in the train_model. py file. Is there any way for me to resubmit without the 20 point late penalty? Sorry for the late post!",
        "target": "Yes, please go ahead and resubmit today. "
    },
    {
        "source": "#440: Submission. Should every py file, h5 file, and vocab file be in the same directory in the zip file? or should there be a data folder as well?",
        "target": "Not a TA. I just included all 6 . py, 1 h5 file, and 2 vocab file into one folder and zip it."
    },
    {
        "source": "#438: HW3 Part 4: my evaluate. py doesn't recognize dependency relation. I get a score of . 76 and . 77 for my micro and macro unlabeled attachment scores, however, my labeled attachment score is 0. Actually, I get a division by zero error without adjusting the main. py in evaluate. py. This appears to be because the program doesn't recognize any of my dependency relations in each edge. What should the argument for left and right arc look like exactly? I'm currently keying into the output label dictionary, but this doesn't seem to be working.",
        "target": "Nevermind, found the answer, it turns out that if you just print the target_labeled list you can check to see the correct format for the labeled dependency relations."
    },
    {
        "source": "#435: Runtime for part 4 is 1 hour but no infinite loop, should I submit ? . I let my part 4 run around 45 minutes and it's not finished so I cancelled it. When I try running it on smaller data set that I made, it produces 0. 5 -&gt; 0. 6 accuracy (around 100 sentences). Took it to an OH, and the TA was kind to walked through my code but we still coulnd't figure out what's wrong. (My loss value for part 3 is around 0. 3 after 5th epoch). I also ran out of GPU so testing is gonna be slower, should I just submit ? Appreciate any advice.",
        "target": "Sounds like its working  there are some reason why part 4 may be slow. Check that you have eager execution turned off. Another possible issue might be that the sorting of transitions is slow. Make sure you use np. argsort. "
    },
    {
        "source": "#434: Google Colab GPU Limit. Has anyone got this message in google colab and know how to get around it?  ",
        "target": "maybe try another Google account where you still have credits?"
    },
    {
        "source": "#433: HW3 Part 4 empty stack clarification. When checking the legality for arc-right, the condition requires that the stack is not empty, does this mean that the stack contains no root node as well or would it be empty (not including root) i. e. its length equals 1? Thank you!",
        "target": "right_arc is permitted if the stack contains root. It is not permitted if the stack is empty. Left_arc however is not permitted if the top of the stack is root. "
    },
    {
        "source": "#432: Bizarre Python Error for HW3 Part 4. I keep getting this error while running the decoder and then Python crashes:UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown\n\nwarnings. warn('resource_tracker: There appear to be %d 'For reference, I am on an M1 Mac but am using CPU TensorFlow. Any ideas what could be causing this? I've checked my Part 2 and Part 3 and they look mostly right. Not sure what's going on here.",
        "target": "This is just a warning you get because Python didnt shut down properly. Its not the reason for why it crashes. Are there any other error messages?  This happens in part 4 only? Its odd because if it was a tensorflow issue it should have also shown up in part 3. "
    },
    {
        "source": "#431: Dhaarna OH. Does anyone know if Dhaarna OH is supposed to be in the CS TA room today? I've been here for 10 minutes but the only person here is the whiteboard with NLP written on it",
        "target": "I think there was an announcement on canvas they got moved to tmrw :("
    },
    {
        "source": "#430: TensorFlow GPU on Linux (with just conda + NVIDIA drivers). Hi all, If you're working on a Linux machine and don't want to install specific CUDA and cuDNN libraries system-wide, you might find figuring out your TensorFlow installation a bit tricky like I did. I am sharing the installation method that I have found most desirable for my needs (on Linux with just conda + NVIDIA drivers and no cuda installation system-wide) in hopes of it being useful. You most likely have NVIDIA drivers installed, you can just verify it via nvidia-smiIf the above command does not display a nice output of your GPU processes, then you should figure out installing NVIDIA drivers first. Setup a new conda environment with Python 3. 10 (due to TF compatibility, we will not use newest Python versions):conda create -n tf python=3. 10Next, review the TF compatibility website as well as the conda-forge packages for cudatoolkit and cudnn to find the newest version of TF that you can install via conda alone. As of today, this is 2. 11, which came out about a year ago. The tradeoff with this installation method is that you won't be able to use the shiniest TF release but you will have the ease of installation that comes with conda. https://www. tensorflow. org/install/source#gpuhttps://anaconda. org/conda-forge/cudatoolkit/files? version=https://anaconda. org/conda-forge/cudnn/files? version=Install the cudatoolkit and cudnn packages first:conda install -c conda-forge cudatoolkit=11. 2. 2 cudnn=8. 1. 0Configure conda activate scripts to set your $LD_LIBRARY_PATH (which is where TF looks for your cuda/cudnn libraries) whenever this environment is activated: mkdir -p $CONDA_PREFIX/etc/conda/activate. d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate. d/env_vars. shNow you need reload the conda environment for the above changes to go in effect. Easiest way is to close your terminal and reopening&amp;reactivating the environment. Next, install the latest TF version that you've identified that can be installed via this method (2. 11) as of today:pip install tensorflow==2. 11This is it! You can verify by running:python -c \"import tensorflow as tf; print('GPUs Available: ', tf. config. list_physical_devices('GPU'))\"which should print a ton of TF warnings/messages but also:GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nwhich confirms the installation is successful! Hope it's useful!",
        "target": "Nice, thanks for the detailed instructions. "
    },
    {
        "source": "#429: HW3 Colab. Hi Everyone! I'm having a problem running my code for part 3 on Colab. When I run it on my computer (Mac) it works fine, but when I try to run it on Colab, I get this error:ValueError: cannot reshape array of size 33816560 into shape (1899519, 91)From this line:outputs = np. load(sys. argv[2])Does anyone know if there are extra steps you need to take to run on Colab? Thanks!",
        "target": "Did you rerun the feature extractor on colab or upload the . npy file from your local machine? It could be an encoding issue. "
    },
    {
        "source": "#428: HW 3 submitting with tf. compat. v1. disable_eager_execution(). If we used it for part 4, should we include tf. compat. v1. disable_eager_execution() in our submission? Or should that be left out in the submission and only used when we are testing our own code?",
        "target": "To my knowledge, it shouldn't matter that you have tf. compat. v1. disable_eager_execution() in your code because you only need to submit the . py files, . vocab files, and the . h5 model file. Unless this line is in one of these files (which it probably wouldn't be since you would be using an external file such as a . ipynb notebook file ffor testing), this shouldn't matter or cause any issues."
    },
    {
        "source": "#428: HW 3 submitting with tf. compat. v1. disable_eager_execution(). If we used it for part 4, should we include tf. compat. v1. disable_eager_execution() in our submission? Or should that be left out in the submission and only used when we are testing our own code?",
        "target": "You can leave that line in your code when you submit. "
    },
    {
        "source": "#426: Part 4 : Infinite Loop. Hello, has anyone else run into a problem where they get stuck in an infinite loop of shifting and right-arcing? I am not sure why this is, I'm thinking perhaps my checks are incorrect but I'm not sure where to start with debugging.",
        "target": "(Not a TA) I have run into a similar issue. I recommend you check your right-arc implementation to see if the correct word is removed from the buffer."
    },
    {
        "source": "#426: Part 4 : Infinite Loop. Hello, has anyone else run into a problem where they get stuck in an infinite loop of shifting and right-arcing? I am not sure why this is, I'm thinking perhaps my checks are incorrect but I'm not sure where to start with debugging.",
        "target": "Isolate the sentence for which this happens, then run the decoder on this sentence only and trace the steps. "
    },
    {
        "source": "#426: Part 4 : Infinite Loop. Hello, has anyone else run into a problem where they get stuck in an infinite loop of shifting and right-arcing? I am not sure why this is, I'm thinking perhaps my checks are incorrect but I'm not sure where to start with debugging.",
        "target": "I would also try to check that you properly mapped the actions to their probabilities before sorting."
    },
    {
        "source": "#425: Worse scores after fixing bug? . Hi, I believe i found an issue in my code for get_input_representation() method. My old program had the representation as [ stack[-3], stack[-2], stack[-1] buffer[-3], buffer[-2], buffer[-1] ]. so I fixed it to be:  [ stack[-1], stack[-2], stack[-3], buffer[-1], buffer[-2], buffer[-3] ]However, even with the incorrect representation, my old program was giving me loss values of 0. 5 (epoch 1) down to 0. 28 (epoch 5). and attachment scores between 0. 75-0. 8My fixed program is giving me loss values of 0. 55 (epoch 1) down to 0. 38 (epoch 5) and attachment scores of around 0. 67-0. 74. So it seems like my model got worse even though I fixed the input representation. Can someone explain how this is possible? ",
        "target": "The specific representation shouldnt matter as long as its consistent between training and deciding. Theres some randomness in the training process which might explain this difference. "
    },
    {
        "source": "#423: HW3 Part 3 Loss Value of 0. Hi! I know there have been several posts about loss values, but I haven't seen anyone else mention loss values of 0. Whenever I run my code, the loss value only shows as 0. All my code from part 2 is running as predicted, and I don't see any obvious errors in the build_model function. Does anyone have any idea what might be causing this behavior? Thanks!",
        "target": "Hey Anonymous, If you are getting loss values of 0 it is most likely because your input feature vector is not in the correct format and doesn't match up at all with how you are training it. I would recommend looking specifically at the example the professor gives you in the spec with \"dog eats a\" and \"the\" and &lt;ROOT&gt; are on the stack. Pay attention to the order of how he mentions it and then also how it is put in root and whatnot. IE: it is stack for the first 3 and then buff for the last 3. Also, make sure it is numpy of length (6, )Your Favorite Nguyen, Nguyen Quoc Tran &lt;3"
    },
    {
        "source": "#420: necessity of cuDNN, cuFFT, cuBLAS, and NUMA. Hi, My current build for tensorflow on WSL for windows 11 successfully runs on Nvidia GPU, but fails to support cuDNN, cuFFT, cuBLAS, and NUMA (see image below), despite numerous attempts to fix these issues. In this homework my runtime is acceptable (slightly over 10 mins on part 4 without eager execution) and the score is greater than 0. 7, but I wonder if in future assignments these libraries would be necessary to ensure performance. Thank you!",
        "target": "No I dont think you will need those. "
    },
    {
        "source": "#419: Part4 tf. compat. v1. disable_eager_execution(). Hi, Once I add tf. compat. v1. disable_eager_execution(), the runtime for part 4 is reduced to less than a minute when using just the CPU. In contrast, the runtime is approximately 2 hours without tf. compat. v1. disable_eager_execution(). Does this speed improvement seem plausible? The accuracy appears fine, but I'm concerned about whether such a significant runtime reduction does not negatively affect the implementation.",
        "target": "Yes that seems reasonable. :) Essentially with eager execution, rather than using the compiled computation graph, the model is evaluated layer by layer. This is good for debugging and implementing certain control flow mechanism, but it slows things down considerably. "
    },
    {
        "source": "#418: Cannot use Tensorflow on Apple M1 Ventura. Hi, I'm using Anaconda's virtual environment, and on that environment, I installed Tensorflow. The installation of Tensorflow was successful, but I could not use it and got the same error \"illegal hardware instruction\" in post #343. I tried both Python 3. 9 and 3. 11, and neither works. I wonder how I can solve this issue.",
        "target": "Hmm thats different from the increasing loss issue other people have encountered. Which version of tensorflow is this? You could always switch to colab. "
    },
    {
        "source": "#417: HW3 Loss. Question to understand things better: I ran my model locally (M2 Max Ventura) and it gave me a loss that was increasing in value and way greater than 1. When I ran the same code on Google Colab, the model trained as expected and gave me a more reasonable loss that got less than 0. 2956 after the 3rd epoch. I am curious why the training loss varies based on what device we train the model on even though the training data and model structure stays the same. Is it that tensorflow computes calculations differently across different device packages? ",
        "target": "To be honest I dont quite understand whats going on there. Its definitely an issue in the M1/M2 version of tensorflow. I initially thought only the GPU (tensorflow-Metal) version was affected, but it seems to actually affect the CPU version as well. I think the most recent version of the Adam optimizer is at fault. There is a way to use a legacy version  maybe something to look up? "
    },
    {
        "source": "#416: HW3 loss. I was wondering if I should be concerned with my model loss (around 0. 39 after epoch 5) as it's a bit higher than what other people have got. Despite this, I get a Labeled Attachment Score of around 0. 7 and an Unlabeled Attachment Score of around 0. 75. Should I still look at my model more closely? Thanks!",
        "target": "That seems okay. "
    },
    {
        "source": "#415: oh today tony yu. hi wondering if the OH will be in the CS TA room or online? Thanks! ",
        "target": "It will be in the CS TA room."
    },
    {
        "source": "#414: Part 4 Logic. Hello, I have two questions regarding part 4:1) When looking for legal transitions, are we to check if state. stack is empty, or if the stack that is presented in the 1x6 input representation vector is empty (or are both of these stacks the same structure)? 2) How are we to update the current state after we find a legal transition? My current implementation  uses conditional statements to apply a transition on the state given a legal action, but my output is returning None.",
        "target": "1) The 1x6 input representation vector should include the top three elements of the stack (or stack[-1], stack[-2], stack[-3]) in the first three indices. Thus, if the first three elements of the vector are all equal to the index for &lt;NULL&gt;, the state. stack would be empty, but I think it's more straightforward to just check if state. stack is empty. 2) The State class comes with functions that allow you to perform shift, left_arc, and right_arc, so calling those functions with the dependency label should update your state."
    },
    {
        "source": "#412: hw3 pt3 & pt4 output. Hi, I get the following outputs in pt3 and pt4, not sure if they are okay or should I update my code to improve the result?",
        "target": "This looks great! Better than some other results I have seen. "
    },
    {
        "source": "#411: HW3 Part 4 Avg Runtime. Hi! I was wondering what a good time/step would be for part 4 to make sure it won't take forever to run. Thanks!",
        "target": "(Not a TA)You could use the trick in #361. And it would take several minutes. "
    },
    {
        "source": "#411: HW3 Part 4 Avg Runtime. Hi! I was wondering what a good time/step would be for part 4 to make sure it won't take forever to run. Thanks!",
        "target": "I'd say 5-10 minutes if you turn off eager-executions "
    },
    {
        "source": "#409: How layers and density are chosen (HW3 Part 3). This is more of a conceptual question, but how were the densities and number of layers chosen here? For example, why 100 and then 10? And why only 2 hidden layers? ",
        "target": "I am also curious about the conceptual basis of this model as well, can you please explain how the \"Dense\" layers work? Based on the documentation, it appears that they apply a weight and an activation function to the previous layer's output, but I am not certain my understanding is correct."
    },
    {
        "source": "#409: How layers and density are chosen (HW3 Part 3). This is more of a conceptual question, but how were the densities and number of layers chosen here? For example, why 100 and then 10? And why only 2 hidden layers? ",
        "target": "My understanding is that the number of hidden layers is up to personal choice and what you think is best for the problem you are trying to solve; basically it is arbitrary, three hidden layers could have probably done the job or even one, but finding a good balance between accuracy and training time is the main goal, and whenever you add hidden layers the training time goes up, especially for larger datasets like the one we are working with. A \"Dense\" layer is basically a normal neural network layer, similar to what we have seen in lecture. The values for these layers (such as 100 or 10) correspond to the number of nodes/input size of the layer, which depends on the value of the output of the previous layer/the general input. For \"Dense\" layers, you tend to have a lot more freedom on these values, as they usually can be any value, but remember, the more nodes you have, the more the training time increases, albeit while also most likely increasing accuracy."
    },
    {
        "source": "#408: Part3. When I'm trying to train the model, I get the Value Error above for the data shapes. In my part 2, I return the np arrays of shape (6, ) and (91, ) which I think is right. These get put into the npy files by the code provided. Do you know why these shapes could be incorrect and what I can do to fix this? ? ",
        "target": "I see this was answered with #368, disregard! https://edstem. org/us/courses/46417/discussion/3797602 "
    },
    {
        "source": "#407: HW3 Part 3 Loss Values. Hi everybody, I know there have been a couple of posts about the loss values already but I think all of them have been about getting extremely large values or showing values as low as . 2. My loss values just steadily stay around . 43 which is neither extremely high nor as low as many people are getting. Does anyone have any ideas on how to get it lower or are these values also ok? (For reference I'm running on CPU on my local Windows machine). Thanks!",
        "target": "That seems okay to me. I would try to see what happens when you continue with this model in part 4. "
    },
    {
        "source": "#407: HW3 Part 3 Loss Values. Hi everybody, I know there have been a couple of posts about the loss values already but I think all of them have been about getting extremely large values or showing values as low as . 2. My loss values just steadily stay around . 43 which is neither extremely high nor as low as many people are getting. Does anyone have any ideas on how to get it lower or are these values also ok? (For reference I'm running on CPU on my local Windows machine). Thanks!",
        "target": "I was also getting similar pattern and loss values when running locally on CPU. (I'm on a Macbook whose OS is quite old). I was able to get ~0. 25 loss value by the end of 5 epoch when running the same code on Google Colab, so I was suspecting that there may be potential issues with the Keras or tensorflow installed on my local computer. "
    },
    {
        "source": "#405: Part 2 output. Hello, I just wanted to ensure that my part 2 code is working as expected. After completing get_input_representation, my vector of length 6 is outputted as:[4. 4. 4. 2. 4. 4. ]Indicating that the stack is empty, and the top (only) item in the buffer is an unknown token. My intuition makes me believe that the top item in the buffer should be \"root\", not an unknown token, can you please correct my intuition if it its wrong?",
        "target": "Yeah. If there is only one token left, that one token should be root. "
    },
    {
        "source": "#404: HW3 part 0. Hello, Every time I run the command for part 0, I get a different words. vocab file. I have not been able to get it to match the spec (the first few lines of mine look like this): &lt;CD&gt;    0\n&lt;NNP&gt;    1\n&lt;UNK&gt;    2\n&lt;ROOT&gt;    3\n&lt;NULL&gt;    4\nsilly    5\nvaluing    6\nembezzling    7\nIs this intended behavior?",
        "target": "Hey Anonymous, That is the intended behavior. If you look at the spec it actually directly says \"(Your indices may differ from this example because the get_vocab script outputs a different index mapping each time it is run. )\". You can also look into how it is getting made and you will see that it will be a little diff each time. Because of the iteration. Happy to help, Nguyen Quoc Tran &lt;3"
    },
    {
        "source": "#403: HW3 Zip File. For the files we submit, should there/can there be a subfolder \"data\" containing the model and vocab files?",
        "target": "Yes that would be a good idea. "
    },
    {
        "source": "#402: Low Score for HW3 Part 4. Hi, I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0. 28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this? Thank you!",
        "target": "Hi! I had a similar issue and resolved it by recloning hw3 and running everything from scratch and then running the evaluate/decode commands. I'm not sure if I accidentally corrupted a data file or something or if this is the same issue as what I had, but hopefully this helps!"
    },
    {
        "source": "#402: Low Score for HW3 Part 4. Hi, I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0. 28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this? Thank you!",
        "target": "If your loss values look normal and only the attachment scores look abnormally low, this might be an indication that something is off with your one-hot encoding. The mapping of indices to output actions in self. output_labels (inside of decoder. py) performs the one-hot encoding in a very particular way, so I would make sure that it's consistent with how you performed one-hot encoding in get_output_representation() of extract_training_data. py."
    },
    {
        "source": "#402: Low Score for HW3 Part 4. Hi, I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0. 28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this? Thank you!",
        "target": "Its also possible that there is an issue with the decoder. Even though your transitions are predicted correctly, the way you select the best transition from the predicted probabilities may not be (make sure you are sorting in reverse order). "
    },
    {
        "source": "#402: Low Score for HW3 Part 4. Hi, I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0. 28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this? Thank you!",
        "target": "I also had that extremely low score. I ran my decoder evaluation function on colab and the numbers were fixed!"
    },
    {
        "source": "#401: Running HW 3 Part 3 on Colab. Hi, I'm trying to test my part 3 but I can't figure out how to run the given command on Colab since input_train. npy, target_train. npy, and model. h5 don't exist in data. ! python '/content/drive/MyDrive/hw3_files/train_model. py' '/content/drive/MyDrive/hw3_files/data/input_train. npy' '/content/drive/MyDrive/hw3_files/data/target_train. npy' '/content/drive/MyDrive/hw3_files/data/model. h5' is the code I'm using but I get this error and I'm not sure how to fix it. ",
        "target": "input_train. npy and target_train. npy should exist from p2 -- I ran part 2 locally and then uploaded those files into my folder on Drive. It's okay that model. h5 doesn't exist yet in the Drive when running p3 since it will be created after training."
    },
    {
        "source": "#401: Running HW 3 Part 3 on Colab. Hi, I'm trying to test my part 3 but I can't figure out how to run the given command on Colab since input_train. npy, target_train. npy, and model. h5 don't exist in data. ! python '/content/drive/MyDrive/hw3_files/train_model. py' '/content/drive/MyDrive/hw3_files/data/input_train. npy' '/content/drive/MyDrive/hw3_files/data/target_train. npy' '/content/drive/MyDrive/hw3_files/data/model. h5' is the code I'm using but I get this error and I'm not sure how to fix it. ",
        "target": "You can use import os\nprint(os. getcwd())\n! ls . /drive/MyDrive/. ..\nto check the path."
    },
    {
        "source": "#399: Giant loss value on Colab. Hello, I am using Colab w GPU so I assume it's not a hardware issues. My model is just like what's laid out in the description, and I even changed it according to other posts on Edstem, so can it be because of part2 ? (I am not sure how to test this). I don't know why the loss value is 9 digits and gets worse after each iteration. Any pointers on this would be so appreciated. Thanks all.",
        "target": "Yes I suspect there is something wrong with your training data, unfortunately. I would recommend trying it out on a single training sentence so that you can verify that the resulting input and output representations are correct. Just copy the sec0. conll file and delete everything except for one sentence. "
    },
    {
        "source": "#395: HW3 Part 2 \"None\". How are we supposed to deal with \"None\" in the get_input_representation function? Should we assign it the POS &lt;NULL&gt; or is there another value that would make more sense here? Thanks!",
        "target": "None represents the ROOT POS. If you see the code, you will observe that it is added to the front of each sentence to serve as the Root."
    },
    {
        "source": "#394: hw3 final result inconsistency. This isn't about the inconsistency between mac and windows that other people have mentioned. But, when i run the final evaluate script, roughly half the time there's some error in the execution (tensorflow or threading probably) and it either fails or doesn't terminate. When it does work, I get acceptable results, but I'm concerned that I might be graded on a failing execution. Is that okay as long as it eventually works?",
        "target": "Whats the error message you get? The results you posted look good. "
    },
    {
        "source": "#393: Order of Precedence. Hi, Part 4 is timing out even when I disable eager execution. When attempting to improve my model, I was wondering if there should be an order of precedence between the different cases in get_input_representation. For instance, if a certain word is unknown and it has a special tag, which special case should it fall under? Thanks so much!",
        "target": "The special tag should take priority in that case. "
    },
    {
        "source": "#392: HW3 Part 3. In the build_model function, what is the pos_types argument used for?",
        "target": "It is not used at all. Refer to this Ed post - https://edstem. org/us/courses/46417/discussion/3712855"
    },
    {
        "source": "#391: file to submit. Hi, I just wanna check that we should submit 6 . py files2 . vocab files1 . h5 fileright?",
        "target": "Yes. "
    },
    {
        "source": "#389: HW3 Part4 Different result. Hello Professor and TA. I ran this part of my code and got two different results on Mac and Windows computers, I would like to ask if this result is acceptable? Thank you.",
        "target": "Hm people have reported similar observations. I dont really have any idea why. Did you train to the same loss on both machines? Either way, the results look good.  "
    },
    {
        "source": "#388: HW 3 - Part 4 - Scores. I just wanted to confirm, do these scores look reasonable?",
        "target": "The labeled scores seem a little low. Take a look at the output of decoder. py (the actual dependency trees) to see if there is any systematic issue you see with the labels. "
    },
    {
        "source": "#387: Extremely low LAS. Hi Professor and TAs, The accuracy of my decoder seems to be extremely low (~0. 03). I've checked my methods carefully but am struggling to figure out where I went wrong. Any hint/tip would be greatly appreciated! Thank you.",
        "target": "Check the actual output of the decoder (the dependency trees printed by decoder. py). You may be able to see some systematic issue. "
    },
    {
        "source": "#386: Part4 prediction method. Hi, For part 4, when I make a prediction, can I use predict_on_batch instead of predict?",
        "target": "You will have to make predictions for one input at a time, unfortunately. "
    },
    {
        "source": "#384: last day of class. Hi, I want to double-check that whether the last day of this course is Mon 12/11 for exam 2? Thanks.",
        "target": "That's correct. "
    },
    {
        "source": "#383: HW 3 Part 4 Output. Hello TAs and Professor, Is the following output acceptable? Thank you, Shimon",
        "target": "Yes, that looks okay to me. "
    },
    {
        "source": "#382: Extremely Large Loss Values. Hi, My losses are EXTREMELY large. I think my build_model is fine because I compared it to a previous post (which you had told them that theirs looked fine). Ive pretty much boiled it down to how I am handling input/output representation. The vectors are both correctly formatted with the correct lengths respectively and everything compiles, but the loss results are clearly terrible. Wsa just wondering if my thought process of focusing on input/output was on the right track and if there are any insights/tips anyone has. Below is the summary of the losses (2020 Macbook Pro on Google Colab):Epoch 1/5 18996/18996 [==============================] - 146s 8ms/step - loss: 86050283520. 0000 Epoch 2/5 18996/18996 [==============================] - 143s 8ms/step - loss: 1137886625792. 0000 Epoch 3/5 18996/18996 [==============================] - 151s 8ms/step - loss: 4919651205120. 0000 Epoch 4/5 18996/18996 [==============================] - 147s 8ms/step - loss: 14147200745472. 0000 Epoch 5/5 18996/18996 [==============================] - 146s 8ms/step - loss: 32414466310144. 0000Thanks!",
        "target": "Hm, that almost looks to me like an issue with the model. The predicted vector will all have values between 0 and 1, and the target should be 1-hot. The reported loss is actually the average loss of the batches, not the sum of losses of the epoch. "
    },
    {
        "source": "#382: Extremely Large Loss Values. Hi, My losses are EXTREMELY large. I think my build_model is fine because I compared it to a previous post (which you had told them that theirs looked fine). Ive pretty much boiled it down to how I am handling input/output representation. The vectors are both correctly formatted with the correct lengths respectively and everything compiles, but the loss results are clearly terrible. Wsa just wondering if my thought process of focusing on input/output was on the right track and if there are any insights/tips anyone has. Below is the summary of the losses (2020 Macbook Pro on Google Colab):Epoch 1/5 18996/18996 [==============================] - 146s 8ms/step - loss: 86050283520. 0000 Epoch 2/5 18996/18996 [==============================] - 143s 8ms/step - loss: 1137886625792. 0000 Epoch 3/5 18996/18996 [==============================] - 151s 8ms/step - loss: 4919651205120. 0000 Epoch 4/5 18996/18996 [==============================] - 147s 8ms/step - loss: 14147200745472. 0000 Epoch 5/5 18996/18996 [==============================] - 146s 8ms/step - loss: 32414466310144. 0000Thanks!",
        "target": "I am facing the same issue, was anyone able to solve it?"
    },
    {
        "source": "#381: Tensorflow Installation Issue. Hi! I get the following error when trying to run $ python -m pip install tensorflow. I have deduced that it is an issue with Windows 11. Is there an easier workaround for this besides importing the code into Colab when I want to train the model?",
        "target": "Hi, this is due to window limiting its path to &lt;260 characters iirc. What you have to do is moving tensorflow folder to another directory with shorter path or enable long-path. Step: Open \"Run\" (The window app), you should be able to look it up w window search bar. Type \"%systemroot%\\syswow64\\regedit\". This should pops up sth called \"Registry Editor\", which resembles a big directory. You should follow this path into the subfolders: Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem. You can either paste the path or just manually click on them until you get to  \"FileSystem\". At this point, you should see \"LongPathEnabled\", click on it and set value from 0-&gt;1. If this works for you, lmk so I can put it on my academic highlight mixtapes for gradschool"
    },
    {
        "source": "#380: HW3 Part 4: difference in output with tf. compat. v1. disable_eager_execution(). Hello, I was curious as to why I have a different output when running with eager execution disabled. When it is disabled, there is no constant stream of outputted text, as opposed to when it is enabled. Is this to be expected? ",
        "target": "You mean the progress-bar information isn't showing up? "
    },
    {
        "source": "#379: HW3 Part 4 prediction input. I'm a little confused on the input being used in this part.  When it comes to parsing sentences and making predictions on the transitions, should we be extracting the input representation of a given state of the sentence using get_input_representation that we created in part 2?",
        "target": "That's right, you'll be using get_input_representation on the current state of the parser. "
    },
    {
        "source": "#378: HW 3 Part 4 Running Time. I'm using Colab for this assignment. For part 4, I already turned off \"eager execution\" for TensorFlow, but it has been running for 40 minutes and still has not finished. I wonder approximately how long it takes to run each of these two executions on Colab:1. python decoder. py data/model. h5 data/dev. conll2. python evaluate. py data/model. h5 data/dev. conllWill the evaluation step take a shorter amount of time? If it's running too long, does it indicate there is an error? Thanks in advance! \"",
        "target": "That seems a bit long. Make sure you are correctly updating the parser state after each prediction, otherwise you may end up in an infinite loop. You could try to print out the current state and see if it repeats. "
    },
    {
        "source": "#377: HW03 Part3 Loss Result. Hello Professor and TA. I am running the same code and getting different loss results on Windows and Mac computers. windows computer has a lower loss. what should I do in this case? Thank you for being so helpful and understand. ",
        "target": "It may based on the calculation ability about the chip? I guess."
    },
    {
        "source": "#377: HW03 Part3 Loss Result. Hello Professor and TA. I am running the same code and getting different loss results on Windows and Mac computers. windows computer has a lower loss. what should I do in this case? Thank you for being so helpful and understand. ",
        "target": "Assuming that the second screenshot is from mac, I have very similar results. Also very interested in knowing the answer. "
    },
    {
        "source": "#377: HW03 Part3 Loss Result. Hello Professor and TA. I am running the same code and getting different loss results on Windows and Mac computers. windows computer has a lower loss. what should I do in this case? Thank you for being so helpful and understand. ",
        "target": "Hu, I assume that is running on the CPU in both cases (I. e. not GPU) "
    },
    {
        "source": "#375: HW 3 Using Colab. Hi everyone, To run the program on Colab, do we have to create a notebook and then upload all the files into it? Thanks!",
        "target": "I don't know if I'm doing it the most efficient way, but I put all my files into a Google Drive folder and then am accessing them through my notebook."
    },
    {
        "source": "#373: Error: \"Cast string to float not supported\". Hi, When running:python train_model. py data/input_train. npy data/target_train. npy data/model. h5\nI get the error:Error: \"Cast string to float not supported\"\nThis is my code:def build_model(word_types, pos_types, outputs):\n    # TODO: Write this function for part 3\n    model = Sequential()\n\n    # Embedding Layer\n    model. add(Embedding(input_dim=word_types, output_dim=32, input_length=6))\n\n    # Flatten Layer\n    model. add(Flatten())\n\n    # 2 Layers, 100, 10\n    model. add(Dense(100, activation='relu'))\n    model. add(Dense(10, activation='relu'))\n\n    # Output Layer with softmax\n    model. add(Dense(91, activation=keras. activations. softmax))\n\n    model. compile(keras. optimizers. legacy. Adam(learning_rate=0. 01), loss=\"categorical_crossentropy\")\n    model. summary()\n    return model\nThis is the full error:Compiling model.\n/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/optimizers/legacy/adam. py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(). __init__(name, **kwargs)\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 6, 32)             484896    \n                                                                 \n flatten (Flatten)           (None, 192)               0         \n                                                                 \n dense (Dense)               (None, 100)               19300     \n                                                                 \n dense_1 (Dense)             (None, 10)                1010      \n                                                                 \n dense_2 (Dense)             (None, 91)                1001      \n                                                                 \n=================================================================\nTotal params: 506207 (1. 93 MB)\nTrainable params: 506207 (1. 93 MB)\nNon-trainable params: 0 (0. 00 Byte)\n_________________________________________________________________\nDone loading data.\nEpoch 1/5\n2023-11-05 12:36:07. 078331: W tensorflow/core/framework/op_kernel. cc:1816] OP_REQUIRES failed at cast_op. cc:121 : UNIMPLEMENTED: Cast string to float is not supported\nTraceback (most recent call last):\n  File \"/Users/omernauer/Desktop/Columbia Studies/Fall 2023/NLP/hw3_files/train_model. py\", line 52, in &lt;module&gt;\n    model. fit(inputs, outputs, epochs=5, batch_size=100)\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 70, in error_handler\n    raise e. with_traceback(filtered_tb) from None\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/tensorflow/python/eager/execute. py\", line 60, in quick_execute\n    tensors = pywrap_tfe. TFE_Py_Execute(ctx. _handle, device_name, op_name,\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntensorflow. python. framework. errors_impl. UnimplementedError: Graph execution error:\n\nDetected at node sequential/Cast defined at (most recent call last):\n  File \"/Users/omernauer/Desktop/Columbia Studies/Fall 2023/NLP/hw3_files/train_model. py\", line 52, in &lt;module&gt;\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1783, in fit\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1377, in train_function\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1360, in step_function\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1349, in run_step\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1126, in train_step\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 589, in __call__\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/base_layer. py\", line 1149, in __call__\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 96, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/sequential. py\", line 398, in call\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/functional. py\", line 515, in call\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/functional. py\", line 654, in _run_internal_graph\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/functional. py\", line 751, in _conform_to_reference_input\n\nCast string to float is not supported\n         [[{{node sequential/Cast}}]] [Op:__inference_train_function_760]\n\nAny help is appreciated!",
        "target": "I suspect theirs is an issue in your input matrices. Are you correctly replacing the tokens from the input string with integer indices? "
    },
    {
        "source": "#372: Part2 stack & buffer representation. Hi, From what I understand, in part 2, the task is to represent the top 3 words from both the stack and the buffer in a single array. ['the', 'root', 'null', 'dogs', 'eats', 'a']. What if the buffer has less than 3 items? For instance, if we only have two words, 'dog', and 'eats', should it be ['the', 'root', 'null', 'dog', 'eats', 'null'] or ['the', 'root', 'null', 'null', 'dog', 'eats']? Thanks!",
        "target": "I believe the array should essentially be [stack[-1], stack[-2], stack[-3], buffer[-1], buffer[-2], buffer[-3]]. But you need to make sure those indices exist. If they don't, you should replace any non-existent ones with &lt;NULL&gt;'s representation (i. e. , 4 from the words. vocab output in Part 1). You also have to check for the special symbols (e. g. , &lt;CD&gt;) by looking at the part of speech tag. Finally, you want to map the string words to their index value from the word-to-index dictionary in the attribute word_vocab. The actual state. stack and state. buffer values are just a word's index in the sentence (e. g. , 3 to mean the third word in the sentence)."
    },
    {
        "source": "#371: HW4 Output. Hi , Does my output needs to be under 70? Or is this fine.  Thank you! Best, Adrian",
        "target": "These look great! "
    },
    {
        "source": "#369: Large Loss Values in Part 3. Hi, Like a few others have mentioned, I'm running into large loss values when attempting to train the model in Part 3. During the first epoch, the loss value appears to continue reducing until around 1. 95, at which point it begins to steadily increase. What could be causing this behavior? Thanks so much!",
        "target": "Hi, I had a similar issue training my model locally on my M1 Macbook Air where the number went down to 0. 6 before it exploded into a really large value (~200). I trained my model on Google colab and it seems to fix my problem though. My loss values are now more reasonable, it was around 0. 6 at the end of epoch 1 and I ended with 0. 29 at the end of epoch 5. Hope this helps!"
    },
    {
        "source": "#369: Large Loss Values in Part 3. Hi, Like a few others have mentioned, I'm running into large loss values when attempting to train the model in Part 3. During the first epoch, the loss value appears to continue reducing until around 1. 95, at which point it begins to steadily increase. What could be causing this behavior? Thanks so much!",
        "target": "If you are not trying to use GPU acceleration on an M1/M2 unit, maybe there is something wrong with your training data?"
    },
    {
        "source": "#368: Shapes are incompatible from model. fit. Hi! I'm getting this error from the \"model. fit(inputs, outputs, epochs=5, batch_size=100)\" line in train_model. py and am not sure where it may be coming from (probably from my 6 inputs and 91 outputs dimensions), but am not sure how to resolve it-- I would appreciate any insight on this, thank you! !",
        "target": "You can also try feature_vector = feature_vector. reshape(1, -1). I threw this into my code and it seems to have fixed something for now."
    },
    {
        "source": "#368: Shapes are incompatible from model. fit. Hi! I'm getting this error from the \"model. fit(inputs, outputs, epochs=5, batch_size=100)\" line in train_model. py and am not sure where it may be coming from (probably from my 6 inputs and 91 outputs dimensions), but am not sure how to resolve it-- I would appreciate any insight on this, thank you! !",
        "target": "Yes, the first dimension required is generally the batch size, so instead of feeding an input of dimensionality (, 6) you should feed an input of (1, 6). You can get this using . reshape(1, -1). Similarly, the output will be an array of size (1, 91). "
    },
    {
        "source": "#367: Question about hw3 part4. Dear TAs, When I was working on hw3 part4, I met this problem when running evaluation program: I'm wondering what will cause this error? Thank you!",
        "target": "I just modified my codes and I noticed that the scores seem to be too low:I'm wondering where should I modify my codes? Thanks!"
    },
    {
        "source": "#367: Question about hw3 part4. Dear TAs, When I was working on hw3 part4, I met this problem when running evaluation program: I'm wondering what will cause this error? Thank you!",
        "target": "Hi dear TAs and professor:After modification, I finally got this output, does it reach the assignment's requirement? Thank you!"
    },
    {
        "source": "#366: Issues installing Tensorflow for GPU on Windows. When I tried installing the tensorflow 1 GPU version on my windows 11 laptop that has a nvidia GPU, I get prompted with the following errorAnd because the latest version of tensorflow requires linux environment on windows, I installed WSL and minicando3, then installing tensorflow ontop of that, but it's not recognizing my GPU, or skipping the process of checking all together ",
        "target": "Hm, sorry I don't really have a good answer to this as a non-windows user. If you can't figure it out, you could try Google Colab. It's actually possible to run training on the CPU as well, so if it runs without GPU support that's okay for now. For homework 4 you will likely use GCP anyway. "
    },
    {
        "source": "#363: HW3 Part 4. Hello! I've been working on part 4 for a while now and I can't seem to solve the following errorThis is my model:And when I try to call model. predict:I get the following error:I double-checked the output of currentState, and it is indeed an np array of integers of length 6. I don't understand what is going on with the dimensions (192) and (32). Could you please help me understand what's going on? Thanks in advance for your help",
        "target": "Hi the predict method expects the input to be of shape (1, 6), that is a 2D matrix (the first dim is the batch size), instead of (6, ). Try to reshape the result if get_input_representation using . reshape(1, -1). "
    },
    {
        "source": "#362: May:  Zoom. ",
        "target": "Solved in OH!"
    },
    {
        "source": "#361: Runtime of decoder. py and evaluate. py. Hi Prof. Bauer and the TAs, Is it normal to have decoder. py to run for more than 30min? I can't remember exactly how long but somewhere in 30-40min range. I was using the accelerator on M2 chip. I also tried on Colab with T4 GPU. The step time seems to be longer on Colab (~20ms/step) than locally on M2 chip (~10ms/step). Best, Ansen Gong",
        "target": "I recommend turning off eager execution.  tf. compat. v1. disable_eager_execution()"
    },
    {
        "source": "#360: HW3 Part 2 - Output. Hello, I would like to confirm whether I understood the task for get_output_representation. It's asking us to make a one-hot representation of the transition. Does that mean that we should make a function that maps each of the 91 possible transitions to a unique index in the output array, leaving the rest as 0? If this is right, I am guessing there are multiple ways of doing this, and it won't matter as long as we're consistent. Thanks for your time and the clarificationsEDIT: Now I'm realizing that function make_output_labels already creates a dictionary of all possible transitions. Maybe we can take each value in the dictionary (which represents the index and goes from 0 to 90)?",
        "target": "Thats correct. You can use the output labels dictionary to find the numeric representation for the transition. Then turn it into a one-hot vector. "
    },
    {
        "source": "#359: hw3 part1. Hi, sorry to ask a silly question, I can't type \"\\$ python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\"in the terminal in my VScode, I can only type \"python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\". All command contain \"$\" is unable to run on my terminal. Btw, I find every time I run \"python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\", I get a different words. vocab, is it normal?",
        "target": "The $ is a common prompt character, not part of the actual command. :)Getting different words. vocab files is expected. The types are collected in a set, which is unordered so when they are written to the file, the order is arbitrary. "
    },
    {
        "source": "#358: Regarding trying to reach SOTA. The current state of the art for dependency parsing is ~97 ( http://nlpprogress. com/english/dependency_parsing. html). Feel free to experiment with additional features to improve the parser. Dear Prof. Bauer and TAs, Does this experimentation part hold any marks? I would like to do it but probably later sometime due to the number of assignments lined up this week. Thank you, Shreyas.",
        "target": "I feel like if you could build state of the art in a week or two you'd have a future in NLP research"
    },
    {
        "source": "#355: Adam not working on Colab. Hi Prof. Bauer and TAs, As I am trying to migrate the code to Colab, I have encountered this issue on the optimizer Adam below: My Colab uses Tensorflow version 2. 14. 0. Have there been similar issues? And if so, what were the solutions? Best, Ansen",
        "target": "Hm -- probably a version incompatibility. Try using tf. keras. optimizers. legacy. Adam."
    },
    {
        "source": "#355: Adam not working on Colab. Hi Prof. Bauer and TAs, As I am trying to migrate the code to Colab, I have encountered this issue on the optimizer Adam below: My Colab uses Tensorflow version 2. 14. 0. Have there been similar issues? And if so, what were the solutions? Best, Ansen",
        "target": "I met the same issue when I disabled eager execution before fitting. I guess that after disabling it the backpropagation is also blocked."
    },
    {
        "source": "#354: HW03 Part2. Hello Professor and TA, I'm curious about the output result of part 2. Below is what I've obtained. Is it the expected result? Thank you for your help and response. [None, 'BUSH', 'AND', 'GORBACHEV', 'WILL', 'HOLD', 'two', 'days', 'of', 'informal', 'talks', 'next', 'month', '. '][None, 'NNP', 'CC', 'NNP', 'MD', 'VB', 'CD', 'NNS', 'IN', 'JJ', 'NNS', 'JJ', 'NN', '. '][None, '. ', '. ', 'GORBACHEV', 'AND', 'BUSH'][3, 4, 4, 4, 5106, 4]",
        "target": "The first line are the words, second line are the POS -- what is the third line? And the last line is the actual input representation? For which state?"
    },
    {
        "source": "#353: Printing Twice. Hi, I hope you are doing well. My output looks like:I see that the scores are very bad, but i also see that it is printing twice.  Is that supposed to happen? Thank you so much",
        "target": "The scores are different though. First one is Micro, the other is macro LAS. They are calculated differently in the code."
    },
    {
        "source": "#353: Printing Twice. Hi, I hope you are doing well. My output looks like:I see that the scores are very bad, but i also see that it is printing twice.  Is that supposed to happen? Thank you so much",
        "target": "I think it is supposed to happen because I am also getting two separate sets of scores, one for the micro averages and other for the macro averages of the labeled and unlabeled attachments. Also, Micro LAS and Macro LAS are calculated and printed separately in the evaluate. py file."
    },
    {
        "source": "#348: HW3 Part 2. Hello, For the get_input_representation method, if our stack/buffer is shorter than 3 words, should we replace the corresponding elements in the array we'll return with the index of \"&lt;NULL&gt;\" in the word vocab?",
        "target": "Yes, that is what I did, and it worked for me."
    },
    {
        "source": "#347: words in extract_tarining data. Hi i hope you are greatI am having issues with my code so I printed out words in extract training data. This is what printed:[None, '``', 'Feeding', 'Frenzy', \"''\", '-LRB-', 'Henry', 'Holt', ', ', '326', 'pages', ', ', '$', '19. 95', '-RRB-', ', ', 'a', 'highly', 'detailed', 'account', 'of', 'the', 'Wedtech', 'scandal', ', ', 'begins', 'on', 'a', 'reassuring', 'note', '. ']I have a few questions about this parameter, is None the root? what are -LRB- and  -RBR-thank you so much!",
        "target": "Yes, None will always appear in position 0 and represents the root symbol. -LRB- stands for the symbol (  left round bracket. "
    },
    {
        "source": "#346: Error in Obtaining Vocabulary. Hi after running $python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\n\n\ni do not get the screenshot on Courseworks, instead i get the following: Is there something I did wrong with uploading the information? ",
        "target": "The order of tokens in the vocabulary may come out differently since we are keeping track of words in a set, which is unordered. "
    },
    {
        "source": "#345: Mac M1 vs Google Colab: Training Model Loss Function. Hello, I had different results when I ran part 3 locally vs Google Colab. I am using MacBook Pro (M1 chip) , macOS : Sonoma and I got a very high loss value which is &gt;700 and it kept increasing on every epoch. However, when I ran my part 3 at Google Colab not only its significantly faster in time but my loss value is also significantly lower, which is &lt;0. 5 and decreasing on every epoch. I am curious on why this happened, and since I get a very drastic output when running on both platforms which one is more reliable? Thank you ",
        "target": "Same questions^"
    },
    {
        "source": "#343: Error: zsh: illegal hardware instruction. Hi, when trying to run:python extract_training_data. py data/dev. conll data/input_dev. npy data/target_dev. npy\nI get the error: zsh: illegal hardware instruction  python extract_training_data. py data/dev. conll data/input_dev. npy \nIs anyone else getting this error or knows how to solve this? Thanks",
        "target": "What architecture are you running this on, and are you using the CPU or GPU version of tensorflow? Is this really an error or just a warning?"
    },
    {
        "source": "#341: Hw3 Part 4 understanding predictions. Hi NLP teaching staffs - I am having trouble understanding the output of model. predict. When I print predict out, the result is a 2D array consisting of numbers. How can I tell 1) which transition the model predict 2) the probability of the prediction? Appreciate any guidance/ pointer. Thank you so much!",
        "target": "The output is a (batch_size, vocab_size) array. Since you are just passing a single input you can just take the first entry from the array in the batch dimension (index 0). Then you end up with a 1D array of length vocab_size. The probability values represent the probability of each word.  For each dimension you get a probability value. You need to sort the dimensions according to the probability, which should result in an array of integer dimensions sorted in decreasing order of the probability. This can be done using np. argsort"
    },
    {
        "source": "#340: HW 3 PART 2. Hi, I'm still a little confused here, just wanna check I correctly understanding this. For part2:the word and pos contains every possible word and corresponding tags from the input file and state can be any middle transition state for one sentence?",
        "target": "In get_input_representation(self, words, pos, state), words and pos refer to the actual input sentence and corresponding pos sequence. State is the current (intermediate) state in the transition sequence. state. stack and state. buffer contain token positions (that is, you can treat them as indices in the words and pos list). "
    },
    {
        "source": "#339: Hw3 Pt 4 Accuracy. For labeled attachment score, I'm getting around 0. 68 and 0. 75 for unlabeled attachment score. Are these scores sufficient for the assignment? If they're not sufficient, what's the best way to go about testing the code to find places to improve?",
        "target": "That sounds pretty good to me!"
    },
    {
        "source": "#337: Very low accuracy. Hello, I just ran evaluate. py on my decoder, but my accuracy is super low, as copied below:Micro Avg. Labeled Attachment Score: 0. 04646650580831323Micro Avg. Unlabeled Attachment Score: 0. 20907440113430015Macro Avg. Labeled Attachment Score: 0. 052479354272750564Macro Avg. Unlabeled Attachment Score: 0. 21695312916782164I think there's something really wrong in my code for it to be this low, so does anyone have any suggestions on where to even start debugging or has anyone come across this? Totally understand this is hard to help with without seeing my code, but I'd appreciate any pointers. Thanks!",
        "target": "I would look at the output produced by decoder to see if there are any obvious systematic mistakes (such as disconnected trees, or everything depending directly on 0-root). With low scores like these, I think something systematic is likely. At the very least, you should be able to identify if the issue is the model or the decoder. Then you can try to find possible causes in your code. "
    },
    {
        "source": "#337: Very low accuracy. Hello, I just ran evaluate. py on my decoder, but my accuracy is super low, as copied below:Micro Avg. Labeled Attachment Score: 0. 04646650580831323Micro Avg. Unlabeled Attachment Score: 0. 20907440113430015Macro Avg. Labeled Attachment Score: 0. 052479354272750564Macro Avg. Unlabeled Attachment Score: 0. 21695312916782164I think there's something really wrong in my code for it to be this low, so does anyone have any suggestions on where to even start debugging or has anyone come across this? Totally understand this is hard to help with without seeing my code, but I'd appreciate any pointers. Thanks!",
        "target": "I had the same issue and to fix it I printed out the state. stack and the state. buffer and every action I was checking the validity of in the while loop. I found a lot of implementation issues when I did this where certain arcs or shifts weren't done properly and once I fixed these, the accuracy increased."
    },
    {
        "source": "#336: Possible Error in HW3 Q2. In the instructions it says: Take a look at the file extract_training_data. py States: The input will be an instance of the class State, which represents a parser state. The attributes of this class consist of a stack, buffer, and partially built dependency structure deps. --stack and buffer are lists of word ids (integers). --\"Based on this it means that the values on the stack and buffer are the word positions in the sentence, and I don't see if there is a way to reverse engineer it to find the word itself. I have been at a few OHs and no one can figure out and TAs think it could be a wording mistake because it would make more sense that the values on the stack and buffer are the words themselves. Can you please clarify if there is a mistake or not? Thank you!",
        "target": "Hi Omer, Responding here -- I also saw your email. The description is correct. Let's say the input sentence is 0-root 1-the 2-cat 3-sleeps. The initial configuration would look like this stack: [0]  buffer: [3, 2, 1]  To find the specific tokens you need to do two lookup steps. First, find the word at the specific index, then lookup the integer representation of that word in the vocabulary dictionary.  For example, for token 1, you would use word_vocab[words[1]] to get the integer representation of \"the\". Does that help? Daniel "
    },
    {
        "source": "#335: HW3 Output labeling. Hi, Can I label my outputs in a different one-hot format? Thank you ",
        "target": "Hi, I initially tried to use my own scheme, but I ran into an issue because of this during the decoder and evaluation stage since I utilize self. output_labels from the Parser class which is defined as dict([(index, action) for (action, index) in extractor. output_labels. items()]) and imposes a certain structure on the output. It looks like this: \"{0: ('shift', None), 1: ('left_arc', 'tmod'), 2: ('right_arc', 'tmod'), 3: ('left_arc', 'vmod'), 4: ('right_arc', 'vmod'). .. }\". My code ended up working after following the same scheme as shown here. It may be possible to do it with your own output ordering scheme, but alternating left and right arcs like done in self. output_labels worked best in my case. Hope this helps!"
    },
    {
        "source": "#334: Q2 Finding root. How do I find root? \"So for example, if the next words on the buffer are \"dog eats a\" and \"the\" and &lt;ROOT&gt; are on the stack , the return value should be a numpy array numpy. array([4047, 3, 4, 8346, 8995, 14774]). Here 4 is the index for the &lt;NULL&gt; symbol, 3 is the index for the &lt;ROOT&gt; symbol, 4047 is the index for \"the\" and 8346, 8995, 14774 are the indices for \"dog\", \"eats\" and \"a\". \"Based on this quote, if root is sitting on the stack, and I call state. stack[root index] would it return the string \"&lt;ROOT&gt;\"?",
        "target": "Both the stack and the buffer contain word ids. You'd want to extract the word (string) using its id. Then, think about how to incorporate word_vocab in order to switch between the word and the mapped index!"
    },
    {
        "source": "#332: Office Hours During Fall Break. Hello! I just wanted to confirm if and which office hours will be available during fall break since the homework is due the day after? Thank you!",
        "target": "Yes, I will hold office hours on Monday and Tuesday on Zoom. I may have to schedule a different time slot than usual on Monday -- I will send an announcement through Courseworks. I don't think the TAs will hold office hours Mon/Tue."
    },
    {
        "source": "#330: Part 4 Warning. Hi I hope you are doing great.  When I run python evaluate. py data/model. h5 data/dev. conll, I get the warning below:(base) venkatramamoorthi@dyn-160-39-55-55 hw3_files % python evaluate. py data/model. h5 data/dev. conll 2023-10-31 09:18:52. 036024: I tensorflow/core/platform/cpu_feature_guard. cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropri ate compiler flags. 2023-10-31 09:18:57. 104034: I tensorflow/compiler/mlir/mlir_graph_optimization_pass. cc:375] MLIR V1 optimization pass is not enabled 2023-10-31 09:18:57. 227919: W tensorflow/c/c_api. cc:304] Operation '{name:'dense_2/bias/Assign' id:87 op device:{requested: ' ', assigned: ''} def:{{{node dense_2/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/bias, dense_2/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. Evaluating. (Each . represents 100 test dependency trees) /Users/venkatramamoorthi/opt/anaconda3/lib/python3. 8/site-packages/keras/src/engine/training_v1. py:2359: UserWarning: `Model. state_updateswill be removed in a future version. This property should not be used in TensorFlow 2. 0, asupdates` are appl ied automatically. updates=self. state_updates, 2023-10-31 09:18:57. 453587: W tensorflow/c/c_api. cc:304] Operation '{name:'dense_2/Softmax' id:93 op device:{requested: '', a ssigned: ''} def:{{{node dense_2/Softmax}} = SoftmaxT=DT_FLOAT, _has_manual_control_dependencies=true}}' w as changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error i n the future. Either don't modify nodes after running them or create a new session. I let this run for 4  hours and it stayed like this.  I worked with a TA and we moved:import tensorflow as tftf. compat. v1. disable_eager_execution()between files.  If I dont have the lines above, it just keeps outputting lines like:1/1 [==============================] - 0s 15ms/stepIs there anything else I can try? Thank you so much! Based on another question in the ed, i think something larger is wrong with my code. it seems someone had this error but their code compiled.  I am unsure of how to troubleshoot this.  thank you!",
        "target": "Hm, so the warnings won't affect your code, as far as I can tell. The output is expected. 15 ms/step seems still very slow, but should allow you to evaluate the model. After the progress bars, does it fail to compute the actual scores? "
    },
    {
        "source": "#329: M1 Chip Computer Warnings during HW3 Part 4. Hi, I'm running the decoder and evaluator on an M1-chip computer, and I'm encountering numerous warnings in part 4. While the decoder and evaluator functions execute correctly and produce results, I'm noticing that the accuracy is significantly low. Due to the warnings, I'm uncertain about whether the issues mentioned in the warnings may be having some impact on my performance that may be attributed to my M1 and some error in my initial setup. Here are some examples of the warnings I get: \"2023-10-30 23:08:58. 971460: I tensorflow/compiler/mlir/mlir_graph_optimization_pass. cc:382] MLIR V1 optimization pass is not enabled 2023-10-30 23:08:59. 002409: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_2/kernel/Assign' id:82 op device:{requested: '', assigned: ''} def:{{{node dense_2/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/kernel, dense_2/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. 2023-10-30 23:08:59. 094512: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_1/bias/v/Assign' id:265 op device:{requested: '', assigned: ''} def:{{{node dense_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/bias/v, dense_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. Evaluating. (Each . represents 100 test dependency trees) /Users/anaconda3/lib/python3. 11/site-packages/keras/src/engine/training_v1. py:2359: UserWarning: Model. state_updates will be removed in a future version. This property should not be used in TensorFlow 2. 0, as updates are applied automatically. updates=self. state_updates, 2023-10-30 23:08:59. 187791: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_2/Softmax' id:93 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = SoftmaxT=DT_FLOAT, _has_manual_control_dependencies=true}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. \"Thanks!",
        "target": "Are you just running the CPU version on an M1, or are you attempting to use the tensorflow-metal GPU acceleration? There are known issues with tensorflow-metal, so I would recommend to just use the CPU version for now. The warnings should not affect performance in that case. "
    },
    {
        "source": "#328: Cumulative Exam 2. Hi! Thank you so much for a great course so far! I was wondering if the second exam includes what was on the first exam or if the scope is just what was covered after? Thank you!",
        "target": "I think it will be cumulative, based on this comment - https://edstem. org/us/courses/46417/discussion/3807069? comment=8777621"
    },
    {
        "source": "#327: followup to best transition. Hi I hope you are great. I asked this question on ED:Once we figure out the best transition in Part 4, how should we store it? I'm unsure of how finding the best transition connects with the given part:        result = DependencyStructure()\n        for p, c, r in state. deps:\n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return resultThe answer was:The best transition is found and used to update the parser's state within the while loop of the parse_sentence method. There's no need to separately store the best transition because it is immediately applied to the parser's state. So the result here is already the best transitionI have my while loop with my logic to see if its a legal transition.  I'm not sure how to update the parser's state once i get the best transitionI think i should call the legal transition' function from the state class, prior to          result = DependencyStructure()\n        for p, c, r in state. deps:\n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return resultthank you again!",
        "target": " Hm, I'm not sure I understand your follow-up question. Once you have the best transition, you can call the corresponding method on the parser state. for example, result. shift() or result. left_arc(\"nsubj\"). This will change the state of result -- then you proceed with the next prediction based on the new state. "
    },
    {
        "source": "#326: HW3 Part 2. Hi, I would like some clarification on get_input_representation(self, words, pos, state). First, are words and pos basically used to check if any entry in the buffer/stack needs to be replaced by a tag? Secondly, are words and pos structured such that pos[i] is the pos tag of word[i]? If there is no tag, does pos[i] equal to None? Also, regarding to the comment in post #315, can we assume if we see None(or 0, as described in the instructions) in the stack or buffer, we can replace it with &lt;ROOT&gt;? Thank you",
        "target": "You're given a list of words that are just words, prepended with NoneYou're given a list of POS that correspond 1:1 with the wordsStack and buffer contain a list of indexes where 0 is root, but these index the word/POS list. I think the wording of the assignment is awkward for thatYou need to identify which words are being referenced and when, but also handle finding their actual IDs to create the vectors. you can use the POS to check if some words need to be converted to a special ID corresponding to a category of words, like NNP and CD."
    },
    {
        "source": "#325: part 4 best transition. Hi I hope you are great. Once we figure out the best transition in Part 4, how should we store it? I'm unsure of how finding the best transition connects with the given part:result = DependencyStructure()\n        for p, c, r in state. deps:\n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return resultthank you so much!",
        "target": "The best transition is found and used to update the parser's state within the while loop of the parse_sentence method. There's no need to separately store the best transition because it is immediately applied to the parser's state. So the result here is already the best transition"
    },
    {
        "source": "#324: HW3 Part 4 - name 'extractor' is not defined error. Hello, I am getting a strange error when attempting to run:python evaluate. py data/model. h5 data/dev. conll\nWhen I try and run it on Google Colab, I get the following output:Evaluating. (Each . represents 100 test dependency trees)\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n/content/evaluate. py in &lt;module&gt;\r\n     45             words = dtree. words()\r\n     46             pos = dtree. pos()\r\n---&gt; 47             predict = parser. parse_sentence(words, pos)\r\n     48             labeled_correct, unlabeled_correct, num_words = compare_parser(dtree, predict)\r\n     49             las_s = labeled_correct / float(num_words)\r\n\r\n/content/decoder. py in parse_sentence(self, words, pos)\r\n     38 \r\n     39             transition = False\r\n---&gt; 40             features = self. extractor. get_input_representation(words, pos, state). reshape(1, -1)\r\n     41             status = self. model. predict(features)\r\n     42             while transition == False:\r\n\r\nNameError: name 'extractor' is not defined\nThis is very confusing, since running decoder. py does not bring up any errors at all. I suspected that it might be my indentation, but I went over it and it seems fine to me. I have not touched evaluate. py. I even redownloaded it just to make sure. What could be causing this? Any help would be greatly appreciated. Thank you.",
        "target": "I assume this might be an import problem. Make sure your working directory on your google colab is correct. It's hard to solve this kind of problem on ed discussion. Could you go to one of the OH and ask the TA for help?"
    },
    {
        "source": "#323: HW3 Part 4 - Difficulty Reading In Feature Into Prediction. Hello, I am getting a error I do not understand when attempting to predict using extractor. get_input_representation(). Right now, I have it so I read it in through:status = self. model. predict(extractor. get_input_representation(words, pos, state))\nHowever, when this line is run, I receive the following error:ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\r\n    \r\n    Input 0 of layer \"1stHidden\" is incompatible with the layer: expected axis -1 of input shape to have value 192, but received input with shape (None, 32)\nI do not understand what might be causing this. extractor. get_input_representation() returns a numpy array of shape (6, ), which is what I expect. Here is how my model is set up for reference:def build_model(word_types, pos_types, outputs):\r\n    m = Sequential()\r\n    m. add(Embedding(word_types, 32, input_length=6, name=\"Embedding\"))\r\n    m. add(Flatten(name=\"FlattenEmbed\"))\r\n    m. add(Dense(100, activation=\"relu\", name=\"1stHidden\"))\r\n    m. add(Dense(10, activation=\"relu\", name=\"2ndHidden\"))\r\n    m. add(Dense(outputs, activation=\"softmax\", name=\"Output\"))\r\n\r\n    m. compile(keras. optimizers. Adam(lr=0. 01), loss=\"categorical_crossentropy\")\r\n    return m\nAny help regarding what might be causing this would be greatly appreciated. Thank you.",
        "target": "Hi the predict method expects the input to be of shape (1, 6), that is a 2D matrix (the first dim is the batch size), instead of (6, ). Try to reshape the result if get_input_representation using . reshape(1, -1). "
    },
    {
        "source": "#322: Homework 3 Part 4. For the part 4, I want to get the input representation, so I wrote something like input_representation = self. extractor. get_input_representation(words, pos, state) but it throws an error Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 192, but received input with shape (None, 32) So I change my code to input_representation = self. extractor. get_input_representation(words, pos, state). reshape(1, -1) and it works. But I just want to make sure this is a necessary step to do to reshape or it indicates something went wrong with my previous code? ",
        "target": "Yes thats fine. By reshaping this way you are turning the d-dim vector (i. e a 1-D Matrix) into a (1, d) 2-D matrix. This allows the training data to be stacked together in batches of 192 of shape (192, d). "
    },
    {
        "source": "#320: Homework 3 Part 2. 1. To get output representation, can I use return keras. utils. to_categorical(self. output_labels[output_pair], num_classes=91) to get the output matrix? Or I have to implement this manually? 2. To get input representation, when we considering the special case of root, I am confused about do we compare the word with None or 'None'? i. e. with quotation mark or without? So, I am confused about how the stack is initialized for the special case &lt;ROOT&gt;?",
        "target": "1. Yes that sounds okay. 2. I think the entry in the word list is just the value None (not a string). Note that the stack contains word positions, so for the root it will contain the value 0. In the initial state, it will be [0]. "
    },
    {
        "source": "#319: Loss values for HW3? . Hello, Is it possible to know (or share) the approximate loss values people are getting for their training? ",
        "target": "Hi, When I was training I started at like a loss of 2. 5 and it went down to about 0. 25 by the end of the 5 epochs. Hope this helps."
    },
    {
        "source": "#319: Loss values for HW3? . Hello, Is it possible to know (or share) the approximate loss values people are getting for their training? ",
        "target": "Hi, I got: 0. 4751 0. 3072 0. 2737 0. 2544 0. 2413I'm not sure whether this is the expected answer]"
    },
    {
        "source": "#319: Loss values for HW3? . Hello, Is it possible to know (or share) the approximate loss values people are getting for their training? ",
        "target": "Thanks all, at time of writing my model wasn't converging so I asked but now it's consistent."
    },
    {
        "source": "#317: HW3 part 4. Hi! I am trying to run the evaluation (python evaluate. py data/model. h5 data/dev. conll) for the model and I found it took quite a long time. On average it takes 9ms / step on a A6000 gpu and it went for nearly two hours. Is this normal evaluation time or something is wrong in my setup? For reference my final results are as follows:5039 sentence. Micro Avg. Labeled Attachment Score: 0. 7391242832055795Micro Avg. Unlabeled Attachment Score: 0. 786863415811507Macro Avg. Labeled Attachment Score: 0. 7480423144749794Macro Avg. Unlabeled Attachment Score: 0. 7964001075386273Thanks!",
        "target": "Did you disable eager execution? I found that it speeds up the last step significantly"
    },
    {
        "source": "#315: HW3 part 2. Hi! In HW3 part 2 get_input_representation(self, words, pos, state), I am not sure what the pos variable can be used for, is it supposed to be used in the function? Also, I found that the list of words for a sentence always start with none, for example:words: [None, '``', 'Feeding', 'Frenzy', \"''\", '-LRB-', 'Henry'. .. ]. Is this normal and we leave it as it is? Thanks!",
        "target": "For the input representation you need the POS to replace some of the input tokens with special symbols. Note that you need to account for the special symbols (&lt;CD&gt;, &lt;NNP&gt;, &lt;UNK&gt;, &lt;ROOT&gt;, &lt;NULL&gt;) in creating the input representation. Make sure you take into account states in which there are less than 3 words on the stack or buffer. The first token (position 0) corresponds to the \"root\" symbol. None is included in the list of input tokens so that the first \"actual\" token has the index 1."
    },
    {
        "source": "#314: Annotated Dependency Tree Bank. Hello, When starting HW 3, I wanted to make sure I had a solid understanding of Dependency Parsing. To clarify, we manually create an annotated dependency tree that is the gold tree from a treebank. I am confused as to how this dependency tree is different from the one we are trying to construct. If we already have the gold tree, why are we training this model to construct transitions if the transitions are already from the gold tree? Are we using the model to create transitions on unseen inputs that are not from the gold tree?",
        "target": "I believe you are training it to construct dependency trees for any input, not just the training set."
    },
    {
        "source": "#314: Annotated Dependency Tree Bank. Hello, When starting HW 3, I wanted to make sure I had a solid understanding of Dependency Parsing. To clarify, we manually create an annotated dependency tree that is the gold tree from a treebank. I am confused as to how this dependency tree is different from the one we are trying to construct. If we already have the gold tree, why are we training this model to construct transitions if the transitions are already from the gold tree? Are we using the model to create transitions on unseen inputs that are not from the gold tree?",
        "target": "I think your understanding is correct here. We have the manually annotated treebank. We obtain transition sequences for these trees using the oracle algorithm. The transition sequence for each training sentence make up the training data. We are training the model to predict good transition sequences for unseen sentences. "
    },
    {
        "source": "#313: target_dev. npy and target_train. py. Hi I hope you are doing great! I think I have completed part 2. I just ran these lines:python extract_training_data. py data/train. conll data/input_train. npy data/target_train. npypython extract_training_data. py data/dev. conll data/input_dev. npy data/target_dev. npySo now I have target_dev. npy and target_train. py as files in my data folder. I am using atom and when I clicked on the files, I got a message that: 'Atom will be unresponsive while loading such large files\"I procceded and target_dev. npy has a bunch of lines like:  ? ?and target_traim. np seems to just be blank. Is this normal? Thank you so much!",
        "target": "Yes. These are binary files, so you cant just open them in a text editor. However target_train. npy should not be empty  I dont know if there is an issue with your code or if it simply doesnt show the content in Atom. Whats the actual file size? "
    },
    {
        "source": "#311: Teacher Forcing. Hello, I had a quick clarification question regarding what \"teacher forcing\"  is. In the slides today, we saw the outputs for an RNN LM being fed independently into each other, as well as outputs being pipelined together in a Generator. My question is in which application of RNNs would we consider the inputs to be \"teacher-forced? \" I am under the assumption that \"teacher-forcing\" means to over-write an incorrect model prediction output with the correct training data as an input in pipelining, but we mentioned teacher-forcing in the RNN LM context as well.",
        "target": "So the generator and RNN language model is trained exactly the same way. But you are right that you cant really speak of teacher forcing in the context of the language model. "
    },
    {
        "source": "#310: Different evaluation result in Colab and Pycharm. Hi there, when I ran the same code in both Colab and PyCharm on the \"dev. conll\" set to evaluate the model performance, it gave me very low accuracy on PyCharm (roughly Micro Avg. Unlabeled Attachment Score 0. 05), but the code run in Colab gave me an accuracy of \"Micro Avg. Unlabeled Attachment Score: 0. 77, \" so I am not sure what happened here. Also, I further trained the model on the \"input_dev\" and \"target_dev\" sets and named the model \"model_dev. h5\". However, when I ran the original model which is not further trained on the \"input_dev\" and \"target_dev\" sets on \"test. conll\" file, it returned the \"ZeroDivisionError: float division by zero\", but if I ran the evaluation by using the \"model_dev. h5\" model, it worked just fine.  I am also unsure what is wrong here. Thanks a lot in advance.",
        "target": "It was solved by reinstalling all libraries."
    },
    {
        "source": "#307: indicies question. Hi i hope you are doing great.  I get that the words map to indicies.  But do the particular indicies have a meaning, or are they just a mapping of a word to an arbitrary numeric value. Thank you! ",
        "target": "They are just arbitrary numeric values (since you need to represent the input to the neural network numerically). "
    },
    {
        "source": "#306: The usage of \"pos_types\" parameter in build_model function in HW3. Hi there, I am doing the HW3 part 3, the input parameter \"pos_types\" for function \"build_model\" seems a bit redundant and is not used in constructing the neural network. Just want to confirm if it is right or if I missed something. Thanks a lot in advance!",
        "target": "That's correct, that parameter is not used. "
    },
    {
        "source": "#304: Ungraded Exercise for HMM Questions Part c and d. for c, wouldn't the tri-grams be on applied to the transition probabilities rather than the emission probabilities? Meaning it should be p(V|N, start) rather than the example with P(START time flies)? In that case, would it be P(V|N, start) = P(V, N, start) / sum of P(x, N, start)and we should do it for as the full set? P(N| N, start)P(Prep| N, start)P(N| V, start)P(Prep| V, start)P(Prep|V, N)P(N|V, N)P(Prep|N, V)P(N|N, V)P(V|N, V)P(N|Prep, V)P(N|Prep, N)For part (d), I don't really understand why don't the solution use a count variable but instead uses: pi[k, t] += pi[k-1][s]is this assuming the first p[0, start] is initialized as 1 and that 1s will keep accumulating? what is sum_s pi[n, s]  ? is sum_s a function or variable here? ",
        "target": "Hi Millie, I think we discussed this during office hours. Please ask again if I misremember. Daniel "
    },
    {
        "source": "#303: Ungraded Excercises Parsing. In step 11, one might think that a RIGHT-ARC is possible. However, since there is no subsequent connection between root and today, we cannot actually make that move. This makes this example unique. Am I interpreting this correctly?",
        "target": "I think we cannot perform right arc when sent still have a child (today), therefore we have to shift first). Once the child (today) is processed, we can right arc root to sent"
    },
    {
        "source": "#302: Basic doubt regarding probabilities. Given that this is our data in Ungraded Exercise: Naive BayesEmail1 (spam): buy car Nigeria profitEmail2 (spam): money profit home bankEmail3 (spam): Nigeria bank check wireEmail4 (ham): money bank home carEmail5 (ham): home Nigeria flyI want to calculate simple P(buy). One way I go is P(buy) = count(buy)/total_no_words = 1/19The other way to go is using total probability theorem, P(buy) = Summation P(buy|x)P(x) = P(buy|spam) * P(spam) + P(buy|ham) * P(ham) = 1/12 (considering the answer is right) * 3/5 + 0/7 * 2/5 = 1/20Where am I going wrong here? I definitely know that total probability theorem is not wrong.",
        "target": "It's a bit late in the night, so I can't really think this through fully, but consider this:Email1 (spam): buy * infinityEmail2 (ham): somethingIn this case, using the first method to calculate P(buy), we get 1, because inf/(1+inf) goes to 1. But it can be seen that P(buy) should be 1/2, because if only half of all emails are spam (in this data), then it should not be guaranteed that we see \"buy\". So it seems like the count(buy)/total_words method of calculation is invalid, although I can't think of why right now - maybe that method requires P(buy) to be independent of P(class = x)?"
    },
    {
        "source": "#302: Basic doubt regarding probabilities. Given that this is our data in Ungraded Exercise: Naive BayesEmail1 (spam): buy car Nigeria profitEmail2 (spam): money profit home bankEmail3 (spam): Nigeria bank check wireEmail4 (ham): money bank home carEmail5 (ham): home Nigeria flyI want to calculate simple P(buy). One way I go is P(buy) = count(buy)/total_no_words = 1/19The other way to go is using total probability theorem, P(buy) = Summation P(buy|x)P(x) = P(buy|spam) * P(spam) + P(buy|ham) * P(ham) = 1/12 (considering the answer is right) * 3/5 + 0/7 * 2/5 = 1/20Where am I going wrong here? I definitely know that total probability theorem is not wrong.",
        "target": "I think that P(HAM) and P(SPAM) need to be considered on a per word basis to use the law of total probability in this way. Thus, in this case, P(HAM) = 7/19 and P(SPAM) = 12/19, P(buy) = 1/19, P(buy|HAM)P(HAM) + P(buy|SPAM)*P(SPAM) = (1/12)*(12/19) = 1/19. I believe this is because the definition of the Law of Total Probability states the events you are conditioning over must partition the same sample space as the event you're trying to find the probability of, and that the law of total probability seems to fail in the original example because the sample space over documents and the sample space over words are being conflated. If all documents contained the same number of words, I don't think this would have been an issue."
    },
    {
        "source": "#301: ungraded linear model and feature problem #2. I am confused about how to tackle this subproblem:b) Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus. Can anyone provide a hint? The solution didn't offer me any clue. ",
        "target": "Hello! My understanding when I was going through it is that we have a really small lexicon and set of possible labels, so our rules can be super specific. Because our rules are so specific, we can make it so they don't have any false positives, hence we probably don't even need to try to weight them differently. "
    },
    {
        "source": "#301: ungraded linear model and feature problem #2. I am confused about how to tackle this subproblem:b) Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus. Can anyone provide a hint? The solution didn't offer me any clue. ",
        "target": "Not sure if my approach is correct, so take it with a huge pinch of salt. Basically I try to approach it like a super simplify Softmax. It would great that TA can point out where Im doing wrong. Based on how the feature function is set up in the solution, as long as the weights are all greater than 0, then all then all POS should correctly assigned."
    },
    {
        "source": "#301: ungraded linear model and feature problem #2. I am confused about how to tackle this subproblem:b) Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus. Can anyone provide a hint? The solution didn't offer me any clue. ",
        "target": "My intuition is that since the feature functions are defined based on what has appeared in the training corpus, the POS tag/token combination that matches what appeared in the training corpus will always have more 1s in the features comparing to another POS tag/token that hasn't appeared. Therefore to make sure that the POS tag/token combination gets the highest score among all possible POS tag choices, we should set all positive numbers for the weights. "
    },
    {
        "source": "#300: Clarification on Linear Interpolation. Hi, I'm studying the different smoothing methods and wondering what value we should be using for the unigram probabilities in linear interpolation. Should we use  count(word_i)/(number of tokens in the lexicon including END) or count(word_i)/(number of tokens in the lexicon not including END)",
        "target": "I believe that we typically include the END token but do not include the START token, similar to how we calculated it for HW 1. Hope that helps!"
    },
    {
        "source": "#299: Constituent Labels Clarification. Hi, I just wanted to clarify a few things about the slide on constituent labels. Since the slide states that constituents should have one non-bracketed word, would this mean the constituents in the example are the NP consisting of DetP and noun \"girl\", VP consisting of verb \"likes\" and NP, and the NP consisting of AdjP and noun \"boys\"? Or is the logic behind this that all phrases XP containing X (regardless of how many non-bracketed words there are) would be a constituent with X as the head?",
        "target": "One assumption here is that the splits are binary and are chose in such a way that the single non-bracketed word is the head. This approach (called X-bar theory) is not always consistently used. The S constituent, for example, does not have an un-bracketed head. Instead, the head of that constituent is the verb of the VP. "
    },
    {
        "source": "#298: hmm-ungraded assignment-a. for part a, why we have 2 routes of red back pointer, one start from \"leaves\" and one start from \"like\"",
        "target": "This may be an oversimplification but, if you check the lecture slides for 05 hmms, toward the middle it goes over the Viterbi algorithm and explains it as d^n paths for n words and d tags. Essentially a matrix of possibilities to work out all possible paths forward then the most probable paths backward. For the red paths I believe it shows multiple because you want to see the most probable path from node to node where one exists &gt;0. Not until youve moved front to back then back to front do you pick the most probable singular path. Until then you want to keep your options on the table, per say."
    },
    {
        "source": "#297: Number of feature vectors. What's the process of knowing how many features to list out for the feature function assignment? The solution has 8, but I'm assuming that depends based on what our features are. ",
        "target": "Not a TA, but in lecture the instructor made it sound like it was kind of an art form of picking the right features that capture the right aspects and perform well. I'm pretty sure the dimensionality is just based on however many features we determine are sufficient for the given task. "
    },
    {
        "source": "#296: Request for Guidance on Zoom Examination Process. Dear Professor and TA, Unfortunately, I've been falling sick and have decided to opt for the Zoom alternative for tomorrow's exam after discussing with the professor. Could you provide any instructions or guidelines on how this will be conducted? I greatly appreciate your assistance. Thank you, Xinyue",
        "target": "Please log into the regular classroom Zoom meeting. I will have the exam set up in Gradescope for you. You will have to either print it and then complete the template, or respond on separate sheets of paper. One you are done, please scan your work (I recommend using a smartphone scanner app, such as GeniusScan), and upload to Gradescope. During the exam, please keep your camera turned on. One of the TAs will proctor the exams on Zoom. "
    },
    {
        "source": "#295: Complexity of retrieving a single parse tree. . In lecture07, it says that the complexity of retrieving any single parse tree is O(N^2). Why is it not O(2*N)?",
        "target": "You start at [0, n, S] and then follow the backpointers deterministically. The table has n^2 entries for the spans, and at most you will touch each cell once. "
    },
    {
        "source": "#293: HMM - Ungraded Exercise part c. Hello, I wonder how could we get sum_z P(u, v, z) in this specific case? What other z tokens can it take except \"flies\"? Thanks in advance!",
        "target": "In principle, the z can range over all words in the vocabulary. Some words may not be possible at all in the context u, v thought.  Note that the computation for P(START time flies) is just an example. You would have to do this for all trigrams that the model could generate. "
    },
    {
        "source": "#292: Katz's Backoff. Hi, I'm looking at the formula for Katz' backoff for bigram models. and I was wondering if there was a typo in the formula? should it be SUM v: count(v, w) Pmle(v) in the denominator of the backoff? I don't understand what the variable u is here.",
        "target": "In my opinion, the variable 'u' here is different from 'w', it is used to describe the condition : 'count(v, u) = 0'. P_mle is used to describe the viewed variable 'w' and 'u' is the hidden(not viewed) variable."
    },
    {
        "source": "#291: Perplexity Formula. Do we include the counts for the start and end markers when determining M for perplexity calculations using the formula given in the lecture notes? ",
        "target": "Correct me if I'm wrong, but I believe you count the end markers and not the start markers (at least this is what I remember doing in the homework)."
    },
    {
        "source": "#291: Perplexity Formula. Do we include the counts for the start and end markers when determining M for perplexity calculations using the formula given in the lecture notes? ",
        "target": "I agree with Alexandra, the logic was that we count everything that can be generated by our model. In our ngram model, the START token were never generated therefore we don't include it in the perplexity calculations. "
    },
    {
        "source": "#291: Perplexity Formula. Do we include the counts for the start and end markers when determining M for perplexity calculations using the formula given in the lecture notes? ",
        "target": "The student answers are correct. You count the end markers but not the start markers. "
    },
    {
        "source": "#289: Clarification on ungraded exercise: Linear Models and Feature Functions. Hi there, I am a bit confused about the answer to the feature functions to this question. It defines eight feature functions, does it mean that for each:\\left(w_{i-1}, \\ w_i, \\ t_{i-1}, \\ t_i\\right), should we compute all the eight feature functions for the single input? If this is the case, for the input (time, flies, N, V) from sentence 1, the resulting feature vector would be (0, 0, 0, 0, 0, 0, 0, 1), which only has one dimension to be 1. So I am a bit confused about what the \"specifying the conditions under which each dimension of the feature vector is 1. \" in the question means. Thanks a lot in advance!",
        "target": "Hello! Obligatory not a TA but was just thinking through the same questionthis is the formula from the ungraded exercise, plus a few details I added when I was going through it to make it clearer for myself. We have a set of possible labels T = {N, V, P}, a d-dimensional vector of weights v (d being 8 for the feature function here), and our feature function converts its inputs w_i, w_i-1, t_i-1, and t_i to a d-dimensional feature vector. t_i^*=\\arg\\max_{t_i\\in T}\\sum_{j=1}^d\\Phi_j(w_{i-1}, \\ w_i, \\ t_{i-1}, \\ t_i)\\cdot v_jSo the argmax is basically a for loop for t_i in T , the sum is basically sum(dimension for dimension in feature_function), so for each context, we consider each label, then sum up the result of applying each dimension of the feature function on it.  Basically, we sum up all the weighted values from each dimension, so the more dimensions that are set to one, the higher our unweighted chance to pick that label. Based on the way the individual functions that build up our feature function are written, (time, flies, N, N) and (time, flies, N, P) would both be (0, 0, 0, 0, 0, 0, 0, 0). If we sum up the values from each dimension, we get 1 when we try V and 0 when we try N or P, so our model picks t_i* = V. This ended up being a bit long winded but hope that helps! And teaching staff please correct anything I got wrong here. "
    },
    {
        "source": "#287: Question about definition of token. In the lecture 3 slides, a token is defined as \"total number of word occurrences\", but during lecture, when we went over the exam material, I believe it was defined as \"an instance of word in particular context\". To clarify, is a token a number or an instance of a word?",
        "target": "I believe usually when we refer to tokens we are referring to the instance of a word in a particular context. If you look at the definition of \"tokenization\" presented in lecture three, it is the process of segmenting text (a sequence of characters) into a sequence of tokens (words). From my understanding, the number of tokens is the number of unique words in a particular context. "
    },
    {
        "source": "#286: Homework 2 Submission. I did assignment 2 this week and submitted it on Monday. Now when I check canvas, it's listed as a past assignment and doesn't show that I submitted. I was wondering if someone could check Courseworks from the instructor side and verify that you can see my submission? My name is Sean Harrison and my uni is sph2146I really appreciate your help!",
        "target": "Hi Sean, I can see your submission on Courseworks. Daniel "
    },
    {
        "source": "#285: Incomplete Answer for Linear models Ungraded Exercises. Dear Professor and TA, may I ask if there is a full version of the Linear Model exercise answer? I cannot see the answer for b) except the first line, and all for c) is missing. Many thanks!",
        "target": "The answer for b) is just the weight vector itself. For c) I dont have a written up answer, and I didnt have time to write it up today. However, its not critical in terms of exam prep. "
    },
    {
        "source": "#284: N-gram. I was doing n-gram ungraded exercise. I was wondering why vocabulary size of a bigram is all the types and END. Why is END included and not START? Why don't we calculate the vocabulary size by the number of every bigram seen, but instead look at individual words? ",
        "target": "professor has answered a similar question, just see #51 "
    },
    {
        "source": "#284: N-gram. I was doing n-gram ungraded exercise. I was wondering why vocabulary size of a bigram is all the types and END. Why is END included and not START? Why don't we calculate the vocabulary size by the number of every bigram seen, but instead look at individual words? ",
        "target": "Hi, I am not a TA but I believe that END is included because you will need to predict and include END in order to mark the sentence boundaries. For instance, consider the sentence \"time flies like flies END\". You will actually have to deliberately make the decision of END in that sentence. As opposed to START which might not be necessarily needed because you are looking at sets of words. As for your second question, I believe that we look at individual words + END because the individual words could result in more bigrams than what is seen Blike pairings and stuff. Bigrams are from individual words."
    },
    {
        "source": "#283: ungraded viterbi question. Hi I hope you are doing great. part b states:Use the Forward algorithm to compute the probability of the surface sequence \"time flies like leaves\". I get how that works in general, but the table ended with this:[4, N] = [3, V] P(N|V)  P(leaves|N) + [3, Prep]  P(N|Prep)  P(leaves|N)) = (0. 004918  2/3  1/4), (0. 021267  1  1/4 )) = 0. 00614[4, V] = [3, V] P(V|V)  P(leaves|V) + [3, Prep]  P(V|Prep)  P(leaves|V) = 0 // because of the 0 transition probabilitiesso if 4, V was a possibility, we would just pick whichever had the higher probability between [4, N] and [4, V] as our final answer, right? Thank you!",
        "target": "Hi, Opps, I thought you were talking about the part a Virterbi algo. where you would pick the higher probability between [4, N] and [4, V] if they were both non-zero. This is because you want to select the state of the position of the sequence that has the most likelihood. In this context, there is no transition probability for leaves to be a verb which means it is most likely a noun in that contextI think you can look from slide 37 onwards in the slide deck lecture05-hmm_pos_tagging. pdf for more information, or you can look on page 4 of this page: https://web. stanford. edu/~jurafsky/slp3/A. pdf"
    },
    {
        "source": "#283: ungraded viterbi question. Hi I hope you are doing great. part b states:Use the Forward algorithm to compute the probability of the surface sequence \"time flies like leaves\". I get how that works in general, but the table ended with this:[4, N] = [3, V] P(N|V)  P(leaves|N) + [3, Prep]  P(N|Prep)  P(leaves|N)) = (0. 004918  2/3  1/4), (0. 021267  1  1/4 )) = 0. 00614[4, V] = [3, V] P(V|V)  P(leaves|V) + [3, Prep]  P(V|Prep)  P(leaves|V) = 0 // because of the 0 transition probabilitiesso if 4, V was a possibility, we would just pick whichever had the higher probability between [4, N] and [4, V] as our final answer, right? Thank you!",
        "target": "I might be mistaken, but I think you actually sum them instead of taking the max, at least for the forward algorithm. As seen in the forward algorithm, you loop through each word in the sentence, summing probabilities of each incoming sequence of POS, until reaching the final word. However, after the loop, you can see that we return the sum of the probabilities. \\sum_s^{ }\\pi\\left[n, s\\right]"
    },
    {
        "source": "#280: Feature functions, feature vector. I'm having some trouble understanding the solution for ungraded exercise #6. What's the logic behind specifying conditions under which each dimension of the feature vector is 1?",
        "target": "From my understanding, feature value = 1 when the feature is true for all training data in the corpus, and 0 otherwise. For example, for all training data, start are always followed by N. Therefore, when we set feature factor N follow start, the feature function will = 1. "
    },
    {
        "source": "#279: arc-eager transition problem. Hi, I am a bit confused about using \"reduce\" in arc-eager transition, are we only allowed to use it  at the end or we can use it anytime once we want to drop a token in the stack?",
        "target": "It can be used anytime (but reducing too early may lead to incorrect results / bad parse trees, of course). "
    },
    {
        "source": "#278: Exam Requirement clarification. Do we need to memorize the formule of the partial derivative of the Log Likelihood to prepare for the mid-term exam? Thank you.",
        "target": "No calculus will be required on the exam. You certainly don't have to know the formula for the partial derivatives of the log likelihood function. You should know the formulas for ngram probabilities, smoothing, HMMs, etc. My advise would be to focus on how these models work, rather than specifically memorizing the formulas. You should then be able to reconstruct the formulas if you need them. "
    },
    {
        "source": "#277: Ungraded Exercise - Show arc-standard sequences are not unique. Does anyone know the transitions different than the one on questino a? I have been thinking for a long time about the solution to this question. Is there any other arc-standrard transitions that can generate the same dependency tree?",
        "target": "It's very possible that the dependency tree in part a) has a unique transition sequence. Try to construct a minimal example for which there are multiple sequences. "
    },
    {
        "source": "#276: Derivation vs Parse Trees. Just to double check, are derivation trees and parse trees  essentially the same thing? ",
        "target": "Essentially :) The derivation tree specifies the order of production application. The nodes in the derivation tree (including the terminals) correspond to specific productions. In the parse tree, the internal nodes correspond to phrase labels, and the terminal nodes correspond to  tokens in the sentence. Because each production introduces a nonterminal (phrase label) derivation trees and parse trees are essentially identical (except for the terminals, which the derivation tree does not have). "
    },
    {
        "source": "#275: calculator in exam 1. It states that \"You should bring a calculator\" on the exam 1 topics. How necessary is this? Are we going to answer some hard calculation question?",
        "target": "In lecture he mentioned it's helpful for computing the probabilities to arrive at an answer. I believe he also mentioned something along the lines of, if you don't have access to a calculator you would need to write out the calculations to show you would otherwise know how to do the multiplication if you had a calculator."
    },
    {
        "source": "#273: Confusion about lecture 11 Log-Linear Models as Neural Networks. Hi there, In lecture 11 slide 2, it describes that the log-linear models can be considered as a neural network with one hidden layer. But it says the input of the neural network is:X\\ =\\ \\left[\\phi\\left(x, y\\right)_1, \\phi\\left(x, y\\right)_2, \\ . .. ,\\ \\phi\\left(x, y\\right)_d\\right]I am not sure why the input is this form. I can understand that for the log-linear model, we take the original input and each potential class label y  as the inputs to the feature function and generate a new d-dimensional input. But in the neural network scenario, since the output contains the probabilities for each class label y , so why do we still need to take the output of the feature function as the input of the neural network? Or does it mean that we should input each class label y  to the feature function and generate a corresponding feature vector to the input of the neural network? I might make the question a bit hard to understand, sorry about that, and I attached a screenshot of the slide as well. ",
        "target": "Thanks, this is not a great slide and potentially misleading. The analogy is a bit more loose than the slide makes it appear. In the log-linear model, we have the same weight vector for all classes. In the neural network, you can have different weights for each output unit. Ordinarily in a neural network, you would not use the feature function approach. "
    },
    {
        "source": "#272: Projectivity for Transition Based Parsing. Hi, Why can transition based parsing with arc-standard or arc-eager only produce projective dependency structures? Thank you! ",
        "target": "I don't know the details specifically but the intuition I observed is that these algos essentially looks at stuff on your immediate left and right and form dependency between them (this is why we have Maxent tagger function, which can establish dependency anywhere in the setence). For example, normally you have NP (5-&gt;7) -&gt; N (5-&gt;6), VP (6-&gt;7); this means that NP (which spans word_5 -&gt; word_7) can be broken down into N and VP which spans word 6 and 7 respectively. What a non-projective algo would do is that it will still see NP but now NP is (5-&gt;8), and NP is made up of N(5-&gt;6) and VP (7-&gt;8). There would be sth else in between them (6-&gt;7) but the algo somehow skipped over that to connect N and VP together into NP despite them being separated by sth in the middle. In your questions, arc-standard and arc-eager only looks at the immediate left/right neighbor, they can't \"skip\" a word like I mention =&gt; they are projective. Lmk if this helps"
    },
    {
        "source": "#271: Question about regularization. I understand that the idea behind regularization is to subtact spikey parameter vectors from the log likelihood function, but can someone explain why we are multiplying by lamba/2 (what is lambda here? )",
        "target": "objective_function = min(loss) + lambda * max(regularization)lambda is one of those common parameters along with the learning rate alpha that you tuned usually via cross-validation. it balances the trade off between minimizing loss and maximizing the margin. using a boundary maximization model like SVM as example, the purpose of a high lambda value put more emphasis on the margin boundaries in an attempt to make the model more generalizable to unseen data. could however potentially lead to more errors if it is too large. conversely, a small lambda value will de-emphasize the regularization (thereby the margin-boundaries), thus more emphasis on the loss-function, which could make the model overfits to the training data. less generalizable to unseen data. lambda / 2 could just be it will simplify the derivative of the regularization term. when differentiating theta^2 using the power rule, the 2 being pull down will nicely cancel off with the 1/2, thereby leaving the derivative of a clean lambda * theta for the purpose of computing the gradient descent."
    },
    {
        "source": "#270: bar notation. Hi I hope you are doing great. In a previous post about dependcy parcing an answer was:The bar | separates the first or last symbol from the remainder of the stack or buffer. w  stands for a buffer whose first token is w (read left-to-right) and w stands for a stack with top (the top is conventionally shown on the right). Is it just convention that the first element of the buffer is on the left of the bar. while the first element of the stack is on the right of bar?  So, they will always be depicted this way? Since they both operate as stacks, I am a little confesued as to why they are depicted differentlyThank you so much!",
        "target": "Yes, thats the convention. For the buffer, it makes sense to have the first word on the left, as the buffer initially contains all tokens in the sentence from left to right. For the stack, I think the idea is that you can easily visualize the shift operation if the top of the stack is on the right, and the tokens involved in a dependency edge will be adjancent. "
    },
    {
        "source": "#269: N-gram Vocab Clarification. I was reviewing the lecture 4 notes for ngram language models and have a couple of questions regarding the terminology. 1. When we used N for the number of tokens, are tokens just the unique set of all unigrams excluding START? 2. For the tokens, do we include STOP, and why is it that we choose to exclude start/stop/both? 3. When we are talking about the size of the lexicon, are we referring to the number of tokens or the total number of words present? 4. When we did additive smoothing, for constant V (size of vocabularly), is that just the total number of words in the file? Thank you for the clarifications!",
        "target": "I could be wrong on these, so take them with a grain of salt. 1. N is the total number of words in the training data, not unique words but the total number of words. 2. I believe you include STOP and exclude START.  See #51. 3. The size of the lexicon is the number of unique words (or types--the number of distinct words). 4. V is the number of types (unique words) in the corpus.  Since you are adding one to each count in the numerator in Laplacian smoothing (over-counting every word by 1), you need to do the same for the denominator, which is why you add the size of the lexicon."
    },
    {
        "source": "#268: Question Regarding Overfitting. Hi, I just had a question about overfitting - the definition/when it occurs wasn't really clear to me from the lecture/slides, so i was wondering if someone could explain it. Thanks!",
        "target": "Overfitting means when your model fits too well that it loses generalization ability. Say you have a bunch of (x, y)s and you want to fit a model to predict y from x. You chose a very complicated model with tons of parameters and it worked very well: it reached 100% accuracy rate on this training set of (x, y). Now you receive another set of (x_new, y_new) and you are using it as your test set, your model is likely not going to do well on this new set of data because it's very specially designed for (x, y). This problem is overfiting: the model tried so hard on the training dataset that it can't generalize to other circumstances. Let me know if this make sense. "
    },
    {
        "source": "#266: Exam Content: Lexical Semantics. Will any of the content from Wednesday's lecture on Lexical Semantics be on the midterm? I don't see it explicitly mentioned on the topics sheet, but just wanted to confirm since we did cover the material in class.",
        "target": "The last topic covered on the midterm will be lecture 11. "
    },
    {
        "source": "#265: Ungraded Exercise: n-gram language models. The size of the vocabulary (excluding the [START] and [END] markers) is 6. Was wondering why the denominator of the laplace bigram probability has 7 in it? Why is the [END] marker considered in the count of the vocabulary?",
        "target": "The probability distribution needs to assign a probability over all possible events so that they sum to 1. 0. The possible events are the possible next words given the left context. END is one of these possibilities, so it should receive some probability. Essentially we are treating it as just another possible event. "
    },
    {
        "source": "#264: Ungraded Exercise: Linear Models and Feature Functions part b. Hi, I do not fully understand why (1, 1, 1, 1, 1, 1, 1, 1) is the answer for part bThank you",
        "target": "The features are designed in such a way that only features with the correct tag will be activated for any of the tokens in the data. So there is never any ambiguity between features that would encoder a preference for different tags. "
    },
    {
        "source": "#263: HMM as language models advantages over an n-gram model. Hi there, I am reviewing the lecture notes and lecture 05 page 40, asks us what the advantage of using HMM as a language model over the plain word n-gram model. I am actually not sure about the answer to this question, any hints would be very appreciated. Thanks a lot in advance.",
        "target": "HMMs explicitly model a limited amount of syntactic structure (verbs typically follow nouns, etc. ). HMMs dont assume direct dependencies between words which makes the model generalize better. For example, it may have never seen the sequence the green cheese during training, but knows that green is an adjective and cheese is a noun. During training, the model has learned that nouns can follow adjectives, so this sentence should receive a high probability. "
    },
    {
        "source": "#261: Ungraded Exercises And Midterm Question. Hi TAs and Professor, It seems that the part b and part c in the solutions of 'Linear models and feature functions' are missing, would you please provide a full edition? And I'm wondering whether there will be a piece of exam paper for reference, thank you! Sincerely, Yuning",
        "target": "What do you mean by a \"piece of exam paper\"? I don't know if i will be able to provide answers to the last part by tonight, unfortunately. Sorry I missed this one. "
    },
    {
        "source": "#260: Ungraded Exercise 1: Naive Bayes. Hello, I hope you are all doing well! I had a question regarding the probability calculations for Naive Bayes. In the exercise, I noticed that the probabilities don't divide by P(B), and I was wondering why that was the case? For example, this calculation (P(class=ham, Nigeria) = P(class=ham) * P(Nigeria | class=ham) = 2/5 * 1/7 = 2/35) doesn't divide by P(Nigeria). Should we always do this in Naive Bayes? How should we approach problems like this in the midterm? Best, Aishwarya",
        "target": "Because we are usually only interested in selecting one of the classes, not computing the exact conditional probability, its sufficient to compute the joint probability. The denominator for computing the conditional probability is the same for all classes, so you can think of this as a constant factor (in the slides, this was called alpha). "
    },
    {
        "source": "#259: Midterm Exam Time and Position. Hi Professor, I'm wondering whether can I take the exam on zoom? And what is the time for us to finish the exam? Do I need to open the camera or download any software to monitor my exam? Thanks! Sincerely, Yuning",
        "target": "In general, all regular (non-CVN) students are expected to take the exam in-person. If you need an exception, for example because you are sick, please reach out to me by email. The exam will be in 309 Havemeyer at 1:10pm and will take 65min. "
    },
    {
        "source": "#257: midterm position. want to make sure if the exam place is in the 309 HAVEMEYER ",
        "target": "Yes correct. "
    },
    {
        "source": "#256: Part d of Ungraded Exercise - HMM. Hello everyone, For the solution to part d of the question as in the title, \"s\" is a bit unclear in this context. So I added an extra line to the solution like the below, please correct me if I'm wrong:Input: An input sequence w, consisting of tokens w[1] . .. w[n]\n//initialization\nInitialize pi[0, start] = 1\nFor each tag t in T, initialize pi[0, t] = 0\n\n//main loop\nfor k = 1 . .. n:\n    for each tag t in T:\n      for each tag s in T:\n        if pi[k-1, s] &gt; 0 and  P(t|s) * P(w[k] | t) &gt; 0:\n          pi[k, t] += pi[k-1][s]   \nreturn sum_s pi[n, s]\nThank you :)",
        "target": "Yes, you're right. This looks more complete. "
    },
    {
        "source": "#253: Ungraded Exercise Question: Transition-based dependency parsing. Hi, I have few questions of this ungraded assignment:1). For question a, do we must follow the format write in the answer, or it's okay that we write in other way, i. e. : {shift, left-arc_{nsubj}, shift, right-arc_{jobj}, shift, shift, shift, . .. .}, or just draw the graph like it shows in lecturenote. 2) For question a again, since it's an \"arc-standard\" transition, so if I remembered correctly, we should finish left-arc then use right-arc, but in this example, we do use a right-arc before we finish left-arc, does that mean it's not a strict rule? 3) For question b, can we use \"arc-eager\" transition or we still have to use \"arc-standard\" transition? If we have to use \"arc-standard\" transition, I got no idea how to do write a different one. Thanks in advance! !!",
        "target": "1) as long as you write down the required sequence of transitions, that would be okay on the exam. But note that its difficult to give partial credit if we dont have intermediate dependency structures (drawing the graph for those is fine as long as the transitions are also listed).  2) thats not a strict rule  in fact this relates to your question about part b). In some cases you can reach the correct final dependency tree by creating the left and right children in either order. 3) for part b) you should still use arc standard. I dont remember if the example sentence in a) can be created in different ways, but you can try to come up with a new (minimal) dependency structure that has an ambiguous transition sequence. "
    },
    {
        "source": "#252: Linear Model and Feature Functions Questions for Review and Ungraded Exercise. Weight bias didnt understand when the professor was explaining the difference between when setting j = 0 to d vs 1 to d? For feature function: Input: n-dimensional vector where x is an input object and y is a possible outputOutput: n-dimensional vector feeds into another indicator function, where value is 0 or 1? Ungraded exercise: Can you please upload the solution for the last portion \"More advanced\"? For (a), is the answer here pretty open-ended, for instance, if I were to add a condition where wi = Fruit, Ti =N, Ti-1 = V, it would be correct as well? For (b), the vectors are all 1s because all the conditions are true based on the example? Thank you! ",
        "target": "1. Often, linear models are formulate like this: (\\sum_{i=1}^dx_iw_i) + bWhere b is a bias. Instead, it's convenient to just set w_0 = b and then add x_0 =1, so you get \\sum_{i=0}^dx_iw_i = (\\sum_{i=1}^d x_i w_i) + 1 \\cdot w_02. Not sure what you mean here. The feature function maps an (x, y) pair to a feature vector. x is the original input object, and y is one possible prediction candidate. The value of the feature function is a vector. The individual values are typically (but not always) binary indicator features. 3. Not sure I will get to the writeup for the last part before the exam, but it's not really relevant. For a) and b) Correct, there are many different feature functions. The weight vector can be all 1 because only features for the correct output (POS) will fire."
    },
    {
        "source": "#251: Transition-based Parsing Questions for review. Why does the divide line mean? e. g. $(\\sigma, w_i | \\beta, A) =&gt; (\\sigma | w_i, \\beta, A)$? Why during the arc left operation word deleted from the stack side but not the buffer side? Why during the arc right operation, $w_i$ moved from top of the stack to the buffer? What is r and w in this case? difference between Ad and A? does it has something to do with not moving root to the right until everythings been parsed essentially? does it mean this rule doesnt apply to the arc eager example? What is lambda in the score equation for the graph-based approach?",
        "target": "1. The bar | separates the first or last symbol from the remainder of the stack or buffer. w  stands for a buffer whose first token is w (read left-to-right) and w stands for a stack with top (the top is conventionally shown on the right). 2. You only delete the dependent but not the head! If you delete the head, then neither of the tokens would be able to participate in further dependency relations. You would never be able to create more than one edge. 3. The reason the head shifts back to the buffer is that there is no other operation to shift it back.  You dont yet know if you may want to attach additional children to the right or to the left of that token, or if you want the token to receive a head to the left or to the right. You can always shift it back to the stack, but you can only move it to the buffer using the right-arc operation. 4. A is the partially built dependency structure that is part of this state. Ad is the gold dependency tree. w ans r are variables referring to the different tokens and relation labels in Ad. What the condition says is that, before you are allowed to do a right-arc with beta[0] as a dependent, all tokens w that are dependents of beta[0] in the gold tree Ad must have already been introduced in A. 5. Lambda is a scoring function computed by some machine learning model, for example a log linear model or a neural net. "
    },
    {
        "source": "#250: Typo in slide of Arc-right. Hi! I'd like to make sure that there is a typo in the following slide from lecture 8:Arc-right should build an edge from the top word on the stack to the next word on the buffer; while the arc-left builds an edge from the next word on the buffer to the top word on the stack. Thank you!",
        "target": "Yes, good catch. Thank you. "
    },
    {
        "source": "#249: CVN Instructions for Midterm. Hello, I hope you are doing well. When will the CVN instructions for the midterm be released? I am mostly curious about the time frame we have to take the exam within. Most classes I have taken so far provide a 48 hour time period within which we need to take the exam via Proctorio (~ same amount of time as in person students). Is it the same for this course? Knowing this will be immensely helpful in planning the next couple of days out. Thanks in advance! ",
        "target": "I will send out an email in a few minutes. You have the option of either taking the exam synchronously on Zoom, or using the CVN Proctorio system (like you have done in the past). For the asynchronous option, the exam will be open until Wednesday night. "
    },
    {
        "source": "#247: Logit Function. Hi, I had a question about the relationship between the logit and sigmoid function presented in lecture. I am a bit confused as to why we are able to interpret z\\ =\\ \\sum_{ }^{ }x\\cdot was the log-odds. I understand how logit and sigmoid are inverses of each other, however this fact seemed like it is making an assumption. Thanks!",
        "target": "Yes, we are assuming that the log-odds are a linear function of the x values. Logistic regression is sort of the simplest model you can assume if you want the output to be a probability, but there are other options. "
    },
    {
        "source": "#245: Ungraded HW dependency parsing question b. Question b asked us to show that the arc standard sequences are not unique, and there is no answer to this question on the posted solution page. Could you explain the way to answer this question? Thank you!",
        "target": "You would have to provide two different transition sequences resulting in the same dependency tree. "
    },
    {
        "source": "#245: Ungraded HW dependency parsing question b. Question b asked us to show that the arc standard sequences are not unique, and there is no answer to this question on the posted solution page. Could you explain the way to answer this question? Thank you!",
        "target": "If it helps, looking specifically at transitions we can do to build the relations between he, sent and sent, her made things clearer for me."
    },
    {
        "source": "#244: Ungraded Exercise HMM. In this question, Why do we need to compute the conditional probabilities?",
        "target": "Because the parameters of the trigram model are specified as conditional probabilities. In other words, you want P(flies | START, time) instead of P(START time flies). "
    },
    {
        "source": "#243: Question on Ungraded Exercise: CKY parsing. Hi NLP teaching staffs - I am wondering if the order of the right hand side matter here. For example:S -&gt; NP VVP -&gt; V NPWe record S -&gt; NP (cell 1-2) and V (cell 2-3) in the solution, however we do not record rule VP -&gt; V (cell 2 - 3) NP (cell - 2). Just want to make sure that VP -&gt; V NP # VP -&gt; NP V. Thank you,",
        "target": "Yes, the order matters! Otherwise, you would be able to swap the left and right constituent and that would give you \"ungrammatical\" sentences. Instead of \"the dog eats the bone\" you would be able to do things like \"the dog the bone eats\". "
    },
    {
        "source": "#242: COVID during Exam 1. Hi Professor and TAs, I just tested positive for COVID and have been running a fever and experiencing symptoms since yesterday. I am not sure if I will be able to take Monday's exam in-person due to the isolation protocol and my current symptoms - I was wondering how or when I can take the exam in this case? I apologize if this is the wrong place for this question, and I am happy to follow up through email and copy my class dean/advisor and provide a doctor's note if needed. Thank you for any help.",
        "target": "Hi sorry for the slow response. Typically email is better for questions like this one. If you feel well enough to take the exam, you can take it on Zoom. Otherwise, we can try to schedule a make-up exam once you have recovered. "
    },
    {
        "source": "#241: Midterm Topics. In Monday's lecture, Professor went through a list of topics to know for the midterm but I wasn't able to find the file. Where can I find that document?",
        "target": "This is listed under the Pages section in Courseworks."
    },
    {
        "source": "#239: HW2 Return Type. Hi, I just realized that in my verify_grammar code for part 1 of the coding assignment, I return 2 things - a boolean true or false and a sentence that pinpoints exactly what causes the PCFG to not be in CNF because the assignment said to print out an error or confirmation message. Depending on how this code will be tested, would it be better to resubmit and take the 20 point deduction (same worth as the part) or is it possible to get partial credit on this? Thanks!",
        "target": "This is certainly not worth a 20 point deduction. I dont recommend resubmitting."
    },
    {
        "source": "#238: hw2 submission. Hi, I made two submissions for hw2 as I realized that I didn't upload the most updated zip file for my first submission, which was on time. The second submission was 40 min late. I made some minor updates to my codes and did more testing in my second submission, but there shouldn't be that much of a difference. Is it OK to grade whichever submission that gives me the most points? If late penalty was gonna apply to my second submission, could you grade my first submission instead? Thanks!",
        "target": "No worries. Thats well within the grace period. "
    },
    {
        "source": "#237: Question on N-gram ungraded excercises? . Hi NLP teaching staffs - I am wondering why END symbol is included in the calculation of V, but not START sumbol? Tysm! !!",
        "target": "The START symbol is never predicted by the model and should therefore not have a probability. Its therefore not a word in the lexicon. In other words, the START symbol is not one of the possibilities for the next word that the model would consider. "
    },
    {
        "source": "#237: Question on N-gram ungraded excercises? . Hi NLP teaching staffs - I am wondering why END symbol is included in the calculation of V, but not START sumbol? Tysm! !!",
        "target": "I would also like to share that It is essential for the model to predict END symbol too, because the model must know how significant it is for END token to occur with other tokens (certain tokens, such as . (Full stop)).  Thats why it can be considered just as any other token, as it helps with the predictions. "
    },
    {
        "source": "#236: Syntax Error when Evaluating Parser. Hello NLP Teaching Team. I am encountering this SyntaxError when I attempt to evaluate my parser, although am unsure why it is incorrect? I am using Spyder (Python 3. 10).",
        "target": "This happens because you are running the command in the Python console, not the terminal. One way of running it from within Python is to use! python evaluate_parser. py . .. The ! runs a shell command in the Python console. "
    },
    {
        "source": "#235: Submission Format. Hi, Sorry, just to check, should we submit the assignment as 3 separate zip files, or put them all into a folder and then compress the entire folder? Thanks in advance! ",
        "target": "I believe we are to take those 3 files and put them into a singular . zip folder:\"Pack these files together in a zip or tgz file as described on top of this page. Please follow the submission instructions precisely! \""
    },
    {
        "source": "#235: Submission Format. Hi, Sorry, just to check, should we submit the assignment as 3 separate zip files, or put them all into a folder and then compress the entire folder? Thanks in advance! ",
        "target": "Put them all in a folder and then zip and submit the folder. "
    },
    {
        "source": "#233: Tips on Error Checking Grammars. Hi everyone, I see the assignment says to try and create small toy grammars to error check with and I was just wondering if there are any good examples or tips when it comes to making these grammars (i. e. covering all the potential edge cases). Thank you!",
        "target": "Not sure about edge cases, but you want some limited amount of ambiguity in the grammar for your test sentence. Three example from the lecture notes is a good starting point. "
    },
    {
        "source": "#232: HW2: Tricky Bug on p3, p4 - Couldn't figure out with one of the TAs.     def parse_with_backpointers(self, tokens):\n        \"\"\"\n        Parse the input tokens and return a parse table and a probability table.\n        \"\"\"\n        # TODO, part 3\n        n = len(tokens)\n\n        # Create parse table and probability table as dictionaries\n        table = {}\n        probs = {}\n\n        # Initialize the tables\n        for i in range(n):\n            for j in range(i, n):\n                table[(i, j)] = {}\n                probs[(i, j)] = {}\n\n        # Fill in the diagonal of the parse table with nonterminals that can produce terminals\n        for i in range(n):\n            word = tokens[i]\n            for lhs, rhs, prob in self. grammar. rhs_to_rules. get((word, ), []):\n                table[(i, i)][lhs] = word\n                probs[(i, i)][lhs] = math. log(prob)\n\n        # Fill in the upper triangle of the parse table\n        for length in range(2, n + 1):\n            for i in range(n - length + 1):\n                j = i + length -1\n#                 j = i +length\n                for k in range(i, j):# i\n                    for lhs1 in table[(i, k)]:\n                        for lhs2 in table[(k +1, j)]: #k+1\n                            # Check if there are rules that can combine lhs1 and lhs2\n                            for lhs, rhs, prob in self. grammar. rhs_to_rules. get((lhs1, lhs2), []):\n                                new_prob = math. log(prob) + probs[(i, k)][lhs1] + probs[(k+1 , j)][lhs2] #k+1\n                                if lhs not in table[(i, j)] or new_prob &gt; probs[(i, j)][lhs]:\n                                    table[(i, j)][lhs] = ((lhs1, i, k), (lhs2, k+1 , j)) #k+1\n                                    probs[(i, j)][lhs] = new_prob\n        \n        return table, probs\n\ndef get_tree(chart, i, j, nt): \n    \"\"\"\n    Return the parse-tree rooted in non-terminal nt and covering span i, j.\n    \"\"\"\n    # Check if the span is valid and if the non-terminal exists in the chart\n    if (i, j) not in chart:\n        print(f\"Chart at ({i}, {j}) does not exist. \")\n        return None\n\n    if nt not in chart[(i, j)]:\n        print(f\"Chart at ({i}, {j}) does not contain non-terminal {nt}\")\n        return None\n\n    # Retrieve the backpointer for the non-terminal\n    backpointer = chart[(i, j - 1)][nt]\n\n    # If the backpointer is a string (terminal), return it as a terminal node\n    if isinstance(backpointer, str):\n        return (nt, backpointer)\n\n    # If the backpointer is a pair of backpointers, recursively construct the tree\n    left_child, right_child = backpointer\n    left_tree = get_tree(chart, left_child[1], left_child[2], left_child[0])\n    right_tree = get_tree(chart, right_child[1], right_child[2], right_child[0])\n\n    # Return a tuple representing the parse tree\n    return (nt, left_tree, right_tree)\n \n \n for the backpointers function, I'm getting \n \n True for both check table and check probs \n \n output: \n ({(0, 0): {'NP': 'miami'},\n  (0, 1): {'NP': (('NP', 0, 0), ('FLIGHTS', 1, 1)),\n   'FRAG': (('NP', 0, 0), ('NP', 1, 1)),\n   'FRAGBAR': (('NP', 0, 0), ('NP', 1, 1)),\n   'SQBAR': (('NP', 0, 0), ('NP', 1, 1)),\n   'VPBAR': (('NP', 0, 0), ('NP', 1, 1))},\n  (0, 2): {'FRAG': (('NP', 0, 0), ('FRAGBAR', 1, 2)),\n   'FRAGBAR': (('NP', 0, 0), ('FRAGBAR', 1, 2)),\n   'NP': (('NP', 0, 1), ('NP', 2, 2)),\n   'SQBAR': (('NP', 0, 0), ('SQBAR', 1, 2)),\n   'VPBAR': (('NP', 0, 0), ('VPBAR', 1, 2))},\n  (0, 3): {'FRAG': (('NP', 0, 0), ('FRAGBAR', 1, 3)),\n   'FRAGBAR': (('NP', 0, 0), ('FRAGBAR', 1, 3)),\n   'NP': (('NP', 0, 1), ('NP', 2, 3)),\n   'SQBAR': (('NP', 0, 0), ('SQBAR', 1, 3)),\n   'VPBAR': (('NP', 0, 0), ('VPBAR', 1, 3))},\n  (0, 4): {},\n  (0, 5): {},\n  (1, 1): {'FLIGHTS': 'flights', 'NP': 'flights'},\n  (1, 2): {'FRAG': (('NP', 1, 1), ('NP', 2, 2)),\n   'FRAGBAR': (('NP', 1, 1), ('NP', 2, 2)),\n   'NP': (('NP', 1, 1), ('NP', 2, 2)),\n   'SQBAR': (('NP', 1, 1), ('NP', 2, 2)),\n   'VPBAR': (('NP', 1, 1), ('NP', 2, 2))},\n  (1, 3): {'FRAG': (('NP', 1, 1), ('NP', 2, 3)),\n   'FRAGBAR': (('NP', 1, 1), ('NP', 2, 3)),\n   'NP': (('NP', 1, 1), ('NP', 2, 3)),\n   'SQBAR': (('NP', 1, 1), ('SQBAR', 2, 3)),\n   'VPBAR': (('NP', 1, 1), ('NP', 2, 3))},\n  (1, 4): {},\n  (1, 5): {},\n  (2, 2): {'CLEVELAND': 'cleveland', 'NP': 'cleveland'},\n  (2, 3): {'FRAG': (('NP', 2, 2), ('PP', 3, 3)),\n   'NP': (('NP', 2, 2), ('PP', 3, 3)),\n   'SQBAR': (('NP', 2, 2), ('PP', 3, 3)),\n   'VPBAR': (('NP', 2, 2), ('PP', 3, 3))},\n  (2, 4): {},\n  (2, 5): {},\n  (3, 3): {'FROM': 'from', 'PP': 'from'},\n  (3, 4): {},\n  (3, 5): {},\n  (4, 4): {'TO': 'to', 'X': 'to'},\n  (4, 5): {},\n  (5, 5): {'PUN': '. '}},\n {(0, 0): {'NP': -4. 706522680248739},\n  (0, 1): {'NP': -12. 121095561599688,\n   'FRAG': -10. 17027139774193,\n   'FRAGBAR': -10. 466537213884134,\n   'SQBAR': -10. 240986922539397,\n   'VPBAR': -8. 54766752612518},\n  (0, 2): {'FRAG': -16. 02265925150763,\n   'FRAGBAR': -16. 318925067651136,\n   'NP': -20. 683040775760592,\n   'SQBAR': -15. 549370753841544,\n   'VPBAR': -16. 030803078912392},\n  (0, 3): {'FRAG': -26. 066987069809162,\n   'FRAGBAR': -26. 36325288595267,\n   'NP': -30. 727368594062128,\n   'SQBAR': -24. 882202252912503,\n   'VPBAR': -26. 075130897213928},\n  (0, 4): {},\n  (0, 5): {},\n  (1, 1): {'FLIGHTS': 0. 0, 'NP': -3. 195065176174159},\n  (1, 2): {'FRAG': -9. 4443343943576,\n   'FRAGBAR': -9. 740600210499807,\n   'NP': -11. 757010390335061,\n   'SQBAR': -9. 515049919155068,\n   'VPBAR': -7. 82173052274085},\n  (1, 3): {'FRAG': -19. 488662212659136,\n   'FRAGBAR': -19. 78492802880134,\n   'NP': -21. 801338208636594,\n   'SQBAR': -18. 847881418226027,\n   'VPBAR': -17. 866058341042386},\n  (1, 4): {},\n  (1, 5): {},\n  (2, 2): {'CLEVELAND': 0. 0, 'NP': -3. 9805856768644103},\n  (2, 3): {'FRAG': -12. 020710341309226,\n   'NP': -14. 024913495165945,\n   'SQBAR': -14. 325018087614133,\n   'VPBAR': -13. 631870907056262},\n  (2, 4): {},\n  (2, 5): {},\n  (3, 3): {'FROM': 0. 0, 'PP': -6. 618738983514369},\n  (3, 4): {},\n  (3, 5): {},\n  (4, 4): {'TO': 0. 0, 'X': -1. 2527629684963681},\n  (4, 5): {},\n  (5, 5): {'PUN': 0. 0}})\n  \n  But for get trees, I keep getting: \n  print(get_tree(table, 0, len(toks) - 1, grammar. startsymbol))\n  Chart at (0, 5) does not contain non-terminal TOP\nNone",
        "target": "We discussed this during my office hours. Id be interested to hear what the issue was eventually. "
    },
    {
        "source": "#230: Late Policy. Hi, I just wanted to clarify what the late policy for this class is? On the syllabus it says we can submit a maximum of 4 days late with a 20 point deduction, does that mean 5 points are deducted each day, or the deduction happens as soon as it becomes late?",
        "target": "There is a one-time deduction of 20 points for any late submission. Please reach out to me if you think your circumstances merit an extension. "
    },
    {
        "source": "#229: HW 2 Part 3. I was wondering if someone could clarify why table[(2, 3)][\"FLIGHTS\"] should be flights? Using my understanding, using the hw example with tokens of ['flights', 'from', 'miami', 'to', 'cleveland', '. '] , table[0][1][\"FLIGHTS\"] would be flights but table[(2, 3)] should only have the rules that have miami  on the right side? Additionally, is there any particular way we should map the \"parent\" indices for terminals? Ex: for the case of table[(0, 1)'][\"Flights\"], what should we set this equal to? Thanks so much!",
        "target": "So, some of the examples in the assignment description exist only to illustrate the format of the table, not the specific values in a specific example. I don't quite understand what you mean by \"parent indices\". table[(0, 1)'][\"Flights\"] should be \"flights\". "
    },
    {
        "source": "#228: Will there be ungraded/ practice assignment for lecture 11? . Hi NLP teaching staffs - I am wondering if there will be ungraded/ practice assignment for lecture 11. Thank you so much!",
        "target": "Ill try to get one up, but it doesnt currently exist. "
    },
    {
        "source": "#227: HW2 P1: Just want to double-check that helper function is allowed. For checking the lhs_to_rules and rhs_to_rules, I use the same helper function to reduce the redundancy in code. I just want to double make sure that helper function is allowed for this assignment :)",
        "target": "Thats fine. "
    },
    {
        "source": "#226: What are the acceptable ranges for scores in HW2? . What are the acceptable ranges for Coverage, Average F-score (parsed sentences), Average F-score (all sentences) in part 5 of hw2? Is there any way to improve the Average F-score (parsed sentences)?",
        "target": "What is it now? There shouldnt really be too much variation on these. "
    },
    {
        "source": "#225: Part 5 F-Score. Hi, I wonder if anyone got:Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9526771952649747, Average F-score (all sentences): 0. 6405932864712761Is it considered as a acceptable result? Thank you so much!",
        "target": "Got the same, pretty sure this matches the performance of Dr. Bauers solution "
    },
    {
        "source": "#224: HW2 P3:. For assigning the inner dictionary values of span length 1, since it still needs to be a pair would I make the two instances in the pair the same since it is only based off the one word. For example, when assigning the inner dictionary values for the first word 'flights' in a sentence, the inner dictionary would look like {'flights': ((0, 1, 'NP'), (0, 1, 'NP'))}.",
        "target": "flights should never be a key in the dictionary. Instead, the dictionary might look something like this {(0, 1): {NP: flights}}"
    },
    {
        "source": "#223: Not Recognizing grammar. py File. Hi! While working on part 3, my visual studio code stopped processing any changes I made. When I closed and reopened the window, I got the below error. What should I do to fix this? Thanks!",
        "target": "Make sure grammar. py is in the same directory.  You probably done any to edit this code in your Downloads directory because thats prone to being deleted. "
    },
    {
        "source": "#222: Ungraded n-Gram Language Models exercise. Hi! Will something similar to the inductive proof in n-Gram Language Models be tested in midterm or final exam? Thanks!",
        "target": "No there will not be any proofs. "
    },
    {
        "source": "#221: CYK Algorithm Question. Hello, apologies if this was already answered somewhere. Let us consider the following situation:If there was such a rule in the form \"S --&gt; NP\", would an S be added to pi[0, 3]? Or would nothing be added, since the Cartesian product of a set with the empty set is always the empty set?",
        "target": "S --&gt; NP would not be a valid rule according to chomsky normal form."
    },
    {
        "source": "#220: F-score for part 5. Upon running evaluate_parser, I'm getting values that are somewhat lower than expected:Coverage: 62. 07%, Average F-score (parsed sentences): 0. 876237979991289, Average F-score (all sentences): 0. 5438718496497656Does anyone know where I should look to correct this?  Will I need to trace out the parses to see what's going wrong?",
        "target": "I suspect that you only keep the highest scoring nonterminal for each span, non the highest-scoring split for each nonterminal.  "
    },
    {
        "source": "#219: HW2 Q1 Start Symbol. Hi, In HW2 Q1, suppose S is the start symbol, and A, B are nonliterals, are rules such as A -&gt; SB or A -&gt; BS allowed? These rules do not seem to make sense, but they do not seem to contradict the rules of CFG.",
        "target": "These rules would be permitted. The start symbol is just another nonterminal. In fact, it's okay for the start symbol to appear on the right hand side of a rule. "
    },
    {
        "source": "#218: HW2 print all results. Hi, just want to confirm whether should we print all results of functions in HW2, like shown in the assignment example code chunks (also for part 5), or just complete the functions and essential output?",
        "target": "The intermediate results on the assignment description are just shown for illustration purposes. They do not have to be part of the output of your submission. "
    },
    {
        "source": "#216: Midterm Preparation Tips. Hi! I was wondering if the teaching team (or anyone else) had any good tips to prepare for the midterm next week? Would it be most helpful to review previous lectures or HW assignments? Are there any other resources you recommend we look at? Thanks!",
        "target": " I think ungraded exercises are helpful to practice for the exam. Also, seek advice/helpful resources."
    },
    {
        "source": "#215: Verify Grammar for HW 2. Hello! I hope you are all doing well. I had a question regarding the first part on the homework. Would zeros and punctuation be counted as terminals? For example, if I had PP-&gt; '0' or NP-&gt; '. ', would this be considered valid CNF Form? Thank you so much for your help!",
        "target": "Yes, these would be counted as terminals. Anything that never appears on the LHS of any rule is a terminal. "
    },
    {
        "source": "#213: HW2 P1: Difference from 1. 0 Question. According to the HW spec:\"Because of numeric inaccuracies the sum may not be exactly 1. 0. \"When using the isclose method, can we leave the maximum allowed difference as default, or what should be the accepted difference from 1. 0?",
        "target": "You can use the default tolerance. "
    },
    {
        "source": "#212: Ungraded Exercises Solutions. Hello, May I ask when will the solutions for ungraded exercises in dependency parsing and linear model &amp; feature functions be released? Thank you :)",
        "target": "Here is the solution for the dependency parsing problem:https://courseworks2. columbia. edu/courses/179333/pages/ungraded-exercise-solution-transition-based-dependency-parsingI will post the feature function solution early this week. Sorry for the delay. "
    },
    {
        "source": "#211: CKY for PCFG parsing. In the main loop of the algorithm for PCFG parsing, we have to iterate through all X that belongs to N and all X-&gt;BC that belongs to R. Why can't we do something similar to the original CKY algorithm where we only calculate the probabilities where span(i, k) can form B and span(k, j) can form C? Since for the ones that do not meet this criteria, the probability will be 0 anyways. ",
        "target": "Im not sure I fully understand the question. There is a difference between how the algorithm is conceptually defined in pseudocode and how it is implemented. You could implement the rule lookup by taking all nonterminals with &gt;0 probability in (i, k), and and all nonterminals with &gt;0 probability in (k, j),  then go through all combinations B C and check it there is a matching A -&gt; B C rule. "
    },
    {
        "source": "#210: Part 4 (get tree) where nt is not in the table. If get_tree is called with a non-terminal thats not in the tree (for example, calling get_tree(0, len(toks), grammar. startsymbol) on a table of a sentence that is not in the language (and therefore does not contain TOP), what should be returned? Is it an empty tuple? Thanks in advance! ",
        "target": "Correct :) the evaluation script catches the KeyError and counts the sentence as not parseable. "
    },
    {
        "source": "#209: Midterm coverage. Hi, I am wondering what is the coverage for our upcoming midterm? Which materials would be included and tested? Thank you.",
        "target": "#184"
    },
    {
        "source": "#208: Mulipying probabilities. When calculating probabilities in part 3, Lecture 7 seems to suggest multiplication of the probabilities. However, should we add the log probs instead of multiplying because they are log probs? Also, is it enough to multiply by the backpointers and assume that that will propagate all the multiplying across the probabilities?",
        "target": "Hello! Not a TA but based on my understanding, yes and yes. Because of the base case initialization, you can think of the probabilities getting multiplied (aka log probs getting added) recursively on each step."
    },
    {
        "source": "#207: Part 3, Stucture of Dictionaries for probablitities and backpointers: What does it mean to only keep the most probable expansion? . I'm a little confused by this. In question 3 we are asked to make a data structure that has a dictionary in a dictionary in order to store back pointers and probabilities. My question is about whether we can store all the back pointer possibilities or just the one best backpointer. In other words, should each entry in the dictionary be a dictionary with only one key? In the problem it is states \"This value represents the log probability of the best parse tree (according to the grammar) for the span 0, 3 that results in an NP. \" This seems to suggest as long as we store the best tree for NP mapped to NP we can still have other mappings. For instance{Np : {NP : (npsource, num, num), P: ((source, i, k), (source2, k, j)})}Or would one need to REMOVE either NP or P from this list in order to only have the biggest one? This seems to be what is suggested here, but because of the structure of the dictionary is very unintuitive to me. Why store objects in a dictionary when we would only every have one key in every dictionary? Plus most parses have 2 things they point to.",
        "target": "I think I understand the wording a bit more now. Am I correct in understanding we ONLY replace those with a better probability that line up the same coordinates AND values. For example if there is already a an NP in (0, 2) and we find an NP with a larger probability then we switch them out? But if there is an NP in (0, 2) and we found a P with a smaller probability than the NP we could keep both because they are different keys?"
    },
    {
        "source": "#206: HW2 P1: Result. Curious if the answer we should be getting is a single output stating that the grammar is not valid? Are we expected to return additional outputs?",
        "target": "The main method in part 1 simply needs to print if the grammar is valid or not. The verify_grammar method simply needs to return True or False. No other output is needed. "
    },
    {
        "source": "#205: HW2 Part 5: Coverage 0. 00%. Not a question, but here's an issue I ran into in Part 5 that will hopefully help some other people as well. For context, I ran the command provided in the instructions python evaluate_parser. py atis3. pcfg atis3_test. ptb , but was getting the following output: Coverage: 0. 00%, Average F-score (parsed sentences): 0. 950447540861, Average F-score (all sentences): 0. 639094036096\nThe 0. 00% coverage was confusing, especially since the F-scores matched those in the instructions. After looking into the code for evaluate_parser. py, I realized that the code for computing coverage coverage = (parsed / total) *100 was operating on two ints (parsed and total), which defaulted to integer division (since my python command still points Python 2). Long story short, use python3 for testing evaluate_parser. py , though the regular python command might've been updated to point to Python 3 in newer systems.",
        "target": "Thanks. Good point in general, we assume Python 3 for all assignments. "
    },
    {
        "source": "#204: HW 2 Part 3 Log Probabilities. Hello, For Part 3 of HW2, when it says to use \"log probabilities\", are we expected to just use the natural log? Or should we use log to a specific base?",
        "target": "Hello! On the previous hw we used math. log2 "
    },
    {
        "source": "#204: HW 2 Part 3 Log Probabilities. Hello, For Part 3 of HW2, when it says to use \"log probabilities\", are we expected to just use the natural log? Or should we use log to a specific base?",
        "target": "It doesnt really matter. In my implementation i just used math. log which is the natural logarithm (base e). "
    },
    {
        "source": "#203: Is \"TOP\" the equivalence for sentence in HW2? . Hello! I'm doing the assignment and I am trying to understand what 'TOP' means. In this case, is 'TOP' what we want to find in our table[0, len(tokens)] at the end of the CKY algorithm? Meaning, is 'TOP' the non-terminal that originates a sentence within the language? I'm confused because we also have 'S' as a non-terminal in the rules. Thank you!",
        "target": "TOP is the startsymbol in this particular grammar. So unless you find TOP in the span for the entire input, the sentence cannot be parsed. You should probably use grammar. startsymbol instead of hardcoding a specific startsymbol. "
    },
    {
        "source": "#202: HW3 release. Hi, Would HW3 be release this week or will be release during the week of Midterm 1? Thank you! Best, Adrian Harischand ",
        "target": "Im planning to release homework 3 toward the end of this week, after the due date for homework 2. "
    },
    {
        "source": "#200: HW2: Question about the example in Part 3. In the description of part 3, it is given that \"the value of table[(0, 3)]['NP'] could be ((\"NP\", 0, 2), (\"FLIGHTS\", 2, 3))\", and \"the table entry for table[(2, 3)][\"FLIGHTS\"] should be \"flights. \" Are these table entries for the tokens ['flights', 'from', 'miami', 'to', 'cleveland', '. ']? If we run parse_with_backpointers on the above list of tokens, should we get the same entries in the table?",
        "target": "No the table entries do not match that example sentence. Its just an illustration of what the table format should look like. "
    },
    {
        "source": "#198: HW Part 5 Evaluate Parser score check. Dear Professor, I am getting the following scores. On rounding them the scores are roughly matching. Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9504475408614075, Average F-score (all sentences): 0. 6390940360964636Just wanted to confirm if these are fine.",
        "target": "Hi, we have the same scores, refer to here: https://edstem. org/us/courses/46417/discussion/3606779? answer=8303893"
    },
    {
        "source": "#197: HW2 part 1 question. For the verify grammar problem, will there ever be a scenario where the inputted lhs has multiple words?  If so, do we just consider it as one word (since it is one string) and it is valid, or is it considered as multiple words and it is invalid?",
        "target": "No that cant happen. There will always be a single LHS symbol. "
    },
    {
        "source": "#196: Verify Grammar Questions. 1. is it okay if there are many for loops in verify grammar and running it on a bit dataset takes some time? I'm talking like 10 seconds2. In order for the grammar to be verified, all these following the only things we need to check? A) from the slides lecture 7 page 4, the node goes to either non-terminals or a terminal   (also does the amount of terminals/non-terminals matter? Should it be going to ONLY 2 non-terminals or 1 terminal? Or  could it go to 4 non-terminals or 1 non-terminal or 3 terminals? The 2 and 1 rules seems to be true for the grammar we are getting and the rules on the slides I just want to check)B) all of the probabilities when going to non-terminal nodes add up to 1if A) and B) are true can we assume this is a good grammar? Thanks",
        "target": "Hello! Not a TA but my understanding is we're checking if it's in Chomsky Normal Form, meaning every rule maps from a non-terminal to a single terminal, or from a non-terminal to two nonterminals. As far as I know, if every rule is in the form A -&gt; B C or A -&gt; b and the probabilities add up it should be good by the homework specs. Also, if all we're checking is the above conditions I'm pretty sure verify_grammar() can be O(n) for n production rules. I ran my implementation on my pretty old laptop on atis3. pcfg and it was quite a bit less than a second, maybe make sure you're using the provided dictionaries correctly?"
    },
    {
        "source": "#196: Verify Grammar Questions. 1. is it okay if there are many for loops in verify grammar and running it on a bit dataset takes some time? I'm talking like 10 seconds2. In order for the grammar to be verified, all these following the only things we need to check? A) from the slides lecture 7 page 4, the node goes to either non-terminals or a terminal   (also does the amount of terminals/non-terminals matter? Should it be going to ONLY 2 non-terminals or 1 terminal? Or  could it go to 4 non-terminals or 1 non-terminal or 3 terminals? The 2 and 1 rules seems to be true for the grammar we are getting and the rules on the slides I just want to check)B) all of the probabilities when going to non-terminal nodes add up to 1if A) and B) are true can we assume this is a good grammar? Thanks",
        "target": "Im not a TA, but heres my understanding. Chomsky Normal Form allows production rules only of the form where there are two non-terminals or one terminal on the right hand side. The left hand side is always a Non-terminal. The probabilities for each of the non-terminals should add up to 1. Sometimes they might not add up exactly to 1, so the assignment suggests using math. isclose() for verification. "
    },
    {
        "source": "#193: Getting 0. 0 for all. . Hi, I'm getting all zeros. Coverage: 0. 00%, Average F-score (parsed sentences): 0. 0, Average F-score (all sentences): 0. 0I'm trying to find what's wrong with my setting but have no clue. Does anyone have the same problem and can give a hint about which part might have gone wrong?",
        "target": "I'd look into evaluate_parser. py in the actual evaluate_parser(), and maybe add some logging? There's a chance you're throwing a keyerror or something similar every time"
    },
    {
        "source": "#193: Getting 0. 0 for all. . Hi, I'm getting all zeros. Coverage: 0. 00%, Average F-score (parsed sentences): 0. 0, Average F-score (all sentences): 0. 0I'm trying to find what's wrong with my setting but have no clue. Does anyone have the same problem and can give a hint about which part might have gone wrong?",
        "target": "Its possible to at the output format is correct. Make sure each tree returned is a nested tuple. "
    },
    {
        "source": "#192: question about #175. Is a TA able to endorse the answer to #175 please? I have the same question of can you assume all lhs are in the right format of only one nonterminal since you assume lhs make up inventory of nonterminals ",
        "target": "I just responded to that thread. The student response is correct.  "
    },
    {
        "source": "#191: HW2 F-scores for parsed and all sentences are the same. Hi, As title, I'm getting ~0. 63 as the both F-scores, and they're exactly the same. The scores for every sentence are the same too. Any idea why? Thanks!",
        "target": "I suspect that the evaluation script is not registering when a sentence cannot be parsed. It expects that in such cases, the get_tree method throws a KeyError (because the start symbol will not be in the table for the full span). It likely also accepts None for unparseable sentences, but I'm not quite sure I remember correctly. "
    },
    {
        "source": "#189: len(bps) ! = 2 table check. Hi, Correct me if I misunderstood, but according to #173, as long as our get_tree consistently works does it matter if we continue to receive the len(bps) ! = 2 error? My get_tree continues to work despite me receiving this error",
        "target": "The problem is likely in the way you represent terminal symbols in the table. The scripts expects that these are plain strings -- not, for example, single element tuples with a string in it, such as ('flights', ). I would recommend you try to fix this before submitting, but as I mentioned in #173 it doesn't really matter. If you choose a different representation for terminals, I recommend you adapt the checker function, to accept the new format. "
    },
    {
        "source": "#188: Follow up questions regarding HW1 solution. I have 3 questions regarding our hw1 solution. 1. In part 3) raw_bigram_probability function, Is the expected return value for input bigram ('START', 'START') 0? 2. In part 3) raw_bigram_probability function, why are we returning bigramcounts/total#ofwords like below? Aren't we supposed to return unigram probability for v (which in this example is bigram[1]) so P(v | u, w) = P(v)? if self. unigramcounts[(bigram[0], )] == 0: return self. bigramcounts[bigram] / float(self. total_num_words)3. In part 3)  raw_trigram_probability function, why are we handling unseen token by replacing its probability with uniform distribution if we have already replaced all the unseen tokens with UNK? I feel like its a redundant step. if trigram_count == 0: # We don't know anything about the distribution of the trigram[2]# in the context of trigram[0], trigram[1] -- assume# uniform distributionreturn 1 / self. total_num_words",
        "target": "1. That's correct! 2. I'm not sure I understand your question. When self. unigramcounts[(bigram[0], )] == 0, for instance when we have ('START', . . ), we want to ensure the denominator isn't 0 and thus we use self. bigramcounts[bigram] / float(self. total_num_words) (just like we use self. trigramcounts[trigram] for trigrams or self. unigramcounts[unigram] for unigrams in the numerator)3. There's a difference between unseen tokens and unseen context. You have to handle that edge case by returning  1 / self. total_num_words. Please see #15 as the difference was explained well there! Hope that helps!"
    },
    {
        "source": "#187: HW2 Part 5: Reasonable F-score values. Hello! I'm currently running into slightly-lower-than-expected F-scores, does anyone have any tips for areas to debug? I'm going to check things like &gt; vs &gt;= but other than that I'm not sure what to look for. Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9248795500760255, Average F-score (all sentences): 0. 6219017664304309",
        "target": "I managed to match my score with the assignment by going through those tokens where didnt match 100% running evaluate_parser. py. Taking those non 100% matches tokens, using a debugger and check the backtracking and prob tables generated to see why your get_parse_tree is not returning the expected tree. Check to see is the divergence caused by the data or the code. "
    },
    {
        "source": "#187: HW2 Part 5: Reasonable F-score values. Hello! I'm currently running into slightly-lower-than-expected F-scores, does anyone have any tips for areas to debug? I'm going to check things like &gt; vs &gt;= but other than that I'm not sure what to look for. Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9248795500760255, Average F-score (all sentences): 0. 6219017664304309",
        "target": "If anyone ends up with similar numbers to mine here, make sure you're checking the probabilities from the left and right symbols when picking the new parse, not just the probability for the new parsed nonterminal generating the left and right symbols. I missed that somehow and was just checking the probability of new parse -&gt; left_symbol right_symbol."
    },
    {
        "source": "#186: Part 4. Hi, Are we suppose to utilize the probabilities in building our parse tree for part 4 or should  the table from part 3  contain the most probable parse for the given sentence which means that all we have to do in part 4 is just build that parse tree without having to worry about checking the probability of the subtrees? Thank you!",
        "target": "The table in part 3 will only contain the single most-probably expansion for each span and nonterminal. So in part 4 you do not have to consider the probabilities. You can simply traverse the table and assemble the tree, which is unique. "
    },
    {
        "source": "#184: Midterm Material Cutoff. Hi NLP teaching staffs - As exam 1 come close, I am wondering what is the material cutoff for review? Thank you so much",
        "target": "Good question. I will upload a topic overview sheet and we will go through this in class next week. We will cover everything up to and including the material of this week (the last topic word2vec). The material from next week will not be on the midterm. "
    },
    {
        "source": "#183: Question on cky. py main comment. Hi NLP teaching staffs -I am running cky. py main and ran into an error: \"name chart is not definied\". Upon inspecting the pre-populated code for cky. py, I noticed that chart is not defined anywhere. I updated the given code to check_table_format(table) and it works. I am wondering if this is the correct fix or if there is anything wrong with my code? Thank you so much! cky. py main code:#print(parser. is_in_language(toks))#table, probs = parser. parse_with_backpointers(toks)#assert check_table_format(chart) - should this be assert check_table_format(table) instead? #assert check_probs_format(probs)",
        "target": "Hu, odd this wasn't noticed before -- looks like just a typo in the template. Please change \"chart\" to \"table\". "
    },
    {
        "source": "#182: HW1 regrade request. I know I am supposed to email the TA who graded my assignment for a regrade request, but on courseworks it does not say a TA. It says graded by Professor Bauer. ",
        "target": "That's because I uploaded the grades. But if you look at the bottom of the comment field, it will say \"Grader: Shreyas\" "
    },
    {
        "source": "#181: Test part 1. Is there any way to test verify grammar other than running on atis3 and getting true in response? ",
        "target": "Sorry, not that I'm aware. "
    },
    {
        "source": "#181: Test part 1. Is there any way to test verify grammar other than running on atis3 and getting true in response? ",
        "target": "I would say it could be helpful to try out other grammars: make a toy grammar like the one from lecture (someone posted one on Ed), then break it in different ways to make it not in Chomsky Normal Form and verify that your check fails (don't think we're supposed to log the reason, but you could add logging temporarily, make sure the reason it fails is correct, then delete it later)."
    },
    {
        "source": "#181: Test part 1. Is there any way to test verify grammar other than running on atis3 and getting true in response? ",
        "target": "You could try constructing a simple toy grammar, and then use grammary. py over it. Just remember that the response to whether your grammar is in Chomsky Normal Form or not depends on how you define the production rules of your grammar."
    },
    {
        "source": "#180: HW2 Part 4 Print out. Hi, After implemented part 2, 3, 4, my get_tree function printed out this value: ('TOP', ('NP', ('NP', ('NP', ('NP', 'flights'), ('PP', 'from')), ('NP', 'miami')), ('PP', ('TO', 'to'), ('NP', 'cleveland'))), ('PUN', '. '))instead of the one in homework description like this:('TOP', ('NP', ('NP', 'flights'), ('NPBAR', ('PP', ('FROM', 'from'), ('NP', 'miami')), ('PP', ('TO', 'to'), ('NP', 'cleveland')))), ('PUN', '. '))\nI'm just wondering, is this an accepted format? Or, is there something wrong with my implementation of constructing the table of backpointers? I kind of have no clue about finding the bug. Thanks very much for any hint!",
        "target": "Bug resolved! Just for reference, I saved the wrong parse tree because of wrong probabilities."
    },
    {
        "source": "#178: HW Grading. I saw the email that the grade has been released, but I checked my grading, it said 0 out of 0 points and a few comments. I downloaded the file and pretty sure that I did every part of the homework. So I am a little confused. ",
        "target": "That's surprising. Can you send email to May and cc me so we can look into that for you?"
    },
    {
        "source": "#177: check_table_format & Part 5 get tree. 1. In the last if statement of check_table_format, I noticed that the first element of the tuple is a string, while the second and third elements are integers. This seems to be in line with the assignment's instructions. However, the error message states: \"It must be a pair ((i, k, A), (k, j, B)). \" I believe it should be corrected to: \"it must be a pair ((A, i, k), (B, k, j)). \"to avoid confusion. 2. I tried to test my get_tree function by implementing Part 5, but there was an issue when I used the test sentence:toks = ['with', 'the', 'least', 'expensive', 'fare', '. ']I realised that this sentence cannot produce a valid parse tree. How should I handle such exceptions? Initially, I had it return None, but the coverage rate was still 100%, which doesn't seem correct. Do we need to check if it can generate a parse tree before we get it?",
        "target": "1. Thank you. I will make sure to correct this in future iterations if this assignment.  2. I think the evaluation script is looking for an exception, such as a KeyError when the sentence cannot be parsed. "
    },
    {
        "source": "#176: HW2 rhs_to_rules. Hi Prof. Bauer and TAs, I am curious to see whether the rule ('NP', ('NP', 'VP'), xxx) is the same as ('NP', ('VP', 'NP'), xxx). Note, the order of non-terminal changed. In my implementation, recognizing them as different rules helped to increase the coverage by appx. 4% and the overall F-score by 2%, but the parsed F-score decreased by 2. 5%. My original implementation was very close to Prof. Bauer's performance. ",
        "target": "They are two different rules. The order of the nonterminal matters. "
    },
    {
        "source": "#175: HW2 part1 checklist. Hi, For Part 1:We need to check if there are any terminals in lhs_to_rules, right? If so, when I reviewed the pcfg file, all lhs keys were uppercase strings. Can we assume an lhs key is a non-terminal if it is composed of uppercase strings? Do we just need to print a statement indicating whether the pcfg file is valid (e. g. , print \"Valid\" if the above method returns true, and \"Invalid\" if not)? Thanks for your help!",
        "target": "I believe in the assignment, it has been stated that all the symbols present on the LHS are assumed as Non-terminals. "
    },
    {
        "source": "#174: Arc standard transition. Hi, I am not sure when I should right-arc instead of left-arc in arc standard transition. Thank you",
        "target": "Look at the first word on the buffer and the top word on the stack. If the target dependency tree has a left edge between these tokens (buffer[0] to stack[0]), do a left arc. If it has a right edge between the tokens, do a right arc  but only if all dependents of buffer[0] that appear in the target have already been constructed. "
    },
    {
        "source": "#173: HW2 part 3 table format. Hi, I had problems meeting the table checking where error comes from \"len(bps) ! = 2\". However, there is also \"Terminal symbols in the table could just be represented as strings. For example the table entry for table[(2, 3)][\"FLIGHTS\"] should be \"flights\". \" The statement from the homework handout is different from the checking criteria, what should we do? Thanks!",
        "target": "Hmm odd. Implement it according to the checking criteria when in doubt. But you can really do it either way as long as you are consistent and your get_tree function works. "
    },
    {
        "source": "#172: HW2 Part 3 -- parse_with_backpointers. Hi, For Part 3, should we be calling is_in_language within parse_with_backpointers? Currently, my probs table comes out right but the backpointer table does not. The get_tree functions also outputs the correct solution despite this. Was just wondering if I was missing something. Thanks!",
        "target": "No is_in_language uses a different table format, so you do not need to call it in parse_with_backpointers. Is the problem that the check_table method reports and error but your get_tree method works? Whats the specific error in that case?"
    },
    {
        "source": "#171: HW2 part 3 best parse tree. Hi, In part 3, I am not sure how does the algorithm work to obtain the most probable parse tree. For example when I am focusing on the cell x03, do I check all combinations from x01, x13 and x02, x23 and just pick the combination that results in the highest probability to put into x03, discarding the other options? Therefore, when I move on to cell x04, the previously chosen combination stored in x03 will be used? Alternatively, is it that for example when I am focusing on the cell x03, for every type of nonterminals, I check all combinations from x01, x13 and x02, x23 and pick the combination that results in the highest probability to put into x03, discarding the other options of the same type of terminals? Such that I result in a set of unique non-terminals that have the highest probability from its own respective types. With this second approach, I am concerned that the table will discard potential combinations that may become more optimal later in building the table. Thanks!",
        "target": "Piggy back onto this, as I am dealing with the same exact question. Alternative to the greedy approach you mentioned would be to build for all possible parse trees and find the single max before returning from part 3 function. But the assignment explicitly mentioned that we should build the backtrack, and prob tables for the best parse tree during the parsing process. I still couldnt figure out how can this be done on the fly greedily. "
    },
    {
        "source": "#170: HW2 Part 1 CNF. Hi, In HW2 part one, the structure of the dataset we received doesn't strictly follow CNF. For example we have  ('PP', ('ABOUT', 'NP'), 0. 00133511348465) seems to have a terminal ('ABOUT') and a nonterminal ('NP') on the right-hand side, which doesn't match the CNF format. Is this expected and I am not sure what we need to actually check for in part 1 to ensure match to CNF. Thanks!",
        "target": "That's incorrect. ABOUT is a nonterminal because it appears on the LHS of a rule. Note that there is also a terminal 'about'.  "
    },
    {
        "source": "#169: HW2 F-scores and Part 2. Hi Prof. Bauer and TAs, As of now, my parsed average F-score is ~82% and overall at 55%, which are lower than Prof. Bauer's scores. As I am reviewing the function evaluate_parser, it seems like there should be some checks for language attribution in the function parser_with_backpointers. However, I am a bit unsure about how we treat the return value in this case. Also, in the last paragraph of Part 2, you mentioned that the \"this method works for grammar with different start symbols. \" Would you mind elaborate on what will be the start symbols? I am using the grammar. startsymbol attribute in my implementation. I am not sure whether this is what you mean and whether changing this value assignment would increase F-score. My implementation of functions is_in_language and parse_with_backpointers are very similar. Looking forward to hearing back. ",
        "target": "Can you clarify what you mean by language attribution? Your choice of startsymbol is correct. This is essentially just a warning to not hardcode the startsymbol and instead use the one provided by the grammar. "
    },
    {
        "source": "#168: Exam format. Hi! I was wondering what the exam format will be, is a pen and paper and theory kind or more coding based. ",
        "target": "There wont really be any coding on the midterm, except for possibly a modify this pseudocode question, or this is broken, can you find the mistake and fix it question.  Most of the midterm will be constructive problems, working with the algorithms we discussed and performing the steps of some algorithm on paper, similar to the ungraded exercises. There may be some factual short answer questions as well. "
    },
    {
        "source": "#167: HW2 Solutions. Hi, For HW2, are there specific answers we should consistently be getting or is it like HW1 where answers vary?  I saw the HW said its graded on functionality, but I just wanted to receive some clarification. Thanks!",
        "target": "There is less flexibility in homework 2, except there may be some small variation based on how you break ties when two splits have the same probability (updating on &gt; or on &gt;=). I dont have the exact numbers ready right now, unfortunately, but you are welcome to post your result so you can compare with other students. It should be more or less consistent. "
    },
    {
        "source": "#166: HW2 Part2 Question. Hi everyone, For HW2 Part2, the instruction said \"The ATIS grammar actually overgenerates a lot, so many unintuitive sentences can be parsed\", and from what I observed in our atis grammar file, there're more than one non-terminals generate the same terminal. I wonder how do we take care of that cases - for example, should we check all combinations of possible non-terminals existed in the rule during the parsing? Thank you so much!",
        "target": "The CKY algorithm already takes care of this. During initialization, if there are two rules for the same nonterminal, both nonterminals are included in the table.  For example if the input token s[1, 2] is flights, and there are rules NP -&gt; flights and FLIGHTS -&gt; flights, both NP and FLIGHTS cover the span [1, 2] and should be added to the table entry for this span. "
    },
    {
        "source": "#165: CKY parsing. Hi, If there are multiple nonterminals in one box(e. g. the NP, N box), do we try all combinations available (e. g. P NP and P N)? If multiple combinations work, do we list out all possible nonterminals (e. g. if NP -&gt; P N is valid, will the red box become PP, NP? )Thank you",
        "target": "yes, you want to consider all feasible combinations"
    },
    {
        "source": "#164: ungraded exercise. Hi, I am wondering if there is a typo in the solution to ungraded exercise. Why the two terms look exactly the same but have different calculation results?",
        "target": "Yes, you are right. The second entry should be pi[2, V] = max( pi[1, N] * P(VIN) * P(flies | V), pi[1, V] * P(V|V) * P(flies|V)) :max (1/4 * 2/4 * 2/6), (1/12 * 0 * 2/6)) = 1/24"
    },
    {
        "source": "#163: HW2 grammar. py. Hi, Can we add \"import math\" to the grammar. py file? Thank you",
        "target": "Im not a TA, but I believe we probably can. Because the assignment even suggests using math. isclose(), which is basically a function present in the math library of python."
    },
    {
        "source": "#163: HW2 grammar. py. Hi, Can we add \"import math\" to the grammar. py file? Thank you",
        "target": "Yes, importing math is okay. "
    },
    {
        "source": "#162: HW2 Questions. Backtracking Table Structure Does Not Support Multiple Subtrees for Same Non-Terminal. grammar. py1. do we need to explicitly check if a word exist in the non-terminal set ?      a. or can we assume that any word that is not part of the terminal set a valid non-terminal for the CNF ? cky. pt1. can we assume all words in the given input string are in the non-terminal set of the grammar ? 2. Part 3, the desire structure based on what is required by the check_table_format function for the backtrack table does not allow for multiple paths for the same constituency.    a. for example, what happen if cell (i, j) has two different k-splits that resulted in VP like the toy cat with glasses example.    b. the structure below will have the latter split overrides the earlier VP's tuple.    c. so, shouldn't the the bps be a LIST instead of a TUPLE of exactly length 2 or  str ?    d. ```json{      (i, j): {          VP -&gt; ((i, k_0, V), (k_0, j, VP),          ### what happen when k_1 is also another VP ? ??          ### VP -&gt; ((i, k_1, VP), (k_1, j, PP),          ### like above          NP -&gt; ((i, k_n, D), (k_n, j, N)),          . ..      }}```",
        "target": "1. For each symbol on the right hand side of a production, you can assume its a non terminal if it is ever used on the left hand side of any production. Otherwise, assume its a terminal. 2. Thats correct. The table only keeps track of one split per nonterminal. Thats okay because we are only interested in membership checking and then later obtaining the single highest probability parse tree. "
    },
    {
        "source": "#161: Viterbi Algorithm HW. In the HW including Viterbi, I am confused about why we do not include [2, Prep] in our second step? How do we know to include it in the 3rd with [3, Prep]? Computing the DP value makes sense but I don't fully understand how the choice is being made. [2, N] = max( [1, N]  P(N|N)  P(flies|N), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 1/4  1/4), (1/12 2/3  1/4)) = 1/64[2, V] = max( [1, N]  P(V|N)  P(flies |V), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 2/4  2/6), (1/12  0  2/6)) = 1/24[3, V] = max( [2, N] P(V|N)  P(like|V), [2, V]  P(V|V)  P(like|V)) = max( (1/64  2/4  2/6), (1/24  0  2/6)) = 1/384[3, Prep] = max( [2, N] P(Prep|N)  P(like|Prep), [2, V]  P(Prep|V)  P(like|Prep)) = max( (1/64  1/4  1), (1/24  1/3  1)) = 1/72",
        "target": "The emission probability P(flies| Prep) = 0.  In other words, we already know that flies cannot be a preposition, according to the lexicon. "
    },
    {
        "source": "#159: ungraded exercise: n-gram. Follow up #51, (not sure if anyone else asked before) so for questions in later exam, we also follow this rule: to add END as a word type but not count START, so |V| = #. of different word +1. Or will we be given other specific rules in exam?",
        "target": "No, thats the general rule. Any other choice will give you incorrect/inaccurate distributions. "
    },
    {
        "source": "#158: Submission instructions for HW 2. courseworks mentions -Pack these files together in a zip or tgz file as described on top of this page. Please follow the submission instructions precisely! I can't seem to find the submission instructions referenced. thanks! ",
        "target": "Ah. Just zip the files and then upload the zip file as you did for homework 1. What you need to submitcky. pyevaluate_parser. pygrammar. pyDo not submit the data files. "
    },
    {
        "source": "#157: get_tree function indexing. get_tree(table, 0, len(toks), grammar. startsymbol)is given in the written example in hw, and is used in the evaluator. I'm assuming that the ranges indicated in the CKY algo are inclusive [i, j], which means my final parse chart would have a entry at (0, len(toks)-1), 'TOP', but not at (0, len(toks)), since len(toks) is one outside the range of the string. Is it acceptable for evaluation purposes for the get_tree method to work with inclusive indexing for ease of recursion? ",
        "target": "The indices specify the left and right boundaries of the tokens. So [0, 1] would be the single token at index 0, and [0, N] would be the entire sentence of length N. Your code should follow this convention. "
    },
    {
        "source": "#156: Question about backpointers. Hi I hope you are doing great! I worked through  the ungraded ckyand I was a liitle unsure of something  in part b.  so  am i right about thinking that backpointers essentially tell us if there was multiple ways to build up to so block in the sentence. so if there was no point where there were different ways to get to the same box, then there would only be one parse tree? am i right in think that parse trees only come down to the last box we fill? If there is more than one parse tree does that only show up in the last square we fill where we derive the final S?  Like if there were 3 options for that final S then there would be 3 parse trees? thank you so much!",
        "target": "In the example we saw in class, there is only a single S entry in [0, 6] (with backpointers to NP in [0, 1] and VP in [1, 6]). However, there are ultimately two different parse trees because there are two backpointers for VP in [1, 6]. So it doesnt just come down to the last cell in the table. Each time you find a new way of constructing a non terminal, you are creating a new backpointer. "
    },
    {
        "source": "#155: CFG Start Symbol 'S'. from the textbook (17. 2. 1), the start symbol 'S' is considered a member of the non-terminal symbol set. 1. Given that, does that mean we can expect to see 'S' in right-hand-side of CKY's constituency ruie. Is something below legitimate: NP -&gt; S VPthe form above look weird as it doesn't make much sense. A NP can be a constituency describing start -&gt; verb phrase. Basically implying NP -&gt; VP ? 2. If we can't do what above, then how are we suppose to treat 'S' symbols that appear in any other cells not (0, n) memoization matrix during  the CKY algorithm.",
        "target": "The startsymbol is a nonterminal and can appear on the right hand side of productions. An example for this are sentences like he said that S "
    },
    {
        "source": "#154: OH for Vedangi Kishor Wagh in CS TA room. Hi curious if Vedangi Kishor Wagh still holding OH in room 122 Mudd today? Not seeing her. Thanks!",
        "target": "Hi Vedangi sent out a message earlier today to move her office hours this week to Thursday from 12:00 noon to 1:30 pm on Zoom. https://columbiauniversity. zoom. us/j/6826910890? pwd=RVJLeURDemRYSDloZDdSNzQxakhGdz09Make sure you receive notifications sent through courseworks. We will generally use courseworks announcement for communication. "
    },
    {
        "source": "#152: Toy Grammar for HW2. Hi everyone. In case anyone is interested in the \"toy grammar\" for HW2, here is the one I made (based on lecture 7) and used for myself. Please let me know if I need to correct anything. Best.",
        "target": "Do you not need a set of rules that define what the TOP rule can create, or to redefine the start symbol to one or more of the non-terminals? Otherwise, there is no way to get from TOP to anywhere else when parsing."
    },
    {
        "source": "#152: Toy Grammar for HW2. Hi everyone. In case anyone is interested in the \"toy grammar\" for HW2, here is the one I made (based on lecture 7) and used for myself. Please let me know if I need to correct anything. Best.",
        "target": "minor typo in the provided pcfg file. The start symbol need to match up with S -&gt; NP VP in order for CKY to able to work when checking the [0, n] cell. Or change all instance S to TOP to match the given atis3 test file. S ; 1. 0\n#TOP ; 1. 0\n\nS -&gt; NP VP ; 1. 0\nVP -&gt; V NP ; 0. 6\nVP -&gt; VP PP ; 0. 4\nPP -&gt; P NP ; 1. 0\nNP -&gt; D N ; 0. 7\nNP -&gt; NP PP ; 0. 2\n\nNP -&gt; she ; 0. 05\nNP -&gt; glasses ; 0. 05\nD -&gt; the ; 1. 0\nN -&gt; cat ; 0. 3\nN -&gt; glasses ; 0. 7\nV -&gt; saw ; 1. 0\nP -&gt; with ; 1. 0\n"
    },
    {
        "source": "#150: Participation. Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? ",
        "target": "I'm not a TA so take this with a grain of salt but I recall the professor mentioned asking or participating on Edstem + OH could count towards your participation grade!"
    },
    {
        "source": "#150: Participation. Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? ",
        "target": "Correct, participation in Ed courts for participation. Note that this is technically an in person class, so the expectation is that you attend most lectures (although Im not taking attendance). "
    },
    {
        "source": "#150: Participation. Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? ",
        "target": "Not a TA, but I recall that participating in the Ed discussion should count toward the participation grade."
    },
    {
        "source": "#150: Participation. Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? ",
        "target": "Can we have a more clear expectation for participation on Ed + OH? Like is it posting/replying how many times vs. attending OH how often, etc?"
    },
    {
        "source": "#148: Ungraded Exercise Solution. Hi, when do the solution will be released for ungraded exercises for HMM? ",
        "target": "Sorry I thought I had uploaded this already but must have forgotten to release it :) Ill post it later today. "
    },
    {
        "source": "#146: log probability math domain error. Hello, When calculating my log probabilities, I keep getting a math domain error. Stack overflow points to errors with numbers close to 0 triggering this exception. I am not sure if I'm the only running into this error but a suggested fix according to the internet is to calculate the log probability of the probability + epsilon. I wanted to confirm if this is acceptable or if there are better alternatives. Thank you! Best,",
        "target": "This is because you have some trigram_prob = 0. You might want to double check your smoothed probability to avoid this case"
    },
    {
        "source": "#145: Regarding commenting out tests in main. I just went through all the unread discussions on ed and realized that somewhere it was mentioned that we need to comment out the tests we conducted in main. I did not do that and I am sorry for any inconvenience caused. I cannot resubmit now as well. Should I do something about it? I apologize again and will make sure to do so for the next assignment.",
        "target": "Don't worry about it. We will not actually run the program itself, but rather import your class and then call the individual methods. "
    },
    {
        "source": "#144: Request for Extension Due to Serious Illness. Dear Professor Bauer, I hope this message finds you well. My name is Yolanda Zhu, and I am in your Natural Language Processing class. I regret to inform you that I tested positive for COVID-19 earlier this week and was feeling extremely unwell. While I initially tried to push through and complete the homework due tonight, my symptoms made it particularly challenging. Therefore, Im wondering if its possible for me to request a 1-2 day extension for HW1. I understand the importance of meeting deadlines, but under these circumstances, I hope you can consider my request. I truly apologize for the inconvenience and appreciate your understanding during this challenging time. Thank you for considering my situation. I look forward to your response. Warm regards, Yolanda Zhu",
        "target": "Responded by email. Closing this thread. "
    },
    {
        "source": "#143: Accuracy Too High? . Going off of some previous posts regarding accuracy, I was wondering at what point does the accuracy start to become suspiciously high? Would accuracy of 0. 90 or 0. 925 or even 0. 95 seem off? Thanks!",
        "target": "Yes, that's too high. Sorry for the late response. I suspect you didn't set up the experiment in part 7 correctly (you are probably testing one of the models against itself). "
    },
    {
        "source": "#142: Comments in Code. Not sure if this was already addressed, but are comments in the code read and taken into consideration, or should we just not bother writing them? Thank you!",
        "target": "The way we grade assignments in general is to test the individual methods first using an autograding script. If they dont work as expected, the TAs look at your code in more detail. So comments would be read and taken into account at that stage. Comments are not required however. More generally, think of comments as improving readability for your own sake. Its good to get into the habit of writing meaningful comments. "
    },
    {
        "source": "#141: Perplexity for test data. Sorry these questions have been asked a bunch, but does a perplexity score of ~85 for the test data tell me something is not right? For the training data I get a perplexity score of ~11  and my accuracy score comes out to about 87%",
        "target": "Hm, the perplexity scores seem somewhat low, did you maybe include the start symbol in the denominator somewhere?"
    },
    {
        "source": "#140: Laplace Smoothing for Unigram Models. Hi Prof. Bauer and TAs, In lecture 4 slides, I am a bit unclear as to exactly what we should use for N in the unigram probability calculation below. Take the example of ungraded exercise 2 - part 1. START dog bites human ENDSTART dog bites duck ENDSTART dog eats duck ENDSTART duck bites human ENDSTART human eats duck ENDSTART human eats salad ENDWhat value should we use for N? ",
        "target": "N in this case would be 24 (the sum of length of all the sentences, including END, but excluding START). V would be {dog, bites, human, duck, eats, salad, END}  so |V| = 7"
    },
    {
        "source": "#139: hw1 part 7 100% accuracy. For part 7 is it normal to get 100% accuracy?",
        "target": "No, there is likely a bug in your experimental setup, Im afraid. "
    },
    {
        "source": "#138: Edge Case for Bigram. Hi, According to post #55 , we need to handle bigram edge cases of [\"START\", x]. What are we supposed to do in this case exactly? Thanks.",
        "target": "The reason why you need to handle this case separately is that count(START) is 0. My suggestion would be to keep track of the number of sentences in the training data in a separate instance variable, and then use that value for the denominator when computing P(x |start). "
    },
    {
        "source": "#136: What should the probabilites be for these two? . raw_bigram_probability('START', 'START') raw_trigram_probability ('START', 'START', 'START')",
        "target": "These should be 0. "
    },
    {
        "source": "#135: Q6 Question. For Q6 I'm getting:\n(base) omernauer@MacBook-Pro-689 HW1 % python -i trigram_model. py hw1_data/brown_train. txt hw1_data/brown_test. txt\nM: 98203\nl -4. 488916578819041\npp = 22. 454249203523293\n&gt;&gt;&gt; \n(base) omernauer@MacBook-Pro-689 HW1 % python -i trigram_model. py hw1_data/brown_train. txt hw1_data/brown_train. txt\nM: 1042565\nl -2. 1932556839359254\npp = 4. 573363791549528\nHow can I understand where my mistake is?\n\n\nimport sys\nfrom collections import defaultdict\nimport math\nimport random\nimport os\nimport os. path\n\"\"\"\nCOMS W4705 - Natural Language Processing - Fall 2023 \nProgramming Homework 1 - Trigram Language Models\nDaniel Bauer\n\"\"\"\n\n\ndef corpus_reader(corpusfile, lexicon=None): \n    with open(corpusfile, 'r') as corpus:\n        for line in corpus: \n            if line. strip():\n                sequence = line. lower(). strip(). split()\n                if lexicon: \n                    yield [word if word in lexicon else \"UNK\" for word in sequence]\n                else: \n                    yield sequence\n\n\ndef get_lexicon(corpus):\n    word_counts = defaultdict(int)\n    for sentence in corpus:\n        for word in sentence: \n            word_counts[word] += 1\n    return set(word for word in word_counts if word_counts[word] &gt; 1)  \n\n\ndef get_ngrams(sequence, n):\n    \"\"\"    COMPLETE THIS FUNCTION (PART 1)    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.    This should work for arbitrary values of n &gt;= 1     \"\"\"    ngrams = []\n\n    if n == 1:\n        init_tuple = 'START',\n        ngrams. append(init_tuple)\n        for word in sequence:\n            ngrams. append(tuple([word]))\n        ngrams. append(tuple(['STOP']))\n    elif n == 2:\n        bituple = ['START'] * 2\n        for word in sequence:\n            bituple. pop(0)\n            bituple. append(word)\n            ngrams. append(tuple(bituple))\n        bituple. pop(0)\n        bituple. append('STOP')\n        ngrams. append(tuple(bituple))\n    elif n == 3:\n        trituple = ['START'] * 3\n        for word in sequence:\n            trituple. pop(0)\n            trituple. append(word)\n            ngrams. append(tuple(trituple))\n        trituple. pop(0)\n        trituple. append('STOP')\n        ngrams. append(tuple(trituple))\n    return ngrams\n\n\nclass TrigramModel(object):\n    \n    def __init__(self, corpusfile):\n    \n        # Iterate through the corpus once to build a lexicon \n        generator = corpus_reader(corpusfile)\n        self. lexicon = get_lexicon(generator)\n        self. lexicon. add(\"UNK\")\n        self. lexicon. add(\"START\")\n        self. lexicon. add(\"STOP\")\n    \n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self. lexicon)\n        self. count_ngrams(generator)\n\n\n        # number of words = wordcount\n        generator = corpus_reader(corpusfile)\n        unigramcount = defaultdict(int)\n        uc = []\n        for sentence in generator:\n            uc. extend(get_ngrams(sentence, 1))\n        for word in uc:\n            unigramcount[word] += 1\n        self. wordcount = len(unigramcount)\n\n\n    def cut_trigram(self, trigram):  # returns a bigram\n        a = list(trigram)\n        a. pop(-1)\n        b = tuple(a)\n        return b\n\n    def cut_bigram(self, bigram):  # returns a unigram\n        a = list(bigram)\n        a. pop(-1)\n        b = tuple(a)\n        return b\n\n    def count_ngrams(self, corpus):\n        \"\"\"        COMPLETE THIS METHOD (PART 2)        Given a corpus iterator, populate dictionaries of unigram, bigram,        and trigram counts.         \"\"\"        self. unigramcounts = defaultdict(int)  # might want to use defaultdict or Counter instead\n        self. bigramcounts = defaultdict(int)\n        self. trigramcounts = defaultdict(int)\n\n        unigram_corpus = []\n        bigram_corpus = []\n        trigram_corpus = []\n\n        for sentence in corpus:\n            unigram_corpus. extend(get_ngrams(sentence, 1))\n            bigram_corpus. extend(get_ngrams(sentence, 2))\n            trigram_corpus. extend(get_ngrams(sentence, 3))\n\n        for word in unigram_corpus:\n            self. unigramcounts[word] += 1\n\n        for word in bigram_corpus:\n            self. bigramcounts[word] += 1\n\n        for word in trigram_corpus:\n            self. trigramcounts[word] += 1\n\n        #print(\"unigram count:\", self. unigramcounts)\n        #print(\"bigram count:\", self. bigramcounts)\n        #print(\"trigram count:\", self. trigramcounts)\n        return\n\n    def raw_trigram_probability(self, trigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 3)        Returns the raw (unsmoothed) trigram probability        \"\"\"        bigram = self. cut_trigram(trigram)\n\n        if self. bigramcounts[bigram] == 0:  # If we haven't seen the preceeding bigram\n            # Technique A\n            #return 1/(len(self. lexicon)-1)  # Taking out start\n\n            # Technique B\n            unigram = self. cut_bigram(bigram)\n            return self. raw_unigram_probability(unigram)\n        else:\n            return self. trigramcounts[trigram]/self. bigramcounts[bigram]\n\n\n\n    def raw_bigram_probability(self, bigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 3)        Returns the raw (unsmoothed) bigram probability        \"\"\"        unigram = self. cut_bigram(bigram)\n        if self. unigramcounts[unigram] ! = 0:\n            return self. bigramcounts[bigram]/self. unigramcounts[unigram]\n        else:\n            return 0\n    \n    def raw_unigram_probability(self, unigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 3)        Returns the raw (unsmoothed) unigram probability.        \"\"\"        return self. unigramcounts[unigram]/self. wordcount\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.\n\n\n    def generate_sentence(self, t=20): \n        \"\"\"        COMPLETE THIS METHOD (OPTIONAL)        Generate a random sentence from the trigram model. t specifies the        max length, but the sentence may be shorter if STOP is reached.        \"\"\"        return result            \n\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 4)        Returns the smoothed trigram probability (using linear interpolation).         \"\"\"        lambda1 = 1/3. 0\n        lambda2 = 1/3. 0\n        lambda3 = 1/3. 0\n        bigram = self. cut_trigram(trigram)\n        unigram = self. cut_bigram(bigram)\n        return (lambda1*self. raw_trigram_probability(trigram) +\n                lambda2*self. raw_bigram_probability(bigram) +\n                lambda3*self. raw_unigram_probability(unigram))\n\n\n        \n    def sentence_logprob(self, sentence):\n        \"\"\"        COMPLETE THIS METHOD (PART 5)        Returns the log probability of an entire sequence.        \"\"\"        probability = 0\n        trigrams = get_ngrams(sentence, 3)\n        for trigram in trigrams:\n            probability += math. log2(self. smoothed_trigram_probability(trigram))\n        return probability\n\n    def perplexity(self, corpus):\n        \"\"\"        COMPLETE THIS METHOD (PART 6)         Returns the log probability of an entire sequence.        \"\"\"        # Count the amount of words = M\n        counter = 0\n        uni = []\n        m_sum = 0\n        for s in corpus:\n            uni. extend(get_ngrams(s, 1))\n            m_sum += self. sentence_logprob(s)\n\n        for word in uni:\n            if word ! = (\"START\", ):\n                counter += 1\n        M = counter\n        print(\"M:\", M)\n\n\n\n        l = m_sum/M\n        print(\"l\", l)\n        return math. pow(2, -l)\n\n\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n\n        model1 = TrigramModel(training_file1)\n        model2 = TrigramModel(training_file2)\n\n        total = 0\n        correct = 0       \n \n        for f in os. listdir(testdir1):\n            pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n            if pp &lt; pp2:\n                correct += 1\n            total += 1\n\n\n            # . .\n    \n        for f in os. listdir(testdir2):\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\n            pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n            if pp &gt; pp2:\n                correct += 1\n            total += 1\n        print(\"hi\")\n        return correct/total\n\nif __name__ == \"__main__\":\n\n    model = TrigramModel(sys. argv[1])\n\n    # put test code here. ..\n    # or run the script from the command line with \n    # $ python -i trigram_model. py [corpus_file]\n    # &gt;&gt;&gt; \n    #\n    # you can then call methods on the model instance in the interactive \n    # Python prompt. \n\n    \n    # Testing perplexity:\n    dev_corpus = corpus_reader(sys. argv[2], model. lexicon)\n\n    pp = model. perplexity(dev_corpus)\n    print(pp)\n\n\n    # Essay scoring experiment: \n    acc = essay_scoring_experiment('train_high. txt', '/train_low. txt', \"test_high\", \"test_low\")\n    print(acc)\n\n",
        "target": "Hi, one obvious bug is that in your code, you have  self. wordcount = len(unigramcount)-1 \nThis should beself. wordcount = sum(unigramcount. values()) \nBy modifying this I was able to get:M: 98203\nl -6. 8231568321629466\n113. 23348441315729\nYou might also want to consider modifying your calculations for raw probabilities to better handle edge cases. For ex, I modified your smoothed_trigram_probability and raw_unigram_probability and obtained:M: 98203\nl -7. 697321492396698\n207. 5509165052973\nLet me know if you have any other questions."
    },
    {
        "source": "#134: Part 7 Error. I am having the same issue as #93 but I re-downloaded the zip file twice and still get the following error:FileNotFoundError: [Errno 2] No such file or directory: 'hw1_data/ets_toefl_data/test_low/1443872. txt'when I run: python3 trigram_model. py brown_train. txt brown_test. txtI have checked that I am in the correct directory.",
        "target": "I ran into the same issue actually about 10 minutes ago, and I ended up finding what my error was. Check your model1 and model2 testdir's in essay_scoring_experiment! That solved my error on my end! And also, part 7 doesn't take arguments, rather it uses the last few lines of code in the main method!"
    },
    {
        "source": "#132: Math Error when Testing Sentence logprob. While I am able to attain adequate perplexity and accuracy scores through my methods, when I try to run the following test on sentence_logprob, I am getting a math error due the smoothed probability of one of the trigrams being zero. Is this to be expected? test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]log_prob = model. sentence_logprob(test_sentence)print(\"Log Probability:\", log_prob)",
        "target": "Yes -- the tokens int he corpus are all lower-cased. "
    },
    {
        "source": "#130: Question About Part 7. Hi, I'm a bit confused on what essay_scoring_experiment() is supposed to do. I followed what Dr. Bauer said in post #58 but my code isn't working. This is what I have right now:def essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n    model1 = TrigramModel(training_file1)\n    model2 = TrigramModel(training_file2)\n\n    total = 0\n    correct = 0\n\n    for f in os. listdir(testdir1):\n        pp_high = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n        pp_low = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n        if pp_high &lt; pp_low:\n            correct += 1\n        total += 1\n\n    for f in os. listdir(testdir2):\n        pp_low = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\n        pp_high = model1. perplexity(corpus_reader(os. path. join(testdir2, f), model1. lexicon))\n        if pp_low &lt; pp_high:\n            correct += 1\n        total += 1\n\n    return correct/total",
        "target": "That looks okay at first glance. Are you getting an error or incorrect results?"
    },
    {
        "source": "#128: Perplexity Question. Hi, When I run my code I get the following output. Is my perplexity supposed to be this low or is this not actually the perplexity value that I'm returning or something? Edit: Here's my perplexity func",
        "target": "The corpus passed into perplexity() is not necessarily the train corpus and you cannot simply divide by self. total_words. In your case, the input corpus is brown_test. txt. Thus you need to calculate M in the perplexity formula by summing the total number of word tokens in the input corpus. "
    },
    {
        "source": "#127: Debugging Perplexity and Accuracy. Hello TA's! I'm having some trouble getting my model to achieve an acceptable perplexity/accuracy score. Right now I am getting Perplexity = 343 and Accuracy = 72%. I implemented error handling for edge cases and I've also checked that my bigram and trigram probabilities add up to 1, so I'm a bit unsure as to where my error is. If anyone could take a quick look and provide some guidance I would be forever grateful! (': def get_ngrams(sequence, n):\r\n \r\n    new = sequence. copy()\r\n    for i in range(n-1):\r\n        new. insert(0, \"START\")\r\n    new. append(\"STOP\")\r\n\r\n    ngrams = []\r\n    for i in range(len(new)-n+1):\r\n        gram = []\r\n        for j in range(n):\r\n            gram. append(new[i+j])\r\n        ngrams. append(tuple(gram))\r\n\r\n    return ngrams\r\n\r\n\r\nclass TrigramModel(object):\r\n    \r\n    def __init__(self, corpusfile):\r\n    \r\n        # Iterate through the corpus once to build a lexicon \r\n        self. total_sentences = None\r\n        generator = corpus_reader(corpusfile)\r\n        self. lexicon = get_lexicon(generator)\r\n        self. lexicon. add(\"UNK\")\r\n        self. lexicon. add(\"START\")\r\n        self. lexicon. add(\"STOP\")\r\n    \r\n        # Now iterate through the corpus again and count ngrams\r\n        generator = corpus_reader(corpusfile, self. lexicon)\r\n        self. count_ngrams(generator)\r\n\r\n        self. total_words = 0\r\n        for key in self. unigramcounts:\r\n            self. total_words += self. unigramcounts[key]\r\n\r\n\r\n\r\n    def count_ngrams(self, corpus):\r\n     \r\n        self. unigramcounts = {} # might want to use defaultdict or Counter instead\r\n        self. bigramcounts = {} \r\n        self. trigramcounts = {}\r\n        corpus_list = []\r\n        unigrams = []\r\n        bigrams = []\r\n        trigrams = []\r\n\r\n        for sentence in corpus:\r\n            unigrams. append(get_ngrams(sentence, 1))\r\n            bigrams. append(get_ngrams(sentence, 2))\r\n            trigrams. append(get_ngrams(sentence, 3))\r\n\r\n        self. total_sentences = len(unigrams)\r\n\r\n        for sentence in range(len(unigrams)):\r\n            for unigram in unigrams[sentence]:\r\n                if unigram in self. unigramcounts:\r\n                    self. unigramcounts[unigram] += 1\r\n                else:\r\n                    self. unigramcounts[unigram] = 1\r\n\r\n            for bigram in bigrams[sentence]:\r\n                if bigram in self. bigramcounts:\r\n                    self. bigramcounts[bigram] += 1\r\n                else:\r\n                    self. bigramcounts[bigram] = 1\r\n\r\n            for trigram in trigrams[sentence]:\r\n                if trigram in self. trigramcounts:\r\n                    self. trigramcounts[trigram] += 1\r\n                else:\r\n                    self. trigramcounts[trigram] = 1\r\n\r\n\r\n\r\n    def raw_trigram_probability(self, trigram):\r\n       \r\n        uvw = 0\r\n        uv = 0\r\n\r\n        if trigram in self. trigramcounts:\r\n            uvw = self. trigramcounts[trigram]\r\n        if trigram[0] == \"START\" and trigram[1] == \"START\":\r\n            uv = self. total_sentences\r\n        elif (trigram[0], trigram[1]) in self. bigramcounts:\r\n            uv = self. bigramcounts[(trigram[0], trigram[1])]\r\n\r\n        if uv == 0:\r\n            return 1 / (len(self. lexicon) - 1)\r\n\r\n        return uvw / uv\r\n\r\n    def raw_bigram_probability(self, bigram):\r\n       \r\n        uv = 0\r\n\r\n        if bigram in self. bigramcounts:\r\n            uv = self. bigramcounts[bigram]\r\n        if bigram[0] == \"START\":\r\n            u = self. total_sentences\r\n        else:\r\n            u = self. unigramcounts[(bigram[0], )]\r\n        return uv / u\r\n    \r\n    def raw_unigram_probability(self, unigram):\r\n    \r\n        if unigram == (\"START\", ):\r\n            return 0\r\n\r\n        return 1 / self. total_words\r\n\r\n    def smoothed_trigram_probability(self, trigram):\r\n       \r\n        lambda1 = 1/3. 0\r\n        lambda2 = 1/3. 0\r\n        lambda3 = 1/3. 0\r\n\r\n        return lambda1 * self. raw_trigram_probability(trigram) + \\\r\n            lambda2 * self. raw_bigram_probability((trigram[0], trigram[1])) + \\\r\n            lambda3 * self. raw_unigram_probability((trigram[1]))\r\n        \r\n    def sentence_logprob(self, sentence):\r\n       \r\n        ngrams = get_ngrams(sentence, 3)\r\n        prob = 0\r\n        for gram in ngrams:\r\n            prob += math. log2(self. smoothed_trigram_probability(gram))\r\n\r\n        return prob\r\n\r\n    def perplexity(self, corpus):\r\n       \r\n        perplexity = 0\r\n        tokens = 0\r\n        for sentence in corpus:\r\n            tokens += self. count_words(sentence) + 1\r\n            perplexity += self. sentence_logprob(sentence)\r\n        perplexity = perplexity / tokens\r\n        perplexity = 1 / math. pow(2, perplexity)\r\n        return perplexity\r\n\r\n    def count_words(self, sentence):\r\n        count = 0\r\n        for word in sentence:\r\n            count += 1\r\n\r\n        return count\r\n\r\n\r\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\r\n\r\n        model1 = TrigramModel(training_file1)\r\n        model2 = TrigramModel(training_file2)\r\n\r\n        total = 0\r\n        correct = 0       \r\n \r\n        for f in os. listdir(testdir1):\r\n            pp1 = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\r\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\r\n            total += 1\r\n            if 'high' in training_file1:\r\n                if pp1 &lt; pp2:\r\n                    correct += 1\r\n            else:\r\n                if pp2 &lt; pp1:\r\n                    correct += 1\r\n\r\n    \r\n        for f in os. listdir(testdir2):\r\n            pp1 = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\r\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\r\n            total += 1\r\n            if 'high' in training_file1:\r\n                if pp1 &lt; pp2:\r\n                    correct += 1\r\n            else:\r\n                if pp2 &lt; pp1:\r\n                    correct += 1\r\n        \r\n        return correct / total\n",
        "target": "Your raw unigram probability is incorrect. You are always returning 1/self. total_words rather than count(unigram)/self. total_words. "
    },
    {
        "source": "#126: Issues w/ Part 6 & Part 7. When I test my part 6 &amp; 7, I am getting value &gt;400 and &lt;80% (specifically 457 and 76%). I tried testing parts 3-5 using:print(model. raw_trigram_probability((\"START\", \"START\", \"the\"))) # Part 3 print(model. raw_bigram_probability((\"START\", \"the\"))) # Part 3 print(model. raw_unigram_probability((\"the\", ))) # Part 3 print(model. smoothed_trigram_probability((\"START\", \"START\", \"the\"))) # Part 4 print(model. sentence_logprob([\"natural\", \"language\", \"processing\"])) # Part 5The output makes sense to me:4. 0436716538617066e-054. 0436716538617066e-050. 0589200673339312160. 019666980255669483-49. 039356515775864I have attended a few OHs but still no luck. I've included my current code in this post. Thank you in advance for any advice/feedback! import sys\nfrom collections import defaultdict\nimport math\nimport random\nimport os\nimport os. path\n\n\"\"\"\nCOMS W4705 - Natural Language Processing - Fall 2023 \nProgramming Homework 1 - Trigram Language Models\nDaniel Bauer\n\"\"\"\n\n\ndef corpus_reader(corpusfile, lexicon=None):\n    with open(corpusfile, \"r\") as corpus:\n        for line in corpus:\n            if line. strip():\n                sequence = line. lower(). strip(). split()\n                if lexicon:\n                    yield [word if word in lexicon else \"UNK\" for word in sequence]\n                else:\n                    yield sequence\n\n\ndef get_lexicon(corpus):\n    word_counts = defaultdict(int)\n    for sentence in corpus:\n        for word in sentence:\n            word_counts[word] += 1\n    return set(word for word in word_counts if word_counts[word] &gt; 1)\n\n\ndef get_ngrams(sequence, n):\n    \"\"\"\n    COMPLETE THIS FUNCTION (PART 1)\n    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n    This should work for arbitrary values of n &gt;= 1\n    \"\"\"\n    if n &lt; 1:\n        raise ValueError(\"n must be greater than or equal to 1\")\n\n    ngrams = []\n\n    # Pad the list of words with 'START' and 'STOP' tokens\n    padded_words = [\"START\"] * (n - 1) + sequence + [\"STOP\"]\n\n    for i in range(len(padded_words) - n + 1):\n        ngram = tuple(padded_words[i : i + n])\n        ngrams. append(ngram)\n\n    return ngrams\n\n\nclass TrigramModel(object):\n    def __init__(self, corpusfile):\n        # Iterate through the corpus once to build a lexicon\n        generator = corpus_reader(corpusfile)\n        self. lexicon = get_lexicon(generator)\n        self. lexicon. add(\"UNK\")\n        # self. lexicon. add(\"START\")\n        self. lexicon. add(\"STOP\")\n\n        self. total_words = 0\n\n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self. lexicon)\n        self. count_ngrams(generator)\n\n    def count_ngrams(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts.\n        \"\"\"\n\n        self. unigramcounts = defaultdict(int)  # Using defaultdict to initialize counts\n        self. bigramcounts = defaultdict(int)\n        self. trigramcounts = defaultdict(int)\n\n        for item in corpus:\n            # Count the number of word tokens in the corpus\n            self. total_words += len(item) + 1\n\n            unigrams = get_ngrams(item, 1)\n            bigrams = get_ngrams(item, 2)\n            trigrams = get_ngrams(item, 3)\n\n            for unigram in unigrams:\n                self. unigramcounts[unigram] += 1\n\n            for bigram in bigrams:\n                self. bigramcounts[bigram] += 1\n\n            for trigram in trigrams:\n                self. trigramcounts[trigram] += 1\n\n        return\n\n    def raw_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n        u, w, v = trigram\n\n        # Check if the trigram includes two \"START\" tokens at the beginning\n        if u == \"START\" and w == \"START\":\n            bigram = (w, v)\n            return self. raw_bigram_probability(bigram)\n        # Check if count(u, w) is 0\n        elif self. bigramcounts[(u, w)] &gt; 0:\n            return self. trigramcounts. get(trigram, 0) / self. bigramcounts[(u, w)]\n        else:\n            # Use uniform distribution over vocabulary size if unseen\n            return 1 / len(self. lexicon)\n            # return self. raw_unigram_probability((v, ))\n\n    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        u, w = bigram\n\n        # Check if count(u) is 0\n        if self. unigramcounts[u] &gt; 0:\n            return self. bigramcounts. get(bigram, 0) / self. unigramcounts[u]\n        else:\n            # Use uniform distribution over vocabulary size if unseen\n            return 1 / len(self. lexicon)\n\n    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n\n        # hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once,\n        # store in the TrigramModel instance, and then re-use it.\n        w = unigram[0]\n\n        return self. unigramcounts. get(unigram, 0) / self. total_words\n\n    def generate_sentence(self, t=20):\n        \"\"\"\n        COMPLETE THIS METHOD (OPTIONAL)\n        Generate a random sentence from the trigram model. t specifies the\n        max length, but the sentence may be shorter if STOP is reached.\n        \"\"\"\n        return result\n\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation).\n        \"\"\"\n        u, w, v = trigram\n\n        lambda1 = 1 / 3. 0\n        lambda2 = 1 / 3. 0\n        lambda3 = 1 / 3. 0\n\n        # Calculate the individual probabilities using raw methods\n        P_trigram = self. raw_trigram_probability(trigram)\n        P_bigram = self. raw_bigram_probability((w, v))\n        P_unigram = self. raw_unigram_probability((v, ))\n\n        # Compute the smoothed probability using linear interpolation\n        smoothed_prob = lambda1 * P_trigram + lambda2 * P_bigram + lambda3 * P_unigram\n\n        return smoothed_prob\n\n    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n\n        # Initialize the log probability to 0\n        log_prob = 0. 0\n\n        # Calculate the log probability of each trigram in the sentence\n        trigrams = get_ngrams(sentence, 3)\n        for trigram in trigrams:\n            log_prob_trigram = math. log2(self. smoothed_trigram_probability(trigram))\n            log_prob += log_prob_trigram\n\n        return log_prob\n\n    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        log_prob_total = 0. 0\n        total_tokens = 0\n\n        for sentence in corpus:\n            # Calculate the log probability of the sentence\n            log_prob_total += self. sentence_logprob(sentence)\n\n            # Count the number of word tokens in the sentence\n            total_tokens += len(sentence) + 1\n\n        # Calculate the perplexity using the formula\n        perplexity = math. pow(2, -log_prob_total / total_tokens)\n\n        return perplexity\n\n\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n    model1 = TrigramModel(training_file1)\n    model2 = TrigramModel(training_file2)\n\n    total = 0\n    correct = 0\n\n    for f in os. listdir(testdir1):\n        pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n        pp2 = model2. perplexity(\n            corpus_reader(os. path. join(testdir1, f), model2. lexicon)\n        )\n\n        # If model1 has a lower (better) perplexity, then it's a correct prediction for testdir1\n        if pp &lt; pp2:\n            correct += 1\n        total += 1\n\n    for f in os. listdir(testdir2):\n        pp1 = model1. perplexity(\n            corpus_reader(os. path. join(testdir2, f), model1. lexicon)\n        )\n        pp2 = model2. perplexity(\n            corpus_reader(os. path. join(testdir2, f), model2. lexicon)\n        )\n\n        # If model2 has a lower (better) perplexity, then it's a correct prediction for testdir2\n        if pp2 &lt; pp1:\n            correct += 1\n        total += 1\n\n    return correct / total\n\n\nif __name__ == \"__main__\":\n    model = TrigramModel(sys. argv[1])\n\n    print(model. raw_trigram_probability((\"START\", \"START\", \"the\")))  # Part 3\n    print(model. raw_bigram_probability((\"START\", \"the\")))  # Part 3\n    print(model. raw_unigram_probability((\"the\", )))  # Part 3\n    print(model. smoothed_trigram_probability((\"START\", \"START\", \"the\")))  # Part 4\n    print(model. sentence_logprob([\"natural\", \"language\", \"processing\"]))  # Part 5\n\n    # put test code here. ..\n    # or run the script from the command line with\n    # $ python -i trigram_model. py [corpus_file]\n    # &gt;&gt;&gt;\n    #\n    # you can then call methods on the model instance in the interactive\n    # Python prompt.\n\n    # Testing perplexity:\n    # dev_corpus = corpus_reader(sys. argv[2], model. lexicon)\n    # pp = model. perplexity(dev_corpus)\n    # print(pp)\n\n    # Essay scoring experiment:\n    # acc = essay_scoring_experiment(\n    #     \"hw1_data/ets_toefl_data/train_high. txt\",\n    #     \"hw1_data/ets_toefl_data/train_low. txt\",\n    #     \"hw1_data/ets_toefl_data/test_high\",\n    #     \"hw1_data/ets_toefl_data/test_low\",\n    # )\n    # print(acc)\n",
        "target": "Just an idea: In line 127, you check self. unigramcounts[u], but I think the keys in that dictionary are in a tuple format, so self. unigramcount[(u, )]. "
    },
    {
        "source": "#124: division by 0 error. Hello, I'm running into an issue with my sentence_logprob() function where if I run this test code: where 'sample. txt' is a very small corpus, it would give me a 'math domain error' due to the fact that I'm taking the log of 0 because none of the words in \"natural language processing\" are in the corpus. For example, the first trigram created is ('START', 'START', 'natural) which I believe should return 0 for all three unigram, bigram, and trigram probabilities because the model has not seen \"natural. \" How should I take this into account?  When I try to implement a check to see if the smoothed_probability() return 0, I run into a division by 0 error with my bigram probability because it will divide by the unigram probability of \"natural\" --&gt; which will be 0. ",
        "target": "This is not something you need to account for. The smoothed trigram probability will eventually just use the unigram probability. There are no unseen tokens in the test data, because the corpus reader replaces them ass with UNK. So, in a sense, your test case is invalid. The token natural would never be encountered during testing. "
    },
    {
        "source": "#122: Unseen Bigram and Trigram. If the bigram or trigram is not in the count dictionary, should we simply return 0 or should we turn it into UNK and calculate the probability of it?",
        "target": "in the raw probability methods you should just return 0 for unseen bigrams and trigrams. "
    },
    {
        "source": "#120: HW1 - Perplexity Scores in train. txt/test. txt. Hi all, When I'm testing perplexity against the training data, I get a score of 19. xxx (which seems okay? ) but ~4. xxx-5. xxx when running the test data. I saw #57 but I checked in OH that I'm getting the correct values for token/sentence counts so I don't think that is the issue. Anyone else having this problem?",
        "target": "This seems very low. are you maybe using the tokens in the training data for M, instead of the test data?"
    },
    {
        "source": "#119: perplexity: M vs m. Hi, I'm wondering what $M$ and $m$ are in the perplexity equation:$$\\begin{align*}2^{-l}&amp; &amp;\\text{where} \\ \\ \\ \\ \\ \\ \\ \\ &amp;l = \\frac{1}{M}\\sum_{i=1}^{m}\\log_2p(s_{i})\\end{align*}$$Right now, I'm taking $M$ to be the sum of the frequency of all unigrams (not including the (START, ) unigram). Then I multiply $M$ by the sum of the log probability for each sentence in the corpus. My perplexity is too low (less than 10) so I'm wondering if this is the source.",
        "target": "Update: I found the issue! M is the number of tokens in the corpus (test file). Perplexity is normal now."
    },
    {
        "source": "#117: Regarding where to submit HW. Hi Daniel and TAs, I would like to resubmit my assignment but I could not find where to upload it. I submitted my HW1 at Canvas earlier using the upload box at the end of the instruction page but it disappeared after the first submission. I also could not find the course at Gradescope if we are using that. Thank you very much!",
        "target": "I can see your submission on Courseworks. You should be able to resubmit a new version. Did the entire submission box disappear? We will only use Gradescope for providing feedback/grades on the midterm and final. A few students asked about the Gradescope link though -- did I accidentally specify Gradescope as the submission method for assignments somewhere? Thanks ."
    },
    {
        "source": "#116: Raw Probability vs Maximum Likelihood Estimate. Hello, I just wanted to clarify that I understand the difference between the regular probability equation, and the MLE equation for getting the probability of n-grams. E. g. for a bigram:raw probability:P\\left(w\\ \\ \\left|u\\right|\\right)=\\frac{count\\left(u, w\\right)}{count\\left(u\\right)}mle probability:\\left(P\\ \\left(w\\right|u\\right)=\\frac{count\\left(u, w\\right)}{\\sum_{u'\\ \\in u}^{ }count\\left(u', w\\right)}Can you explain what u' means here? Is the count(u', w) the count of the total number of bigrams that contain a bigram? I am confused how the denominator of the MLE probability works, how can you sum of the counts if there is only one count of the bigram (u, w)?",
        "target": "Hm, the \"raw\" probabilities are the maximum likelihood estimates for the training data. Where did you get the second formula from? Did you mean? P(w | u ) = \\frac{count(u, w)}{\\sum_{w' \\in V} count(u, w')}That's equivalent to your first formula because \\sum_{_{w'\\ \\in V\\ }}^{ }count\\left(u, w'\\right)  = count(u)"
    },
    {
        "source": "#115: HW1 - Interlude. Hi, I have 2 questions regarding the interlude:1. When handling the edge case of UNK being the only possible next word, are we allowed to use any methods other than beam search? For instance, if we're stuck on UNK after multiple resamples, can we prematurely end the sentence? 2. The two given examples all end in periods. Is this a requirement at the end of the generated sentence? Thank you!",
        "target": "Since the interlude is ungraded you can solve the UNK edge case either way. The sentence is not required to end in a period. "
    },
    {
        "source": "#112: HW1 - Change in Perplexity but not in Accuracy. Dear teaching team, Just out of curiosity: I made a little change to my code to account for STOP words when calculating perplexity, and it reduced my overall perplexity (from ~290 to ~216 when running for the train file). However, my accuracy remained exactly the same. I wonder if there is any expected theoretical justification for this or if it should be just a random occurrence (I did some sensitivity analysis and didn't seem to be a bug at least). My guess is that it changes perplexity equally for both models when evaluating the essays, and therefore does not change how they are split into the two categories, but I'm not so confident about it. Thank you!",
        "target": "So since you are comparing the perplexity scores between the two models for the same data, the perplexity simply scales (and normalizes) the log probability for each test document. It won't change the relative order of the scores at all. "
    },
    {
        "source": "#112: HW1 - Change in Perplexity but not in Accuracy. Dear teaching team, Just out of curiosity: I made a little change to my code to account for STOP words when calculating perplexity, and it reduced my overall perplexity (from ~290 to ~216 when running for the train file). However, my accuracy remained exactly the same. I wonder if there is any expected theoretical justification for this or if it should be just a random occurrence (I did some sensitivity analysis and didn't seem to be a bug at least). My guess is that it changes perplexity equally for both models when evaluating the essays, and therefore does not change how they are split into the two categories, but I'm not so confident about it. Thank you!",
        "target": "Adding on to the above with some anecdotal evidence: I noticed a small increase in accuracy when i got perplexity down from 400s to 200s, but tweaks beyond that didn't change anything."
    },
    {
        "source": "#111: Comment out main function body? . Hello, quick question: do we have to comment out everything under the main function? Should we be leaving anything within the body of main?",
        "target": "(not a TA) I believe they said they would just be testing individual methods, nothing in main "
    },
    {
        "source": "#110: Handling end-of-sentence indicators. Hi Teaching Staff, 1) Can you elaborate on how periods/exclamation points/question marks (as end-of-sentence indicators) are supposed to be handled when we count_ngrams? Take for instance this processed sentence from brown_train. txt:['the', 'fulton', . .. , 'took', 'place', '. ']\nand accounting for all the edge cases mentioned on Ed:INCLUSIONS: [('STOP'), ] in unigramcounts |  [('word', 'STOP')] and [('START', 'word')] in bigramcounts | [('word', 'word', 'STOP')] and [('START', 'word', 'word') and [('START', 'START', 'word')]in trigramcountsEXCLUSIONS: [('START')] in unigramcounts | [('START', 'START')] in bigramcounts2) Would [('. '), ('STOP')] be a valid bigram? 3) How about [('place'), ('. '), ('STOP')] as a valid trigram? 4) If valid, why are these ngrams important to account for in our model? If not valid, do we implement code to ignore {. ?! }? Thank You.",
        "target": "Punctuation symbols should be handled like any other token. So [('. '), ('STOP')] and [('place'), ('. '), ('STOP')] would be a correct bigram / trigram. We want the model to learn that a sentence typically ends in a period. "
    },
    {
        "source": "#108: Question about HW1 Output. Hello, This is what I got for output:train: 16. 091318233161303test: 208. 49720370974467accuracy: 0. 8446215139442231I wonder if this perplexity is in the acceptable range?",
        "target": "Yes, that seems okay to me. "
    },
    {
        "source": "#107: Assignment Submission. Hello, I just wanted to confirm that we need to submit the assignment only on Courseworks.",
        "target": "Correct. "
    },
    {
        "source": "#105: Division by zero error. Hi, I am getting this super weird division by zero error where my unigram count for a specific unigram for some reason is zero when running Part 7. I thought that if it was zero, it should be replaced by \"UNK\".  Would love any insight into why this could be happening my below code! It works for parts 1-6. For some reason, this happens with a lot of words such as: ('discovers', ). Any help is appreciated! Traceback (most recent call last):\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 273, in &lt;module&gt;\n    acc = essay_scoring_experiment('ets_toefl_data/train_high. txt', \"ets_toefl_data/train_low. txt\", \"ets_toefl_data/test_high\", \"ets_toefl_data/test_low\")\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 228, in essay_scoring_experiment\n    pp_2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 210, in perplexity\n    log_prob. append(self. sentence_logprob(sentence))\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 193, in sentence_logprob\n    nonlog_trigram_prob = self. smoothed_trigram_probability(trigram)\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 172, in smoothed_trigram_probability\n    bigram_probs = self. raw_bigram_probability(bigram)\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 138, in raw_bigram_probability\n    return self. bigramcounts. get(bigram, 0) / unigram_count\nZeroDivisionError: division by zero\nraw_bigram_probability():def raw_bigram_probability(self, bigram):\n    \"\"\"    COMPLETE THIS METHOD (PART 3)    Returns the raw (unsmoothed) bigram probability    \"\"\"    unigram = (bigram[0], )\n    unigram_count = self. unigramcounts[unigram]\n\n    if bigram[0] == \"START\":\n        unigram_count = self. sentence_count\n    if unigram_count == 0:\n        print(unigram)\n        return self. bigramcounts[bigram] / 1\n    return self. bigramcounts[bigram] / unigram_count\nessay_scoring_experimentdef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n\n        model1 = TrigramModel(training_file1)\n        model2 = TrigramModel(training_file2)\n\n        total = 0\n        correct = 0       \n \n        for f in os. listdir(testdir1):\n            pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n            pp_2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n\n            if pp &lt; pp_2:\n                correct += 1\n            total += 1\n        for f in os. listdir(testdir2):\n            pp = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\n            pp_2 = model1. perplexity(corpus_reader(os. path. join(testdir2, f), model1. lexicon))\n\n            if pp_2 &gt; pp:\n                correct += 1\n            total += 1\n        \n        return correct / total\n",
        "target": "Hm your version of raw_bigram_probability does not appear to match the error message. Did you maybe change it? "
    },
    {
        "source": "#102: HW1 debug. Is there any suggestions on how to debug the code, Like any part need to pay attention on? I already go through my code several times but got the followings which I think are not that correct. ..",
        "target": "That looks pretty good to me. "
    },
    {
        "source": "#101: Hw1 part6. Do we need to count the word 1 more than each sentence when computing perplexity? Because I think there will be a hiding STOP for each sentence. So the total word tokens is the words in document plus number of sentences. Am I understanding this correctly?",
        "target": "Yes, you need to include the STOP occurrences -- there will be one per sentence. "
    },
    {
        "source": "#98: No access to gradescope. I found out that I am enrolled in the class but not the gradescope. Is there an entry code that I should use to enroll myself? Clicking the link on canvas didn't seem to help. My netid: zw2958",
        "target": "We wont be using Gradescope until the midterm. At that point we will enroll you automatically. For homework assignments we use Courseworks. "
    },
    {
        "source": "#97: Count clarification. Hello! I have a couple questions regarding the count_ngrams method. 1. For some reason, the counts return 0 for words starting with an uppercase letter (i. e. model. unigramcounts[('State', )] = 0) while returning some number for the same words starting with a lowercase letter ( i. e. model. unigramcounts[('state', )] = 830). Is this phenomenon supposed to happen? 2. To double check what previous people have asked regarding how to count the 'START' and 'STOP' tokens, we do not count the unigram 'START' but we do count the unigram 'STOP'.  (meaning model. unigramcounts[('START', )] = 0). Meanwhile, bigramcounts should count bigrams (START, w_1) and (w_n, STOP) and trigramcounts should include trigrams (START, START, w1), (START, w1, w2) and ( w_n-1, w_n, STOP). The bigram (START, START) should be ignored however in favor of finding the number of sentences.",
        "target": "1. The `corpus_reader` function provided automatically lowercase all incoming words. So all ngrams will be lowercase by default. Which explain why the vocab will not contain any uppercase words. 2. I believe you can either choose to not include it in the Counters, or another approach is to just ignore START and return 0 when raw_unigram_probability is given START as an argument. 3. bigram (START, START) should be ignored as it does not provide any context to the model similar to Unigram (START)"
    },
    {
        "source": "#96: Emergency Extension  Assignment 1. Dear Professor and TAs, Hello, my name is Charles Yoon and I am currently at the hospital after being transported to the ER at around 4 AM. I began having intense back pain Tuesday that immobilized me completely, rendering me unable to walk, stand, or really do anything except lay down and use my phone. I thought it would get better if I rested in bed after taking some Tylenol, but it kept getting worse and once my legs started to lose feeling, I was immediately taken to the ER by CUEMS, where I was diagnosed with a herniated disc. The doctors want me to stay in the hospital for the time being to receive proper care, and unfortunately I will not be able to type or submit a coding assignment by Thursday. Would it be possible to be granted an emergency extension? Thank you. I am willing to provide any documentation that might be necessary.",
        "target": "Sorry to hear; that doesnt sound good. You can submit homework 1 until Monday October 2nd. Beyond that, it may be better to drop homework 1 from grading and reweigh the remaining deliverables. I hope you feel better soon. "
    },
    {
        "source": "#95: Where to start in reducing perplexity (Part 6). Hi! I currently have a low-300s perplexity score and was wondering where to start looking to get this score down? Thank you!",
        "target": "Hello! One thing that helped me was making sure to account for the edge cases mentioned in other discussions here related to 'START' tokens and word counts. For unigram ('START') and bigram ('START', 'START'), making sure that the count and probability of those was handled properly. I. e. , for counts, how many sentences would kind of implicitly begin with ('START') and ('START', 'START'), and for probabilities, what should the chances of our model generating a 'START' token be?"
    },
    {
        "source": "#93: Part 7 Error Running. Hi! When running part 7, I keep stumbling on an error: FileNotFoundError: [Errno 2] No such file or directory: 'hw1_data/ets_toefl_data/test_low/1443872. txt'I have set the file paths properly: Acc =essay_scoring_experiment(\"hw1_data/ets_toefl_data/train_high. txt\", \"hw1_data/ets_toefl_data/train_low. txt\", \"hw1_data/ets_toefl_data/test_high\", \"hw1_data/ets_toefl_data/test_low\") I was thinking maybe it had something to do with the command I was running so could you please provide me with clarification on what to follow with here \"python trigram_model. py ____\"",
        "target": "For part 7 the command line parameters shouldnt matter because the path names are all hardcoded. I wonder if you possible use the wrong directory name for one of the test batches in essay_scoring_experiment. "
    },
    {
        "source": "#92: Is 95% accuracy too high? . My accuracy is surprisingly high (0. 958). My perplexity for Q6 was 196. 5. I've checked my previous methods so many times; not sure which step could've gone wrong. ",
        "target": "You seem to be using model 2 twice when computing perplexity for the \"low\" test data. So essentially all of the \"low\" documents are classified as correct :)"
    },
    {
        "source": "#90: Really low perplexity/high accuracy for question 6&7. Hi, Not sure if I can ask this here therefore put it on private but for question 6 &amp; 7, at first for raw_trigram_probability I returned 1/word_count (where word_count is total number of words in training data) for when the count(u, w) would be 0 and got a perplexity of around 250 and accuracy of 73% for Q6 &amp; Q7. After I changed  the raw_trigram_probability for when the count(u, w) would be 0 to be 1/lexicon_size (number of unique unigrams except for START), I got a perplexity of around 28 and accuracy 99%. Is this possible or is this too good to be true and I accidentally made something wrong here ?",
        "target": "Accuracy of 99% is definitely not right -- I suspect there is something wrong with your experimental setup. "
    },
    {
        "source": "#89: HW1 Part 3 Start. Regarding the probability of the unigram ('START', ), would this be a special case where we need to divide the number of sentences in the corpus by the total unigram count?",
        "target": "The unigram ('START') should never be predicted by the language model and therefore it should have a probability of 0. "
    },
    {
        "source": "#88: Resubmitting File New Name. Hi! I just submitted my lab and I want to double check that it's okay that the file is now called trigram_model-1. pyThanks! ",
        "target": "This is fine -- Courseworks automatically renames resubmissions this way. "
    },
    {
        "source": "#87: debugging perplexity. Hi, I am getting perplexity scores which are very high. 3. 6668123503602652e+50 Is there something I should check that maybe be throwing this number way off. Thanks!",
        "target": "I recalled having the same problem yesterday, several things to check:1. ensure your count_ngram is correct(as the example in the homework description page)2. special cases in raw_trigram_probability, raw_bigram_probability, raw_unigram_probability , I recommend checking #55, and that you are assigning zero to those bigrams/trigrams that didn't appear(thus numerator being zero)3. check the perplexity denominator of l(should be number of tokens appeared in testing)Hopefully these will help you debug this. I recalled not having any problem with sentence_logprob , but having problem mostly with the 4 functions mentioned above"
    },
    {
        "source": "#87: debugging perplexity. Hi, I am getting perplexity scores which are very high. 3. 6668123503602652e+50 Is there something I should check that maybe be throwing this number way off. Thanks!",
        "target": "I also had this issue because I didn't realize there was a difference between m and M. m is the total number of sentences while M is the total number of word tokens"
    },
    {
        "source": "#86: HW1 Part 3 Clarification. Just to ensure my understanding of Part 3 is correct, is calculating the unigram probabilities just P(v) = count(v) / |V|? Meanwhile, is finding the bigram probability just P(w|u) = count(u and w)/count(u) while finding the trigram probability just P(w|u, v) = count(u, v, w)/count(u, v)? (not taking into account the edge cases). ",
        "target": "The unigram probability should be count(u) / N (where N is the total number of tokens in the corpus). "
    },
    {
        "source": "#85: Will we penalized if part 6 and 7 take a min or two to run? . Hello! For clarification purposes, if, for parts 6 &amp;7, our program takes a minute or two to display an output, will that negatively impact our grade? Or as long as an output is rendered, are we fine? Thank you in advance! ",
        "target": "Chances are that, in that case, you are not using dictionaries efficiently. We may take a small deduction if the program is inefficient. That being said, iterating through the training corpora may take a minute or so during training. "
    },
    {
        "source": "#84: in continution to previous post (not sure if approach was allowed to be shared). when I run the \"generate_sentence\" function, I keep getting sentences that have no meaning at all such as the following, and the probability of the \"STOP\" token is very small and almost constant (of the order of e-5) everytime, does this make sense and is it the same for anyone else ? Example outputs with a t of 10:['distinguish', 'proteases', 'sell', 'neil', 'whichever', 'corner', 'dogmatically', 'hilum', 'talking', '%']['yearned', 'dominant', 'timeless', 'townsmen', 'mountains', 'interpreter', 'prize', 'assume', 'depressed', 'agreeing']['instantly', 'covering', 'enforcing', 'disrespect', 'peeled', 'blasphemous', 'analyzing', 'charming', 'wrath', \"o'dwyers\"]Not sure where I'm going wrong. Approach:1) for each word in the lexicon, generate a trigram probability2) Draw a random number r between 0 and 1. 3) Iterate through every word in the lexicon, keeping a sum of event probabilities. 4) Once the sum exceeds r, return the current event. is my step 1 correct? am I approaching it the wrong way?",
        "target": "Any clarification on this would be very much appreciated! Thank you!"
    },
    {
        "source": "#83: Re: \"generate_sentence\" function. when I run the \"generate_sentence\" function, I keep getting sentences that have no meaning at all such as the following, and the probability of the \"STOP\" token is very small and almost constant (of the order of e-5) everytime, does this make sense and is it the same for anyone else ? Example outputs with a t of 10:['distinguish', 'proteases', 'sell', 'neil', 'whichever', 'corner', 'dogmatically', 'hilum', 'talking', '%']['yearned', 'dominant', 'timeless', 'townsmen', 'mountains', 'interpreter', 'prize', 'assume', 'depressed', 'agreeing']['instantly', 'covering', 'enforcing', 'disrespect', 'peeled', 'blasphemous', 'analyzing', 'charming', 'wrath', \"o'dwyers\"]Not sure where I'm going wrong.",
        "target": "Also getting this problem with responses like:['lilly', 'preferred', 'banks', 'the', 'banks', 'preferred', 'the', 'butcher', 'shop', 'shop', 'the', 'took', 'f or', 'acting', 'preferred', 'family', 'butcher', 'butcher', 'preferred', ', ', 'shop', ', ', 'but', 'will', 'took' , 'be', 'took', 'butcher', 'where', 'for', 'the', 'butcher', 'butcher', 'the', 'the', 'took', 'shop', 'chronicle s', 'students', 'help', 'of', 'family', 'leveling', 'relationships', 'took', 'their', 'shop', 'preferred', 'the', 'the', 'reports', 'students', 'privacy', 'her', '. ', 'loneliness', 'continue', 'STOP'] If you increase t you see STOP more frequently but I'm getting a lot of repeated words and nonsensical sentences. "
    },
    {
        "source": "#82: High Perplexity Score. Hi TAs, I believe I have done parts 1-5 correctly; however, my perplexity score is still very high. I have made sure to not count ('START', ) in my unigramcounts, but I still achieve a 325. 7 as my score. I would appreciate any guidance on how I can reduce this number. Thanks! Here is my code for reference:def get_ngrams(sequence, n):\n    \"\"\"\n    COMPLETE THIS FUNCTION (PART 1)\n    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n    This should work for arbitrary values of n &gt;= 1 \n    \"\"\"\n    temp = ((['START']*(n-1)) + sequence + ['STOP']) if n &gt;= 2 else (['START'] + sequence + ['STOP'])\n\n    res = []\n    for i in range(len(temp) - n + 1):\n        res. append(tuple(temp[i:i+n]))\n\n    return res\n    def count_ngrams(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts. \n        \"\"\"\n\n        self. unigramcounts = {} # might want to use defaultdict or Counter instead\n        self. bigramcounts = {} \n        self. trigramcounts = {} \n\n        ##Your code here\n\n        for sentence in corpus:\n            for gram in get_ngrams(sentence, 1):\n                if gram ! = ('START', ):\n                    self. unigramcounts[tuple(gram)] = 1 + self. unigramcounts. get(tuple(gram), 0)\n\n            for gram in get_ngrams(sentence, 2):\n                self. bigramcounts[tuple(gram)] = 1 + self. bigramcounts. get(tuple(gram), 0)\n            \n            for gram in get_ngrams(sentence, 3):\n                self. trigramcounts[tuple(gram)] = 1 + self. trigramcounts. get(tuple(gram), 0)\n\n        self. total_words = sum(self. unigramcounts. values())\n        print(self. total_words)\n\n        return\n    def raw_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n\n        numerator = self. trigramcounts. get(trigram, 0)\n        denominator = self. bigramcounts. get(trigram[0:2], 0)\n\n        if denominator == 0:\n            return 1/len(self. lexicon)\n        \n        return numerator/denominator\n    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        numerator = self. bigramcounts. get(bigram, 0)\n        denominator = self. unigramcounts. get(bigram[0:1], 0)\n\n        if denominator == 0:\n            return 1/len(self. lexicon)\n        \n        return numerator/denominator\n    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.\n        \n        return self. unigramcounts. get(tuple(unigram), 0)/self. total_words\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation). \n        \"\"\"\n        lambda1 = 1/3. 0\n        lambda2 = 1/3. 0\n        lambda3 = 1/3. 0\n\n        return lambda1 * self. raw_trigram_probability(trigram) + lambda2 * self. raw_bigram_probability(trigram[1:3]) + lambda3 * self. raw_unigram_probability(trigram[2:3])\n    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        trigrams = get_ngrams(sentence, 3)\n        res = 0\n\n        for i in sentence:\n            self. total_test += 1\n            \n        for gram in trigrams:\n            res += math. log2(self. smoothed_trigram_probability(gram))\n\n        return res\n    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6) \n        Returns the log probability of an entire sequence.\n        \"\"\"\n        res = 0\n\n        for sentence in corpus:\n            res += self. sentence_logprob(sentence)\n\n        res /= self. total_test\n\n        return 2**(-1 * res)\n",
        "target": "My question has been answered!"
    },
    {
        "source": "#79: Hw 1 Part 6 Perplexity Score. I'm getting a really, really high perplexity and am struggling to figure out why. I went step by step through 1-5 which all seemed to work smoothly, but for perplexity, I am getting around a 6000 score. Did anyone else get a score in this range and figure out why?",
        "target": "If your 1-5 worked smoothly, I would suggest you looking into the part where you calculate the log probabilities.  If you still have the problem, you can make a private post so the TA can take a look at your code."
    },
    {
        "source": "#78: hw1 pt7. Is it acceptable if I get an accuracy of 73. 7% for \"essay_scoring_experiment\"?",
        "target": "That seems a little low, unfortunately. People are reporting accuracies around 84% or 85%. "
    },
    {
        "source": "#77: Testing hw 1 part 6. HelloHow what command do I use to test part 6? When I run python trigram_model. py hw1_data/brown_test. txt I get an IndexError. ",
        "target": "You also need to provide the test corpus as a second command line parameter. "
    },
    {
        "source": "#76: Start Clarifications. Hi, I am confused on how to implement start. I currently have: In get_ngrams I add START as a unigram. I do not count start as one of the words in the lexicon. In raw_bigram_prob, if it is START, WORD i calculate probability by doing bigramcounts[bigram]/sentencecount. In raw_trigram. If I have START, START, WORD, then i get prob by saying trigramcounts[trigram]/sentencecount. (I did not include any special cases for START, WORD, WORD as that did not seem to require one)Are these correct assumptions. I am getting errors in my code and trying to debug. Thanks  ",
        "target": "That sounds all correct to me. "
    },
    {
        "source": "#75: On Suggestion for \"P(v | u, w) not clear\", Part3 HW1. Dear teaching team, I was revising the homework before submitting and one detail made me curious. In Part3, we read the following: My intuition says that the second alternative, using P(v) as a proxy, would be better since assuming a uniform distribution across all words could introduce bias. Is there any reason why the first alternative is the one recommended? Thank you!",
        "target": "The first version empirically works better on this data. I suspect its because the context is a very strong indicator for what the next word should be, so using the unigram probability will assign too much probability mass to some words. "
    },
    {
        "source": "#74: Strange Numbers for Part 6. Hi! I am getting way too big numbers (about 10/10^2 off) from what has been mentioned on Ed. I'm not sure what is wrong with my code as each step seems to work/make sense on its own. Any tips as to what might be causing the issue? Thanks!",
        "target": "One thing Im noticing is that your smoothed_trigram_probability function isnt quite right. For a trigram (u, v, w) you are using u as the unigram and (u, v) as the bigram, rather than w for the unigram and (v, w) as the trigram. Additionaly, i dont think you need to worry about the case (START, w) and (START, START, w) in the smoothed probability if you are handling it correctly in the raw probability functions. "
    },
    {
        "source": "#73: H1 Clarification. Hi, I have a few questions needs to be clarified: In method count_ngrams, should we count the number of START tokens as 0? i. e. model. unigramcounts[('START', )] = 0 or regular counts number? Related to the question 1, if we DO NOT count the START tokens, then case ('START', x) would be a special case (division by zero). If so, should we treat ('START', x) = (x)? If we COUNT the START tokens, then case ('START', x) would be a regular case. Related to the question 2, should we assign 0 possibility to START in raw_unigram_probability? If we do so, then ('START', x) would be a division by zero case again. In method raw_trigram_probability, if count(u, v) = 0, then we use 1 / |V|. For |V|, should we minus the START token from the size of the lexicon? i. e. should we use 1 / (len(self. lexicon) - 1) or 1 / len(self. lexicon)? In method raw_trigram_probability, should we treat ('START', 'START', x) = ('START', x)? Or should we treat it as a regular case, and use number of sentences in corpus for ('START', 'START')? For essay score, I got 83. 6. Is this score acceptable or does it still indicate that there are some minor errors in my code? ",
        "target": "1. I think the START counts should be all 0. At least the probability for the start unigram should be 0. 2. You will have to treat (START, x) as a special case -- i recommend storing the number of sentences in a new instance variable, and then using this for the denominator. The same applies to (START, START, x). 3. Yes P(START) should be 0. 4. START should not be included in the lexicon. So either remove it from the lexicon or use 1 / (len(self. lexicon) - 1). 5. These two approaches should be equivalent since count(START, START, x) = count(START, x). 6. That seems just a tad low. Most implementations I have seen get 84-85%. "
    },
    {
        "source": "#71: HW 1 Part 6 -- Training vs Testing Data. Hello, Just making sure, but, in part 6, m and M refer to sentences and word tokens within the test data, correct? Does this also mean that before part 6, any mention of word count or sentence count refers only to the train data? ",
        "target": "Hm, I'm not sure I understand. When computing the probabilities that make up the mode, you can only rely on the training data. For the perplexity calculation, m and M refer to the sentence and token count in the test data."
    },
    {
        "source": "#69: Interlude hint. Write the method generate_sentence, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\", \"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each. Draw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated. Wanted to get the hint on the above as mentioned in courseworks.",
        "target": "In Python, I believe you can use random. choices while supplying the optional weights variable. "
    },
    {
        "source": "#69: Interlude hint. Write the method generate_sentence, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\", \"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each. Draw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated. Wanted to get the hint on the above as mentioned in courseworks.",
        "target": "There are several approaches here. The simplest one for our use case:1. Draw a random number r between 0 and 1. 2. Iterate through the different event, keep a sum of event probabilities. 3. Once the sum exceeds r, return the current event. Another easy approach is to use the random. choice function: &gt;&gt;&gt; numpy. random. choice(['A', 'B', 'C'], p = [0. 7, 0. 2, 0. 1]) You should also skip UNK tokens. An easy way is to simply sample the next token again if you end up with UNK. "
    },
    {
        "source": "#67: Expressions for computing raw probabilities of the following. Just wanted to ask the exact expressions to compute the Raw Probabilities of (START, START) in case of Bigram, (START, X) in case of Bigram, (START, START, X) for trigram, and (START, X, X) for Trigram, as we would not be computing the unigram probabilities for START.",
        "target": "Hello! One thing that helped me was to distinguish between when we use probability versus count and which formulas use which. Some formulas will want the count of how many times a given ngram (such as START, START) occur in our corpus, and some will want the probability of that bigram, aka, what should the chances be of our model generating that bigram be. "
    },
    {
        "source": "#66: (\"START\", ) unigram. Per previous discussions, I understand that it does not make statistical sense to include the (\"START\", ) unigram when counting the total number of tokens in the corpus and each unigram's counts. Since the get_ngrams is only called when 1) calculating the counts, and 2) calculating sentence log-prob (with n=3), why do we even return (\"START\", )  as part of the unigrams list? Isn't this the equivalent of returning (\"START\", \"START\") when n=2 and (\"START\", \"START\", \"START\") when n=3 both which we do not do. ",
        "target": " In the comments of this https://edstem. org/us/courses/46417/discussion/3467312 post, I saw Prof. Daniel reply:\"It makes sense to not generate it at all. Feel free to implement accordingly (even if this contradicts the example). \" to the same question. Keeping the post up for more visibility if anyone else is curious. "
    },
    {
        "source": "#64: What is an acceptable perplexity score? . Hello, Is 287 an acceptable perplexity score?",
        "target": "That sounds reasonable. "
    },
    {
        "source": "#63: Interlude - Generating text. Hi, I am curious if we get additional marks for implementing Interlude - Generating text. Kind regards, Junbang",
        "target": "No, this part is entirely optional. "
    },
    {
        "source": "#62: Perplexity Score. So the instruction says that for step 6, the perplexity should be less than 400, is 308 acceptable? cuz i saw in other posts saying 304 is too high",
        "target": "Yes, I think most are in the range of 150 or so. There is some variation here in how you handle some corner cases. So 300 may be okay. "
    },
    {
        "source": "#61: Creating \"model\" for Part 2. Hi, I am not sure if this is a naive question but I was a bit confused about how to set up model to test count_ngrams in Part 2. The instructions say \"Where model is an instance of TrigramModel that has been trained on a corpus. \" but I am not totally sure what this means. Thank you in advance for any help!",
        "target": "Oh, we just mean that the variable \"model\" refers to a TrigramModel after it has been trained, for examplemodel = TrigramModel(\"brown_train. txt\")"
    },
    {
        "source": "#60: running test for part 7. Hi I hope you are doing great when I am running the test for part 7, i commented out the two lines of code in the mainwhen i run it on the command line i am runningpython trigram_model. pyI am getting an EOL error so I think I am missing argument.  Is there something else we should be including on the command line for testing part 7thank you so much!",
        "target": "this is resolved, thank you so much!"
    },
    {
        "source": "#59: Query regarding ngrams when n=4. Hello, For the sequence [\"natural\", \"language\", \"processing\"] we know that the first trigram is ('START', 'START', 'natural')I wanted to confirm that for the sequence [\"natural\", \"language\", \"processing\", \"class\"] the ngrams when n=4 would be('START', 'START', 'START', 'natural'), ('START', 'START', 'natural', 'language'), ('START', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'class'), ('language', 'processing', 'class', 'END')",
        "target": "Yes, that's correct. "
    },
    {
        "source": "#58: Question about Part 7. Hi I hope you are doing great.  I have been wrestling with part 7 for a while and I think I am having trouble with the intuition of what its happening in the code of this part. there is one loop going through all the test files of group 1 and for each file is determining a perplexitythis is follwed bya second loop going through all the test files of group 2 and for each file is determining a perplexitybut group1 - is all test_highand group 2 - is all test_lowso how do we actually compare the perplexities to determine if something is correct? thank you so much!",
        "target": "For each document in group 1, run both models. If the \"high\" model has lower perplexity than the \"low\" model, you are classifying the document correctly, so you count 1 \"correct\" point. Otherwise, you are classifying it incorrectly. Then you repeat the same for each document in group 2, except that now the \"low\" model should have lower perplexity than the \"high\" model. "
    },
    {
        "source": "#57: sanity check on # of tokens of brown_train. I'm on part 6 of the hw and I'm getting really low perplexity values--like ~5 and ~3 When I find the summation of tokens in the corpus of brown_train. txt I get 98203 tokens, does this seem right? I figure I'm doing the smoothing or above incorrectly or I'm counting tokens incorrectly. Thanks in advance!",
        "target": "Make sure you use the number of tokens in the test data for M when you compute perplexity."
    },
    {
        "source": "#56: Clarification for M and N. M in the equation for calculating perplexity means the total number of word tokens = number of unigram tokens, so does that mean M = (sum(self. unigramcounts. values()) - 1) The -1 here to exclude the START? Or does that mean M = sum(len(sentence) for sentence in corpus)? Same question for when counting the raw_unigram_probability, do we use(sum(self. unigramcounts. values()) - 1) or sum(len(sentence) for sentence in corpus) for N? ",
        "target": "M is the number of tokens in the test data, so your second expression. Similarly, N is the number of sentences in the test data. "
    },
    {
        "source": "#54: Homework1 Part 3 Trigram ('START', 'START', 'the'). I tested the model. raw_trigram_probability(('START', 'START', 'the'))butself. bigramcounts[('START', 'START')]  does not exist. (KeyError: ('START', 'START'))Should we ignore this case? Thank you.",
        "target": "No, this is a special case you need to account for. My suggestion would be to keep track of the total number of sentences in the training data in a new instance variable, then use that value as the denominator when you compute P(the | START, START). "
    },
    {
        "source": "#53: HW 1 q3. Hi, I wanted to make clarify:The raw uni/bi/trigram probability methods returns the probability for a specific ngram and not for the whole set, correct? Best, Omer",
        "target": "Right this method returns a single P(w|u, v) probability. "
    },
    {
        "source": "#52: Questions regarding Assignment 1. HI! I hope you have a great day! There are a couple of questions in HW1:I know someone already asked this, but I'm unsure if I fully grasped the concept. Is the reason why we don't encounter cases where unigram(prior) = 0 in bigrams (given (prior, word)) because we treat those values as UNK? We don't count the START token, but why do we include START in the lexicon? In raw_unigram_probability, one of the questions that was already asked was if we should return 0 when encountering the 'START' or 'STOP' token, and the answer indicated 'yes'. I understand for START since it's not included, but shouldn't the count for STOP be non-zero since it's included? In raw_bigram_probability and raw_trigram_probability, we didn't include (START) in unigrams and (START, START) in bigrams. Should we set the value to the number of sentences in the corpus when we need to count these cases to get the denominator? Similarly, for the bigram (START, START), is this different from the case where count(u, v) = 0? For computing perplexity, is M the number of words before being processed by get_ngrams? Or is it the count after adding the STOP token? Thank you so much!",
        "target": "1. correct -- all unseen tokens have been replaced with UNK by the corpus reader. There should not be any cases where you encounter a unigram in the test data that has not been seen during training. 2. It's not really necessary to include START in the lexicon. In fact, when you use the size of the lexicon make sure START is not included. 3. It's debatable if you should consider STOP as part of the unigram distribution, but I think it makes sense to include it (thus not set it to 0). Imagine generating a document by randomly sampling from this distribution repeatedly until you encounter STOP. 4. Yes, you can use the number of sentences in the training corpus here. 5. Yes, that's a different scenario. You DO know the distribution of words at the beginning of the sentence. 6. When computing perplexity, M is the number of tokens in the test data, including STOP (i. e. , the number of predictions the language model makes on the data. "
    },
    {
        "source": "#51: Ungraded n-gram Language Model. In the solution \"There are 6 words, plus the END symbol, so |V|=7\". I understand that there are 6 words, but why do we not count \"START + END\" and only count \"END\"? ",
        "target": "When you think of the model as generating sentences by drawing the next token from the conditional distribution, given the context, it will at some point select the STOP token. But it will never sample the START token because there is no probability for P(START | x). "
    },
    {
        "source": "#50: Acceptable perplexity score. Hello, what is an acceptable perplexity score? Is 304 acceptable?",
        "target": "That would be a little bit too high, but we will let you guys know what would be the baseline for this part"
    },
    {
        "source": "#49: HW1 Part 3 Question. Hi, I was wondering if the case that we encounter when \"count(u, w) is also 0\" in the trigram probability model will also apply to the bigram probability model. Will we ever see a time where we haven't seen the first bigram word in a sequence and we should do the same uniform distribution as with the trigram case?",
        "target": "You mean the case where you havent seen the context u for a bigram (u, v)? That cannot happen because there are no unseen unigrams. The corpus reader automatically replaces them with UNK. "
    },
    {
        "source": "#48: Acceptable Essay Scoring Accuracy Score. Hi, What's an acceptable accuracy score for the essay_scoring_experiment() function. Is an 84% accuracy acceptable or should I try and increase it?",
        "target": "That seems okay to me. "
    },
    {
        "source": "#47: Cannot install msilib. Hello, I apologize if this is a very simple fix but I am very, very new to programming in Python and keep getting the following error message: My understanding is that msilib is a Windows specific package (? ) but despite trying to manually install it I keep getting this message. I appreciate any help! Thanks!",
        "target": "Hm, according to the traceback this is an import in the trigram_model. py file? This is definitely not in the template :) see what happens if you delete that line. "
    },
    {
        "source": "#46: HW 1 clarifications. I had a few questions:While computing the raw probability for bigrams, if we come across a word that is not seen before (i. e, unigram count is zero), should we treat it as UNK, and instead use the count for ('UNK', )? While finding the number of words for the denominator in calculating the raw unigram probabilities, if there are repeated words, they should be counted multiple times right? That is, we are not finding the number of unique words but instead finding the total number of words? Similarly, while calculating perplexity, does the denominator (number of word tokens) count repeated words multiple times? Also, while counting the number of word tokens, should we count 'STOP' multiple times, or only once? For the generate_sentence() function, to generate the next word, my understanding is that we should we randomly choose from all words that appears in the given context (last two words). Is this correct?",
        "target": "1.  No the corpses reader does this automatically. If you look at the __init__ method, it takes a lexicon as parameter, which allows the corpus reader to identify unseen tokens. 2. Right. The denominator in the raw unigram probability is the number of tokens (word instances including duplicates) in the training data. 3. Yes. M in the perplexity formula is the number of tokens in the training data. 4. Yes, you would choose the word from the conditional distribution for the context. "
    },
    {
        "source": "#44: Saturday OH. Hi, Is the OH on Saturday still happening or is there a different link that is being used?",
        "target": "Hi, May had to move her office hours to Thursday for this week only. Sorry for the inconvenience. "
    },
    {
        "source": "#43: Assignment 1 format. Hi, I am wondering if we need to keep the directory in the original code when submitting. In order to test, I have changed it to my own directory and is it necessary to change it back to the original? Best, Xiaoyu",
        "target": "Please change any pathnames in the code back to defaults. I actually dont think we use this for testing because we call the methods individually, but better to be safe. "
    },
    {
        "source": "#42: Raw unigram probabilities in HW1. For the denominator of the raw unigram probabilities, should we include \"STOP\" as part of the total word count, but not \"START\"? ",
        "target": "Thats  correct. Include STOP but not START. "
    },
    {
        "source": "#41: More clarification for hw1. Hi, I have following questions for hw1, 1. Since we count ('STOP'), (? , 'STOP'), (? , ? , 'STOP') in unigramcounts, bigramcounts, trigramcounts respectively. How do we deal with ('START') in unigramcounts, (start, ? ) in bigramcounts, (start, start, ? ) and (start, ? , ? ) in trigramcounts. 2. If we do not store ('START') in unigram dictionary, (start, ? ) in bigram dictionary, (start, start, ? ) and (start, ? , ? ) in trigram dictionary. I am not sure how can we deal with P( (u, v) | 'START'), P(u, v, w | ('START', v) ) and P(u, v, w | ('START', 'START') ) as a special case. 3. Can we use basic numpy functions such as np. sum in our scripts? 4.  Shall I assume that the input sentence has already been tokenized in sentence_logprob(sentence)? 5. Shall I return zero as probability for raw_unigram_probability() when I get tokens 'START' or 'STOP'? 6. Is |V| in raw_bigram_probability() or raw_trigram_probability() the total number of distinct unigrams excluding 'START'? Sorry for the long questions. Kind regards, Junbang",
        "target": "1 and 2: you would count bigrams (START, x) and trigrams (START, x, x). I recommend not counting the bigram (START, START) or the unigram (START, ). Instead, keep track of the number of sentences in a separate instance variable.  3: I would prefer you didnt use anything but in the standard library because it slows down grading. The regular sum function or math. fsum should do the job here. 4: yes, the corpus reader tokenizes the sentences. 5: yes returning 0 makes sense. 6: yes. But you should only need |V|for the raw trigram probability when dealing with unseen contexts. I dont think its used for the bigram probabilities. "
    },
    {
        "source": "#40: Assignment 1 last question. Hi, In the last question in assignment 1, I found that it is common to encounter unseen words from training when calculating perplexity of testing dataset. This results in a probability of zero returned by the smoothed_trigram_probability() and calculating the log probability with zero results in math error. Is this expected and how can we resolve this problem. Kind regards, Junbang",
        "target": "Hmm something is wrong with your setup. There should be no unseen unigrams. The idea is that the model collects the lexicon during training and replaces all words appearing only once with UNK. During testing, the lexicon is passed to the corpus iterator, so that unseen words in the test data are replaced with UNK as well. "
    },
    {
        "source": "#37: meeting id not valid zoom. Hi for the office hours today i am getting the message that the meeting id is not valid.  is there something else i can try?  thank you!",
        "target": "Hi can you try it again? Not sure if its my problem or zoom's problem"
    },
    {
        "source": "#36: Perplexity formula. Hi, Does \"total number of words tokens in the corpus\", which is referring to M, include bigrams and trigrams? Thank you",
        "target": "No, just the unigram tokens, including STOP (but not START). "
    },
    {
        "source": "#35: negative perplexity when tested. Hi I hope you are doing great. I just ran the test for my perplexity with argv[1]  being brown_train and argv[2] being brown_testThe value returned is:-1. 1662575414840142Does this make any sense? Thank you so much!",
        "target": "Something is off, that shouldnt happen. "
    },
    {
        "source": "#35: negative perplexity when tested. Hi I hope you are doing great. I just ran the test for my perplexity with argv[1]  being brown_train and argv[2] being brown_testThe value returned is:-1. 1662575414840142Does this make any sense? Thank you so much!",
        "target": "So several things could happen. Perplexity is computed using the equation 2^l, where l represents the average of the log probabilities of sentences. The value of l should be negative, ensuring that 2^l is positive. If l is used directly as perplexity instead of 2^l, the result would be incorrect. Given your provided numbers, assuming that is you value of  l, calculating 2^l yields an implausibly small perplexity. A potential issue might be with the calculation of M. It's important to note that M should not be the total number of words in the train corpus (self. total_words) but rather the total number of words in the testing corpus. When iterating over the log probabilities of sentences, ensure that you count the number of words in the test set. Additionally, remember to count the \"STOP\" token for each sentence."
    },
    {
        "source": "#34: testing raw probabilities for Q3. Hello, Could we possibly get the raw probabilities for some test ngrams just so I can check my understanding of Q3? Using the 1/|V| rule for where the conditioned on ngram = 0 if possible. .. For example:&gt;&gt; model. raw_trigram_probability[('START', 'START', 'the')]\n\n&gt;&gt; model. raw_trigram_probability[('fake, 'example', 'here')]\n\n&gt;&gt;&gt; model. raw_bigram_probability[('START', 'the')]\n\n&gt;&gt;&gt; model. raw_bigram_probability[('fake', 'example')]\n\n&gt;&gt;&gt; model. raw_unigram_probability[('the', )]\nThank you so much!",
        "target": "You could approximate this by doing some calculations on the corpus, especially for the start token:If you iterate through the corpus and check the first word of every sentence and build a distribution table of that, then you'll find approximately (barring differences stemming from tokenization) how common each word is as a starter word. That should roughly match the result of:&gt;&gt; model. raw_trigram_probability[('START', 'START', 'the')] At least. Because all the bigrams's it's checked against with are START START word and the denominator is just the number of words in the sentence. You should get a decently high value for picking a very common word like The, since lots of sentences start with it."
    },
    {
        "source": "#32: Assignment 1 part 3. Hi, Part 3 wants us to compute the raw probability, then are we still supposed to make P(v | u, w) = 1 / |V| (where |V| is the size of the lexicon), if count(u, w) is 0? Thank you",
        "target": "Yes, you will need a workaround for the situation where count(u, w) is 0 -- otherwise you will end up with a division by 0 error. My suggestion is to set P(v | u, w) = 1 / |V|. "
    },
    {
        "source": "#31: Size of lexicon. Hi, Just for clarification, is the size of lexicon basically the same as number of unigrams? Thank you",
        "target": "It's the size of the set of all unigrams. So if a word appears twice, it's only counted once."
    },
    {
        "source": "#30: Is this allowed to count the total tokens in the text? . Hello! Could I declare a total_tokens class variable, and set it to be the return value of count_ngrams? This way I wouldn't have to traverse the text again. I'm asking since  count_ngrams doesn't originally have any return value. Thanks for your help!",
        "target": "Thats fine. But why not just populate the total_token count inside of count_ngrams? "
    },
    {
        "source": "#29: self in python. Hi I hope you are doing great. I would like to double check my understanding of self  in python if that is okayi know that self refers to  an instance of a class, so if I have an instance variable insti can use it by tying self. inst I think that is right butI am less sure  using methods that have self as argumentsfor example in the definition of this function:def raw_trigram_probability(self, trigram):self is in an argument, so does that meanthat if I later use this method in a different method I can use it like: raw_trigram_probability(self, some_trigram)or do i have to say: self. raw_trigram_probability(self, some_trigram)thank you so much!",
        "target": "class Dog:\n    def __init__(self, name):\n        self. name = name\n    def bark(self):\n        print(\"Woof my name is\", self. name)\n\nrover = Dog(\"Rover\")\nrover. bark()\nThe \". \" passes the variable on the left as the first argument to the function. So to have Rover bark, you can either do Dog. bark(rover) or rover. bark()."
    },
    {
        "source": "#27: Homework Submission. Hi, Do we have to comment out all test code before submit? Thank you!",
        "target": "Yes please. "
    },
    {
        "source": "#26: \"UNK\" wrod when calculating perplexity and generating sentences. Hi, Do we need to include the count of the \"UNK\" token when calculating the perplexity and do we need to skip the \"UNK\" word if the next generated word is \"UNK\" and generate a new word instead? Thanks a lot!",
        "target": "You should include it. From the perspective of the language model UNK is just another token (like any other). "
    },
    {
        "source": "#25: Clarification Questions. Hi, a couple clarification questions:What's the newest version of Python we can use for this assignment? May we import anything in the standard library to work on this? May we define custom helper functions? May we add type hints to all the function signatures? May we save the total token count in TrigramModel. init? If not, where? In TrigramModel. count_ngrams, are we obligated to use get_ngrams, or may we use any function that yields the correct answer? Should unigramcounts include \"START\"? Should unigramcounts include \"STOP\"? Should the \"if name == \"main\" clause do anything in particular when we finish? Can we set it to whatever we want? In TrigramModel. generate_sentence, should the model be capable of producing the \"UKN\" token? Do raw_{uni, bi, tri}gram_count_total accept tuples of one, two, and three strings, respectively? Thank you, Daniel Indictor",
        "target": "That's quite a few questions. Thanks for being efficient. 1. I would be careful using anything past 3. 92. Yes, the standard library is fair game. Third party libraries (including spacy, nltk, and even numpy) should not be used, unless otherwise specified. Generally, the assignments are designed to be solvable with only the information provided. That is, if you need something from the standard library (such as collections. Counter or collections. defaultdict) that will be mentioned in the assignment. 3. Sure4. Yes, because type annotations won't affect functionality. 5. Yes, that's a good idea. 6. I would recommend you use get_ngrams. We will test the function by functionality, so if it works correctly without calling get_ngrams that should be fine. But that suggests you are repeating code. 7. No. 8. Yes. The way to think about counting START and STOP is this: STOP is predicted when the model \"generates\" a sentence, but \"START\" is not. So the unigram event STOP should have some probability assigned to it, but START should not. Of course, then you won't be able to use count(START) when computing P(word | START. I suggest keeping track of the number of sentences instead and treating P(word | START) as a special case. 9. We won't test the \"main\" part, so it can be anything you want. 10. This part is optional, so you implement this anyway you like. But I would suggest avoiding the UNK token in the generated output. 11. Yes "
    },
    {
        "source": "#24: raw unigram probability. Hi I hope you are doing great! I am working on raw unigram probability method.  Taking the hint in the code:#hint: recomputing the denominator every time the method is called # can be slow! You might want to compute the total number of words once, # store in the TrigramModel instance, and then re-use it. My idea is to store the length of the corpus_file as an instance variable.  To calculate this it seems I would have to add to the  corpus_reader method that was given to us.  I don't think we are supposed to do thats  though. Is this what the hint meant, or am I misunderstanding something? Thank you so much!",
        "target": "I would count the total number of tokens (and maybe the total number of sentences) in the count_ngrams method. The problem with modifying the corpus_reader is that this function is separate from the TrigramModel class, so you wouldn't be able to directly store the token count as an instance variable. "
    },
    {
        "source": "#23: Perplexity denominator. For the total number of word tokens here, should we count any padding we've added? Or just the actual words?",
        "target": "You should count the END marker, but not START. The idea is that the number or predictions the model makes does include END tokens, but START is never predicted. "
    },
    {
        "source": "#22: Question about Katz' Backoff Trigrams. I'm confused about how Katz' Backoff for Trigrams works. I wonder if there is a more specific example about it, thank you so much!",
        "target": "The essential idea is: if you count for trigram is not 0, use trigram probability; if it's 0, then backoff to bigram. The idea is that the occurrence of trigram is self contained in bigram, but not the other way around. Then you continue the process from bigram to unigram and etc. The alpha term is a normalizing factor representing residual probability for backoff to ensure a normalized final probability distribution (summing up to 1). "
    },
    {
        "source": "#21: testing count ngrams. Hi I hope you are doing great! I have a question about these instructions:Counting n-gramsNow it's your turn again. In this step, you will implement the method count_ngramsthat should count the occurrence frequencies for ngrams in the corpus. The method already creates three instance variables of TrigramModel, which store the unigram, bigram, and trigram counts in the corpus. Each variable is a dictionary (a hash map) that maps the n-gram to its count in the corpus. For example, after populating these dictionaries, we want to be able to query&gt;&gt;&gt; model. trigramcounts[('START', 'START', 'the')]\n5478\n&gt;&gt;&gt; model. bigramcounts[('START', 'the')]\n5478\n&gt;&gt;&gt; model. unigramcounts[('the', )]\n61428\nWhere model is an instance of TrigramModel that has been trained on a corpus. Note that the unigrams are represented as one-element tuples (indicated by the , in the end). Note that the actual numbers might be slightly different depending on how you set things up. I am at this step and have been trying to test my count_ngrams, but I dont think I am doing it correctly. should i be able to just run this line:model. trigramcounts[('START', 'START', 'the')]in the main, or is there some other steps I should be taking? I tried putting the brown data in txt in the same folder, but i think I have gotten myself mixed up. Thank you so much!",
        "target": "you can run the program using command python trigram_model. py hw1_data/brown_train. txt. sys. argv[1] is the parameter name for the input fileif you run the program with brown_train. txt instead of brown_test. txt, you should get the same number as the example. that's what I have at least. .. and of course, you need to print the counts out using print statements under main"
    },
    {
        "source": "#19: testing incrementally. Hi I hope you are having a great day! Im working through part 2, but i am unsure of how to test this part.  Do you have any advice on how to most effectively test the code part by part.  Thank you so much! ",
        "target": "In general, I would try to run the various parts on a small toy training corpus so that you can check counts and probabilities by hand. "
    },
    {
        "source": "#19: testing incrementally. Hi I hope you are having a great day! Im working through part 2, but i am unsure of how to test this part.  Do you have any advice on how to most effectively test the code part by part.  Thank you so much! ",
        "target": "i set up a unittest to do it; it helps a lot for automating testcases for edge cases. It also helps for not flooding stdout as only failed test cases are printed."
    },
    {
        "source": "#18: part 2 default dict or collection. Hi I hope you are doing great! In Part 2s code it has a comment:        self. unigramcounts = {} # might want to use defaultdict or Counter instead\n        self. bigramcounts = {}\n        self. trigramcounts = {}so if i want use a default dict or Counter, is it okay if I just change the definitions above? Also there is an empty return at the end of the method.  If I'm not returning anything should I leave the return there or can i remove it? Thank you so much!",
        "target": "Yes you can simply replace the initialization.  You can remove the return statement at the end. "
    },
    {
        "source": "#17: Part 2 Clarification. Hi I hope you are doing great.  Even though we are not changing this part of the given code, I'm a little unclear on this part : \"The idea is that words that appear only once are so rare that they are a good stand-in for words that have not been seen at all in unseen text. You do not have to modify this function. \"  What does this actually mean? How are they a stand- in? Thank you so much!",
        "target": "This is because the words that appears only once are so rare so that you can treat them as any other OOV words that you might encounter in the future. The main intuition behind this approach is to (1) it allows you to cuts off the long tail of the distribution (2) create smooth probability for new OOV words."
    },
    {
        "source": "#16: Answers to Ungraded Exercise. Where can we find the answers to the ungraded exercises, if there is any?",
        "target": "I will post these usually with a delay. So if it isnt up yet, check back the next day. "
    },
    {
        "source": "#15: Clarification on unseen context & unseen token. As covered during today's lectures I'm a bit confused between the difference of unseen tokens, unseen contexts and unknown tokens. Is unseen tokens merely the process of replacing words with unknown tokens and unseen contexts the process of assigning values to the the unknown tokens to mellow out the spikes? Thanks!",
        "target": "Hello! This is how I understand it, TA's please correct me if I'm wrong:The idea of something being \"seen\" has to do with whether or not the model has encountered it (the token, the ngram) after going through all the training data. For individual unseen tokens, the question is: how do we have our model handle tokens in the test data that we never encountered in the training data? The approach we discussed in lecture was: we treat all tokens we haven't seen as the same token, which we label as unknown (UNK). Now the question is, how do we prepare our model to handle the token UNK? The strategy we discussed was to pick a number k, and say that for any token in our training data, if the token shows up fewer than k times, we replace it with UNK. Then we treat all instances of UNK  in our training data the same as any other token, calculating unigram and ngram probabilities or whatever approach we're choosing. Now that we've made sure that our model thinks it's seen every individual token it encounters in the test data, the question for unseen contexts was: how do we calculate the probability of some ngram, e. g. the trigram (w | u, v), when it's possible that  count(u, v) is 0, meaning, we've never seen v in the context of u. And that part of lecture discussed different strategies for figuring out how to calculate probabilities like (w | u, v) given that possibility. I hope that helps!"
    },
    {
        "source": "#13: HW1 Part 1. Hi Prof. Bauer and TAs, In part 1 example, The first return element has two 'START's, whereas the previous n=1 and n=2 examples have only 1 'START' in the first return element. I am confused on the padding rules here. ..",
        "target": "I have the same question. Is it an arbitrary pattern? "
    },
    {
        "source": "#13: HW1 Part 1. Hi Prof. Bauer and TAs, In part 1 example, The first return element has two 'START's, whereas the previous n=1 and n=2 examples have only 1 'START' in the first return element. I am confused on the padding rules here. ..",
        "target": "From my understanding, we pad n-grams with \"START\" tokens to provide the appropriate context (i. e. , indicate the start of a sentence). In the case of bigrams, we pad the first bigram with one \"START\" token to generate the bigram that only includes the first word. In the case of trigrams, to generate the trigram that only includes the first word, we need to pad it with two \"START\" tokens, and to generate the trigram that only includes the first two words, we need to pad it with one \"START\" token. And so on. .. for quadgrams, we would need three \"START\" tokens in the first quadgram, etc. For instance, in the case of trigrams, if we only had one \"START\" token, the first trigram generated would be (\"START\", \"natural\", \"language\") (i. e. , a sentence that begins with \"natural language\"). But this precludes something else we might be interested in: the sentence that starts with just \"natural\"! So we would also include the trigram (\"START\", \"START\", \"natural\"). In some implementations, we would actually directly add the (n-1) \"START\" tokens to the beginning of the sentence and generate the n-grams that way, but the gist is that the number of \"START\" tokens depends on the number of empty/incomplete \"spots\" at the beginning of the n-gram that we need to fill in. "
    },
    {
        "source": "#13: HW1 Part 1. Hi Prof. Bauer and TAs, In part 1 example, The first return element has two 'START's, whereas the previous n=1 and n=2 examples have only 1 'START' in the first return element. I am confused on the padding rules here. ..",
        "target": "In my opinion, after today's class, I understand that we need two w_0 word vector to create Markov chain to predict the future probability of the next word, so there should be two 'START' to create a beginning. Hope this can help you!"
    },
    {
        "source": "#10: Extra Practice - Naive Bayes. Hi there! I'd like to know if the teaching team has any recommendations as to what sources we should use to get extra experience with topics covered in ungraded assignments such as Naive Bayes. It would be great to train and check solutions to verify our understanding, even if the assignments themselves don't affect our grades. Thank you!",
        "target": "Thats what the ungraded exercises are supposed to be for. You could check the Jurafsky and Martin textbook to see if there are any additional exercises. "
    },
    {
        "source": "#8: Conditional Independence Question. Hi I hope you are doing great,  Sorry for all the questions. On a slide from today:I get the first line that B and C are becoming independent of each other. But I dont get the equivalently part.  Like for each part below is it only true given A, or shouldn't we also meet the criteria of  =  P(B| C), and =P(C| B)  (for each part respectively wouldn't that also have to be true)Thank you!",
        "target": "I believe that if we were to add the criteria of = P(B| C), and =P(C| B) then we would no longer have events B, C solely dependent on A but rather dependent on each other and on A. This would mean that they would no longer be conditionally independent. "
    },
    {
        "source": "#8: Conditional Independence Question. Hi I hope you are doing great,  Sorry for all the questions. On a slide from today:I get the first line that B and C are becoming independent of each other. But I dont get the equivalently part.  Like for each part below is it only true given A, or shouldn't we also meet the criteria of  =  P(B| C), and =P(C| B)  (for each part respectively wouldn't that also have to be true)Thank you!",
        "target": "So B and C are conditionally independent given A. Here is the full chain of equations that might be helpful. Derived using the chain rule of probability (line 1) and rule of conditional independence (line 2), "
    },
    {
        "source": "#8: Conditional Independence Question. Hi I hope you are doing great,  Sorry for all the questions. On a slide from today:I get the first line that B and C are becoming independent of each other. But I dont get the equivalently part.  Like for each part below is it only true given A, or shouldn't we also meet the criteria of  =  P(B| C), and =P(C| B)  (for each part respectively wouldn't that also have to be true)Thank you!",
        "target": "Both the equations make sense. The equation on the top makes sense because both B and C are independent to each other, such that A has already occurred. The second equation simply suggests that the knowledge of C occurring does. Ot provide any knowledge of B occurring and vice-versa. The left equation suggests that even if both A and C were to occur together jointly, only the occurrence of A alone would suffice to suggest about the occurrence of B. The equation on the right suggests that even if A and B were to occur together, only the occurrence of A alone would suffice to suggest about the occurrence. Both together imply that the occurrence of B in no way affects occurrence of C and vice-versa, even if these variables were to occur in the presence of A (jointly), whose occurrence does provide information of the occurrence of both B and C, as these two are dependent on A. "
    },
    {
        "source": "#7: denominator of bayes rules. Hi I hope you are doing great! In class when we were doing examples  and using bayes rule, we were able to just ignore the denominator.  Is this just because we can drop the denominator anytime we are calculating the probability of a particular class,  or is there some other rule that decides when we can do the calculation without the denominator. Thnak you so much!",
        "target": "The denominator can take on the value of some constant alpha (as defined in class). This is because for each calculation the denominator value remains the same. When looking for a label y that can maximize the probability we can choose based on a reduced simpler version that has the alpha."
    },
    {
        "source": "#7: denominator of bayes rules. Hi I hope you are doing great! In class when we were doing examples  and using bayes rule, we were able to just ignore the denominator.  Is this just because we can drop the denominator anytime we are calculating the probability of a particular class,  or is there some other rule that decides when we can do the calculation without the denominator. Thnak you so much!",
        "target": "In other words, we are always comparing two Probabilities, P(y=1 | x) vs P(y=0 | x). When you write them out using Bayes rules, you will notice that on both sides of the equation, the same denominator p(x) is present. Algebraically we can cancel this from our equation, so when we are formalizing what the Naive Bayes Classifier score is, we can just ignore the denominator completely. "
    },
    {
        "source": "#6: symbol question. Hi I hope you  are doing great. What does this symbol stand for:Thank you!",
        "target": "The capital pi is the product operator, and you can think of it as the multiplicative analog of the summation operator (capital sigma). For instance:\\sum_{n=1}^4n=1+2+3+4=10\\prod_{n=1}^4n=1\\cdot2\\cdot3\\cdot4=24"
    },
    {
        "source": "#4: TA's OHs & HW Due Dates. Greetings to all the teaching staffs, Does anyone have an idea when the TAs' OHs and the DUE dates for all assignments will be up for this course? Thank you for your time and attention!",
        "target": "Dear all, We've posted our TA Office Hours on the Canvas syllabus. Although due dates vary from Mon-Fri, we'll try our best to ensure there's an Office Hour on each due date. Check Canvas for details."
    }
]