{"title": "Hw 1 Part 7", "category": "General", "question": "<document version=\"2.0\"><paragraph>I'm getting back around 68% accuracy for the essay_scoring_experiment() function when I test it on the two training text files, and two testing directories. How could I increase my accuracy to reach above 80% as mentioned in the hw instructions?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>What do the perplexity values look like for the earlier parts? </paragraph></document>"]}
{"title": "Are the office hours at 3 pm happening?", "category": "General", "question": "<document version=\"2.0\"><paragraph>^^^</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>joining the question..</paragraph></document>"]}
{"title": "sentence_logprob", "category": "General", "question": "<document version=\"2.0\"><paragraph>hello,</paragraph><paragraph>I get -50.9 when i do (model.sentence_logprob([\"natural\",\"language\",\"processing\"]))</paragraph><paragraph>how is this ok? cant seem to understand the logic behind it... </paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>If you graph log2(x), you'll see that as x --&gt; 0 from the right, log2(x) goes to negative infinity. Normal probabilities (x) range from 0 to 1. Log2(1) = 0, so because we're dealing with small probabilities close to 0, all our log probabilities will be negative numbers. </paragraph><paragraph>The log transformation is monotonic (meaning that the order of the original probabilities is preserved), so the lowest log probability (most negative) corresponds to the lowest original probability (closest to 0). </paragraph><paragraph>We use log probabilities because the numbers are a lot more reasonable (logprob of -50.9 corresponds to 7.8417117e-23 in original probability). We can also add log probs instead of having to multiply original probabilities. Hope this helps!</paragraph><paragraph/></document>"]}
{"title": "Andrew's OH this AM", "category": "General", "question": "<document version=\"2.0\"><paragraph>hello there are a few of us in the zoom, cant tell if Andrew is in a break out room or not.. been here for almost 30min so please let us know!</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Checking with him on Slack ... I'll let you know when I hear back. </paragraph></document>"]}
{"title": "HW1 Part 6", "category": "General", "question": "<document version=\"2.0\"><paragraph>As M is explained in #45, are these \"tokens\" unique? Does M include either \"START\" or \"STOP\" tokens?</paragraph><paragraph>\"M = is the <bold>total number of word tokens in the test corpus\".</bold></paragraph><paragraph>Thank you!</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>the term \u201ctokens\u201d suggests that you count duplicates. Tokens are the specific instances of the unique types as they appear in a corpus. For example \u201cto be or not to be\u201d contains 6 tokens, but only 4 types.\u00a0</paragraph><paragraph>M in this case contains the STOP token, but not START. A better way to think about M, is that this is the number of ngrams the model needs to predict the test data.\u00a0</paragraph></document>"]}
{"title": "solution for ungraded exercise", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>The solutions for the 3rd ungraded exercise (HMM) are just the problems themselves again. Could someone be so kind as to repost the actual solutions? </paragraph><paragraph>Thank you!</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>My mistake, I accidentally linked the assignment itself, not the solutions Page. </paragraph><paragraph>It's fixed now, and here is a direct link: https://courseworks2.columbia.edu/courses/191061/pages/ungraded-exercise-solution-hmm-viterbi-and-forward-algorithm</paragraph></document>"]}
{"title": "HW1 Part 5 - Question about sentence_logprob()", "category": "General", "question": "<document version=\"2.0\"><paragraph>Any time I pass a sentence to sentence_logprob() that is not in the corpus, I get an error because math.log2() is passed a 0. Taking #39 into account, does sentence_logprob() only take in sentences that are already in the corpus? I don't see how the unigram would be non-0 if the is not in the corpus.</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>A word not seen is substituted with \"UNK\" so the unigram probability should not be zero.</paragraph></document>"]}
{"title": "t in the optional part", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hello,<break/><break/>In the optional part, t represents the maximum sequence length. Does it include START and STOP tokens? Include both; only have STOP or exclude them.</paragraph><paragraph>Thank you very much.</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Since it\u2019s optional, you can define it either way.\u00a0</paragraph></document>"]}
{"title": "CS Building 5th Floor", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hello, </paragraph><paragraph>I could not find the CS building 5th Floor for Tony's OH. I first went to the Mudd 5th floor and then got to this place via CS rooms. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/uw0tN7BvQO9rBY6u1CkpVEN6\" width=\"658\" height=\"877.3333333333333\"/></figure><paragraph>But it seems this is not the correct place.</paragraph><paragraph>Does anyone know how to get to the correct place?<break/>Thank you very much.</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>I don't know the exact location of Tony's OH but - if I remember correctly - you can go to the hallway where the CS Lounge entrance is and keep walking to the end to find stairs that can lead you to the 5th floor. That might have been what you were looking for. I hope that helps!</paragraph><paragraph/></document>", "<document version=\"1.0\"><paragraph>This is funny: I\u2019ve been at Columbia since 2010 and I have never been in that specific hallway \ud83d\ude05</paragraph></document>"]}
{"title": "HW1 Part 3: unigram probability is 0, dealing with 'START' unigram", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>in #39 Prof. Bauer said \"For the interpolated probabilities, the result will never be 0 because the unigram will be non-0.\" How should we ensure the unigram probability is non-0? If the unigram v has never appeared in the corpus, should we use P(v)=1/(num_tokens)? Or should we use P(v)=1/count(UNK)?</paragraph><paragraph>Also, in another post #26 , Prof. Bauer said not to count 'START' in our total tokens. How should we handle the unigram probability for 'START'?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>So the corpus reader takes care of replacing unseen unigrams (that is, words not in the lexicon) with UNK. So no unseen tokens remain.\u00a0</paragraph><paragraph>The unigram probability of \u2018START\u2019 should be 0.\u00a0</paragraph></document>"]}
{"title": "Total words", "category": "General", "question": "<document version=\"2.0\"><paragraph/><paragraph>What does it mean when we compute the total number of words in part 3? Is it the size of the lexicon or the actual total number of tokens encountered in the corpus when we iterate through it excluding the start tokens?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>total number of words/tokens encountered per #26 </paragraph></document>"]}
{"title": "Start and Stop", "category": "General", "question": "<document version=\"2.0\"><paragraph>When do we include start and stop tokens? Like when we are counting total word count when calculating the raw unigram probabilities do we count start and stop or just count the words in the lexicon? And then I saw somewhere that for perplexity we include stop and not start? That confused me. Is there a general rule for what to include? </paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>we don\u2019t ever include start because the model can\u2019t predict it \u2014 therefore it shouldn\u2019t be in any counts</paragraph></document>"]}
{"title": "Gurnoor Virdi's OH", "category": "General", "question": "<document version=\"2.0\"><paragraph>I am waiting in the CS TA room in Mudd and no one here do I have the wrong place?</paragraph><paragraph>Or have the OH been rescheduled?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Hi sorry I am running behind just got out of a meeting will start at 10:30 today!\u00a0</paragraph></document>"]}
{"title": "HW1 Part 3 - How to deal with ('START') or ('START', 'START') context grams", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>What should we do for raw trigram probability P(v|u,w)=count(u,v,w)/count(u,w) if (u,v,w) has been seen but (u,w) hasn't been seen? For example if the trigram is ('START', 'START', 'the') then the trigram may appear but the context bigram ('START', 'START') has 0 appearances. How should we calculate the trigram probability in this case? </paragraph><paragraph>Similarly, what should we do to handle the case of a bigram of the form ('START', 'word')? </paragraph><paragraph>Should we use the unigram probability of v in these cases? </paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>This is an edge case that you need to take special care of.</paragraph><paragraph>One solution is to add a class variable that counts the total number of sentences. You can then set count(START, START) to be equal to the total number of sentences.</paragraph></document>"]}
{"title": "HW1 Part 7", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hi there</paragraph><paragraph>I'm confused that what should be sys.argv[1] and sys.argv[2]? In other words, what should be implement in [corpus_file]? Brown_train.text or train_high and train_low. I think I finish all function parts and want to check the accuracy. Thanks</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Sr39cb5DtMl1uYJBrqy9hG2V\" width=\"658\" height=\"401.1958762886598\"/></figure></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>You should use brown_train.txt to train your model and brown_test.txt to evaluate perplexity for parts 1-6. </paragraph><paragraph>A reasonable command might be:</paragraph><paragraph>python -i trigram_model.py &lt;path_to_brown_train&gt; &lt;path_to_brown_test&gt;</paragraph><paragraph>For part 7, you don't actually need to use any of the brown corpus files. Instead train separate models using the ETS TOEFL files input to essay_scorring_experiment. </paragraph></document>"]}
{"title": "HW1 Part 3", "category": "General", "question": "<document version=\"2.0\"><paragraph>I'm struggling to understand how the raw trigram probability would not be 0 if count(u,w,v) = 0? In addition, if count(u,w) is also 0 then I don't understand where to fit 1 / |V| in my equation in a way where my answer will not be 0 every time.<break/></paragraph><paragraph>Any tips for testing my work here for Part 3 would also be greatly appreciated!</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>For trigrams, if count(u,v,w)=0, which might happen, <underline>but also</underline> count(u,w)=0, it's not quite clear what the answer should be. It's expected that we might not know anything about the distribution of <italic>v</italic> in the context of <italic>u,w</italic> so we assume uniform distribution, i.e., we divide by <italic>|V|</italic>.</paragraph><paragraph>Essentially, what you want to focus on is when the <underline>denominator is 0</underline>, use uniform distribution.</paragraph><paragraph>Regarding testing it's probably best to create an example manually, calculate some probabilities and check if the results match by using print statements.</paragraph></document>"]}
{"title": "Hw1 Part 5", "category": "Problem Sets", "question": "<document version=\"2.0\"><paragraph>Is the point of this part that we split the sentence into trigrams ie: </paragraph><pre>[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]\n</pre><paragraph>then calculate the probability of each trigram in the sentence ie:</paragraph><paragraph>{('START', 'START', 'natural'): .032,('START', 'natural', 'language'): .05, ('natural', 'language', 'processing'): .043, ('language', 'processing', 'STOP'): .062} </paragraph><paragraph>and then finding the log probability of each of these and adding them together?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>The idea is to calculate the log probability of an entire sequence, given a corpus. Please refer also to <link href=\"https://edstem.org/us/courses/53508/discussion/4243838\">#45</link>!</paragraph></document>"]}
{"title": "HW1 P6 do words tokens for M include punctions?", "category": "General", "question": "<document version=\"2.0\"><paragraph>HW1 P6 says: \"Here M is the total number of word tokens, while m is the number of sentences in the test corpus\". Does a \"word token\" include punctuation like \",\"?</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Anything returned by the corpus iterator should count as a token. </paragraph><paragraph>Another way to think about M is that it is the number of ngram predictions used to generate all sentence in the test data. </paragraph></document>"]}
{"title": "Hw1 Part4", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Is the expectation for part 4 that the method returns a dictionary with the overall linear interpolation probability of all the words in the text file? </paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>No. You should not return an entire dictionary. You should just be returning the smoothed probability of the trigram provided as an argument to the function call.</paragraph></document>"]}
{"title": "Avighna's Office Hours", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi, I am just wondering if Avighna's office hours are happening/if I will be able to meet with her. I've been in the \"Host has joined. We've let them know you're here\" for a bit now. Thanks!</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Hi! I am taking people one by one out of the waiting room as fast as I can but it is packed right now and ends in 2 mins :(, so I don't think I will be able to get to everyone. There are a lot of other OH today though so please go to one of those if I haven't been able to get to you!!</paragraph></document>"]}
{"title": "HW1 submission main method", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Regarding the submission of HW1, should we keep the main method's contents unchanged as provided, with most lines commented out, or does it not make a difference?</paragraph><paragraph>Thank you!</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>It doesn't really make a difference -- we will test the individual methods, not the main code. </paragraph></document>"]}
{"title": "Homework 1", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>According to #8 , why should get_ngrams(['n','l','p'],1) also add a START even though n doesn't exceed len(sequence)? Is it that there should be at least one START? If it is, what should the output be for get_ngrams(['natural', 'language'],2)?</paragraph><paragraph>&gt;&gt; get_ngrams(['natural', 'language'],2)<break/>[('START', 'natural'), ('natural', 'language'), ('language', 'STOP')]<break/>Is it like this?</paragraph><paragraph>That is we have at least one START and STOP and (n-1)*'START' should be added.</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><blockquote>&gt;&gt; get_ngrams(['natural', 'language'],2)<break/>[('START', 'natural'), ('natural', 'language'), ('language', 'STOP')]<break/>Is it like this?</blockquote><paragraph>Yes, that's correct. You need to add n-1 START symbols and one STOP symbol. </paragraph><paragraph/></document>"]}
{"title": "Waiting to be accepted into zoom link for todays OH", "category": "General", "question": "<document version=\"2.0\"><list style=\"bullet\"><list-item><paragraph>title</paragraph></list-item></list><paragraph>wondering if there was a limit or time schedule or something i missed out on! Or if I joined the wrong link like last time I attended OH on zoom?</paragraph><paragraph/><paragraph>Thanks!</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Hi! I am letting people in one by one from the waiting room as I get to them!</paragraph></document>"]}
{"title": "HW1 - Part 5 Values of Sentence Probs", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>I'm getting very strange values for perplexity in part 6, so I wanted to check to see if my log sentence probabilities were reasonable. </paragraph><paragraph>For most sentences, I am getting somewhere between -10 to -60 when calling model.sentence_logprob(sentence) on some sentence from the training corpus. Is this reasonable?</paragraph><paragraph>I think this is correct as sentences should have a very low probability, but I could be wrong about just how low the probabilities are supposed to be. A negative log prob of &lt; -10 means an extremely low probability. </paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>The range should be more like -10 to -100s (-200, -300, -400, etc.). This may not be the only correct answer though. </paragraph><paragraph>I was incorrectly calculating raw_unigram_probs to be higher than they should have been. </paragraph><paragraph>With this fixed, I am getting good perplexity values and accuracy values. Updating incase someone else runs into this</paragraph><paragraph/></document>"]}
{"title": "Perplexity score", "category": "General", "question": "<document version=\"2.0\"><paragraph>I'm getting around ~8 for a perplexity score when running python trigram_model.py brown_test.txt brown_test.txt. </paragraph><paragraph><break/>I don't really understand why it would be so low, I don't see any big problems with my functions. Am I running this correctly (the above)? Or is it then something in a function of mine? In my perplexity function, I'm using M = number of word tokens (total word count), is that what is meant by tokens?</paragraph><paragraph><break/>EDIT: </paragraph><paragraph>running python trigram_model.py brown_test.txt brown_train.txt: 6e^19<break/>running python trigram_model.py brown_test.txt brown_test.txt: 8.8<break/>running python trigram_model.py brown_train.txt brown_train.txt: 11.3<break/>running python trigram_model.py brown_train.txt brown_test.txt: 1.6</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>I had a similar problem, according to the HW description we should \"compute the perplexity of the model on an entire corpus.\" In other words, M should be calculated based on the test corpus, not the one used to build the model I believe</paragraph></document>", "<document version=\"2.0\"><paragraph>Possibly you are not using the correct value for M when computing the perplexity (see student answer above). M is the total number of tokens (word occurrences, including duplicates) in the corpus you are testing. </paragraph><paragraph>But also, you <underline>would</underline> expect the perplexity of the model trained on \"brown_test\" to have a very low perplexity on \"brown_test\" itself. What's more concerning is your last result:</paragraph><paragraph>running python trigram_model.py brown_train.txt brown_test.txt: 1.6</paragraph><paragraph>This should be much higher -- most people report something around 150 to 300. </paragraph></document>"]}
{"title": "HW1 - Part5", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>How should we handle the log probability calculation when the smoothed probability of a trigram is zero? Thank you!</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>It was mentioned in #39 that the interpolated probability would not be 0</paragraph></document>"]}
{"title": "HW 1 part 6", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>Firstly, when I run : </paragraph><pre>dev_corpus = corpus_reader(sys.argv[2], model.lexicon)\npp = model.perplexity(dev_corpus)\r\nprint(pp)\n</pre><paragraph>I get </paragraph><paragraph>File \"C:\\Users\\HP\\Downloads\\trigram_model.py\", line 208, in  dev_corpus = corpus_reader(sys.argv[2], model.lexicon) IndexError: list index out of range</paragraph><paragraph/><paragraph>Also, it seems my code is going into an infinite loop after I added the code for perplexity function. Any help would be really really appreciated! Thanks! </paragraph><paragraph>Below is my code (attaching everything to increase clarity):</paragraph><pre>def get_ngrams(sequence, n):\r\n   \r\n    if n == 1:\r\n        padded_seq = ['START'] + sequence + ['STOP']  \r\n    else:\r\n        padded_seq = (['START'] * (n-1)) + sequence + ['STOP']\r\n\r\n    res = []\r\n    for i in range(0, len(padded_seq)-n+1):\r\n        res.append(tuple(padded_seq[i: i+n]))\r\n\r\n    return res\r\n\r\n\r\nclass TrigramModel(object):\r\n    \r\n    def __init__(self, corpusfile):\r\n    \r\n        # Iterate through the corpus once to build a lexicon \r\n        generator = corpus_reader(corpusfile)\r\n        self.lexicon = get_lexicon(generator)\r\n        self.lexicon.add(\"UNK\")\r\n        self.lexicon.add(\"START\")\r\n        self.lexicon.add(\"STOP\")\r\n    \r\n        # Now iterate through the corpus again and count ngrams\r\n        generator = corpus_reader(corpusfile, self.lexicon)\r\n        self.count_ngrams(generator)\r\n\r\n\r\n    def count_ngrams(self, corpus):\r\n        self.unigramcounts = Counter() # might want to use defaultdict or Counter instead\r\n        self.bigramcounts = Counter() \r\n        self.trigramcounts = Counter() \r\n\r\n        for s in corpus:\r\n            # print(s)\r\n            unigrams = get_ngrams(s, 1)\r\n            bigrams = get_ngrams(s, 2)\r\n            trigrams = get_ngrams(s, 3)\r\n\r\n            # ref on how to update counter objects https://docs.python.org/3/library/collections.html#counter-objects\r\n            # print(trigrams)\r\n            self.unigramcounts.update(unigrams)\r\n            self.bigramcounts.update(bigrams)\r\n            self.trigramcounts.update(trigrams)\r\n\r\n    def raw_trigram_probability(self,trigram):\r\n        u, v, w = trigram\r\n        tri_count = self.trigramcounts[trigram]\r\n        if u == 'START' and v == 'START':\r\n            bi_count = self.bigramcounts[(v, w)]\r\n        else:\r\n            bi_count = self.bigramcounts[(u, v)]\r\n        \r\n        lexi_size = len(self.unigramcounts) - 1\r\n\r\n        if lexi_size == 0:\r\n            return 0.0\r\n\r\n        if bi_count == 0:\r\n            ans = 1 / lexi_size\r\n        else:\r\n            ans = tri_count / bi_count\r\n        return ans\r\n\r\n    def raw_bigram_probability(self, bigram):        \r\n        u, v = bigram\r\n        bi_count = self.bigramcounts[bigram]\r\n        uni_count = self.unigramcounts[(u,)]\r\n        lexi_size = len(self.unigramcounts) - 1\r\n\r\n        if lexi_size == 0:\r\n            return 0.0\r\n        \r\n        if uni_count == 0:\r\n            ans = 1/lexi_size\r\n        else:\r\n            ans = bi_count/uni_count\r\n        return ans\r\n    \r\n    def raw_unigram_probability(self, unigram):\r\n        total_words = sum(self.unigramcounts.values()) - self.unigramcounts[('START',)]\r\n        uni_count = self.unigramcounts[unigram]\r\n        if total_words == 0:\r\n            return 0.0\r\n        else:\r\n            return uni_count / total_words \r\n\r\n    def generate_sentence(self,t=20): \r\n        \"\"\"\r\n        COMPLETE THIS METHOD (OPTIONAL)\r\n        Generate a random sentence from the trigram model. t specifies the\r\n        max length, but the sentence may be shorter if STOP is reached.\r\n        \"\"\"\r\n        return result            \r\n\r\n    def smoothed_trigram_probability(self, trigram):\r\n        lambda1 = 1/3.0\r\n        lambda2 = 1/3.0\r\n        lambda3 = 1/3.0\r\n        u, v, w = trigram\r\n        p_tri = self.raw_trigram_probability(trigram)\r\n        p_bi = self.raw_bigram_probability((v, w))\r\n        p_uni = self.raw_unigram_probability((w,))\r\n        return (lambda1 * p_uni) + (lambda2 * p_bi) + (lambda3 * p_tri)\r\n        \r\n    def sentence_logprob(self, sentence):\r\n        \r\n        trigrams = get_ngrams(sentence, 3)\r\n        ans = 0.0\r\n\r\n        for t in trigrams:\r\n            # print('hi1')\r\n            p = self.smoothed_trigram_probability(t)\r\n\r\n            if p == 0:\r\n                return float(\"-inf\")\r\n            ans += math.log2(p)\r\n\r\n        return ans\r\n\r\n    def perplexity(self, corpus):\r\n\r\n        sum = 0.0\r\n        \r\n        for s in corpus:\r\n            log_prob = self.sentence_logprob(s)\r\n            \r\n            if log_prob == float(\"-inf\"):\r\n                return float(\"-inf\")\r\n            \r\n            sum += log_prob\r\n\r\n        l = -1 * (sum / (len(self.unigramcounts) - 1))\r\n        print(2**l)\r\n        return  2**l\n</pre></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Make sure to pass the train and test files to check perplexity: </paragraph><paragraph>python trigram_model.py brown_train.txt brown_test.txt</paragraph></document>", "<document version=\"2.0\"><paragraph>I don't think you implementing M for perplexity in the correct way. Follow #45 and feel free to visit OHs</paragraph><paragraph/></document>", "<document version=\"2.0\"><paragraph>corpus_reader() deals with tokenization and you have to get the total number of tokens from the testing dataset. Follow an example below: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/V9gXOSldp5uR0NAAZ6YSyykH\" width=\"682\" height=\"207.36992316136116\"/></figure></document>", "<document version=\"2.0\"><paragraph>Attended OH and got it fixed, thank you!</paragraph></document>"]}
{"title": "Class today?", "category": "Lectures", "question": "<document version=\"1.0\"><paragraph>Is today\u2019s class still going to be remote?\u00a0</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>+1 </paragraph></document>", "<document version=\"2.0\"><paragraph>Sorry I just read this. In case you missed it, the recording of today's class should be up in the Video Library in a bit. </paragraph></document>"]}
{"title": "HW 1 - Part 6", "category": "General", "question": "<document version=\"2.0\"><paragraph>For the perplexity calc and the corpus, the instructions say the following:</paragraph><pre>Write the method perplexity(corpus), which should compute the perplexity of the model on an entire corpus. \nCorpus is a corpus iterator (as returned by the corpus_reader method). \nRecall that the perplexity is defined as 2-l, where l is defined as: \n</pre><paragraph>It's not clear to me how exactly the corpus should be passed into this method as there's an existing method that reads in the corpus. Should it be read in again somehow, local to this function?</paragraph><paragraph/><paragraph>Also, is it possible to get examples of what the log prob of sample sentences would be to help check work along the way?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>You should first call corpus_reader to obtain a corpus iterator object. Then pass this object to the perplexity method.\u00a0</paragraph></document>"]}
{"title": "HW 1 - Part 3", "category": "General", "question": "<document version=\"2.0\"><paragraph>When we calculate raw_unigram_probability(unigram), it's the count(unigram) / (total tokens). In this case should \"total tokens\" include counts of 'START' and 'STOP' or only \"real tokens\"?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>#26</paragraph></document>"]}
{"title": "HW1 - Part 2", "category": "General", "question": "<document version=\"2.0\"><paragraph>For HW1 - Part 2, when we read in line by line, should each line have a number of 'START's inserted in the front, and one 'STOP' at the end? Or, should there only be one 'STOP' and one set of 'START's for the whole corpus?</paragraph><paragraph>I'm assuming the former, but wanted to be sure.</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Each entry you iterate over in the corpus corresponds to a sentence (see excerpt from the HW1 Description:)</paragraph><pre>&gt;&gt;&gt; generator = corpus_reader(\"\")\n&gt;&gt;&gt; for sentence in generator:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0print(sentence)</pre><paragraph>START and STOP are applied on this sentence level rather than on the level of the whole corpus</paragraph><paragraph/></document>"]}
{"title": "HW 1 Reasonable Perplexity + Accuracy", "category": "Problem Sets", "question": "<document version=\"2.0\"><paragraph>Hi, I am getting 293 perplexity for the Brown test set and 84.66% accuracy for the essay classification, do these numbers seem reasonable or do they seem indicative of an underlying error? I get 18 perplexity when I run the Brown train set on the Brown train set</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>These look good.</paragraph></document>"]}
{"title": "Ungraded exercise: Naive Bayes", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi! Thanks for posting the solution for the Ungraded exercise: Naive Bayes.<break/>I am a little confused about the difference between that solution and the worked example on page 7 of chapter 4 in Jurafsky and Martin's book.</paragraph><paragraph>To me, it seems that both b) in the ungraded exercise and the worked example (screenshot attached) present a very similar structure, but different approaches to solving, leading to different solutions. I would appreciate your help figuring out the what the differences are between the problems.</paragraph><paragraph>Thanks!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/GrvqmwFI8qIKxCweVwuTtmj3\" width=\"391\" height=\"459.1334379905809\"/></figure></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>It\u2019s the same approach, but in the textbook example they are additionally using add-one smoothing for the conditional probabilities.\u00a0</paragraph></document>"]}
{"title": "HW1 Part 7", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I got a trigram key error from my code and was wondering how to account for trigrams that occurred in the test file but not in the training file?   </paragraph><paragraph>Thank you!</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Because you are using smoothed_trigram_probability, there will never be a trigram with probability 0. \u00a0</paragraph></document>"]}
{"title": "OH on zoom from 9.30 am", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>Is there still going to be an OH today on zoom as indicated by the calendar?</paragraph><paragraph>Thanks!</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>It's on! Feel free to join with the Zoom link in the calendar.</paragraph></document>"]}
{"title": "How do I test my functions?", "category": "Problem Sets", "question": "<document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>This might be a silly question, but is there a way to test each part of my hw? Or is it only possible to test it once the entire hw is done?</paragraph><paragraph>Thanks!</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>One way is to instantiate a new TrigramModel using your own txt file. For example, you could create a txt file based on the corpus from the ungraded n-gram exercise, instantiate a TrigramModel with that file, and then verify that the probability outputs match up with the solutions.</paragraph></document>"]}
{"title": "Problem Viewing OH Calendar", "category": "General", "question": "<document version=\"2.0\"><paragraph>Is anyone else having problems seeing the calendar for office hours? Could this be because I have a Barnard email and not a Columbia one? If so, could those would a Barnard email be added to it? Thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/E6euChiT2lNYYEWXZOT1sV0S\" width=\"658\" height=\"507.8591117917304\"/></figure><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>I added all the students that responded on #20, but I'll make sure now to add all of the Barnard students from this course manually.</paragraph></document>"]}
{"title": "Final Exam Timing", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I know it's about 3 months away, but due to unplanned circumstances, I might not able to take the exam with my current group on 4/26. I was wondering if there's an option to take the final exam with the second group on 4/29 or any other alternative. </paragraph><paragraph>Thank you in advance,</paragraph><paragraph>Orr</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Please directly email the professor.</paragraph></document>"]}
{"title": "Naive Bayes Warm Up Clarification", "category": "Problem Sets", "question": "<document version=\"1.0\"><paragraph>Super quick question for calculating the class of a document given a word. I know the solutions state that we can disregard the denominator because \"It's sufficient to compute the joint probabilities, because the denominator for the conditional probability is constant between classes.\"\u00a0</paragraph><paragraph>Can someone explain a little bit more what is meant by \"constant between classes\"? Are we saying that bc there are only ten possible words in the \"universe\" for these documents we don't need to calculate the probability that one word occurs because they're all equally likely to occur (1/10 chance of occurring)? Or is it something else.</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/ldrTBh86xJ0wUVOaBbUnxaTi\" width=\"2137\" height=\"686\"/></figure></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Yes, so the goal of the Naive Bayes <underline>classifier</underline> is to make a classification decision: based on the observed words in the given document, which class is the \"best\". <break/><break/>The way you do that is by comparing the probabilities that the model assigns, under the assumption class = Spam vs. the assumption class = Ham. <break/><break/>It doesn't matter if you compare the conditional probabilities P(spam | crown-prince) vs. P(ham | crown-prince) or the conditional probabilities P(spam, crown-prince) or P(ham, crown-prince). The reason is that P(crown-prince) is the same under both assumptions. <break/><break/>Even though the conditional and joint probability are of course not identical, since your goal here is only to classify, you can use either. </paragraph></document>"]}
{"title": "Where is cs ta room?", "category": "General", "question": "<document version=\"2.0\"><paragraph>As title.</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>It's on the first floor of Mudd, room 122A. If you're coming for my OH: I'm sitting on the first floor, right next to the street level doors since it's fully occupied right now. </paragraph><paragraph>Reach out to me at am6203@columbia.edu for any other questions as I figure out the OH location. </paragraph></document>"]}
{"title": "Hw1 Part 6", "category": "Problem Sets", "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>For the value of M in Part 6, should we count the number of 'START' and 'STOP' token too ? I believe we do but we get a value closer to 400 for the perplexity if we don't.</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Include STOP, but exclude START.</paragraph></document>"]}
{"title": "HW1 part 7", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>For part 7, I am a bit confused on what should I do. We need the accuracy of each model, but how do we determine if a perplexity is correct? The two for loops given seems to iterate the high model through the high data set, and iterate the low model through the low data set separately. Are we supposed to loop the high model through both high and low, and loop the low model through both high and low, and compare their results for each test data to see if their perplexity difference is correct?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Hi, I have a similar question as a sanity check- I am assuming we have to compare the perplexity of the same file calculated from one model (trained on high scores) compared to the other model (trained on low scores), i.e. for a given sample file, both models should compute it's perplexity? (and then we label it based on the lower perplexity score, and identify if that labelling is correct based on the directory it originated from?)</paragraph><paragraph/></document>", "<document version=\"2.0\"><paragraph>Yes, I think your understanding is correct. The example code is misleading. </paragraph><paragraph>The idea is that you iterate through all \"low\" essays and apply both the \"low\" and \"high\" models to each. If the \"low\" model has a lower perplexity than the \"high\" model, you would count that essay as classified correctly. </paragraph><paragraph>Then you repeat with all the \"high\" quality essays. </paragraph></document>"]}
{"title": "HW1 Part3 n-gram probabilities are 0", "category": "General", "question": "<document version=\"2.0\"><paragraph>For the case the prob for n-gram is 0, do we need to consider similar case for bigram?</paragraph><paragraph>And what if the count(u,w,v) is 0, while count(u, w) is not 0?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>For the raw probabilities, just return 0 for those cases. For the interpolated probabilities, the result will never be 0 because the unigram will be non-0.\u00a0<break/></paragraph></document>"]}
{"title": "Hw1 Part1", "category": "General", "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/JaFh6e80Be8GbJlZ9tOo8iJp\" width=\"642\" height=\"385.4012539184953\"/></figure><paragraph>Is this the correct logic of how i should be using the START and STOP padding for the ngram? </paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>That looks good to me. (Side note: I recommend that you work on this in a regular IDE or a text editor, rather than Jupyter notebook. You may encounter some issues with being able to test the code as intended because not all methods may be defined correctly \u2014 it\u2019s also difficult to pass command line parameters).\u00a0</paragraph></document>"]}
{"title": "Hw1 Part 2", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>I just have a question about this part of the homework. What exactly does the content in the corpus looklike? Are we counting the instances of specific trigram, bigram and unigrams or are we counting trigram, bigrams and unigrams in general (hence looking at whats in the corpus and looking at the size and counting it occordingly)? Lastly what would be the best way to test this part of the code specifically? Thank you!</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>I had a similar question! What would sys.argv[1] be reading in to generate the corpus? </paragraph></document>", "<document version=\"1.0\"><paragraph>Hm I\u2019m not sure I fully understand your question. The corpus reader iterates through the corpus and produces one sentence at a time. You should count all unigrams, bigrams, and trigrams that appear in the corpus.\u00a0</paragraph><paragraph>To test, I would recommend running the code on a small sample corpus (one or two sentences) \u2014 just put them in a text file. Then you can directly compare the obtained counts to your manual counts.\u00a0<break/></paragraph></document>"]}
{"title": "Hw1 part 3 |V|", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>We need to do P(v | u,w) = 1 / |V| (where |V| is the size of the lexicon), if count(u,w) is 0. But I am confused what |V| represent. Does it mean number of types of bigrams or number of types of single words or frequency of word \"v\"?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>|V| is the magnitude (size) of the set V, which contains unigram types.\u00a0</paragraph></document>"]}
{"title": "HW1 Part 4", "category": "General", "question": "<document version=\"2.0\"><paragraph>Should we just use the given values (1/3 each) for lambda?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Yes.</paragraph></document>"]}
{"title": "HW1 - Interlude", "category": "General", "question": "<document version=\"2.0\"><paragraph>Is the optional portion of the homework extra credit? Or is it strictly for fun?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>There's no extra credit in this course, so it's just for fun! :)</paragraph></document>"]}
{"title": "HW1 Part 5", "category": "General", "question": "<document version=\"2.0\"><paragraph>Are we using bigrams or trigrams to calculate the log probability?</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Use trigrams.</paragraph></document>"]}
{"title": "HW1 Part 3", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Can we compute the total words each time we run this function or will we be docked points for doing this?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Not necessarily; we'll only make deductions if your code takes too long to run when grading. If you want to err on the side of caution, we highly recommend creating a variable to store the total number of words, and reusing that for later computations.</paragraph><paragraph/></document>"]}
{"title": "Possible need for an extension for HW1", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hello, I am a medic in the Army National Guard, and I will have to go away for training for a week starting this Friday. I will try very hard to finish the HW as soon as I can and on time, but how could I obtain an extension given the circumstances? I can submit paperwork that showcases my responsibilities.  <break/><break/>Thank you</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Please email me directly. </paragraph></document>"]}
{"title": "HW1 Part 6", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph/><paragraph>Is M total number of unique word tokens or number of word tokens which might be repeated in the corpus?</paragraph><paragraph/><paragraph>When we sum log probability for each sentence to calculate perplexity, do we include START and END tokens to calculate log probablity of a sentence?</paragraph><paragraph/><paragraph>Do we include START and STOP tokens in M?</paragraph><paragraph/><paragraph>Thanks</paragraph><paragraph>Abhinav</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>For the log probability, you should include the probability of STOP/END, but not START. Think about the model as predicting one token at a time, given the left context. The last predicted token is STOP. START is never predicted, it just provides the context for the first token. </paragraph><paragraph>M is the number of word tokens (i.e. including duplicate counts), not types. It should include STOP but not START (since the STOP token is predicted, but the START token is not). </paragraph><paragraph/><paragraph/><paragraph/></document>"]}
{"title": "HW1 part 6", "category": "General", "question": "<document version=\"2.0\"><paragraph>For part 6, is the \"perplexity\" function supposed to return perplexity for a sentence or the entire corpus? The comments in the function is different from in the assignment on courseworks.</paragraph><paragraph/><paragraph>Also for part 6, when we use the log2 probabilities for calculating perplexity, do we need to log the output of the function sentence_logprob again? or is the output already log2 prob?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>The perplexity function is supposed to return the complexity of the model on the entire corpus. </paragraph><paragraph>The sentence log probabilities should already use log2 (see part 5). </paragraph></document>"]}
{"title": "Hw1 part 7 accuracy file path", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>The starter code (acc = essay_scoring_experiment('train_high.txt', \"train_low.txt\",\"test_high\", \"test_low\") ) does not work for me and it keeps saying No such file or directory: 'train_high.txt' And my python file is in the same directory with them. I am not sure why it says no such file.</paragraph><paragraph>So I used my exact file path which worked.</paragraph><paragraph>I wonder if I should submit with my exact file path or change it back, or should I comment out the testing part?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Your working directory is probably different from the directory where the files are located. I recommend just switching it back after you are done testing for your final submission, but it's not a big deal to leave the absolute pathnames. </paragraph></document>"]}
{"title": "HW1 part3 total # of words", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hi</paragraph><paragraph>If I understand correctly, total number of words is not including the (START,) and (STOP,) token? Why do we not count those tokens in unigrams? (My guess is the start and stop token are meaningless here.)</paragraph><paragraph>Best,</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>This is for the unigram probabilities? START should definitely not be included, because the model would never predict it. I recommend including STOP. You will need the unigram probability of STOP when you compute linear interpolation in part 4. </paragraph></document>", "<document version=\"2.0\"><paragraph>What's the harm in including Start as a normal token?</paragraph></document>"]}
{"title": "HW1 perplexity question", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hi. I am perplexed by a perplexity score \ud83d\ude35\u200d\ud83d\udcab. </paragraph><paragraph>Specifically, I ran perplexity on the brown_train which gave me <bold>14.6</bold>, a low value as expected. However, when running on brown_test I  get <bold>9.07</bold> a low score, but importantly, it's <italic>lower than the training perplexity.</italic> This seemed odd since I expected the test perplexity to be higher than that of the training data. </paragraph><paragraph/><paragraph>Can I assume there is a bug in my code, or is it possible for test perplexity be lower than train perplexity? </paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>The test perplexity should never be lower than the one on the training set. I would double check whether you're using the number of tokens in the training set when computing your test perplexity.</paragraph></document>"]}
{"title": "HW! part 3", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>For HW1 part 3 where we need to compute the total words for unigram, can we edit __init__ directly to add object variables? Or can we only edit the functions specifically for each part?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>That's fine. As long as you keep the function signatures and return types the same, you're free to make any other modifications.</paragraph></document>"]}
{"title": "Add-one smoothing", "category": "Lectures", "question": "<document version=\"2.0\"><paragraph>Does V (the size of the vocab) in the denominator include all the START and END tokens? </paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>We never count START as part of the vocabulary because it never appears on the left side of any conditional probability calculation we do in training. We count END because we will always see something like (END|word).</paragraph><paragraph/></document>"]}
{"title": "Exam format", "category": "General", "question": "<document version=\"2.0\"><paragraph>For the exams will there be coding? How are they formatted? Sorry I know this is super in advance just curious! </paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>The exams will consist of short answer and free response questions that are similar to the ungraded exercises. You may have to adjust a couple lines of pseudocode for a specific algorithm, but we will not ask you to directly code anything from scratch.</paragraph></document>"]}
{"title": "hw1 part2 about Punctuations", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Do we need to regard the punctuations as a \"word\" or we need to remove all of them?</paragraph><paragraph>Eg. For bigram, \"Hi, Bob.\", do we use [...('Hi', ','), (',', 'Bob')...] or [...('Hi', 'Bob'), ...]</paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>The corpus reader is performing any text normalization for you. You don\u2019t have to worry about tokenization or removing symbols.\u00a0</paragraph></document>"]}
{"title": "HW 1 Part 5", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>What is the datatype of \"sentence\" in sentence_logprob(sentence)? Can we assume it will be given as an array? Or is it a string?<break/></paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>It\u2019s a sequence (either a list or tuple) of tokens.\u00a0</paragraph></document>"]}
{"title": "HW1 Part 1", "category": "Problem Sets", "question": "<document version=\"2.0\"><paragraph>Should the case for n = 3 have only one start at the beginning, or is that correct?</paragraph><paragraph>IE:</paragraph><pre>[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]</pre><paragraph>Should be</paragraph><pre>[('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]</pre><paragraph/></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Not at TA but I think #8 answers this!</paragraph><paragraph/></document>"]}
{"title": "HW 1 Part 3", "category": "General", "question": "<document version=\"2.0\"><paragraph>What are the unigram, trigram, bigram arguments for the functions? Are they particular n-gram queries or are they the dictionaries populated in part 2?</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>The arguments will be individual ngrams (formatted as a tuple).\u00a0</paragraph></document>"]}
{"title": "Request for OH Google Calendar", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi! Would it be possible to create a Google calendar of scheduled office hours so we can import them into our own calendar? No worries if not, but it would be really helpful! Thank you so much!</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Here! I'll add it to Courseworks as well:</paragraph><paragraph>https://calendar.google.com/calendar/u/0?cid=Y180ZWMwOWFjZDM0YWU4Yzc1ZWI2ZmY1NTgxYjg1MzY5ZGU3ZjRlNWEwNDg2YTViMzU5ZjI1ZWMzOGM3OTMyNmZhQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20</paragraph></document>"]}
{"title": "HW 1 Part 4", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>For this part, can we use this equation to implement it?<break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/MACE3wNpyq5fcbnS5QfeVl7k\" width=\"658\" height=\"147.7142857142857\"/></figure></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Yes, that's the correct formula. See below for the slide of the lectures that refers to linear interpolation:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ptoP8yX8sUjKeBDqI7bdhrcf\" width=\"758\" height=\"526.9179229480737\"/></figure><paragraph/></document>"]}
{"title": "HW 1 Part 2", "category": "Problem Sets", "question": "<document version=\"2.0\"><paragraph>Just to clarify, nothing is returned in count_ngrams, right?</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>I think that's correct. That function should just populate the associated dictionaries. </paragraph></document>"]}
{"title": "Where to code", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Are we recommended to work in a specific python environment?  </paragraph></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>I\u2019ll leave that up to you. We won\u2019t be using any libraries for homework 1 and 2, but make sure you have Python 3.7 or higher. A lot of people really like vscode, but any editor or IDE will do.\u00a0<break/></paragraph></document>"]}
{"title": "HW1 submission", "category": "Problem Sets", "question": "<document version=\"2.0\"><paragraph>Hi, just wonder how we should submit HW1 since canvas only accepts zip/tgz while we are asked to submit .py only</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>Thanks. I\u2019ll change it to .py.\u00a0</paragraph></document>"]}
{"title": "Bigram Probability, First Word Not in Lexicon", "category": "General", "question": "<document version=\"2.0\"><paragraph>From the homework \"\"\"One issue you will encounter is the case if which you have a trigram u,w,v where count(u,w,v) = 0 but count(u,w) is also 0. In that case, it is not immediately clear what P(v | u,w) should be. My recommendation is to make P(v | u,w) = 1 / |V| (where |V| is the size of the lexicon), if count(u,w) is 0\"\"\"</paragraph><paragraph>Doesn't this same behavior happen for bigrams if u is not in the lexicon? Would the approach be the same?</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>That won\u2019t happen because all unigrams that are not in the lexicon are replaced with the UNK token by the corpus reader.\u00a0</paragraph></document>"]}
{"title": "Question regarding get_ngrams function.", "category": "Assignments", "question": "<document version=\"2.0\"><paragraph>Hi! I had a quick question about the <bold><italic>get_ngrams()</italic></bold> function for HW1. I'm assuming this function should pad out our inputs even if <bold>n</bold> is greater than the number of words in the input sequence? </paragraph><paragraph>Ex: </paragraph><paragraph>sequence = [\"Hi\", \"there\"]</paragraph><paragraph>n = 4</paragraph><paragraph>Output: </paragraph><paragraph>[('START', 'START', 'START', 'Hi'),    ('START', 'START', 'Hi', 'there'),    ('START', 'Hi', 'there', 'STOP')]</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>That's correct!</paragraph></document>"]}
{"title": "Ungraded Exercise Submission", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Do we need to submit the ungraded exercise? </paragraph><paragraph>Thank you very much.</paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>No, they won't be submitted. If you'd like feedback on your solutions though, you're always welcome to stop by any of our office hours!</paragraph></document>"]}
{"title": "What is the term \"Class\" referring to?", "category": "General", "question": "<document version=\"2.0\"><paragraph>I am wondering what the term \"class\" refers to in relation to a document-- is class related to a word? </paragraph><paragraph/></document>", "comments": [], "answers": ["<document version=\"1.0\"><paragraph>The class/label is the type of the document, for example \u201cspam\u201d or \u201cnot spam\u201d. Each document has exactly one class. So this is a property of the entire document, not a single word.\u00a0</paragraph></document>"]}
{"title": "Question about Participation Points", "category": "General", "question": "<document version=\"2.0\"><paragraph>Hi Professor and TAs,</paragraph><paragraph>I hope you are well! I was wondering about how to earn participation points (the 10% of our overall grade) as someone who works 9-5 and needs to watch the recordings asynchronously. If I do not participate during class time, can I still earn the full participation points by participating on Ed instead? </paragraph><paragraph>If it is mandatory that I attend class in-person, I can try and work something out with my boss. For the next few weeks, however, I will need to watch the recordings afterward. And I'm just worried I will miss out on those participation points! :) </paragraph><paragraph>I appreciate your help in advance!</paragraph><paragraph>Best regards,</paragraph><paragraph>Mia</paragraph></document>", "comments": [], "answers": ["<document version=\"2.0\"><paragraph>Participation on Ed will count for that score! I highly recommend responding to the professor's ungraded exercises and discussion questions to show evidence of your participation. In addition, you could also try to answer other students' questions when we start releasing the graded homework assignments.</paragraph></document>", "<document version=\"1.0\"><paragraph>If I recall correctly, you are a CVN student. Participating in Ed is sufficient in your situation, as Shivansh describes. CVN students are explicitly allowed to participate asynchronously.\u00a0</paragraph></document>"]}
{"title": "Game to Discriminate Between Human and AI Conversation", "category": "General", "question": "<document version=\"2.0\"><paragraph>If you think you could be a good candidate to tell the difference between a human and an AI in a Turing Test, check out <link href=\"https://www.humanornot.ai/\">this website</link>. </paragraph><paragraph>After having a 2-minute live conversation with either a randomly assigned other human player or an AI, you submit your guess as to whether or not they were an AI. </paragraph><paragraph>If you end up playing it and get a crazy conversation feel free to comment it!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/DwF3j9ouWxCZCbEJEEerAULF\" width=\"658\" height=\"1121.8360655737704\"/></figure><paragraph/></document>", "comments": ["<document version=\"2.0\"><paragraph>I just played the game and won by identifying that the other participant was a human. Their responses were strange and unnatural, almost like a human trying to act non-human. So, the big takeaway is, \"If you're going to trick someone, don't be too deliberate. Be natural.\"</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/l83NIs6DROBlmbJ7duVmYGgm\" width=\"655\" height=\"642.2969696969697\"/></figure></document>", "<document version=\"2.0\"><paragraph>This is quite a cool site, and I've seen clips of it going around on the internet. I think what's interesting is that this is slightly different from Turing's interpretation of the \"Imitation Game\", where humans try and convince the evaluator that they are human. In this game, human players may try and act like AI, perhaps making it even more difficult than Turing's game. It's also very likely they're reusing different games to fine-tune their models lol. Will be interesting to see how this evolves.</paragraph><paragraph/></document>", "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/jzycXckeAUDjJQtmvoC7fHOk\" width=\"493\" height=\"393.8491620111732\"/></figure><paragraph>I feel like when I played the game some things made me feel like I was talking to a human like the misspelling of certain words and the \"insta?\" at the end which is (sort of) new slang. It was interesting cause at some point I felt like I was being deceived but I just think that was me overthinking it. I have seen a lot of videos on TikTok where people try to guess whether images are AI-generated or real photographs and I feel like that is almost easier than this. I was wondering what everyone else thinks.</paragraph></document>", "<document version=\"2.0\"><paragraph>Pretty sure I just <italic>turing'd</italic> someone. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/6kfjTdvSMfMTmSJRknUMe4c7\" width=\"655\" height=\"733.6430921052631\"/></figure></document>", "<document version=\"2.0\"><paragraph>this was interesting for sure </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XRWX8cRDZcsfsbYQ5CgsZePF\" width=\"542\" height=\"645.1921708185054\"/></figure></document>", "<document version=\"2.0\"><paragraph>Is it just me or does the link not work anymore?</paragraph></document>", "<document version=\"2.0\"><paragraph>Interesting, thanks for sharing Brian! Seems like site is down, will keep trying. </paragraph></document>"], "answers": []}
