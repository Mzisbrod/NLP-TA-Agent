{
    "0": {
        "question": "For the case the prob for n-gram is 0, do we need to consider similar case for bigram? And what if the count(u, w, v) is 0, while count(u, w) is not 0?",
        "answers": [
            "For the raw probabilities, just return 0 for those cases. For the interpolated probabilities, the result will never be 0 because the unigram will be non-0. "
        ]
    },
    "1": {
        "question": "Is this the correct logic of how i should be using the START and STOP padding for the ngram? ",
        "answers": [
            "That looks good to me. (Side note: I recommend that you work on this in a regular IDE or a text editor, rather than Jupyter notebook. You may encounter some issues with being able to test the code as intended because not all methods may be defined correctly  its also difficult to pass command line parameters). "
        ]
    },
    "2": {
        "question": "I just have a question about this part of the homework. What exactly does the content in the corpus looklike? Are we counting the instances of specific trigram, bigram and unigrams or are we counting trigram, bigrams and unigrams in general (hence looking at whats in the corpus and looking at the size and counting it occordingly)? Lastly what would be the best way to test this part of the code specifically? Thank you!",
        "answers": [
            "I had a similar question! What would sys. argv[1] be reading in to generate the corpus? ",
            "Hm Im not sure I fully understand your question. The corpus reader iterates through the corpus and produces one sentence at a time. You should count all unigrams, bigrams, and trigrams that appear in the corpus. To test, I would recommend running the code on a small sample corpus (one or two sentences)  just put them in a text file. Then you can directly compare the obtained counts to your manual counts. "
        ]
    },
    "3": {
        "question": "We need to do P(v | u, w) = 1 / |V| (where |V| is the size of the lexicon), if count(u, w) is 0. But I am confused what |V| represent. Does it mean number of types of bigrams or number of types of single words or frequency of word \"v\"?",
        "answers": [
            "|V| is the magnitude (size) of the set V, which contains unigram types. "
        ]
    },
    "4": {
        "question": "Should we just use the given values (1/3 each) for lambda?",
        "answers": [
            "Yes."
        ]
    },
    "5": {
        "question": "Is the optional portion of the homework extra credit? Or is it strictly for fun?",
        "answers": [
            "There's no extra credit in this course, so it's just for fun! :)"
        ]
    },
    "6": {
        "question": "Are we using bigrams or trigrams to calculate the log probability?",
        "answers": [
            "Use trigrams."
        ]
    },
    "7": {
        "question": "Can we compute the total words each time we run this function or will we be docked points for doing this?",
        "answers": [
            "Not necessarily; we'll only make deductions if your code takes too long to run when grading. If you want to err on the side of caution, we highly recommend creating a variable to store the total number of words, and reusing that for later computations."
        ]
    },
    "8": {
        "question": "Hello, I am a medic in the Army National Guard, and I will have to go away for training for a week starting this Friday. I will try very hard to finish the HW as soon as I can and on time, but how could I obtain an extension given the circumstances? I can submit paperwork that showcases my responsibilities.  Thank you",
        "answers": [
            "Please email me directly. "
        ]
    },
    "9": {
        "question": "Hi, Is M total number of unique word tokens or number of word tokens which might be repeated in the corpus? When we sum log probability for each sentence to calculate perplexity, do we include START and END tokens to calculate log probablity of a sentence? Do we include START and STOP tokens in M? ThanksAbhinav",
        "answers": [
            "For the log probability, you should include the probability of STOP/END, but not START. Think about the model as predicting one token at a time, given the left context. The last predicted token is STOP. START is never predicted, it just provides the context for the first token. M is the number of word tokens (i. e. including duplicate counts), not types. It should include STOP but not START (since the STOP token is predicted, but the START token is not). "
        ]
    },
    "10": {
        "question": "For part 6, is the \"perplexity\" function supposed to return perplexity for a sentence or the entire corpus? The comments in the function is different from in the assignment on courseworks. Also for part 6, when we use the log2 probabilities for calculating perplexity, do we need to log the output of the function sentence_logprob again? or is the output already log2 prob?",
        "answers": [
            "The perplexity function is supposed to return the complexity of the model on the entire corpus. The sentence log probabilities should already use log2 (see part 5). "
        ]
    },
    "11": {
        "question": "The starter code (acc = essay_scoring_experiment('train_high. txt', \"train_low. txt\", \"test_high\", \"test_low\") ) does not work for me and it keeps saying No such file or directory: 'train_high. txt' And my python file is in the same directory with them. I am not sure why it says no such file. So I used my exact file path which worked. I wonder if I should submit with my exact file path or change it back, or should I comment out the testing part?",
        "answers": [
            "Your working directory is probably different from the directory where the files are located. I recommend just switching it back after you are done testing for your final submission, but it's not a big deal to leave the absolute pathnames. "
        ]
    },
    "12": {
        "question": "HiIf I understand correctly, total number of words is not including the (START, ) and (STOP, ) token? Why do we not count those tokens in unigrams? (My guess is the start and stop token are meaningless here. )Best,",
        "answers": [
            "This is for the unigram probabilities? START should definitely not be included, because the model would never predict it. I recommend including STOP. You will need the unigram probability of STOP when you compute linear interpolation in part 4. "
        ]
    },
    "13": {
        "question": "Hi. I am perplexed by a perplexity score . Specifically, I ran perplexity on the brown_train which gave me 14. 6, a low value as expected. However, when running on brown_test I  get 9. 07 a low score, but importantly, it's lower than the training perplexity. This seemed odd since I expected the test perplexity to be higher than that of the training data. Can I assume there is a bug in my code, or is it possible for test perplexity be lower than train perplexity? ",
        "answers": [
            "The test perplexity should never be lower than the one on the training set. I would double check whether you're using the number of tokens in the training set when computing your test perplexity."
        ]
    },
    "14": {
        "question": "For HW1 part 3 where we need to compute the total words for unigram, can we edit __init__ directly to add object variables? Or can we only edit the functions specifically for each part?",
        "answers": [
            "That's fine. As long as you keep the function signatures and return types the same, you're free to make any other modifications."
        ]
    },
    "15": {
        "question": "Does V (the size of the vocab) in the denominator include all the START and END tokens? ",
        "answers": [
            "We never count START as part of the vocabulary because it never appears on the left side of any conditional probability calculation we do in training. We count END because we will always see something like (END|word)."
        ]
    },
    "16": {
        "question": "For the exams will there be coding? How are they formatted? Sorry I know this is super in advance just curious! ",
        "answers": [
            "The exams will consist of short answer and free response questions that are similar to the ungraded exercises. You may have to adjust a couple lines of pseudocode for a specific algorithm, but we will not ask you to directly code anything from scratch."
        ]
    },
    "17": {
        "question": "Do we need to regard the punctuations as a \"word\" or we need to remove all of them? Eg. For bigram, \"Hi, Bob. \", do we use [. .. ('Hi', ', '), (', ', 'Bob'). .. ] or [. .. ('Hi', 'Bob'), . .. ]",
        "answers": [
            "The corpus reader is performing any text normalization for you. You dont have to worry about tokenization or removing symbols. "
        ]
    },
    "18": {
        "question": "What is the datatype of \"sentence\" in sentence_logprob(sentence)? Can we assume it will be given as an array? Or is it a string?",
        "answers": [
            "Its a sequence (either a list or tuple) of tokens. "
        ]
    },
    "19": {
        "question": "Should the case for n = 3 have only one start at the beginning, or is that correct? IE:[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]Should be[('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]",
        "answers": [
            "Not at TA but I think #8 answers this!"
        ]
    },
    "20": {
        "question": "What are the unigram, trigram, bigram arguments for the functions? Are they particular n-gram queries or are they the dictionaries populated in part 2?",
        "answers": [
            "The arguments will be individual ngrams (formatted as a tuple). "
        ]
    },
    "21": {
        "question": "Hi! Would it be possible to create a Google calendar of scheduled office hours so we can import them into our own calendar? No worries if not, but it would be really helpful! Thank you so much!",
        "answers": [
            "Here! I'll add it to Courseworks as well:https://calendar. google. com/calendar/u/0? cid=Y180ZWMwOWFjZDM0YWU4Yzc1ZWI2ZmY1NTgxYjg1MzY5ZGU3ZjRlNWEwNDg2YTViMzU5ZjI1ZWMzOGM3OTMyNmZhQGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20"
        ]
    },
    "22": {
        "question": "For this part, can we use this equation to implement it?",
        "answers": [
            "Yes, that's the correct formula. See below for the slide of the lectures that refers to linear interpolation:"
        ]
    },
    "23": {
        "question": "Just to clarify, nothing is returned in count_ngrams, right?",
        "answers": [
            "I think that's correct. That function should just populate the associated dictionaries. "
        ]
    },
    "24": {
        "question": "Are we recommended to work in a specific python environment?  ",
        "answers": [
            "Ill leave that up to you. We wont be using any libraries for homework 1 and 2, but make sure you have Python 3. 7 or higher. A lot of people really like vscode, but any editor or IDE will do. "
        ]
    },
    "25": {
        "question": "Hi, just wonder how we should submit HW1 since canvas only accepts zip/tgz while we are asked to submit . py only",
        "answers": [
            "Thanks. Ill change it to . py. "
        ]
    },
    "26": {
        "question": "From the homework \"\"\"One issue you will encounter is the case if which you have a trigram u, w, v where count(u, w, v) = 0 but count(u, w) is also 0. In that case, it is not immediately clear what P(v | u, w) should be. My recommendation is to make P(v | u, w) = 1 / |V| (where |V| is the size of the lexicon), if count(u, w) is 0\"\"\"Doesn't this same behavior happen for bigrams if u is not in the lexicon? Would the approach be the same?",
        "answers": [
            "That wont happen because all unigrams that are not in the lexicon are replaced with the UNK token by the corpus reader. "
        ]
    },
    "27": {
        "question": "Hi! I had a quick question about the get_ngrams() function for HW1. I'm assuming this function should pad out our inputs even if n is greater than the number of words in the input sequence? Ex: sequence = [\"Hi\", \"there\"]n = 4Output: [('START', 'START', 'START', 'Hi'),    ('START', 'START', 'Hi', 'there'),    ('START', 'Hi', 'there', 'STOP')]",
        "answers": [
            "That's correct!"
        ]
    },
    "28": {
        "question": "Hello, Do we need to submit the ungraded exercise? Thank you very much.",
        "answers": [
            "No, they won't be submitted. If you'd like feedback on your solutions though, you're always welcome to stop by any of our office hours!"
        ]
    },
    "29": {
        "question": "I am wondering what the term \"class\" refers to in relation to a document-- is class related to a word? ",
        "answers": [
            "The class/label is the type of the document, for example spam or not spam. Each document has exactly one class. So this is a property of the entire document, not a single word. "
        ]
    },
    "30": {
        "question": "Hi Professor and TAs, I hope you are well! I was wondering about how to earn participation points (the 10% of our overall grade) as someone who works 9-5 and needs to watch the recordings asynchronously. If I do not participate during class time, can I still earn the full participation points by participating on Ed instead? If it is mandatory that I attend class in-person, I can try and work something out with my boss. For the next few weeks, however, I will need to watch the recordings afterward. And I'm just worried I will miss out on those participation points! :) I appreciate your help in advance! Best regards, Mia",
        "answers": [
            "Participation on Ed will count for that score! I highly recommend responding to the professor's ungraded exercises and discussion questions to show evidence of your participation. In addition, you could also try to answer other students' questions when we start releasing the graded homework assignments.",
            "If I recall correctly, you are a CVN student. Participating in Ed is sufficient in your situation, as Shivansh describes. CVN students are explicitly allowed to participate asynchronously. "
        ]
    },
    "31": {
        "question": "If you think you could be a good candidate to tell the difference between a human and an AI in a Turing Test, check out this website. After having a 2-minute live conversation with either a randomly assigned other human player or an AI, you submit your guess as to whether or not they were an AI. If you end up playing it and get a crazy conversation feel free to comment it!",
        "answers": [
            "I just played the game and won by identifying that the other participant was a human. Their responses were strange and unnatural, almost like a human trying to act non-human. So, the big takeaway is, \"If you're going to trick someone, don't be too deliberate. Be natural. \"",
            "This is quite a cool site, and I've seen clips of it going around on the internet. I think what's interesting is that this is slightly different from Turing's interpretation of the \"Imitation Game\", where humans try and convince the evaluator that they are human. In this game, human players may try and act like AI, perhaps making it even more difficult than Turing's game. It's also very likely they're reusing different games to fine-tune their models lol. Will be interesting to see how this evolves.",
            "I feel like when I played the game some things made me feel like I was talking to a human like the misspelling of certain words and the \"insta? \" at the end which is (sort of) new slang. It was interesting cause at some point I felt like I was being deceived but I just think that was me overthinking it. I have seen a lot of videos on TikTok where people try to guess whether images are AI-generated or real photographs and I feel like that is almost easier than this. I was wondering what everyone else thinks.",
            "Pretty sure I just turing'd someone. ",
            "this was interesting for sure ",
            "Is it just me or does the link not work anymore?",
            "Interesting, thanks for sharing Brian! Seems like site is down, will keep trying. "
        ]
    },
    "32": {
        "question": "Hi, I made a separate public post, #793, in case another student ran into a similar issue but wanted to post my code as well privately to make things more clear! See #793 for the details and a photo of my issue :)",
        "answers": []
    },
    "33": {
        "question": "I am working on part 4 and I noticed that the highest recommended tag is often 'O' Here is my result with the example given in 4. The 'B-V' tag is accurate but it doesn't seem like the other tags are predicted. It is not because the other predicted tags are not in the id_to_role dictionary, but literally the 'O' tag is getting predicted. Does anyone know why this might be happening? The model inputs are input_ids and attn_mask processed similar to when we loaded in the dataset (part 1).  The input pred_indicator is also similar to the part 1 implementation, I just set pred_indicator[pred_index] = 1",
        "answers": [
            "Because the O tag is by far the majority tag, it seems reasonable that the model would learn to default to it. It may mean you need to train longer. What does the training loss and accuracy look like? ",
            "Hi. I had a similar issue as well. Not sure if we made the same mistake, but my mistake was that I called tokenizer. tokenize(tokens) which messed up the tokens since they were already tokenized. Once I deleted that line, everything worked as intended.",
            "i am having this same issue (#779) but my loss and accuracy are the same as professor bauer after 2 epochs"
        ]
    },
    "34": {
        "question": "Hello, I am currently training my model on part 3, and everything is working well. My issue is after about an hour and a half, the VM shuts down and times out, so all of my training progress gets lost. Is there anything I can do to prevent this from happening? I saw someone on EdStem said to use the \"screen\" command, but the issue stemmed from the VM instance itself timing out.",
        "answers": [
            "What do you mean the VM instance timed out? My problem was with my ssh session timing out so using screen to set the process to be in the background worked for me. "
        ]
    },
    "35": {
        "question": "Hello, I have been working primarily with Google Cloud for HW 5. I have been experiencing on and off issues. I am just checking to make sure Google Colab works and we don't need to do anything else but change to TCP4. Thanks!",
        "answers": [
            "So the assignment is meant to be solved on GCP and doesnt have to run anywhere else. But it should work on colab just fine, provided you get a GPU runtime. No changes to the actual code should be required. "
        ]
    },
    "36": {
        "question": "Hi, I'm getting these values for part 3 after 2 epoch. Is this ok or are these off by too much? I'm not sure what could be wrong because everything up to this point, I've been getting the expected output. (similar loss value in part 2 for example). Some additional weird behavior I'm seeing. 1. The two epochs only take about 20-30 minutes to train for me, not several hours like is expected. The only thing I changed in the given train() code was computing the accuracy part. 2. this is epoch 2. The loss jumps up from 0. 237 to 0. 41 and stays around there until it's done. 3. I saved this model and used it for part 4 and 5. And I got 0. 92227 accuracy for token-based accuracy, which doesn't seem terrible.",
        "answers": [
            "Hm the training loss seems a bit high, but if you get reasonable results during testing it may not be an issue. Its normal for the loss to jump around a little bit. "
        ]
    },
    "37": {
        "question": "Hi, I'm unclear as to what the parameter k is supposed to be. Can anyone clarify?",
        "answers": [
            "I'm not entirely sure, but I interpreted k as the index used to retrieve data from self. items."
        ]
    },
    "38": {
        "question": "is k in this section the actual item we need to process or the index of an item from our list of items?",
        "answers": [
            "k is the index of the item that should be returned. Presumably, the data was just loaded into a list. Its just the index in this list. "
        ]
    },
    "39": {
        "question": "Hi, In part 1. 2. 3, it is required that Each sequence must be a pytorch tensor of shape (1, 128). However, in part 3, each batch are of the shape (32, 1, 128). To make sure that the shape is maintained correctly as (32, 128), do we change part 1. 2. 3 into a tensor of shape 128, or modify relevant codes in the dataloader in part 3?",
        "answers": [
            "I had the same question! Did you find a fix for this? Nvm, looks like it's covered in #773!",
            "I would change part 1. 2. 3. "
        ]
    },
    "40": {
        "question": "Hi, I'm on the last part of part 2. I'm trying to take the argmax over every position. This is the part of the instructions I'm on (listed below).  However, when I do this my result looks like this. For some reason, some of my predictions are 0, and then when I try to use 0 with the id_to_role dictionary to decode the actual tokens, I get a key error because 0 is not a key in the id_to_role dictionary. Does anyone know what may be causing this? The loss I got from part 2 (3. 89) was  close to the expected loss (3. 97) so I think my parts 1 and 2 are correct so far.",
        "answers": [
            "Not a TA, but the ids of the roles are 1-indexed, so I added 1 to the argmax's. Not sure if this is correct though",
            "I think that this is happening because this is before training . One of the posts says to return 'O' for values not in the dictionary "
        ]
    },
    "41": {
        "question": "Hi, when I am trying to train my model on GCP, it keeps on crashing it would run for some time but then would throw an error, how can I fix this?",
        "answers": [
            "What is the error?"
        ]
    },
    "42": {
        "question": "Hi, For HW2 Q2 I have tried the averaging different instantiations of the model trick mentioned in other posts. When averaging 5 results, the average loss ranges from about 3. 84 to 4. 06. When averaging 10 results, the error still goes around 3. 91 to 4. 01. Is this considered acceptable result?",
        "answers": [
            "That looks good to me. There is really no guarantee it will be exactly the expected value, but with more samples you should get closer. "
        ]
    },
    "43": {
        "question": "Hello, I recently saw that I received a 25/30 on Part 3 for having too many dense layers. While I agree that this representation is incorrect according to the specifications of Part 3, I ended up submitting an experiment for changing the model (as was encouraged in Part 4 and performs better than the model specified in Part 3). In doing so, I changed the structure of what was asked from us in Part 3. While I understand, it is my fault for deviating from the instructions and I understand if I do not get the points back, I thought submitting a better version of the model asked from us in part 3 would suffice in its place. ",
        "answers": [
            "Please send an email so I have it in my inbox. Im processing these slowly. "
        ]
    },
    "44": {
        "question": "I'm a bit confused on how to pad the tokens in part 1. 2. 3. The instructions say, \"We need to process the sentence by adding \"[CLS]\" as the first token and \"[SEP]\" as the last token. The need to pad the token sequence to 128 tokens using the \"[PAD]\" symbol. This needs to happen both for the inputs (sentence token sequence) and outputs (BIO tag sequence). \"Does that mean we need to add \"[CLS]\" , \"[SEP]\", and \"[PAD]\"  to the BIO tag sequence too?",
        "answers": [
            "Thats correct. Both the input and target need to be processed that way. "
        ]
    },
    "45": {
        "question": "I made the changes suggested in  #624 and it seemed like my model was created successfully (previously was getting the error described in #624). However, I'm still getting the same error again when computing the loss for one item. I am making sure to input the correct dimensions for output and targets, and I'm not sure how to fix this as I already made sure to edit the model code so it looks like this now. Does anyone know if there is any other ways to resolve the error?",
        "answers": [
            "Update in case others have the same issue: I manually pushed the target tensor to GPU with . cuda() so that both target and outputs were on the GPU and it works now!",
            "I believe targets is not saved to cuda. maybe do targets. to('cuda') before loss(). .. I did this step in SrlData getitem"
        ]
    },
    "46": {
        "question": "For the accuracy calculation, I just noticed that there is already some code in train() calculating the \"predictions\" and \"matching\" however I'm a bit confused at why the \"matching\" calculation does not seem to ignore the cases where [PAD] is predicted correctly. In my code I also stripped those matches out, are we not supposed to do that? ",
        "answers": [
            "The code provided is incomplete. You will have to exclude the padded tokens before computing the accuracy. "
        ]
    },
    "47": {
        "question": "My epoch accuracy is 0. 939 after training but my decoding output is the following. I checked all of my model inputs and outputs, so i think my model is just not predicting the right things. I'm not sure why this is happening when I have a pretty good training accuracy. I also tried using my model after 1 epoch (accuracy = 0. 888) and have the same result. Any suggestions on debugging? ('A', '[CLS]')\n('U. ', 'O')\n('N. ', 'O')\n('team', 'O')\n('spent', 'O')\n('an', 'O')\n('hour', 'O')\n('inside', 'O')\n('the', 'O')\n('hospital', 'O')\n(', ', 'O')\n('where', 'O')\n('it', 'O')\n('found', 'B-V')\n('evident', 'O')\n('signs', 'O')\n('of', 'O')\n('shelling', 'O')\n('and', 'O')\n('gunfire', 'O')\n('. ', 'O')",
        "answers": [
            "Oh my god. Ok, so I was correct that our models were trained properly. The pred_idx  that we're passing into label_sentence doesn't account for the fact that all indices are shifted right one to account for adding the [CLS] token. So to get the correct index for the predicate it's actually pred_idx + 1. "
        ]
    },
    "48": {
        "question": "Hi, For decoding in label_sentence, are we supposed to tokenize each token and send it to Bert for testing? Asking this because the word \"shelling\" is not tokenized in this expected output. Do we have to re-combine the tokenized words before outputting them? Thanks, Samhit",
        "answers": [
            "I remember in a post professor told that Bert accepts tokenized text as input, so I think we need to send tokenized text to input and later convert the tokenized output labels back to labels for individual words and return the output."
        ]
    },
    "49": {
        "question": "Hi, For the average accuracy for each epoch calculation, can we just sum up all the accuracies for each batch (matching divided by predictions for that batch) and divide by the total number of steps, similar to average loss. (Or) should we sum up all the matchings, sum up all the predictions and divide the total_matchings by the total_predictions ?",
        "answers": [
            "I did the former using np. mean. "
        ]
    },
    "50": {
        "question": "Hello, I'm not sure if in the case where the prediction and target set both have the same span but with different tags, should we categorize it as FN or FP. Or both? Thank you!",
        "answers": [
            "That situation would cause both a false negative and false positive. "
        ]
    },
    "51": {
        "question": "Hello, I'm running into some memory issues when trying to run train() and I don't know how to proceed. Before this error appeared, I also ran into the same issues as #773 and #766 with the not enough values to unpack.  Any help would be appreciated. ",
        "answers": [
            "Im not quite sure. What batch_size are you using? Did you make sure the BERT model is instantiated with a token length of 128?"
        ]
    },
    "52": {
        "question": "I'm receiving this error and I'm not sure how to debug (I also don't have the unsqueeze calling in my getitem(). If anybody could help that would be great! Been stuck on this for a while:",
        "answers": [
            "What is the actual input_shape (input_ids. shape). You could print it. ",
            "I believe that the output tensors from __ getitem __ function is supposed to be in the shape of 128 (no need to reshape or unsqueeze). I only applied the reshape or unsqueeze to make it (1, 128) for the individual tests before the \"train\" function since \"1\" refers to the batch size. In train(), the shape should be (32, 128) since the batch size is 32."
        ]
    },
    "53": {
        "question": "I am trying to create a VM instance on GCP. After I follow the instructions and click create VM, I dont't get the quota error. I get this instead telling me GPU instances are unavailable:A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-east4-c zone.\nAny way around this or am I doing something wrong?",
        "answers": [
            "Wait nvm lol: #577. us-west1-b Oregon did the trick for me"
        ]
    },
    "54": {
        "question": "are we supposed to submit the PDF from Print-&gt;Save as PDF as well as the FIle-&gt;Download As-&gt; Notebook?",
        "answers": [
            "Thats right. "
        ]
    },
    "55": {
        "question": "Would it be possible to achieve an F1 score of 0. 87 for evaluate_spans? Is this too high? Should we be including CLS and/or SEP?",
        "answers": [
            "In the span based eval you should not include CLS or SEP (these are not part of any span). This seems high. But not impossible. What was your per token accuracy and loss? "
        ]
    },
    "56": {
        "question": "Hello, For Part 3, when running Train(), my program is able to run for a few batches, but keeps crashing and outputting different errors, saying that it doesn't recognize a key. Is there an issue with the way in which I am invoking my get_item code? My original loss from the end of part 2 is around 3. 8-4. 1 too. I truncated the token sequences before making them tensors, but I am still getting the follow errors:KeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_2827/3364925475. py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_2827/2922944801. py in train()\n     19     epoch_acc=0\n     20 \n---&gt; 21     for idx, batch in enumerate(loader):\n     22         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in __next__(self)\n    519             if self. _sampler_iter is None:\n    520                 self. _reset()\n--&gt; 521             data = self. _next_data()\n    522             self. _num_yielded += 1\n    523             if self. _dataset_kind == _DatasetKind. Iterable and \\\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self. _next_index()  # may raise StopIteration\n--&gt; 561         data = self. _dataset_fetcher. fetch(index)  # may raise StopIteration\n    562         if self. _pin_memory:\n    563             data = _utils. pin_memory. pin_memory(data)\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in &lt;listcomp&gt;(. 0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/tmp/ipykernel_2827/2744729194. py in __getitem__(self, k)\n    120         tag_ids= []\n    121         for tag in padded[1]:\n--&gt; 122             tag_ids. append(role_to_id[tag])\n    123 \n    124 \n\nKeyError: 'B-ARGM-REC'\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_2827/3364925475. py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_2827/2922944801. py in train()\n     19     epoch_acc=0\n     20 \n---&gt; 21     for idx, batch in enumerate(loader):\n     22         print(\"in da loop\")\n     23         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in __next__(self)\n    519             if self. _sampler_iter is None:\n    520                 self. _reset()\n--&gt; 521             data = self. _next_data()\n    522             self. _num_yielded += 1\n    523             if self. _dataset_kind == _DatasetKind. Iterable and \\\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self. _next_index()  # may raise StopIteration\n--&gt; 561         data = self. _dataset_fetcher. fetch(index)  # may raise StopIteration\n    562         if self. _pin_memory:\n    563             data = _utils. pin_memory. pin_memory(data)\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in &lt;listcomp&gt;(. 0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/tmp/ipykernel_2827/2763776596. py in __getitem__(self, k)\n    119         tag_ids= []\n    120         for tag in padded[1]:\n--&gt; 121             tag_ids. append(role_to_id[tag])\n    122 \n    123 \n\nKeyError: 'B-ARG5'\n",
        "answers": [
            "So I think the professor said that if you have a label that is not in the role_to_id dictionary (i. e. key errors like these), you replace it with 'O', so you would key into the 'O' when tag is not in the dictionary. I used a ternary operator for this, so I recommend using that! (formatted as role_to_id[tag if tag in role_to_id else 'O'], something like that)"
        ]
    },
    "57": {
        "question": "Hi, Just to confirm if they are valid scores,  I am getting the following after training for 2 epochs:Training loss epoch: 0. 053995941260483904Training accuracy epoch: 0. 983526443835309Overall P: 0. 7871418357806742 Overall R: 0. 7957545229120103 Overall F1: 0. 7914247481282417",
        "answers": [
            "That looks pretty good. "
        ]
    },
    "58": {
        "question": "Hi, I hope you are doing wellI'm trying to run the jupiter notebook on my browser, but I keep getting this error message even though my virtual machine successfully runs the command. I'm not sure how to proceed in this case, and I would love some guidance on this. Thank you!",
        "answers": [
            "Double check the Jupyter notebook config file to check that the notebook is running on the external IP, not just 127. 0. 0. 1. Also check the firewall config of the VM in the GCP console. "
        ]
    },
    "59": {
        "question": "I'm running just the first part of part 2 where I just pick an item from my dataset and use it in the model. However, I'm running to this error: \"not enough values to unpack, expected 2 not 1\"I double checked that each input to the model (ids, mask, and pred) are each tensors of length 128 -- does anyone know what might be the issue? ",
        "answers": [
            "Hi, I believe that the input tensors have to be of shape (1, 128), where 1 is your batch size."
        ]
    },
    "60": {
        "question": "Hello, For Part 2, the example on the argmax prediction has '[CLS]' as the first token predicted. However, since we have not done any training by this point, it is expected that our example would not have '[CLS]' as its first token, correct? I just want to make sure I understand this part.",
        "answers": [
            "So the model has not been fine tuned, but it has been pretrained in the masked LM objective. So I think its likely that it would predict [CLS] for the first token. "
        ]
    },
    "61": {
        "question": "I'm a bit confused by what's meant in the comments of the second to-do in Question 2: \"Note that you still have to provide a (batch_size, input_pos) tensor for each parameter, where batch_size =1\"What's the input_pos?",
        "answers": [
            "Never mind! Understood after looking at the train() function written in Part 3. "
        ]
    },
    "62": {
        "question": "Hey everyone, I got approved for 2 GPU quota, and its been over 15 minutes, but instance is still not finishing creating. Anyone else having this issue? ",
        "answers": [
            "You can try hovering over the red exclamation point to see exactly what's going wrong, but I think that there might not be any GPUs available in the region that you're trying to create your VM instance. Prof Bauer talks about it in this question: #577 "
        ]
    },
    "63": {
        "question": "Hi everyone, I'm having a problem with accessing Jupyter notebook page. I followed the instructions and setup the firewall rules for jupyter notebooks, however when I tried to open the page [my vm ip]:8888 and it shows error message 'The site can't be reached' and 'refused to connect'. Is there anything I could do to fix this? Thank you so much for the help! !",
        "answers": [
            "Hard to tell without a closer look. I suspect the Jupyter config may not be correct and the notebook server is only listening on the 127. 0. 0. 1 IP, not the external ip of the VM. Make sure you copied the notebook config file correctly.  "
        ]
    },
    "64": {
        "question": "I followed all the steps in the instructions, and cannot open the site. Can anyone tell me which step I did wrong? ",
        "answers": [
            "It looks like Jupyter is only running on the local IP. Make sure you copied the Jupyter configuration correctly.  "
        ]
    },
    "65": {
        "question": "Hi, I've tried for a couple of hours to figure out how to open the Jupyter notebook from Google cloud VM. But it still does not work. Is it possible to solely use Colab to finish homework 5? ",
        "answers": [
            "Hey can you rejoin the zoom? Thanks!"
        ]
    },
    "66": {
        "question": "Hello Professor Bauer and Teaching Staff, I was wondering how to address this given that we can not respond  is the second screenshot a possible correct approach? ",
        "answers": [
            "Seems good. But it looks like you already had a version.  "
        ]
    },
    "67": {
        "question": "How long does \"a few hours\" mean for part 3?",
        "answers": [
            "Hi, For the T4 GPU, I used tqdm and I am seeing as per the SS below, around 2 hrs per epoch"
        ]
    },
    "68": {
        "question": "how should we handle this case?",
        "answers": [
            "In my implementation I just truncated these. But it may be better to just ignore the entire sentence. "
        ]
    },
    "69": {
        "question": "Hi, In HW5 part 1. 1, is it that the \"tokenize_with_labels\" function is already completed and we don't need to do anything about it? When I run \"tokenize_with_labels(\"the fancyful penguin devoured yummy fish . \". split(), \"B-ARG0 I-ARG0 I-ARG0 B-V B-ARG1 I-ARG1 O\". split(), tokenizer)\", I am already getting \"(['the', 'fancy', '##ful', 'penguin', 'dev', '##oured', 'yu', '##mmy', 'fish', '. '], ['B-ARG0', 'I-ARG0', 'I-ARG0', 'I-ARG0', 'B-V', 'I-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'O'])\" as output. Thanks!",
        "answers": [
            "Yes, Professor Bauer left it completed for us. #606"
        ]
    },
    "70": {
        "question": "Has anyone had an issue with their notebook appearing blank? Its been working fine the last few days but now when i open the notebook its completely blank. ssh into my instance is fine, its showing an active kernel, correct files are showing, it's showing my last save point. ..",
        "answers": [
            "have you try restarting the kernel?",
            "Does your ssh terminal say anything? I had this issue once, and i found that my jupyternotebook got untrusted. I used this jupyter trust 4705-f23-hw5/4705_f23_hw5. ipynb  to solve :)[W 21:11:19. 919 NotebookApp] Notebook 4705-f23-hw5/4705_f23_hw5. ipynb is not trusted^Ctl3189@instance-2:~$ jupyter trust 4705-f23-hw5/4705_f23_hw5. ipynb"
        ]
    },
    "71": {
        "question": "similar question to #616: My average loss is 3. 99 but i've seen values as low as 3. 73 and high as 4. 14 after instantiating my model 10. Is this normal? I'm not really sure what I would change to get my loss consistently closer to the approximation. Is anyone able to explain why the loss should be close to the approximation and what would be affecting my values?",
        "answers": [
            "Yeah I was getting similar results. Eventually I got a result, after reinstantiating the model many times, that was 0. 03 off, and then I somehow managed to lose the result through restarting the kernel so that was frustrating. The professor said to just keep instantiating the model until you get something close to the desired result. It's not a very efficient way to handle this but I think it's the only way. Could be wrong though, so if anyone can confirm that would be helpful too."
        ]
    },
    "72": {
        "question": "Hi, i was trying to follow the  setup instructions in jupyter notebook by running the code in each of the blocks. How do i fix the error in part 1 data preparation.  It says transformers module not found. I assume i have to install the transformers module somewhere, but I'm unfamiliar with Jupyter notebooks so not sure how to do fix this.",
        "answers": [
            "in Jupiter terminal \"! pip install transformers datasets\"https://huggingface. co/docs/transformers/quicktour"
        ]
    },
    "73": {
        "question": "What should I select? If 'Specified target tags\", what should I input:And why?",
        "answers": [
            "I picked all instances in the network and it worked. Not sure if this is correct though",
            "All instances. "
        ]
    },
    "74": {
        "question": "Hi, I was following the steps for setting up HW5. as seen in this screenshotAfter accessing from my local browser by running http://[your ip]:8888. I get this page. How should I proceed? I didn't see any instructions for how to handle this page in the HW directions.",
        "answers": [
            "Try adding this to the Jupyter config:c. NotebookApp. token = ''Alternatively, just copy/paste the token thats being displayed on the terminal when you run Jupyter notebook. "
        ]
    },
    "75": {
        "question": "hi! in the future, I think a video tutorial for setting up this assignment (from gcp all the way to opening the notebook) would be extremely helpful. just wanted to suggest!",
        "answers": [
            "Thanks. I had an old tutorial from past semesters that was outdated, so I didnt distribute it this time. We can produce a new one for the upcoming spring iteration of the course. "
        ]
    },
    "76": {
        "question": "Hello all, When trying to initially train the model in part 3, before modifying the train method at all, I get the below error (I also get it after modifying it).  The issue seems to be in the forward method of class SrlModel(). Does anyone know what might be going wrong here?",
        "answers": [
            "Can you print out the actual input shape? Seems like the shape tuple has more than two entries. ",
            "I also observed this error, if I do torch. tensor(token_ids, dtype=torch. long). unsqueeze(0) , but removing \"unsqueeze\", makes it work fine. If you have the unsqueeze, try removing and retry "
        ]
    },
    "77": {
        "question": "Hi, Wondering if there's any TA OH left for HW5? highly appreciate it! ",
        "answers": [
            "I will be holding one tmr"
        ]
    },
    "78": {
        "question": "Hello, my batch loss values for part 3 seem to very low (as in less than 0. 01), I don't think this is normal, but just wanted to confirm. I'm unsure of what to do, and they keep getting smaller. Below is a snippet of the progression of the batch loss values:Batch loss:  4. 054444313049316\nCurrent average loss: 4. 054444313049316\nCurrent average accuracy: 0. 0\nBatch loss:  3. 979296922683716\nBatch loss:  3. 768834114074707\nBatch loss:  3. 614400625228882\nBatch loss:  3. 4809179306030273\nBatch loss:  3. 3388655185699463\nBatch loss:  3. 2510297298431396\nBatch loss:  3. 065067768096924\nBatch loss:  2. 8237223625183105\nBatch loss:  2. 6501760482788086\nBatch loss:  2. 451078176498413\nBatch loss:  2. 256450891494751\nBatch loss:  2. 005648374557495\nBatch loss:  1. 8249739408493042\nBatch loss:  1. 5963672399520874\nBatch loss:  1. 3876043558120728\nBatch loss:  1. 2990769147872925\nBatch loss:  1. 1228885650634766\nBatch loss:  0. 996213436126709\nBatch loss:  0. 8626056909561157\nBatch loss:  0. 8244842886924744\nBatch loss:  0. 707217276096344\nBatch loss:  0. 6257396340370178\nBatch loss:  0. 5275598764419556\nBatch loss:  0. 5193887948989868\nBatch loss:  0. 5367732644081116\nBatch loss:  0. 432065486907959\nBatch loss:  0. 4994810223579407\nBatch loss:  0. 45364564657211304\nBatch loss:  0. 4385327100753784\nBatch loss:  0. 47460734844207764\nBatch loss:  0. 47314372658729553\nBatch loss:  0. 43053632974624634\nBatch loss:  0. 4229948818683624\nBatch loss:  0. 38370954990386963\nBatch loss:  0. 41373613476753235\nBatch loss:  0. 38112300634384155\nBatch loss:  0. 36516323685646057\nBatch loss:  0. 3554108142852783\nBatch loss:  0. 3198952376842499\nBatch loss:  0. 3561134934425354\nBatch loss:  0. 31086522340774536\nBatch loss:  0. 32146918773651123\nBatch loss:  0. 35578617453575134\nBatch loss:  0. 3337218165397644\nBatch loss:  0. 3126886188983917\nBatch loss:  0. 33468666672706604\nBatch loss:  0. 34820085763931274\nBatch loss:  0. 3210916519165039\nBatch loss:  0. 27926957607269287\nBatch loss:  0. 3248721659183502\nBatch loss:  0. 2393699735403061\nBatch loss:  0. 23967529833316803\nBatch loss:  0. 2542567551136017\nBatch loss:  0. 2415236085653305\nBatch loss:  0. 22153104841709137\nBatch loss:  0. 21964891254901886\nBatch loss:  0. 22180967032909393\nBatch loss:  0. 2165355235338211\nBatch loss:  0. 21262942254543304\nBatch loss:  0. 1998571902513504\nBatch loss:  0. 21602031588554382\nBatch loss:  0. 21546243131160736\nBatch loss:  0. 1910235732793808\nBatch loss:  0. 18305090069770813\nBatch loss:  0. 17495006322860718\nBatch loss:  0. 17573660612106323\nBatch loss:  0. 16947096586227417\nBatch loss:  0. 16375981271266937\nBatch loss:  0. 15941256284713745\nBatch loss:  0. 1348469853401184\nBatch loss:  0. 13177306950092316\nBatch loss:  0. 1412310004234314\nBatch loss:  0. 13377036154270172\nBatch loss:  0. 14451517164707184\nBatch loss:  0. 12922604382038116\nBatch loss:  0. 12140139937400818\nBatch loss:  0. 105585478246212\nBatch loss:  0. 10778787732124329\nBatch loss:  0. 09824687242507935\nBatch loss:  0. 10125046968460083\nBatch loss:  0. 11317369341850281\nBatch loss:  0. 10061492025852203\nBatch loss:  0. 08653963357210159\nBatch loss:  0. 08661562204360962\nBatch loss:  0. 10136072337627411\nBatch loss:  0. 07846256345510483\nBatch loss:  0. 08183478564023972\nBatch loss:  0. 07600025087594986\nBatch loss:  0. 06759410351514816\nBatch loss:  0. 07063036412000656\nBatch loss:  0. 06919985264539719\nBatch loss:  0. 06868188083171844\nBatch loss:  0. 06514791399240494\nBatch loss:  0. 054077647626399994\nBatch loss:  0. 05911911278963089\nBatch loss:  0. 04881056770682335\nBatch loss:  0. 04994519054889679\nBatch loss:  0. 05233999341726303\nBatch loss:  0. 04855324700474739\nBatch loss:  0. 04035058990120888\nCurrent average loss: 0. 6900796895053717\nCurrent average accuracy: 0. 9048667738031466\nBatch loss:  0. 04774666950106621\nBatch loss:  0. 043077122420072556\nBatch loss:  0. 04092007130384445\nBatch loss:  0. 039590347558259964\nBatch loss:  0. 04119435325264931\nBatch loss:  0. 03814026340842247\nBatch loss:  0. 03875689581036568\nBatch loss:  0. 03783031925559044\nBatch loss:  0. 035457808524370193\nBatch loss:  0. 038035888224840164\nBatch loss:  0. 033280640840530396\nBatch loss:  0. 031726885586977005\nBatch loss:  0. 034284159541130066\nBatch loss:  0. 03501279279589653\nBatch loss:  0. 028807245194911957\nBatch loss:  0. 029841458424925804\nBatch loss:  0. 030320294201374054\nBatch loss:  0. 025569308549165726\nBatch loss:  0. 02447674423456192\nBatch loss:  0. 028052041307091713\nBatch loss:  0. 0260455459356308\nBatch loss:  0. 02362748235464096\nBatch loss:  0. 025321949273347855\nBatch loss:  0. 02356087788939476\nBatch loss:  0. 024245664477348328\nBatch loss:  0. 02370499260723591\nBatch loss:  0. 024651404470205307\nBatch loss:  0. 0232594832777977\nBatch loss:  0. 023778511211276054\nBatch loss:  0. 02079637348651886\nBatch loss:  0. 022749941796064377\nBatch loss:  0. 021912850439548492\nBatch loss:  0. 01965963840484619\nBatch loss:  0. 020016152411699295\nBatch loss:  0. 020961331203579903\nBatch loss:  0. 01930428296327591\nBatch loss:  0. 019029129296541214\nBatch loss:  0. 019100934267044067\nBatch loss:  0. 018171805888414383\nBatch loss:  0. 01855277083814144\nBatch loss:  0. 018262235447764397\nBatch loss:  0. 018863961100578308\nBatch loss:  0. 01757778599858284\nBatch loss:  0. 017264558002352715\nBatch loss:  0. 017259657382965088\nBatch loss:  0. 01627066545188427\nBatch loss:  0. 015738120302557945\nBatch loss:  0. 01766519621014595\nBatch loss:  0. 015532076358795166\nBatch loss:  0. 01640811562538147\nBatch loss:  0. 0161956287920475\nBatch loss:  0. 01676386035978794\nBatch loss:  0. 015004479326307774\nBatch loss:  0. 020208697766065598\nBatch loss:  0. 016666989773511887\nBatch loss:  0. 015439004637300968\nBatch loss:  0. 015238978900015354\nBatch loss:  0. 014591080136597157\nBatch loss:  0. 014566686004400253\nBatch loss:  0. 01617591269314289\nBatch loss:  0. 01442203763872385\nBatch loss:  0. 01352754421532154\nBatch loss:  0. 013138306327164173\nBatch loss:  0. 013419908471405506\nBatch loss:  0. 014974895864725113\nBatch loss:  0. 01407176349312067\nBatch loss:  0. 01786687597632408\nBatch loss:  0. 0132637619972229\nBatch loss:  0. 012018700130283833\nBatch loss:  0. 012571961618959904\nBatch loss:  0. 012387474067509174\nBatch loss:  0. 013380499556660652\nBatch loss:  0. 013376233167946339\nBatch loss:  0. 012046496383845806\nBatch loss:  0. 014537307433784008\nBatch loss:  0. 01196714024990797\nBatch loss:  0. 01229437068104744\nBatch loss:  0. 012623605318367481\nBatch loss:  0. 011935274116694927\nBatch loss:  0. 011496961116790771\nBatch loss:  0. 012824351899325848\nBatch loss:  0. 011729823425412178\nBatch loss:  0. 011400659568607807\nBatch loss:  0. 011539318598806858\nBatch loss:  0. 010882369242608547\nBatch loss:  0. 012176182121038437\nBatch loss:  0. 010736091062426567\nBatch loss:  0. 010822015814483166\nBatch loss:  0. 011666384525597095\nBatch loss:  0. 010679413564503193\nBatch loss:  0. 012515945360064507\nBatch loss:  0. 011127859354019165\nBatch loss:  0. 011381746269762516\nBatch loss:  0. 011736201122403145\nBatch loss:  0. 012096414342522621\nBatch loss:  0. 01054787915199995\nBatch loss:  0. 01099485531449318\nBatch loss:  0. 010997350327670574\nBatch loss:  0. 011011684313416481\nBatch loss:  0. 01095783431082964\nCurrent average loss: 0. 3565346281604832\nCurrent average accuracy: 0. 9523748064016521\nBatch loss:  0. 009713069535791874\nBatch loss:  0. 01033339835703373\nBatch loss:  0. 010414314456284046\nBatch loss:  0. 010733190923929214\nBatch loss:  0. 009922059252858162\nBatch loss:  0. 010035117156803608\nBatch loss:  0. 008730228058993816\nBatch loss:  0. 009885470382869244\nBatch loss:  0. 010021351277828217\nBatch loss:  0. 009636195376515388\nBatch loss:  0. 009516282938420773\nBatch loss:  0. 009978237561881542",
        "answers": []
    },
    "79": {
        "question": "Hi professor and staff, I was wondering if there was anyway to submit a regrade request for my Exam 1, while reviewing for the final I noticed multiple grading errors on my exam and, having taken Exam 2, I really think this could be the difference between me passing and failing the course. Specifically for problem 4 I gave EXACT answer from the solutions and still got -4 for not using left_arc even though the solution posted also doesn't include a left-arc. In my problem 3 I get -4 for not \"Initializing all pi[i, i+1, A] to 1 even thought I explicitly do that in my solution. I apologize for waiting until now to make the request, ordinarily I wouldn't if there was any ambiguity in my solutions but having seen my answers directly match the solutions in preparation for exam 2 I just needed to try. Thank you for your time.",
        "answers": []
    },
    "80": {
        "question": "RuntimeError                              Traceback (most recent call last)\r\n/tmp/ipykernel_17324/2697911999. py in &lt;module&gt;\r\n----&gt; 1 model = SrlModel(). to('cuda') # create new model and store weights in GPU memory\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in to(self, *args, **kwargs)\r\n    897             return t. to(device, dtype if t. is_floating_point() or t. is_complex() else None, non_blocking)\r\n    898 \r\n--&gt; 899         return self. _apply(convert)\r\n    900 \r\n    901     def register_backward_hook(\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self. children():\r\n--&gt; 570             module. _apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self. children():\r\n--&gt; 570             module. _apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    568     def _apply(self, fn):\r\n    569         for module in self. children():\r\n--&gt; 570             module. _apply(fn)\r\n    571 \r\n    572         def compute_should_use_set_data(tensor, tensor_applied):\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in _apply(self, fn)\r\n    591             # `with torch. no_grad():`\r\n    592             with torch. no_grad():\r\n--&gt; 593                 param_applied = fn(param)\r\n    594             should_use_set_data = compute_should_use_set_data(param, param_applied)\r\n    595             if should_use_set_data:\r\n\r\n/opt/conda/lib/python3. 7/site-packages/torch/nn/modules/module. py in convert(t)\r\n    895                 return t. to(device, dtype if t. is_floating_point() or t. is_complex() else None,\r\n    896                             non_blocking, memory_format=convert_to_format)\r\n--&gt; 897             return t. to(device, dtype if t. is_floating_point() or t. is_complex() else None, non_blocking)\r\n    898 \r\n    899         return self. _apply(convert)\r\n\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1. I keep getting this error when I try to instantiate SRLModel, I'm unsure what to do. Here is the code block I'm running:model = SrlModel(). to('cuda') # create new model and store weights in GPU memory",
        "answers": [
            "Nevermind I just restarted the notebook server and it worked"
        ]
    },
    "81": {
        "question": "I recently sent an email to Professor Bauer to request an incomplete grade for this class. I was hoping to receive his approval given that the deadline is tonight. Thank you!",
        "answers": [
            "Responded by email. "
        ]
    },
    "82": {
        "question": "Hi TAs and Prof. , Wanted to understand if my understanding of TP, FP, FN is correct because I'm getting very weird results for the span-based evaluation: True Positives (TP): Count a prediction as TP if the predicted span exactly matches a target span, including both the span and the label. False Positives (FP): Count a prediction as FP if the predicted span does not match any span in the target. This includes:Predicted spans that don't exist in the target. Predicted spans where the span is correct, but the label is incorrect. False Negatives (FN): Count as FN if a span in the target:Does not exist in the predicted spans. Exists in the predicted spans but with a different label. Span-based valuation results: Overall P: 0. 16827258962528463  Overall R: 0. 7939763509332494  Overall F1: 0. 27769208370387277Token-based valuation results: Accuracy: 0. 9246983648015192 (Looks decent)Latest training epoch (after 2 epochs): Training loss epoch: 0. 2138111023245621, Training accuracy epoch: 90. 54548921959775% (Similar to what was given)Not sure why I'm getting such a low F-1 given other metrics seems to be correct? Thanks in advance!        # Calculate TP, FP, FN\n        for span, label in predicted_spans. items():\n            if span in target_spans:\n                if target_spans[span] == label:\n                    total_tp += 1  # Correct span and label\n                else:\n                    total_fp += 1  # Correct span but incorrect label\n            else:\n                total_fp += 1  # Span not in target\n\n        for span, label in target_spans. items():\n            if span not in predicted_spans:\n                total_fn += 1  # Span missing in predictions\n            elif predicted_spans[span] ! = label:\n                total_fn += 1  # Span present but labeled incorrectly in predictions",
        "answers": [
            "Hi, I think you might forget to only consider the span within [CLS_idx+1 : SEP_idx]. Otherwise, there would be too many FP leading to super low Precision and hence low F1. In the case where the span exists in both prediction and target but with different tags, I'm not sure how to categorize this case (either FN or FN or both lol). We might need further clarifications."
        ]
    },
    "83": {
        "question": "Hello, Part 3 asks us to \"modify the training loop. .. so that it also computes the accuracy for each batch and reports the average accuracy after the epoch. \" I just wanted clarification as to where exactly we implement this. Should this be under the part that says, \"# Compute accuracy for this batch\"? ",
        "answers": [
            "Yes I would add it there. "
        ]
    },
    "84": {
        "question": "Hello, I have been trying to open jupyter from my browser, but it's not working. My config should be correct. I've attached a screenshot just in case. I've tried a few different IPs. http://127. 0. 0. 1:8923, http://10. 208. 0. 2:8923, and http://34. 165. 248. 255:8923. I know it's not 8888, but according to the screenshot below, it seems to be in use. I have a feeling this is the issue, as the firewall rule is configured for 8888. The latter two IPs just load forever, while the first immediately fails (as expected as it is the loopback). This is what I currently see. ",
        "answers": [
            "I seem to be having the same issue. There are no error messages in the terminal, but Jupyter won't open.",
            "Odd. I assume restarting the vm instance would fix this. But you could also try to find which process is using that port and then kill it. To find the process try:sudo netstat -nlp | grep :8888",
            "I'm also running into the same issue! 8888 is available and I also followed the same instructions, but am unable to access the notebook. Has anyone been able to fix it? "
        ]
    },
    "85": {
        "question": "I was just wondering, what is the procedure to request regrades for HW 3? The instructions on the home page say to email the TA who graded the assignment, but I can't see the name of the TA who graded my HW 3. Is there any other way to submit a regrade request?",
        "answers": [
            "Please email me directly. I forgot to include the TA names."
        ]
    },
    "86": {
        "question": "When I'm training, I found cases where the label (e. g. 'B-ARGM-COM') is not a part of the dictionary created by the role_list. txt The assignment didn't specify how to handle these specifically. Would treating them like ['PAD'] works? What would be the preferred approach? Thanks in advance! Training output: Batch loss:  4. 183680534362793Current average loss: 4. 183680534362793, Current average accuracy: 0. 45558086560364464%Batch loss:  4. 087785243988037Batch loss:  4. 029718399047852Batch loss:  4. 006991863250732Error:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/tmp/ipykernel_11409/3364925475. py in &lt;module&gt;\n----&gt; 1 train()\n\n/tmp/ipykernel_11409/2737204925. py in train()\n     18     model. train()\n     19 \n---&gt; 20     for idx, batch in enumerate(loader):\n     21 \n     22         # Get the encoded data for this batch and push it to the GPU\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in __next__(self)\n    519             if self. _sampler_iter is None:\n    520                 self. _reset()\n--&gt; 521             data = self. _next_data()\n    522             self. _num_yielded += 1\n    523             if self. _dataset_kind == _DatasetKind. Iterable and \\\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/dataloader. py in _next_data(self)\n    559     def _next_data(self):\n    560         index = self. _next_index()  # may raise StopIteration\n--&gt; 561         data = self. _dataset_fetcher. fetch(index)  # may raise StopIteration\n    562         if self. _pin_memory:\n    563             data = _utils. pin_memory. pin_memory(data)\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in fetch(self, possibly_batched_index)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/opt/conda/lib/python3. 7/site-packages/torch/utils/data/_utils/fetch. py in &lt;listcomp&gt;(. 0)\n     47     def fetch(self, possibly_batched_index):\n     48         if self. auto_collation:\n---&gt; 49             data = [self. dataset[idx] for idx in possibly_batched_index]\n     50         else:\n     51             data = self. dataset[possibly_batched_index]\n\n/tmp/ipykernel_11409/835064567. py in __getitem__(self, k)\n     48 \n     49         #labels to ids\n---&gt; 50         label_ids = [role_to_id[label] for label in label_sentence]\n     51 \n     52         # Create attention mask\n\n/tmp/ipykernel_11409/835064567. py in &lt;listcomp&gt;(. 0)\n     48 \n     49         #labels to ids\n---&gt; 50         label_ids = [role_to_id[label] for label in label_sentence]\n     51 \n     52         # Create attention mask\n\nKeyError: 'B-ARGM-COM'\n",
        "answers": [
            "If a label doesnt appear on the role dictionary it should be replaced with O. "
        ]
    },
    "87": {
        "question": "After running model here In this step or the prior step, I keep getting Kernal Death or Kernal Restart automatically I tried starting my Notebook, restarting my SSH, and laptop, none of them works Are we allocated the right amount of memory for this task?",
        "answers": [
            "What's the specific error message you encounter (if any)? Are using a T4 GPU? How much RAM did you set up for the VM?"
        ]
    },
    "88": {
        "question": "I encountered the following error when creating the instance as required. Create VM instance \"instance-1\" and its boot disk \"instance-1\"A n1-standard-4 VM instance with 1 nvidia-tesla-t4 accelerator(s) is currently unavailable in the us-central1-a zone. Alternatively, you can try your request again with a different VM hardware configuration or at a later time. For more information, see the troubleshooting documentation. What should i do? how to fix it? I have changed and tried several zones. All unavailable.",
        "answers": [
            "Yeah it can be tricky to find an instance. Keep trying (its often easier during evenings/weekends). ",
            "I know this might not be particularly helpful but I just kept trying different zones until it worked!"
        ]
    },
    "89": {
        "question": "Hi, On the exam today, For Q3, I used capital L instead of lowercase l because i didn't want to confuse it for 1. I forgot to write that clarification down, is that ok? Thanks",
        "answers": [
            "Thats fine. "
        ]
    },
    "90": {
        "question": "I hope you are doing well. I stayed up very late preparing for todays NLP exam, and I ended up falling asleep a few hours before it started and completely sleeping through it. Ive never done something like this before with an exam and I've been doing well in the class up to this point, so I would really appreciate it if I could somehow make up for this. Would it be at all possible for me to still take the exam, either by taking the CVN version or otherwise? I'm taking another exam from 4-6pm, but otherwise I could do a makeup at any point. Im very sorry for the inconvenience this has caused and I appreciate your understanding. Sincerely, John",
        "answers": []
    },
    "91": {
        "question": "For any English-&gt;AMR question on the final, can we skip the graph and go straight to the PENMAN representation or do we have to draw the graph first?",
        "answers": [
            "Either output representation would be fine. "
        ]
    },
    "92": {
        "question": "For the problem \"He admired Bob's love of math\", the solution is (a/ admire\n    :ARG0 (h / he)\n         :ARG1 (l / love\n                    :ARG0 (b / Bob)\n                    :ARG1 (m / math))) It seems that \"love\" is arg1 of he, rather than arg1 of admire. Is this true? I feel like for \"He admires love\", love should be arg1 of admire.",
        "answers": [
            "I believe this is just a weird formatting issue. \"love\" is ARG1 of admire here. ChatGPT confirms this:",
            "I think you can think in such way:He admired (Bob's) love (of math) -&gt; He (ARG0) admired love (ARG1). Bob's love of math -&gt; Bob(ARG0)'s love of math(ARG1)."
        ]
    },
    "93": {
        "question": "(y / yield-03 :ARG0 (f / fund :mod (t / top) :mod (m / money)) :ARG1 (o / over :op1 (p / percentage-entity :value 9) :degree (w / well)) :time (c / current))\" do we need to know the exact number like '03' behind 'yield' in the exam2",
        "answers": [
            "Its been confirmed that you do not need to know how to do AMR to this level of detail."
        ]
    },
    "94": {
        "question": "There was another thread saying how semantic parsing wasn't going to be on the final, but are there other sections that will not be covered on the final? Or in other words which topics will be covered on the final?",
        "answers": [
            "There's been a Page on CourseWorks with the entire topic list: https://courseworks2. columbia. edu/courses/179333/pages/exam-2-topic-overview"
        ]
    },
    "95": {
        "question": "Is there a typo in part 3 of the IBM Model ungraded exercise? q(5 | 1, 5, 5) appears twice in the equation but I think of them is supposed to be q(1 | 5, 5, 5)?",
        "answers": [
            "Yes, it's a typo, according to professor's comments in #678 "
        ]
    },
    "96": {
        "question": "I don't understand the answer to #670. How can I know that the alignment is inconsistent and crosses between noun phrases when \"e\" in the foreign sentence 2 to map to position 4 in the English sentence? In addition, is there any general strategy to find q probability? This is confusing since It looks like there are many possible cases.",
        "answers": [
            "If the alignments for the nouns were different, you would get phrases that are not well nested. Of course this _could_ be the case, but seems highly unlikely given the general patterns implied by the other examples in the data. "
        ]
    },
    "97": {
        "question": "Hi, I'm recapping for Exam 2 and I've been struggling to understand the difference between Generative and Discriminative models. Are BERT and Naive Bayes discriminative or generative models? ",
        "answers": [
            "I'm pretty sure that the fundamental difference between generative and discriminative models is that  generative models capture the joint probability P(x, y) while discriminative models capture the conditional probability P(y|x). Where y is a class and x is some input (what x is depends on the model we are working with I believe). What they actually do with the probability they capture varies between implementation of specific models. Generally though generative model is modelling the probability of a input and class occurring as a pair, this allows them to be better at modelling patterns within the data making them well suited to tasks needing the generation of new of data. Discriminative models instead capture the probability of class given some input, which makes them better at modelling the difference between classes and thus are often used for classification tasks. Also BERT is a discriminative model and Naive Bayes, despite being commonly used as a classifier, is a generative model.",
            "In a generative model, you model the distribution of the data, given some hidden variable, i. e. P(x|y). You then use Bayes rule to find the most likely y to explain your observation (input data) x. In a discriminative model you directly model the distribution P(y | x). "
        ]
    },
    "98": {
        "question": "While reviewing the ELMo slides, I realized that the \"s\" task-specific parameter looks the same as the \"\" parameter mentioned in the slides for the attention mechanism. Does this mean that the ELMo representations also use the attention mechanism to generate the task-specific, contextualized embeddings? Below are the relevant slides: Attention: ",
        "answers": [
            "I believe that in the context of ELMo the j's correspond to the layers in a stacked bidirectional RNN. For some tasks, researchers found that more heavily weighing the layers deeper into the model (where the embeddings have become more abstract) gives better results than just summing them all together equally, and sometimes they only use the top layer (layer L). This is different from the alpha parameter in attention, which corresponds to how relevant the key embedding of a token is to the query embedding of another token in a given layer. The attention mechanism involves a sum over N (the number of tokens), while the sum in the provided ELMo equation is over L (the number of bidirectional layers in the model)."
        ]
    },
    "99": {
        "question": "Hi, I was wondering if there were any more resources to practice translating from AMR-to-English  in addition to the ungraded excercise.",
        "answers": [
            "I found this video to be really helpful. Especially the last 20 minutes. Start at 16:32 and watch in 2x speed. It goes through examples similar to the ungraded exercises. https://www. youtube. com/watch? v=uCZ9nAe76Ss"
        ]
    },
    "100": {
        "question": "Dumb quesiton but what does transformer return here for classification task ? Processed input representation with context ? Next word ? How does that translate to classificaiton ? Any help is appreciated, thank you! ",
        "answers": [
            "If you take a look at a GPT classifier (like GPT2ForSequenceClassification in https://github. com/huggingface/transformers/blob/v4. 24. 0/src/transformers/models/gpt2/modeling_gpt2. py#L1328), you will see the original head of the pre-trained model (which was used to train the model) is not used. Instead, a new head which is a singular Linear layer is used to convert the transformer outputs to logits. These then get converted to class scores via either MSELoss (regression), CrossEntropyLoss (binary classification), or BCEWithLogitsLoss (multiclass classification).",
            "The idea is that you augment the input with an Extract token, then you use the representation that GPT computes for this token (prior to the classification head that was used for pre training) and feed it into a classifier. "
        ]
    },
    "101": {
        "question": "Hi, In the slides, it says that the way we compute the similarity score for attention is by doing matrix multiplication and scaling. I was wondering if there are other ways of doing this. Thanks,",
        "answers": [
            "There are other ways to do this, but GPUs are really good at doing matrix multiplication as it is a highly parallelizable task. The alternative will dramatically increase the runtime needed to do attention. "
        ]
    },
    "102": {
        "question": "Hello! I think the regrade window is closed, and that's honestly my bad, but I knew I got this question right. I assumed the grade I got was the grade I deserved. However, the misgrade on Question 1 cost me a large number of points. Although it's technically my fault for missing the regrade deadline, as I am reviewing for the final exam, this -10 points on my midterm is just too large of a number for me to not try to ask for a regrade request even at this time of the semester. As is apparent from the screenshots below, I did not use a table to show my work for the problem. Instead, I did it the other allowable way with equations. I did note to go to page 12 for my work on this problem, as this problem did not have an extra scratch page next to it on the exam. This regrade request is not a petty few points. Getting these points back would solidly put my midterm grade in average zone. I would really appreciate if staff could make this change. Thank you so much for the consideration.",
        "answers": [
            "Hi please send this to my by email. There is a good chance it will get lost on Ed. "
        ]
    },
    "103": {
        "question": "Just to check my understanding =&gt; let's say we put 1 input token in, process and get a positional-encoded representation of the tokenAfter that it's fed to attention block, which spits out a context vector of every ( is it every or is it every previous tokens ? ) in the input, with respect to the token that we are trying to guess. We do this by taking the weighted sum of those tokens and return that weighted sum (which is a context vector that contains similarity scores of each token)This context vector (I assume the dimension is #of inputs *1) is then fed into a feed forward network. =&gt; My question is: is the process above correct ? What does the feed forward network do ? Finally, I remember Dr. Bauer mentioned that we can have multiple context vector for the same token but I can't imagine how this is done, if anyone can give an example it would be amazing. Thanks",
        "answers": [
            "I think one point of confusion is that there is no designated target token. All tokens in the input are passed into the encoder at the same time. The encoder produces a contextualized representation for each of the input tokens. The output will be a (batch_size, input_length, embedding_dim) tensor. "
        ]
    },
    "104": {
        "question": "Hey everyone, I was pretty confused about this naming convention for e, f (and the associated terms p(f|e), p(e) etc. ) Kept thinking \"e\" stood for evidence in my attempts to internalize the content. Could not come up with a good reason why the Spanish text was called \"f\". I had almost given up in deciphering the naming method used here. Turns out, historically, the setup/explanation of the MT task is usually made in the scenario where we are attempting to translate French (f) to English (e). [source1, source2] It helped me to reason about the \"f\" as \"s\" in the lecture slides for Spanish, sharing in case it is helpful or if someone else was curious why they were called \"f\" and \"e\". ",
        "answers": [
            "f is for french and e is for english in the examples",
            "i think it means \"foreign\" and \"english\"?"
        ]
    },
    "105": {
        "question": "Just to make sure: Semantic Parsing this whole part will not be tested in the final right? Thank you.",
        "answers": [
            "I just checked the final review recording, it will not be tested.",
            "It won't be tested. "
        ]
    },
    "106": {
        "question": "In this formulation and in the slides following, it appears that m and n are used interchangeably. ",
        "answers": []
    },
    "107": {
        "question": "Extremely silly question incoming. I was reading the original GPT paper and they say that:the inputs are passed through our pre-trained model to obtainthe final transformer blocks activation $h_l^m$, which is then fed into an added linear output layer with parameters $W_y$ to predict $y$. This mirrors discussion during lecture that we only care about $h_l^m$ in predicting $y$. But why do we only care about the last position? It makes intuitive sense that we only care about the result of the final layer $l$, but why also the final position $m$ corresponding to the extract token? Does the extract token hold all the information necessary for the classification task?",
        "answers": [
            "I thought through it for a few more minutes. Is it because it is an auto-regressor, and thus it's sensitive to all of its left context? That is, is $h_l^m$ not just the vector representation of just extract, but the vector representation that predicts extract based on the entire left context? I think my confusion may entirely be a result of being unsure what $h_l^m$ is. It seems to the the final transformer block's activation, so it's not just the mth generated word or anything like that. The paper implies it is a vector, but maybe it's a matrix instead? Hmm, maybe I'm entirely off on the wrong foot  and it's just the output of the final layer of the transformer with the entire left context as an input?"
        ]
    },
    "108": {
        "question": "Will the final review session slides be posted?",
        "answers": [
            "Hey Jennifer, the slides should be here #630 ! Hope this helps-SN"
        ]
    },
    "109": {
        "question": "I think the concept of attention as a soft lookup makes a lot of sense. You use the dot product as a sort of \"similarity\" score, and then use these as weights to fine-tune an aggregate of vectors. This basically is storing information about what values to pay \"attention\" to in the context vector. However, when this translates to attention matrices in transformers, I have a hard time understanding the concept. We're still taking the similarity score of Q and K (this time as a matrix multiplication so we don't have to do it for every step), but what is V? What is the Value? In the above example, after calculating the score, we just multiply back the keys with high score (values and keys seem to be the same). What exactly is the V matrix here? How do we calculate it? Is something different about Q and K? When I googled this, it made it sound like Q, K, and V are all matrices that result from a matrix multiplication of the word embeddings with some weight matrix that is trained in the transformer. But that just made me more confused. Any help would be appreciated.",
        "answers": [
            "I believe V is a matrix where each column is an input vector (following the positional encoding). The output is a matrix of vectors that have been adjusted for self-attention.",
            "V is a transformation matrix acting on the embeddings. It is is something different than Q, K. Maybe this, somewhat lengthy response in a different thread might help. https://edstem. org/us/courses/46417/discussion/4035163? answer=9287537"
        ]
    },
    "110": {
        "question": "In the slide below, For the denominator, we need probability of X, i. e. P(X_1, .. ., X_d) which is presented as product over P(X_i). My understanding was that Naive Bayes makes a conditional independence assumption s. t. all X_i are conditionally independent given the label. So to decompose P(X_1, . .. , X_d) to P(X_1) x P(X_2) x . .. x P(X_d) should not be allowed in the absence of Label. Instead we need a sum term over the labels where the inside of the sum is the same as the nominator. ",
        "answers": []
    },
    "111": {
        "question": "Hi, I just wanted to know when we get some information on how the test is going to take place on zoom and general instructions for the exam. Thanks",
        "answers": [
            "For those of you signed up for this in advance, I will distribute the link tomorrow morning. You will get access to the exam on Gradescope at 1:10pm. You can then either print it, complete, and scan the final version back in -- or just use separate sheets of paper for your answers. There will be extra time after the exam to finish scanning and uploading. "
        ]
    },
    "112": {
        "question": "Hi! In class we went through two types of tokenization that BERT uses (bye-pair encoding and WordPiece) are these things we should be able to replicate or is understanding the concept alone sufficient? Thank you! ",
        "answers": [
            "Understanding the concept is sufficient. "
        ]
    },
    "113": {
        "question": "Hi! I am just wondering is it acceptable  if we output sentence with a different structure than the solution provided during AMR translation (still have the same meaning)thank you",
        "answers": [
            "found answered in #702 thanks"
        ]
    },
    "114": {
        "question": "Hi, I'm confused on the difference between BERT and GPT and Transformer Models. Is the transformer model itself a combination of both BERT and GPT? If you are to choose between using BERT vs GPT is it just GPT if the task falls into one of the 4 categories (MCQ, Classification, Similarity, or Entailment) and BERT otherwise?",
        "answers": [
            "not a TA so correct me if I am wrong:BERT is implemented with a [encoder-only] transformer architecture, that's why it's called Bidirectional Encoder Representations from Transformers. GPT is implemented with [decoder-only] transformer architecture, that's why it's called Generative Pre-trained Transformer. So, in short, both of them are transformer models, I'd say it's a model architecture that's implemented by the above two models. I am unsure of what exact model you want to choose between BERT and GPT. BERT is a masked model, which means that you can \"mask\" a word in a sentence and let BERT predict what the word would be(thus doesn't have to be a next word prediction). GPT is a generative model and thus can only predict the next word to appear. I don't think there are tasks that reply on specifically BERT or GPT. ",
            "I think BERT is good for classification task and GPT is mainly used on generation task, generally speaking.",
            "I think BERT is better for understanding context because it considers left and right context, while GPT is more suitable for generative tasks. The inputs are also quite different for the two models, as the start/sep tokens are different for BERT and GPT(different input tokens depending on MC, similarity, entailment). "
        ]
    },
    "115": {
        "question": "Hello, What exactly is the output of running ELMo? My understanding was that it returned a word embeddings vector (but with context), similar to Word2Vec. But I got confused reviewing the nearest neighbors slide in Lecture 13; is the implication that ELMo is actually a sentence embeddings vector?",
        "answers": [
            "I think it might be the contextual embedding. For the \"nearest neighbors\", I think that refers to when you're trying to map the contextual embedding back into the lexeme during running time, you need to find the nearest contextual embedding to be its output lexeme."
        ]
    },
    "116": {
        "question": "Hi, While performing AMR translation is it okay if the sentence we make is structured differently than the original solution if they have the same meaning?",
        "answers": [
            "Yes, AMR generalized away from the surface form of the sentence, so many different sentences can have the same AMR representation. "
        ]
    },
    "117": {
        "question": "Hello! I am a little confused about whether context size is uni- or bi-directional in the scope of this class. By which I mean, for the sentence: \"The Quick Brown Fox\" with a context of 2, would the context for the word Quick be {\"Quick, The\", \"Quick, Brown\", \"Quick, Fox\"} or only {\"Quick, Brown\", \"Quick, Fox\"}.",
        "answers": [
            "I guess you could define this either way. Typically I would say something like \"a context window of +-2\". But when it just says \"2-word window\" I would read this as +-1"
        ]
    },
    "118": {
        "question": "For AMR translation to English such as in the ungraded exercise, does the verb tense of our answer matter as long as the overall semantic roles and meanings are captured? For example, would \"Currently, the top money fund yields well over 9%\" be accepted versus the answer in the solution \"Currently, the top money fund is yielding well over 9%? \" Thanks! ",
        "answers": [
            "No, tense is one of the generalizations that AMR makes. It's not reflected in the AMR and therefore the English sentence could have any tense. "
        ]
    },
    "119": {
        "question": "In lecture 18, the slides show that a_i = argmax q(j|i, l, m) * t (fi, ej)I'm wondering is it equivalent to a_i = argmax q(j|i, l, m) * t (fi | ej)In IBM exercise problem, we are also calculating the conditional probability instead of joint probability.",
        "answers": [
            "Ah, there's a typo in the slides. The t parameters are conditional probabilities, conditioned on e. So it should always be t (fi | ej)."
        ]
    },
    "120": {
        "question": "hi i hope you are great, I dont understand the solution for 6 BIn part A we calculated 3ed +dv parametersIn B we move from being giving embedding to, to do doing one hot. so we add layer for one hotso this would add: 3V * 3E parametersi thinkso wouldnt the total be:(3V * 3E) + 3ed +dv parametersi dont see how the actual answer is calculatedthank you so much!",
        "answers": [
            "We use the same shared VxE embedding matrix for each of the tokens. We do compute the embedding matrix multiplication three times, but the matrix we multiply the one-hot encoding by is the same for each of these three multiplications, so there are only V*E new learnable parameters."
        ]
    },
    "121": {
        "question": "Hi Professor Bauer and TAs, I received a 5 point deduction on part 4 of HW 3 because I \"did not sort predictions. \" If I'm not mistaken, I believe this is referring to this step in the homework doc: \"The easiest way to do this is to create a list of possible actions and sort it according to their output probability (make sure the largest probability comes first in the list). \"In my case, I think I did actually do this in lines 47 and 48 of my decoder. py:# select and perform the action with the highest probability\nbest_transition = max(transition_scores, key=lambda k: transition_scores[k])The only difference is that instead of sorting the transitions by probability first and then iteratively removing illegal ones, I assigned 0 score to all illegal transitions a priori (in the for-loop iterating over all transitions) and then simply returned the highest probability transition. Also, my attachment scores are consistent with those in the homework doc: Micro Avg. Labeled Attachment Score: 0. 7459847599147617. I apologize for the inconvenience, but would it be possible for me to receive points back in this case? Or was the deduction referring to a different mistake? Either way, thank you so much!",
        "answers": []
    },
    "122": {
        "question": "Hi, I wonder how could we differentiate between ARG2 and ARG3 in AMR parsing? Thank you!",
        "answers": [
            "You would need to look up the definition for ARG2 and ARG3 in the propbank framefile as these differ per predicate. Clearly not something you can do on the exam. "
        ]
    },
    "123": {
        "question": "Hello, I recently saw that I received a 25/30 on Part 3 for having too many dense layers. While I agree that this representation is incorrect according to the specifications of Part 3, I ended up submitting an experiment for changing the model (as was encouraged in Part 4 and performs better than the model specified in Part 3). In doing so, I changed the structure of what was asked from us in Part 3. While I understand, it is my fault for deviating from the instructions and I understand if I do not get the points back, I thought submitting a better version of the model asked from us in part 3 would suffice in its place.   ",
        "answers": []
    },
    "124": {
        "question": "Just to confirm that the exam2 will start at  1:10pm Dec 11th in 309 Havemeyer ?",
        "answers": [
            "Yes, see #464",
            "Yes, that's correct -- the usual time and place. "
        ]
    },
    "125": {
        "question": "Hi, I'm a bit confused about IBM Model 2 after looking at the ungraded exercise. I saw that, for t(skwk|fly) and q(5 |1, 5, 5), the \"5\" in q corresponds to \"skwk\" and the \"1\" corresponds to \"fly\". If we have a problem like this on the final, should the foreign or the english word come first in both t and q? In the ungraded exercise, the foreign word comes first in t but the english word comes first in q.",
        "answers": [
            "For IBM Model 2, I believe it's:$t(f|e)$ for the probability that the output word $f$ is generated from the input word $e$. $q(j|i, l, m)$ for the probability that the $i$-th alignment variable (i. e. , $a_i$) is $j$ for input sentence length $l$ and output sentence length $m$. In other words, the probability that the $i$-th output word (i. e. , $f_i$) is aligned to the $j$-th input word (i. e. , $e_j$). The probability of the whole sentence is $$\\prod_{i=1}^m q(a_i | i, l, m) t(f_i | e_{a_i}). $$"
        ]
    },
    "126": {
        "question": "For \"(y / yield-03 :ARG0 (f / fund :mod (t / top) :mod (m / money)) :ARG1 (o / over :op1 (p / percentage-entity :value 9) :degree (w / well)) :time (c / current))\" I didn't get what does the \"-03\" mean in this case",
        "answers": [
            "The -03 is just there to note that its the third sense/subsense of \"yield\" as defined in proppbank"
        ]
    },
    "127": {
        "question": "For the exam, are we expected to know the format for NERs like time, person, date, etc? (I mean this: time: (t/ time-12 :value \"4:00pm\"))Or will we be given this formatting information on the exam? Just want to know if I should be placing it on my cheatsheet.",
        "answers": [
            "You don't have to know the exact format. "
        ]
    },
    "128": {
        "question": "I understand that in the transformer model, query, key, and value vector representations are calculated using trainable weight matrices and the input embeddings. I'm curious how these different vector representaions can be interpreted. I'm wondering if there these embedding-like vectors carry different meanings of the same word? Like does the key form of a token supposedly represent a \"contextual meaning\" of a word whereas the query form captures a more central meaning of the word as a target (whatever that means)?",
        "answers": [
            "This is a cool question. reminds me of a talk I had listened to either with Andrej Karpathy or Ilya Sutskever (can't remember which) discussing something similar. This is how I internalized some of that discussion. At each attention layer, we have embeddings for each position. As we get deeper in the model, 2 things happen. 1. The embeddings get more complex, due to additional non-linearities. 2. They also get more contextualized. The attention mechanism, with Q, K, V matrices (which are shared for each position's embedding) facilitate 2. And a nice analogy to how it does this is \"message passing\" or an \"online quorum\".   When an embedding e_i is multiplied by Q(uery), q_i is generated. q_i, in my understanding, can be thought of as: \"As embedding e_i, here is a representation of the things I would like to know\"So it's sort of a question that is asked by e_i, in the same way you might ask an online forum about what you'd like to know. The k_i generated from the K(ey) is the natural analog to the above. It represents something along the lines:\"As embedding e_i, I can answers these kinds of questions\" The score we calculate is then, kind of a matchmaking between askers and answerers. Note that what the questions an embedding might be able to answer (it's k_i (key) value) and the actual answers to those questions might be different. And that's where the V(alue) comes in. When two indices agree (via the score) that one can answer the other's question, it seeks the answer from the value vector. Thus v_i denotes something like:\"As embedding e_i, here is the knowledge I possess that would be helpful to others\" It happens that most of the time, the best answerer to a question an embedding might ask is itself. (i. e. the softmax is highest between its own query and key values) But attention basically presents an opportunity to see if there are other positions that possess something that might be helpful, which turns out to be super effective. (and the residual connections ensure that we don't lose track of what e_i was originally in this chatter) So I think yes, the training does push these q, k, v vectors to capture different meaning, perhaps not semantically but contextually. I know this about the world e_i (i. e. my semantics are set), What can I do with this? 1. Ask questions2. List questions I can answer3. Answer questionsSorry for the long answer, I really like your question! "
        ]
    },
    "129": {
        "question": "Bayesian approaches: Model P(x|y) and P(y) using Bayes Rule. Assume observed data generated by some hidden class label. Naive BayesGaussian Mixture ModelsHMMApplications: N-gram Language ModelCKY Parsing, PCFGsTransition-based Dependency ParsingIBM Model 2JAMRDiscriminative approaches: Predicting P(y|x) directly with inductive learning: given a bunch of input/output pairs, find h(x) that best approximates f(x) that maps inputs to outputs Linear and Log-linear modelSVMDecision trees, random forrestApplications: Neural NetworksRNNTransformersSummarizationLanguage and VisionHi TAs and Prof. Bauer, want to understand if my understanding of the different approaches and Applications are correct for all the models we covered. Please point out if I missed or misunderstood anything as well. Thanks in advance!",
        "answers": [
            "Not sure about JAMR being a discriminative model. I think they are using a log-linear model in the original implementation. You also seem to be mixing together models and applications. For example, summarization and language and vision would be an application, while RNN or transformer would be a model. "
        ]
    },
    "130": {
        "question": "Hi, In this equation, what exactly are all the variables? (If translating from english to French, which is the english word and french (for both j, i, and l, m). q(j | i, l, m) Also, are l and m necessarily equal. Can we also compare things which are 3 words and 5 words and find patterns? Thanks!",
        "answers": [
            "i is a token position in the source sentence (called \"foreign\" in the slides) and j is a token position in the target sentence (called \"English\" in the slides). l is the number of tokens in the target, m the number of tokens in the source. l and m are not necessarily equal. You can have several source tokens aligned to the same target token, or even to nothing at all. You can also have unaligned tokens in the target language.  "
        ]
    },
    "131": {
        "question": "Confused about the difference between Keys values and queries without the context  of transformers. if they are all input representations what makes them different? and do they change how  different they are for instance when calculating  cross attention (vs self attention) in the  decoder?",
        "answers": [
            "In self attention, k, v, and q are all the token representations from the previous transformer layer. In cross attention k and v are the token representations computed by the encoder, while q are the token representation from the previous decoder layer. "
        ]
    },
    "132": {
        "question": "I am confused by what the encoder attention block should return. My understanding is that for each input, it returns a context vector of that input (i. e: if I put in \"the\", it should return [0. 5 0. 1 0. 1 0. 3]). And if we do that for all inputs we get 4 context vectors as such. 1) Do we then take average of these 4 context vectors for a final grand vector and then feed it to the multi-headed attention block ? Does this vector stay the same for each next word we try to predict then ? 2) How do we process all of our inputs in parallel ? What would it look like in this example",
        "answers": [
            "The encoder will return an encoded (contextualized) representation for each input position. 1) I'm not sure I understand the question. Are you asking how the encoded input representations are used in the decoder? The encoded representations are not combined into a single vector. Rather, the cross-attention mechanism selects which of the encoded input representations are relevant to compute the outputs. 2) The transformer has a fixed input and output size. All input tokens + output tokens (up to the current output position t) are processed in parallel to predict the next output token. The example in your screenshot doesn't specify what the output looks like. "
        ]
    },
    "133": {
        "question": "Hello! For a stacked RNN model since we are encoding different vectors for each token, does it matter that the number of layers corresponding to each word in the text might not be the same or am I perhaps misunderstanding something? Thank you!",
        "answers": [
            "Hm yeah Im not sure I understand the question. The number of layers is a hyper parameter of the model. Its the same for any input. "
        ]
    },
    "134": {
        "question": "Hello! I am still a little confused on what exactly the difference is between the unfolded RNN and the traditional feed forward RNN is? I understand that we are now accounting for the vector encoded by the hidden layer (W*x) as a representation of the context to the left of the target word. Is the objective here that instead of each word having the semantic rep using 2 vectors (the word as a target and as a context word), we are combining into one vector? And if yes, what advantage does that hold over the regular NN model? Thank you!",
        "answers": [
            "Are you asking about unfolding for BPTT? The idea is that this provides a way to train the model and propagate later errors back to earlier input positions. There is no benefit in terms of the prediction itself and the model does not use more information than the not-unfolded RNN. "
        ]
    },
    "135": {
        "question": "According to definition in slides, A-&gt;aB or A-&gt;a is considered Regular Grammar. Is A-&gt;Ba also considered part of Regular Grammar?",
        "answers": [
            "No"
        ]
    },
    "136": {
        "question": "Hello, I'm confused about the AMR solution to the sentence \"We are expected to meet him at 4:00pm in the courtyard. \" Why are ARG1 and ARG0 both (w / we)? I appreciate your help! !",
        "answers": [
            "(e / expect-01\n          :ARG1 (w / we)\n          :ARG2 (m / meet\n                    :ARG0 (w / we)\n                    :ARG1 (h / him)\n                    :time (t / time-12 :value 4:00pm)\n                    :location (c / courtyard)))\"We are expected to . .. .\", thus, with respect to the verb expect, \"we\" are being affected and thus become the patient/theme, hence tag ARG1. What about the word \"meet\" ? Who is the person that takes this action ? The answer is \"we\", hence the tag ARG0 under meet."
        ]
    },
    "137": {
        "question": "Hi, Can we have more exercises related to IBM model 2 that have more than one alignment to enhance our understanding? Thank you.",
        "answers": []
    },
    "138": {
        "question": "hi i hope you are wellI dont quite understand why we say we take the max at every step for vertibi.  This is from the ungraded question solution:[0, Start] = 1[1, N] = [0, Start]  P(N|start)  P(time |N) = 1  1/2  2/4 = 1/4[1, V] = [0, Start]  P(V|start)  P(time |V) = 1  1/2  1/6 = 1/12[2, N] = max( [1, N]  P(N|N)  P(flies|N), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 1/4  1/4), (1/12 2/3  1/4)) = 1/64[2, V] = max( [1, N]  P(V|N)  P(flies |V), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 2/4  2/6), (1/12  0  2/6)) = 1/24[3, V] = max( [2, N] P(V|N)  P(like|V), [2, V]  P(V|V)  P(like|V)) = max( (1/64  2/4  2/6), (1/24  0  2/6)) = 1/384[3, Prep] = max( [2, N] P(Prep|N)  P(like|Prep), [2, V]  P(Prep|V)  P(like|Prep)) = max( (1/64  1/4  1), (1/24  1/3  1)) = 1/72[4, N] = max( [3, V] P(N|V)  P(leaves|N), [3, Prep]  P(N|Prep)  P(leaves|N)) = max( (1/384  2/3  1/4), (1/72  1  1/4 )) = 1/288[4, V] = max( [3, V] P(V|V)  P(leaves|V), [3, Prep]  P(V|Prep)  P(leaves|V)) = max( (1/384  0  1/6), (1/72  0  1/6)) = 0The most likely state sequence is N V Prep N. I dont understand why we saying we take the max at each step.  It seems like we are work out each sequence from start to end and picking the one with the highest probability.  thank you!",
        "answers": [
            "I think the reason of taking the max at each step is to avoid the need to explicitly calculate and store probabilities for all possible sequences since that would be computationally expensive. "
        ]
    },
    "139": {
        "question": "Hi all, When finishing ungraded assignment, I got confused about this:How can we get this distribution? Why all the probabilities are 1? And how can we get this: Thank you!",
        "answers": [
            "And in this assignment, it seems that there is something wrong with the answer: or I have a misunderstanding about this part? This is my answer:",
            "I think there are some typos in the solution. The last one should be q(1|5, 5, 5). ",
            "All alignment possibilities are 1 because there is only one possible alignment per sentence pair, and for the same length these alignments are always the same. In the data, for each sentence pair of length 1, token 1 is aligned to 5, token 2 is aligned to 4,  etc"
        ]
    },
    "140": {
        "question": "In the ungraded HW problem, we see an example of lexical divergence, resulting in non-1 translation probabilities. I was wondering in what scenario will there be a non-1 alignment probability and wanted to check if my understanding is correct. If this is the example translation: He drinks coffee daily &lt;--&gt; on p'et kofe kazhdyy denAnd this is the dictionary: He : on, drinks : p'et, coffee : kofe, daily : kazhdyy denWill the alignment probabilities look something like this? q(1 | 1, 4, 5) = 1, q(2 | 2, 4, 5) = 1, q(3 | 3, 4, 5) = 1, q(4 | 4, 4, 5) = 0. 5, q(5 | 4, 4, 5) = 0. 5. Thank you in advance.",
        "answers": [
            "Yes, if that one sentence was your only training data, you would get q(4 | 4, 4, 5) = 0. 5, q(5 | 4, 4, 5) = 0. 5. More typically, you will have some inconsistent alignment  between multiple sentences, i. e. in some sentence pairs token 4 is aligned to position 4 and sometimes to position 5. "
        ]
    },
    "141": {
        "question": "Hello, I am looking over the lecture slides again and wanted to make sure my understanding of ELMo was correct. Would it be that for a given sentence such as \"the cat is happy\" where you are analyzing the word \"cat\" ELMo would go forward for the context before the word \"cat\" (while attempting to predict \"cat\") and backwards for the words \"is happy\" (while attempting to predict \"cat\")? Then finally both of these predictions would be combined to form an output?",
        "answers": [
            "That sounds almost correct. There are initially two different embedding for each token, one using the left context and one using the right context. Then, these are just concatenation. "
        ]
    },
    "142": {
        "question": "For Exam1 Question6 b), when using one-hot representation for input words and computing its embeddings, why does the embedding layer only add V*e parameters instead of 3V*e since there are 3 embeddings? How do we decide the embedding matrix is shared or not?",
        "answers": [
            "I think the embedding matrix weights are shared across each context word because we want to generalize a common matrix which encodes all the input word representations together."
        ]
    },
    "143": {
        "question": "Hello! I was wondering where we can find the recording of the review session from today (Sat. Dec 9th). I understand it just concluded, but wondering if it is already posted. Thanks!",
        "answers": [
            "i think its already posted :))"
        ]
    },
    "144": {
        "question": "Hi, I wanted to know for assignment 5, is it necessary that we perform the assignment on GCP or can we use our local machine or Colab?",
        "answers": [
            "The problem is that you need a GPU with about 16GB memory. You can probably run it on colab, but I havent tried that. Its good practice to setup the GCP environment though. "
        ]
    },
    "145": {
        "question": "Hi there, In the lecture notes, for the graph-based dependency parsing, we start from a completely connected graph and then find the maximum spanning tree from the fully connected graph as the dependency tree, so why the answer to the first question is \" The approach might create graphs that are disconnected\" given the generated tree is MST. Thanks a lot!",
        "answers": [
            "The approach described on the ungraded exercise does NOT use an MST algorithm. It simply greedily selects the highest scoring head for each token. "
        ]
    },
    "146": {
        "question": "Hi, I understand attention as a concept generally (weighting hidden states depending on the decoding timestep) but I don't understand the \"attention as lookup\" idea. If the context vector is a aggregate of the hidden states, how does \"lookup\" fit in here? In the diagram below that I found, the query maps to all keys, so isn't it better to just describe this as a weighted sum? Why do we talk about attention as lookup? I feel like I'm missing something.",
        "answers": [
            "You are right that it feels like a weighted sum, at the end of the day there is a weighted sum that happens. Why this \"weighted sum\" is often though as a ~fuzzy~ lookup is because the mixing ratios in your weighted sum (for each embedding index) are determined/\"personalized\" by each position's query. So people call this \"queried\"/\"personalized\" weighted sum that happens in attention a sort of lookup table by virtue of it being \"queryable\" like a lookup table. ",
            "Very nice description, thanks.  "
        ]
    },
    "147": {
        "question": "For ungraded assignment IBM Model 2 in example #4, isn't it possible for the \"e\" in the foreign sentence position 2 to map to position 4 in the English sentence?",
        "answers": [
            "Sure, but that would mean that the alignment is inconsistent and crosses between noun phrases. So while possible, it's not the most intuitive alignment. "
        ]
    },
    "148": {
        "question": "Suppose we use RNN to some downstream task on a whole sentence (with input tokens x1, x2, .. .. ,xk). Do we use the final hidden state h_k or use the final output y_k (or both) to input in a linear model?",
        "answers": [
            "I think it depends on what you are trying to do with the sentence. Using hidden state can help understand the whole sentence. Final output can help understand what the sentence might llead to or its conclusion.",
            "Typically you would feed the hidden representations into the downstream classifier. Note that there is a difference between token classification tasks and sentence classification tasks. In sentence classification you would use the hidden state computed after the last time step. "
        ]
    },
    "149": {
        "question": "Hi, I am a little confused about the IBM Model 2, and I have some questions:1. When calculating the probability q(ji), is the index i representing the position in the source language or the target language? 2. How does this choice impact the alignment and translation probabilities, and what considerations should be taken into account for optimal model performance?",
        "answers": [
            "To answer your first question, it would make sense that i represents the position in the source language since q(j|i, l, m) represents the probability that the alignment variable i takes value j (from the slides). In other words, what is the probability that the value of variable i actually takes the value of j?  For instance, if \"gato\" is at position i in some Spanish sentence and \"cat\" is at position j in the English sentence, then q(j | i, l, m) would represent the probability that the alignment variable i (representing the position of \"gato\" in the source language Spanish) takes the value j (the position of \"cat\" in the target language). https://web. stanford. edu/class/archive/cs/cs224n/cs224n. 1162/handouts/Collins_annotated. pdf"
        ]
    },
    "150": {
        "question": "I'm trying to better understand the usage of ARG0 and ARG1 in AMR. Could you explain when to use ARG0 as opposed to ARG1? (a/ admire\n    :ARG0 (h / he)\nHere we use ARG0 following the verb admire. (e / expect-01\n          :ARG1 (w / we)Here we use ARG1.",
        "answers": [
            "Here's my interpretation (which may or may not be right): ARG0 is typically used to represent the entity or entities that perform the action denoted by the main verb. In \"(a/ admire :ARG0 (h / he)), \" ARG0 is used to represent the agent, which is \"he\" (referring to someone) who is performing the action of admiring. On the other hand, ARG1 is generally used to represent the entity or entities that undergo the action or are affected by it. In \"(e / expect-01 :ARG1 (w / we)), \" now ARG1 is used to represent the patient or theme, which is \"we\" which refers to a group of people who are being expected. Overall, use ARG0 to represent the agent or entities performing the action and use ARG1 to represent the entities undergoing or affected by the action.",
            "I find that this example do a good job of understanding AMR for me. Notice that for line 2 and 3; we are tending to predicate b: beg -01. With respect to the word beg, we could see that \"I beg you\" would make \"I' the action-taker (hence ARG-0) and you the affected (hence ARG1). However, when we apply this recursive structure to the predicate \"excuse\", the roles are reveresed and so are ARG0 and ARG1 labels."
        ]
    },
    "151": {
        "question": "If so, why hasn't a distilled version of say GPT4 been developed? I would think a smaller, more portable version of GPT would be immensely useful for potential applications like on-device ChatGPT. I found this Microsoft paper (https://arxiv. org/pdf/2108. 13487. pdf) that seems to indicate that at least on GPT3 similar training methodologies can lead to the new model outperforming the original model:\"Brown et al. (2020) propose to directly use GPT-3 for downstream tasks, with the n given labeled instances and no fine-tuning. We refer to this strategy as raw GPT-3. We note that raw GPT-3 is expensive, as its cost goes linearly with the number of instances during inference. Also, it has a relatively high latency when deployed for real applications. However, even in terms of accuracy, we observe in the experiments from section 3. 3 that the in-house models trained with GPT-3 labels can often outperform raw GPT-3. We argue that by using data labeled by GPT-3, we are essentially performing self-training: the predictions on unlabeled samples act as regularization on induced models and help improve the performance. In particular, for classification problems, we can theoretically upper-bound the error rate of the best in-house model using the labels generated by GPT-3. \"I am just curious why if this is the case has it not been used or deployed in the real world.",
        "answers": [
            "Correct me if I misunderstood or if I am wrong. If you are asking about why we are not using labels from GPT4 to train models, I believe this is a (maybe widely) used approach for finetuning many LLM now: to use GPT4 feedback in finetuning as there has been evidence of positive correlation between human preference and GPT4 selection and always collecting human response is quite expensive. It's then collectively passed to a reinforcement strategy called DPO(direct policy optimization). I suggest checking out Prof. Rush's video here explaining how they did this on Zephyr(finetuned Mistral) https://www. youtube. com/watch? v=cuObPxCOBCw&amp;t=34s (paper here: https://arxiv. org/abs/2305. 18290) "
        ]
    },
    "152": {
        "question": "What's the purpose of ARG1 (w/we) after the line (e / expect -01)? My understanding was that \"we are expected to meet him\" could be covered by:(e / expect-01)     :ARG1 (m/meet)           :ARG0 (w/we)            :ARG1 (h/him)    ",
        "answers": [
            "I suppose this is a bit up to discussion, based on the propbank frameset for expect-01. Does this sense just cover ARG0 expects an event ARG1, or would it instead be ARG0 has an expectation of ARG1 to do ARG2. I actually think your interpretation is better (and matches the one we came up with in class iirc). "
        ]
    },
    "153": {
        "question": "Hello, I hope you are doing well. I was wondering when we will get the CVN instructions for those of us taking the exam asynchronously. (Mostly just want to know the time frame in which we need to take the exam. )Thanks in advance!",
        "answers": [
            "Hi, The time window will open at 1:10pm on Monday and extend until Tuesday night, 11:59pm. "
        ]
    },
    "154": {
        "question": "Dear Prof, I'm wondering what kind of cheat sheet is accepted. Can we use printed cheat sheet? Thanks!",
        "answers": [
            "#384"
        ]
    },
    "155": {
        "question": "1) for the probability q(j|i) in IBM model, is the dimension i standing for the dimension of source language or target language? 2) In order to make a one to many translation, does it mean the token number in source language (foreign language) is always more than the token number of target language (ie. English)? 3) If source language has 7 words and target language has 9 words, Can we use IBM Model?",
        "answers": [
            "1) i is in the source language, j is in the target language. 2) Not necessarily, some words in the target language may simply not be aligned to anything in the source language. 3) Yes, see 2. "
        ]
    },
    "156": {
        "question": "Hi, Want to ask for any helpful notes about using GPT 3. 5-Turbo, GPT 4 API for fine-tuning? Any source available at Columbia? Thanks.",
        "answers": [
            "Hm, I'm not aware of a specific resource maintained at Columbia -- are you wondering about best practices etc. ? I think most researchers are a little skeptical about using the API for fine-tuning since that doesn't give them full control of the process and transparency. So, in most recent work I have seen people tend to use open source models (like Llama). "
        ]
    },
    "157": {
        "question": "Hi, I want to know if there are any requirements for participation on Ed in order to get participation points, such as a certain number of times or should not be anonymous? Thanks!",
        "answers": [
            "No require except for occasional posting. I can see you in the statistics tab, even when you post anonymously. "
        ]
    },
    "158": {
        "question": "Hi, last exam we had a problem about how to count the trainable parameters of a model. I am still a bit confused about this concept. Can I ask the relationship between trainable parameters with input embedding, hidden layer, possible output words. And how this concept apply to like RNN or some other models.",
        "answers": [
            "Since feed-forward layers are fully connected, the weights between two layers can be expressed as a (e, d) matrix, where e is the dimensionality of the previous layer and d is the dimensionality of the successor vector. For a simple Elman RNN model in the slides, the parameters consist of the weight matrices U and W, and a weight matrix to compute the output from the input. For some of the more advanced models (like LSTMs and transformers etc. ) things are more complicated because the internal architecture of the different components is more complex (think of the LSTM gates or the attention mechanism in the transformer). "
        ]
    },
    "159": {
        "question": "Is there a reading material to understand in detail about how GLUE and SuperGLUE are computed? ",
        "answers": [
            "You could take a look at the glue and superglue websites:https://gluebenchmark. comhttps://super. gluebenchmark. comThis will not be in the exam (though it helps to have an idea how some of these problems can be solved with pretrained language models) "
        ]
    },
    "160": {
        "question": "There are two modeling objectives while training BERT. MLM and Next Sentence Prediction. Are these two tasks trained together? Specifically, I want to know how the loss function would look like.",
        "answers": [
            "Hi, Based on what I read online, I believe that these two tasks are trained together. If you look at the diagram below from the research paper introducing BERT, it seems like when they are feeding in the pre-training examples that they are masking two sentences and then concatenating them together with the [SEP] token. Then, the special classification token at the start is used to predict the NSP. Based on this, I believe that the loss function would probably be the combined MLM loss and NSP loss. Based off this medium article and the BERT paper:https://arxiv. org/abs/1810. 04805https://medium. com/@Suraj_Yadav/what-is-bert-how-it-is-trained-a-high-level-overview-1207a910aaedBest Regards, @fr. baebae",
            "Yes thats correct. Both objectives are used simultaneously and the losses are combined. "
        ]
    },
    "161": {
        "question": "Hi, In the objective function proposed in question c, I don't see how this object optimize for labeling the type of the edge. Do we need to think about this? Thanks!",
        "answers": [
            "The problem assumes that dependencies are unlabeled  but its probably a good idea to think about how you could modify the approach to deal with labeled edges. "
        ]
    },
    "162": {
        "question": "Hi Professor and TAs, I am just now realizing that the filename path for W2VMODEL_FILENAME in my lexsub_main. py submission is my local file path ('/Users/manasisoman/Downloads/GoogleNews-vectors-negative300. bin. gz') instead of ('GoogleNews-vectors-negative300. bin. gz'). I'm assuming this affects the autograder, so I just wanted to let you know that my Word2VecSubst class might not work as intended due to the file name error. This is completely my bad, and I'm really sorry if this adds additional work to the TAs when grading. Let me know if there's anything I should do / if you think it's best if I resubmit the same code but with the different file name. ",
        "answers": [
            "This will be fine. The TAs will be able to change the pathname during grading.  "
        ]
    },
    "163": {
        "question": "Hi, I feel not clear about the def of predicate and function in FOL models, why \"Student\" is a predicate not a function(on slide 12). In my understanding, predicate is a verb, argument represents each single word( not sure if I misunderstand these ). Thanks in advance!",
        "answers": [
            "A predicate makes an assertion Student(Mary) asserts that the individual designated by constant Mary is a student. The result is a truth value, either true or false. A function maps one constant to another one. Brother(Mary) returns another constant who is the brother of Mary. Its not asserting that Mary is a Brother. "
        ]
    },
    "164": {
        "question": "Hi. Recently when I was doing hw4 part 6 I tried to solve the question by trying out a few methods including extracting word embedding from bert and using cosine similarity. when I was trying to use pytorch to manipulate the tensors since I don't have pervious experience with large language models and pytorch I consulted chatgpt. I worked on it for hours but in the end it didn't work, I did not include part6. predict in my files because I know my method didn't work but I did included whatever I had at the last minute in the code because I was hoping to get some advice from the TAs about why it didn't work and I leaved a comment stating that my code for part 6 does not work. yesterday while I was going through my code for part 6 again I realized I included some code from chat gpt. I am not sure about the policy about chatgpt regarding this class but I think I should report this because I am not sure if this is academic dishonest.  I am sorry for any trouble I have caused you and I will definitely learn from this in the future.",
        "answers": [
            "Thanks for coming forward and letting us know. We probably wouldn't have noticed, in all honesty. We will just grade your code as submitted without any issues. Note though that the policy on the syllabus states \"AI-Tools/LLMs: The use of language model based AI to complete programming assignments is not permitted in this course, unless specified as part of an assignment. \"So if you are working on homework 5, please do not use ChatGPT. "
        ]
    },
    "165": {
        "question": "What is reentrancy in the context of AMR and where in the slides is it explained?",
        "answers": [
            "Hi there, from my understanding reentrancy means that we are reusing the variables when something is referenced again in the sentence. I find this graph helps me a lot to understand the elements in AMR:",
            "\"reentrancy\" simply refers to the idea that a vertex may have multiple incoming edges in the AMR graph. An entity may play multiple roles in relation to different predicates. "
        ]
    },
    "166": {
        "question": "I think this is a long shot, but I thought it was worth asking. I am redoing the my midterm and I noticed for question 1, I got points off for my chart, but I got the right answer. I am realizing that because of the lack of space, i could not write my full calculations, so I just wrote down the answer and not all the fractions that went into it. I did make a mistake  when starting the viterbi algorithm, I did not multiple the starting word with the corresponding pos value. This is a mistake, but I carried the mistake throughout the problem, which demonstrates an understanding of the algorithm. Looking at the rubric, I think i deserve 2 or 4 points off rather than 6 because I still got the final answer, I just made one wrong calculation that I carried throughout the problem. I know there is a lot of grading going on at the moment, and it is way past the midterm, so I understand if my request is denied. ",
        "answers": [
            "Hi, sorry for the slow response. The expectation was that you demonstrate understanding of the algorithm. Unfortunately, even though the answer is correct, your chart does not suggest that you computed the intermediate probabilities correctly (as previous_probability * transition_probability * emission_probability). A agree with the TAs' grading here. Also note that the regrade period for exam 1 has passed. "
        ]
    },
    "167": {
        "question": "Hello! I am not sure I entirely follow the logic behind wanting to keep the window size as large as possible to capture semantic relatedness (this alone makes sense since we want to have broader context for the target word) but keep the window small for semantic similarity? Is this simply because we dont want too many synonyms of hypernyms used in wrong contexts because as we broaden the context window, with more words substituting for the target word, we could have mismatches? Thank you!",
        "answers": [
            "For small context windows, the idea is that other words that appear in the same context must be substitutes, so at least they need to have the correct part of speech. By capturing the immediate context, you will also likely capture some of the tokens context words have a direct semantic relation with the target (either arguments or the predicate of the target word, for example). So other words will share the same relation. If you make the context bigger it becomes less likely for similar words to be exact substitutes. But they will still co-occur with the same content words, so they should be about the same topic and thus generally related. "
        ]
    },
    "168": {
        "question": "Hi is there a copy of the blank midterm? I want to use it as practice without the solutions",
        "answers": [
            "I don't think there is a blank copy. The files section on Canvas only contains the solution copy. "
        ]
    },
    "169": {
        "question": "Hi, I wonder how does the cross attention work and its algorithm? From my understanding the cross attention is implemented in the decoder layer as the multi-head attention to the input representation. Is that correct?",
        "answers": [
            "Yes, the only difference is between self-attention and cross-attention is that in the former all of key, query, and value vectors are from the same encoder embeddings whereas in the latter key and value from encoder embeddings while the query is constructed from decoder-side. Check this neat illustration, especially the section \"Decoder Side\"http://jalammar. github. io/illustrated-transformer/"
        ]
    },
    "170": {
        "question": "the bird ate a fly -----&gt; skwk e qta skwk etIt's unclear here whether the first skwk means bird or the second means bird. As a result, q(1|2, 5, 5) should not be zero because  the first skwk (at position 1) may correspond to bird (in the 2nd position), right? Similarly,  q(4|5, 5, 5) should not be zero because there's a possibility that in sentence 3, j =4 and i = 5. Is my analysis correct?",
        "answers": [
            "So since \"e\" must mean \"a\" and \"et\" must mean \"the\" (from example 1 and 2), in sentence 3 the nounphrase \"the bird\" must appear to the right of the verb while \"a fly\" must appear to the left of the verb. It would be very odd if the alignments were just crossing between noun phrases. "
        ]
    },
    "171": {
        "question": "In question 5 of midterm, Specify a set of concrete features and a corresponding weight vector w that will produce the correct classification for the documents in the training data. In the solution the weight vector is given as (1, 2, 3). I did not understand how it has been calculated.",
        "answers": [
            "The weights were selected to break ties between sentence 1 and 2 and sentence 2 and 3. In sentence 2, both feature 1 and 2 would be active, so by assigning a higher weight to feature 2, the model selects the correct label. Similarly in sentence 3, both feature 2 and 3 would be active, so you need to select a higher weight for feature 3 in order to predict the correct label. There are many feature vectors that satisfy these conditions, it could have been (1, 10, 100)  or (1, 3, 9) just as long as weight 2 &gt; weight 1 and weight 3 &gt; weight 2. "
        ]
    },
    "172": {
        "question": "How to represent \"am/is/are\" in AMRFor instant, \"I am good at coding\". or \"I am so tired about the final exam\". How to use AMR to represent this?",
        "answers": [
            "Hi, I believe that you don't need to ever represent \"am/is/are\" in AMR. If you think about it AMR is to just convey the meaning away from the syntactical representation. Am/is/are do not add meaning to any sentence and they are more like linking some subject to its predicate. They are also just versions of \"be\", so you can just look here in the documentation to see: https://www. isi. edu/~ulf/amr/lib/popup/be. htmlalso this video was in that documentation: https://www. youtube. com/watch? v=h66NudhsJgM&amp;ab_channel=isinlpAs for your example I am not 100% but they way I would represent the first one would be(c / code-01\n           :ARG0 (i / I)\n           :manner (g / good))\nI am not sure if the top node could be \"excel\", \"proficient\" or something like that since the main concept is the skill level. "
        ]
    },
    "173": {
        "question": "I am a little lost on 1) the importance and use of weight vectors in Linear Models where we are manually identifying features and 2) how we solve for these weight vectors. There are 2 problems I am reviewing - the ungraded and the Exam1 problem, but I can't find the approach or reasoning to solve these. Can someone please provide insight into how I go about solving this problem? Thank you.",
        "answers": [
            "1) What do you mean by importance? Without the weight vector you would not be able to predict anything. 2) There is, in general, not an analytical way to find the optimal weight vector. Instead, training is done incrementally (using perceptron training for the linear model or gradient ascent on a loglinear model). But on the specific problems we have seen, the weight vector was easy to find, mostly because there were only very few trainign examples. The trick is to look at which features will fire for each instance, and then select the weight vector so that the feature that is paired with the correct class has the highest score. I think there was an Ed post asking about this specific question a while ago (when the midterm solutions first came out), but I couldn't find it right now. "
        ]
    },
    "174": {
        "question": "Suppose in we are given a task similar to what we did in CFG Parsing, ie. finding the Non terminal for each word, but using a BERT/GPT to do so. We talk in class to decapitate the last layer of BERT/GPT, and add a classifier. 1) If we are asked about this kind of down stream task, do we need to specify the linear classifier (ie. it may be a multi-class logistic regression or a Feed forward network)? Do we need to specify the loss function or training objective of the task (ie. maximize the sum of log probability of each true Nonterminal label)? 2) Generally I remember we use CLS token or Extract token for downstream tasks. In this case, we need to classify each token's non terminal, do we need these CLS/Extract tokens to input them into linear classifier. I can think of several casesa) only input CLS/Extract Tokenb) forget about CLS/Extract, input BERT/Transformer embeddings for each wordc) Use a augmented input ie. [CLS, Vi] for each token where Vi is the embedding for each token. Which method should I select? Generally in any tasks, how should we select for (a), (b), (c)3) To do So, there are two methods, one is fixing the original BERT/GPT's parameter fixed, the other is changing BERT/GPT's parameter according to downstream task. Which should we choose if we are given these types of problem?",
        "answers": [
            "So for this kind of problem there are generally multiple approaches (all of which would give you full points on an exam). It sounds like your understanding here is sound. 1) Yes, you would have to specify in detail what the downstream classifier looks like. The question would ask for training objective and loss function specifically if that was expected. 2) It depends on the ask. For per-sentence or multi-sentence tasks, you would use the CLS or Extract token. For per-token tasks you would use the individual token embeddings. I think your approach c) is not super common -- I have seen this before, but the applications are limited. 3) It depends! On some tasks fine-tuning the model on the downstream task is necessary, but it's usually much more expensive. On other tasks, the pre-trained contextualized representations may be good enough (for example, because the task is somewhat similar to the pretraining objective). "
        ]
    },
    "175": {
        "question": "So in lecture, the model we go over is called GPT (I assume this is GPT1). Are there major training/design differences between the different GPT models (GPT1 vs GPT2 vs GPT3 vs GPT3. 5 vs GPT4), or are larger number versions typically just larger models trained on larger corpuses?",
        "answers": [
            "As far as I understand, for the most part the models are identical just with exponentially more parameters and trained on larger and more varied data sets.  GPT3 adds instruction tuning. "
        ]
    },
    "176": {
        "question": "Hi Teaching Team, By when can we expect hw3 and hw4 grades to be released? ",
        "answers": [
            "Hey! I recommend checking out #618! Hope this helps!"
        ]
    },
    "177": {
        "question": "Will there be reduced office hours after the final exam? Or will they continue as normal for students doing HW 5? Thanks!",
        "answers": [
            "They will likely be reduced, but some office hours will continue throughout reading week. "
        ]
    },
    "178": {
        "question": "Hi all, Will we take the final exam at 309 Havemyer? Thanks!",
        "answers": [
            "We will take the final at the lecture room on the day of if I recall correctly from what Professor Bauer said in class (same situation as midterm)."
        ]
    },
    "179": {
        "question": "Will homework 3 and 4 grades be released before homework 5 is due?",
        "answers": [
            "Hi! I recommend checking out #618! Hope this helps!"
        ]
    },
    "180": {
        "question": "Hello! Was wondering if participation on EdStem would suffice for a 10/10 on the participation grade. Thanks!",
        "answers": [
            "Yes, the participation grade is all-or-nothing. Participation on Ed is sufficient to get this part of the grade. "
        ]
    },
    "181": {
        "question": "While we translate AMR into sentences, could we have multiple correct sentences derived from the same ARM? Thank you.",
        "answers": [
            "Not a TA, but since the AMR tree is just a complication of lemmas, there are many different ways to interpret the sentence while preserving the semantics of the sentence. As shown in the paper \"Abstract Meaning Representation for Sembanking \":AMR aims to abstract away from syntactic idiosyncrasies. We attempt to assign the same AMR to sentences that have the same basic meaning. For example, the sentences he described her as a genius, his description of her: genius, and she was a genius, according to his description are all as- signed the same AMR. ",
            "Yes, thats a crucial idea behind AMR: many sentences can have the same annotation. AMR is abstracting from the specific surface form. "
        ]
    },
    "182": {
        "question": "For the AMR problem, are we supposed to know the two approaches discussed in class to parse (JAMR and bauers method)? I was wondering if there is more than one correct solutions. In other words, does my answer have to be identical to the solution? I get the general idea of how to parse (main verb up top, recursively find arguments or concepts to further break down) but I am confused about the step by step 'formula' or details. Just a little concerned about how to solve this type of problem on a test.",
        "answers": [
            "There is no specific algorithm for doing these translations  and since you dont know the full annotation guidelines, chances are you wont be able to get English-&gt;AMR exactly correct. Note that even trained annotators only get about a 90% agreement. For AMR-&gt; English, there are clearly many different sentences (all with the same meaning). What we are after is simply who does what to whom, when, where, why. .. .  As long as the basic predicate/argument structure (including reentrancy) reflects the sentence, your solution would count as correct. "
        ]
    },
    "183": {
        "question": "Hi there, Just wondering if these terminologies are interchangeable? Thanks ",
        "answers": [
            "For the most part. They originate from a slightly different branch of the literature, but they essentially mean the same."
        ]
    },
    "184": {
        "question": "My question is towards the Arc-standard system and Oracle algorithm. In Oracle algorithm we perform Right-Arc only if there's no relationship pointing to the right element. Is this rule giving us a unique sequence of actions? In Arc Standard rule, do we have this rule for right-arc? ",
        "answers": [
            "Im not quite sure I understand this question. In the arg standard oracle algorithm, you perform the right_arc transition if 1) there is a dependency edge between the first word on the stack and the first word on the buffer and 2) all dependents of the first word on the buffer (according to the gold tree) have already been connected in the partially derived tree up to this point. This oracle algorithm is deterministic, so it produces one specific sequence of actions. Its possible that there are multiple sequences that result in the same tree. However, the oracle algorithm will produce only one of them. "
        ]
    },
    "185": {
        "question": "Q3: Why is arg0 h/he and arg1 l/love not in the same indentation? Q4: Clarify difference and definition of arg1 and arg2? Whys is we arg1 and meet arg2? Thanks in advance!",
        "answers": [
            "Yes, I had the same question on Q3. I assumed that 'he' and 'love' would both be arguments for admire. I also was hoping to get some clarity about using pronouns as arguments. AMR is meant to omit details, but do pronouns not count as details?",
            "same here. .. some clarifications would be nice"
        ]
    },
    "186": {
        "question": "Logical form:What are unbound variables? Does vocabulary or signature always mean arity value? Machine translation:For back translation, why create synthetic back-translation from French to English instead of English to French directly? IBM: why P(f, a| e, m)? BLUE: what does it mean by recall is ignored? Using the example in class exp(log(0. 8)+log(0. 5)*(1/2)) =0. 78 and not 0. 6325",
        "answers": [
            "Logical form (note this is not part of the material for the final): What are unbound variables? Bound variables are bound to a quantifier \"forall x. student(x)-&gt;smart(x)\". Unbound variables appear without a quantifier. Vocabulary/Signature refers to the collection of all predicates. But each predicate has a specific arity (number of arguments) and that information is part of the signature. MT:The assumption is that you have more mono-lingual data for french. m is the number of tokens in the F sentence. The q parameters depend on m, so the entire distribution depends on m. BLUE: Blue score only measure n-gram precision, i. e. how many of the system output/predicted ngrams appear in any of the reference translations. It does not measure n-gram recall, i. e. how many of the ngrams in the references appear in the system output. Thanks, this may be a typo. "
        ]
    },
    "187": {
        "question": "I am having trouble getting the correction dimensions from the cross_entropy loss(input, target) function in part 2. For the input parameter, I tried to mimic the 'outputs. transpose(2, 1)' code in the given training cell for part 3, but I am not sure why this tensor must be transposed if the model output already returns the logit tensor anyway. For the target parameter, I am very confused as to what 'input_pos'  is referring to in the hint \"# complete this. Note that you still have to provide a (batch_size, input_pos)  tensor for each parameter, where batch_size =1\" . Is the 'input_pos' the index of the role our sentence is predicated on form role_ids? The pytorch documentation keeps referring to the target parameter in reference to the \"class indice\" but I was not sure what this meant in this context, or if \"input_pos\" was the class indice. ValueError: Expected input batch_size (1) to match target batch_size (2).\n",
        "answers": [
            "The input should be (1, 128) since there are 128 input positions. The output will be (1, 128, num_labels). "
        ]
    },
    "188": {
        "question": "Hello, When obtaining the different tensors from my data model and running the model on them, I kept encountering: RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)\nTo resolve this, I was able to find that I had to add these lines of code to the model definition, is this alright for me to add this to the original code (new lines in bold):class SrlModel(Module):\n    \n    def __init__(self):\n        \n        super(SrlModel, self). __init__()\n        \n        self. encoder = BertModel. from_pretrained(\"bert-base-uncased\")\n        self. device = torch. device('cuda:0' if torch. cuda. is_available() else 'cpu')\n\n        # The following two lines would freeze the BERT parameters and allow us to train the classifier by itself.\n        # We are fine-tuning the model, so you can leave this commented out!\n        # for param in self. encoder. parameters():\n        #    param. requires_grad = False\n        \n        # The linear classifier head, see model figure in the introduction. \n        self. classifier = Linear(768, len(role_to_id))\n                \n        \n    def forward(self, input_ids, attn_mask, pred_indicator):\n    \n        # This defines the flow of data through the model \n        input_ids = input_ids. to(self. device)\n        attn_mask = attn_mask. to(self. device)\n        pred_indicator = pred_indicator. to(self. device)\n        # Note the use of the \"token type ids\" which represents the segment encoding explained in the introduction. \n        # In our segment encoding, 1 indicates the predicate, and 0 indicates everything else. \n        bert_output =  self. encoder(input_ids=input_ids, attention_mask=attn_mask, token_type_ids=pred_indicator)\n\n        enc_tokens = bert_output[0] # the result of encoding the input with BERT\n        logits = self. classifier(enc_tokens) #feed into the classification layer to produce scores for each tag.\n        \n        # Note that we are only interested in the argmax for each token, so we do not have to normalize \n        # to a probability distribution using softmax. The CrossEntropyLoss loss function takes this into account.\n        # It essentially computes the softmax first and then computes the negative log-likelihood for the target classes. \n        return logits        \n",
        "answers": [
            "Thanks for the insight dude!",
            "life saver, i also find this helpful if anyone is looking for more info:https://stackoverflow. com/questions/56360644/pytorch-runtimeerror-expected-tensor-for-argument-1-indices-to-have-scalar-t"
        ]
    },
    "189": {
        "question": "Is word2vec used to create the embeddings that get fed into a transformer typically? Does it vary based on transformer architecture and downstream task?",
        "answers": [
            "Transformer models usually have a trainable embedding layer (as in the neural language model, for example), but this layer is initialized with static embedding obtained from word2vec or glove. "
        ]
    },
    "190": {
        "question": "Hi! For the single prediction result on the untrained model, after doing argmax, for some indexes, I got 0 id,  while 0 is not presented in the keys of \"id_to_role\", so I am wondering if it is normal for our model to predict 0 at this stage? Thanks!",
        "answers": [
            "Yes thats possible. ",
            "gocha"
        ]
    },
    "191": {
        "question": "Hello, was just wondering what is to be expected of the structure of the conceptual questions. Someone of Ed mentioned that they would be similar to the last question on the first exam. Is this accurate? Or would these questions be more theoretical?",
        "answers": [
            "Just saw the newly posted ungraded exercises, so that kind of answers my question",
            "Id recommend watching the lecture from this Wednesday! Professor Bauer throws out some examples of what conceptual questions could look like (I. e. using RNNs/Transformers to compute WSD the same way as the Lesk algorithm)"
        ]
    },
    "192": {
        "question": "Hi- I was wondering if it would be possible to have a solution set released for homework 4 before the final if the grades will not be released in time. Thanks!",
        "answers": [
            "Yes, I can likely get these up -- but note that there is significant variation in how things can be implemented. "
        ]
    },
    "193": {
        "question": "Hi there, In the training corpus, there is an example that contains an unknown role As    long    as    these    two    countries    stand    together    ,    then    it    seems    that    other    countries    could    not    do    anything    to    them    .\n O    O    O    B-ARG0    I-ARG0    I-ARG0    B-V    B-ARGM-REC    O    O    O    O    O    O    O    O    O    O    O    O    O    OMay I ask how to handle this one? Also, for part 1. 1, it looks like the function \"tokenize_with_labels\" is finished, do we need to add anything to the function? Do I miss anything here? Thanks! ",
        "answers": [
            "The completed tokenize_with_labels function was an oversight :) I forgot to remove it-- feel free to use my version. I filtered out roles that appeared very infrequently (iirc less than 1k occurences in the dev data). So if you find a label in the corpus that is not in the list, you can just ignore it. I removed both the B and I tags in those cases, so simply ignoring the individual tags will not lead to inconsistent annotations. "
        ]
    },
    "194": {
        "question": "Hi--I wanted to ask if the teaching staff had an approximate guideline of when HW 3 and 4 would be finished grading. I just want to figure out if we would know any more of our grades before the final and if I should start working on HW5. Thank you so much!",
        "answers": [
            "We should have homework 3 done very soon (missing for one block of grades from one TA). Homework 4 won't be done in time, unfortunately. "
        ]
    },
    "195": {
        "question": "Hello, I am currently working on HW 5, and I am trying to complete the init method for loading the dataset. Despite my best efforts, and after trying multiple different loop structures, I still have been unable to tokenize the data within a reasonable time frame. It takes about 10 minutes to tokenize 10, 000 annotations, and there are over 1, 200, 000 total annotations. I am reading in the file first, storing the tokens and bio_tags in separate lists, then after I am tokenizing the lists while adding them to the items array. This approach stores the values properly, as I tried with 40 annotations, but just seems to be taking too long. Do you have any suggestions as to how I can optimize my loop structure?    def __init__(self, filename):\n\n        super(SrlData, self). __init__()\n        \n        self. max_len = 128 # the max number of tokens inputted to the transformer. \n        \n        self. tokenizer = BertTokenizerFast. from_pretrained('bert-base-uncased', do_lower_case=True)        \n        \n        self. items = []\n        # complete this method \n        with open(filename, 'r') as file:            \n            \n            tokens=[]\n            bio_tags=[]\n            # Loop through every 4 lines\n            line_num=0\n            annotations=0\n            print(\"Start\")\n            while True:\n                line= file. readline()\n                tok=False\n                bio=False\n                # Extract the 2nd and 4th lines and store them in variables\n                if line_num % 4 ==1:\n                    tokens. append(line. replace('\\t', ' '))\n                    tok=True\n                    #print(tokens)\n                    \n                if line_num % 4 ==3:\n                    bio_tags. append(line. replace('\\t', ' '))\n                    bio=True\n                    annotations+=1\n                \n                if not line:\n                    break\n                line_num+=1\n\n            \n            for i in range (0, annotations):\n                tokenized=tokenize_with_labels(tokens[i]. split(), bio_tags[i]. split(), self. tokenizer)\n                self. items. append((tokenized[0], tokenized[1]))\n                if line_num % 10000 == 0:\n                    print(line_num)\n                \n            print(\"FININISHED\")\n\n",
        "answers": [
            "Nevermind! After sleeping on my code, I was able to optimize the loop by storing the current token, bio_tag instead of all of them!    def __init__(self, filename):\n\n        super(SrlData, self). __init__()\n        \n        self. max_len = 128 # the max number of tokens inputted to the transformer. \n        \n        self. tokenizer = BertTokenizerFast. from_pretrained('bert-base-uncased', do_lower_case=True)        \n        \n        self. items = []\n        # complete this method \n        with open(filename, 'r') as file:            \n            \n            # Loop through every 4 lines\n            line_num=0\n            annotations=0\n            print(\"Start\", \"new\")\n            current_item=[]\n            while True:\n                line= file. readline()\n                tok=False\n                bio=False\n                \n                #reset tuple every 4 lines\n                if line_num % 4 == 0 and line_num &gt; 1:\n                    tokenized=tokenize_with_labels(current_item[0]. split(), current_item[1]. split(), self. tokenizer)\n                    self. items. append((tokenized[0], tokenized[1]))\n                    current_item=[]\n                \n                # Extract the 2nd and 4th lines and store them in variables\n                if line_num % 4 ==1:\n                    current_item. append(line. replace('\\t', ' '))\n                    \n                if line_num % 4 ==3:\n                    current_item. append(line. replace('\\t', ' '))\n                    annotations+=1\n                \n                if not line:\n                    break\n                line_num+=1\n\n                if line_num % 10000 == 0:\n                    print(line_num)\n                \n            print(\"FININISHED\")\n"
        ]
    },
    "196": {
        "question": "Hi there, For part 2, by running a single input on the untrained model, I got a loss value to be 4. 164388656616211. Given the desired initial loss is 3. 970291913552122, I am wondering if my initial loss value is acceptable. Thanks!",
        "answers": [
            "That seems a bit high. Try to instantiate the model multiple times, then compute the initial loss and average. "
        ]
    },
    "197": {
        "question": "Is the following logic correct? If the input consists of 4 words and the task involves assigning BIO tags to each word indicating their semantic roles, the number of outputs generated by the model would typically be 4 times the number of possible BIO tags or semantic role categories. For instance, considering a simplified set of BIO tags:B-Agent: Beginning of an Agent role. I-Agent: Inside an Agent role. B-Patient: Beginning of a Patient role. I-Patient: Inside a Patient role. O: Outside any role. If each word in the input sequence is assigned one of these tags (B-Agent, I-Agent, B-Patient, I-Patient, or O), the number of possible outputs for each word would be 5 (including O). Therefore, for an input sequence of 4 words, the total number of outputs would be 4 multiplied by the number of possible tags for each word, which in this case would be 4 * 5 = 20 outputs. These outputs correspond to the predicted BIO tags for each word in the sequence, indicating their assigned semantic roles or lack thereof (as denoted by O). Therefore, in the following LSTM, there should be more than 4 output probabilities, correct?",
        "answers": [
            "Sorry for the slow response. I'm not sure I understand the question here. Yes, there will be number_of_possible_labels output units. So if you unroll the RNN, there would be number_of_tokens * number_of_possible_labels output units. During decoding, that isn't typically done though -- you only consider one time-step at a time/ Are you just asking why it only shows P(Arg0) for the first token, and not all of them? The figure is demonstrating the training process, so P(BArg0) is the output that matters for computing the loss on the training data. "
        ]
    },
    "198": {
        "question": "What do we get out of finding the path from argument to target here?",
        "answers": [
            "The parse tree path is a relevant feature for determining if the candidate constituent is an argument and what its label may be. "
        ]
    },
    "199": {
        "question": "https://jalammar. github. io/illustrated-gpt2/",
        "answers": [
            "thank you so much! this seems really cool"
        ]
    },
    "200": {
        "question": "Hi! I was setting up my instance and got to the last steps but I can't connect to the notebook in my browser :(The firewall rules are set per the spec, and the notebook looks like its up and running when I run:jupyter notebook &gt;&gt; jup_notebook. log 2&gt;&amp;1 &amp; tail -f jup_notebook. logBut I'm never able to connect, has anyone else had this issue?",
        "answers": [
            "Did you change the jupyter config file? "
        ]
    },
    "201": {
        "question": "Hi! i am wondering if tomorrow's lecture will be an exam 2 review session (as indicated in the syllabus)? Thanks! ",
        "answers": [
            "#609 "
        ]
    },
    "202": {
        "question": "Will the Saturday final review session have a Zoom option as well? Also, will it be recorded for us to review after the session ends? Thanks!",
        "answers": [
            "Yes, we will try to broadcast it on zoom and record. "
        ]
    },
    "203": {
        "question": "Is there a final review session today and if there is when and where is it being held?",
        "answers": [
            "There will be a review session on Saturday (tentatively at 4pm). Room TBD. We will also have a review session in class tomorrow. "
        ]
    },
    "204": {
        "question": "Hello, I am currently trying to populate the self. items array by reading in the entire tsv file into an array, and parsing every 4 lines into their tokens and bio_tags. Although this method makes sense to me logically, it is very inefficient as the file has over a million lines. Is there anything I can do to optimize this parsing? Is there a better approach I could take maybe?",
        "answers": [
            "So there is no (efficient) way around storing the data in memory at some point. Its probably better to only store the final dictionary rather than the initial lines from the file. I would read the file line-by-line rather than all at once and keep count of the lines. After every four lines, process the data you have read to create a new item. "
        ]
    },
    "205": {
        "question": "Hi, Thanks Prof. Bauer and the TA's for a wonderful semester! I was wondering if there are any books/articles on linguistics that can help us understand natural language processing better, especially for those without a linguistic background. Thank you very much!",
        "answers": [
            "Are you more interested in textbooks or in casual reading? One of the books that got me interested/started in linguistics was Stephen Pinker's \"The Language Instinct\". It's not completely up-to-date in terms of the consensus within cognitive/neuro science, but still a fascinating read. ",
            "not a TA but did my undergrad in linguistics major, I can suggest some textbooks we read for some courses in my ugrad:1. Intro to linguistics: Language Files: Materials for an Introduction to Language and Linguistics2. Intro to Phonology/Phonetics: Zsiga, Elizabeth, C. (2013) The Sounds of Language: An Introduction to Phonetics and Phonology. Malden: Wiley-Blackwell. [I actually found this book quite interesting]3. Intro to historical linguistics: Lyle Campbell, Historical Linguistics: An Introduction, 3rd ed. 4. Intro to syntax/semantics: syntax: Sportiche, Dominique, Hilda Koopman and Edward Stabler 2014. An Introduction to Syntactic Analysis and Theory. Wiley Blackwell ; semantics: Elbourne, Paul. 2011. Meaning: A Slim Guide to Semantics. Oxford University Press5. Pragmatics: https://plato. stanford. edu/entries/pragmatics/ while these maybe too specific and too academic, feel free to take a look at the linguistics branch that corresponds to your interest in nlp. Hope this helps :)"
        ]
    },
    "206": {
        "question": "Hello, I wanted to get a head start on hw 5 after finishing hw4. I noticed for part 1. 1, where we are to map the tokenized sentence to the proper BIO tags, that the code given produces the gold output shown in the prompt. Was this code left completed on purpose? def tokenize_with_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n    Word piece tokenization makes it difficult to match word labels\n    back up with individual word pieces. This function tokenizes each\n    word one at a time so that it is easier to preserve the correct\n    label for each subword. It is, of course, a bit slower in processing\n    time, but it will help our model achieve higher accuracy.\n    \"\"\"\n\n    tokenized_sentence = []\n    labels = []\n    \n    for word, label in zip(sentence, text_labels):\n\n        # Tokenize the word and count # of subwords the word is broken into\n        tokenized_word = tokenizer. tokenize(word)\n        n_subwords = len(tokenized_word)\n\n        # Add the tokenized word to the final tokenized word list\n        tokenized_sentence. extend(tokenized_word)\n\n        # Add the same label to the new list of labels `n_subwords` times\n                  \n        if label. startswith(\"B\"):\n            labels. append(label)\n            labels. extend([\"I-\"+label. split(\"-\", 1)[1]] * (n_subwords-1))\n        else: \n            labels. extend([label]* n_subwords)\n            \n\n    return tokenized_sentence, labels\n",
        "answers": [
            "Ha, yes it was supposed to be removed. Feel free to use this part, obviously. "
        ]
    },
    "207": {
        "question": "Hi Professor and TAs, When testing HW4 part 2 and part 3, I noticed that the number of attempts shown is slightly smaller than total, but this situation does not occur in part 4, 5 &amp; 6. I'm curious about what could lead to the failure of some attempts. ",
        "answers": [
            "So several people have made this observation and I havent been able to figure out why this is happening. I suspect some form of encoding issue that causes the Perl script to ignore some lines. "
        ]
    },
    "208": {
        "question": "Hello! I submitted at 12 a. m. and not 11:59 p. m. Will my score by deducted by 20%? Also, I am not really sure how it is scored, but say there is an error in my syntax that results in a compile error. How is that graded?",
        "answers": [
            "No that falls within the gray area. "
        ]
    },
    "209": {
        "question": "Hello Professor, I just wanted to give you an update on my efforts in using chat-gpt as a predictor. Once I purchased $5 worth of credits to leave the free tier, I was able to freely query chatgpt-3. 5 at any request rate I wanted. I was able to give chat a few examples in the prompt and asked for a specific format:f'get the single best SINGLE WORD subtitute for the word between ** in {context}. Be sure to maintain the meaning of the sentece with the substitute. '\n                                f'For example, given the following sentence: '\n                                f'-Anyway, my pants are getting tighter every day- '\n                                f'the goal is to propose an alternative word for tight, such that the meaning of the sentence is preserved. '\n                                f'like return \"constricting\", \"small\", or \"uncomfortable\". '\n                                f'In the sentence '\n                                f'-If your money is tight, don\\'t cut corners. - '\n                                f'the substitute small would not fit, and instead, possible substitutes include scarce, sparse, limited, constricted. '\n                                f'You will implement a number of basic approaches to this problem and compare their performance. '\n                                f'The response to my request should only be A SINGLE STRING'\n                                f' e. g. for the substitute for \"red\" return \"scarlett\" I am storing these in a single word variable, so do not include any external information like a sentence. Just the word please. '\ndespite my explicit prompt, chat still gave 3/300 synonyms using a sentence, not just the word and the performance was as follows:I found that when I manually formatted the . predict file, the performance improved slightly as well:I later tried to include some more examples to the prompt, but chat kept formatting the output as sentences, so I kept with the original prompt where only 3 synonyms were mis-formatted. I think this could be a good extension of practicing prompting to increase performance, but chatgpt-3. 5 is tough to get the properly formatted/consistent format from. I wonder how using chatgpt4 would improve the performance of this approach. Overall, definitely worth trying: very fun to see that I can send a request to chat using the openai api! ",
        "answers": [
            "Very cool, thanks :) I'm definitely planning to introduce some prompting assignment into future iterations of the course. Maybe this is one option. "
        ]
    },
    "210": {
        "question": "Hi, just want to get an idea about when will the grade of hw3 be released? thank you so much!",
        "answers": [
            "Im hoping tomorrow or Wednesday. Theres one part missing. "
        ]
    },
    "211": {
        "question": "Hi teaching staff! I implemented the part 3 algorithm the way that Dr. Bauer explained in the EdStem post about part 3, but for some reason (as you can see in the screenshot), my accuracy/precision results are extremely low. I tried debugging by implementing different data structs/print statements but nothing has been working. Is there any advice I can get on my code? If not, no worries. Thank you so much!",
        "answers": [
            "nevermind, I think I figured it out. thanks tho!"
        ]
    },
    "212": {
        "question": "For part 6, would I be allowed to call another HF models api/the openai api to test how it performs?",
        "answers": [
            "Yes. Thats a good idea. "
        ]
    },
    "213": {
        "question": "Do we need to process any tokens outputted from the BERT model like . replace(\"_\", \" \") and . lower() or can we assume the tokens outputted by BERT are singular lower-case words? ",
        "answers": [
            "Technically the tokens outputted by BERT are wordpieces. But since the output is filtered by matching against the candidates you can expect the remaining predictions to be individual lower-case words. "
        ]
    },
    "214": {
        "question": "I call part 2, 3, 4, 5 in part 6 and it takes a really long time for my code to run (over an hour). Is that okay, or do I need to come up with a part six that has a quicker runtime to get the points?  Thanks!",
        "answers": [
            "I had this problem too! I just decided to modify my code in a slightly different way because I didn't want to risk it :)",
            "Not a TA, but according to the assignment, \"Any small improvement (or conceptually interesting approach) will do to get credit for part 6. \" So I think you're totally fine even if the \"performance, \" however this is being defined, isn't better. See #594.",
            "A little late, but one thing that could cause this is if your code for part 6 (or main) were creating the model multiple times (if you're using BERT or Word2Vec as base). "
        ]
    },
    "215": {
        "question": "I got the warning like this:/Users/. .. /venv-metal/lib/python3. 9/site-packages/urllib3/__init__. py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1. 1. 1+, currently the 'ssl' module is compiled with 'LibreSSL 2. 8. 3'. See: https://github. com/urllib3/urllib3/issues/3020 warnings. warn(when I tries to run the code python lexsub_main. py lexsub_trial. xml  &gt; smurf. predict. Would this affect my implementation or setup? How to fix it?",
        "answers": [
            "Um since its just a warning I dont see why it would affect the output. But Im confused why urllib would be needed for anything. .."
        ]
    },
    "216": {
        "question": "would the result like this:Total = 298, attempted = 298precision = 0. 092, recall = 0. 092Total with mode 206 attempted 206precision = 0. 131, recall = 0. 131good enough for this part?",
        "answers": [
            "someone posted these exact stats in a previous post and Dr. Bauer approved it, I think you're all good."
        ]
    },
    "217": {
        "question": "Are these acceptable outputs? Did you guys get similar?",
        "answers": [
            "See #585. They have similar scores, and Professor Bauer said they look normal. "
        ]
    },
    "218": {
        "question": "Hi everybody! I was wondering if it is okay if our part 6 score is significantly worse than some of the other predictors as long as it is conceptually interesting/unique. Thanks for the help!",
        "answers": [
            "Hi David! I'm not super sure, but the assignment says, \"Any small improvement (or conceptually interesting approach) will do to get credit for part 6\". I assumed we could have a good approach that performs only as well as one of the other methods. Hope this helps!",
            "As long as the approach is interesting/different from the other parts, its okay if it doesnt outperform the other approaches. "
        ]
    },
    "219": {
        "question": "Does the Lesk algorithm consider punctuation such as \"? \" and \", \" as tokens when calculating overlap?",
        "answers": [
            "No punctuation should be ignored for part 3.  "
        ]
    },
    "220": {
        "question": "Hi-- I was wondering how we are defining precision/recall in this assignment given we are dealing with non-binary outputs. Should these values always be the same in the context of this assignment?",
        "answers": []
    },
    "221": {
        "question": "Hi - I am wondering whether the ARM exercises solution has been posted because I was not able to find it. Tysm!",
        "answers": [
            "Posted in Pages now."
        ]
    },
    "222": {
        "question": "Are we allowed to import collections?",
        "answers": [
            "Yes according to https://edstem. org/us/courses/46417/discussion/3974511",
            "Thats fine. "
        ]
    },
    "223": {
        "question": "hi i hope you are great.  I have an idea for part 6 that involves importing a new library in hw.  is that okay? Thank you!",
        "answers": [
            "Yes for part 6 this is fine. Make sure you list any necessary dependencies in a comment. "
        ]
    },
    "224": {
        "question": "Do we have to write explanation for the part 6 of HW 4 or is it sufficient to just include the code.",
        "answers": [
            "An explanation in a comment would be useful for the graders. "
        ]
    },
    "225": {
        "question": "Sorry if this was announced already, but I was wondering in there would be a weighting on the final between material covered before and after exam 1?",
        "answers": [
            "It will be approximately 60% new material, 40% old material. "
        ]
    },
    "226": {
        "question": "For some reason my part3 result # Total = 298, attempted = 298\r\n# precision = 0. 112, recall = 0. 112\r\n# Total with mode 206 attempted 206\r\n# precision = 0. 155, recall = 0. 155\r\nis better than part2 result# Total = 298, attempted = 298\n# precision = 0. 074, recall = 0. 074\n# Total with mode 206 attempted 206\n# precision = 0. 107, recall = 0. 107Does this result looks normal?",
        "answers": [
            "Yes, that looks normal. "
        ]
    },
    "227": {
        "question": "Hi, I am getting the following scores for part 2, but they are quite low compared to the other ones I have seen in Ed. Is this within the reasonable range? This is how I am implementing it. Is there something I'm overlooking? Thanks in advance!",
        "answers": [
            "You need to use the . count() method on the lexemes to obtain the frequency. "
        ]
    },
    "228": {
        "question": "Where should we download rolelist. txt? After unzipping ontonotes_srl. zip, I only got propbank_dev. tsv  propbank_test. tsv  propbank_train. tsv",
        "answers": [
            "I think you are the first person working on this. :) I added the rolelist. txt file to the zip file. Please download it again. "
        ]
    },
    "229": {
        "question": "Hi, I just want to confirm my understanding of calculating overlap in simple lesk is correct:1. Suppose a word appears in the definition of synset m times and in the context of the target word n times. Then when calculating the overlap, we count this as 1 overlap, instead of min(m, n)? 2. We need to discard the occurrences of the target word in the definition/contextWith the above understanding (and the score based approach in #477) I get the following result for part 3:which is significantly better than the result for part 2:Is this expected behavior, or did I do something wrong?",
        "answers": [
            "Yes, both of these points are correct. Point 1 is a bit up to interpretation, but in my implementation I simply used set intersection, so duplicate words were discarded. The results look good tome. "
        ]
    },
    "230": {
        "question": "Hi, I got exactly the same score for part 4 and part 5 . Is that an issue or Ok? \"(base) qmn@qmndeMBP hw4_files % perl score. pl part5. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170(base) qmn@qmndeMBP hw4_files % perl score. pl part4. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170'Also, Am I run part 5 correctly? ",
        "answers": [
            "I got the same result, see #520 ",
            "Same to you. PART4:\nTotal = 298, attempted = 298\nprecision = 0. 115, recall = 0. 115\nTotal with mode 206 attempted 206\nprecision = 0. 170, recall = 0. 170\n\nPART5:\nTotal = 298, attempted = 298\nprecision = 0. 115, recall = 0. 115\nTotal with mode 206 attempted 206\nprecision = 0. 170, recall = 0. 170\n"
        ]
    },
    "231": {
        "question": "Hello, I have written my code for part 3, and I followed the logic as recommended in the hw and EdStem posts, but I am getting a low performance as shown below:Any tips on how I can improve this performance? I checked, and my tokenization is correct. I dont include the target word in the full context to make it easier to parse later too.",
        "answers": [
            "Hi, this might not fix everything but I noticed \"use_up\" in your prediction - make sure to replace all \"_\" in the predictions with a space \" \". You can do this by using the replace() function, for example: word. replace(\"_\", \" \")",
            "Im getting similarly low numbers for this part. Im combining left/right context, tokenizing the context, getting synsets for the target word with the appropriate POS. Then I strip stop words from the context, iterate through the synsets, obtain definition, examples, hypernyms for each, tokenize the definition, remove stop words from definition and each example.  Next, I look for overlap in the definition, incrementing counter each time a match is found. Then I check for overlap in each example (tokenizing each example first), obtain examples and definition for each hypernym, remove stop words and tokenize those, iterate through them and again search for overlap. This in the end gives me a dict of synsets with total overlap counts for definition, examples, hypernym examples and definition. If the dict is empty (no overlap), I iterate through the synsets, then through the lemmas in each synset and store the lemma count for each synset in another dict. I take the max from this dict to get the most frequent synset, then iterate through the lemmas in this synset and store their counts. The lemma with the maximum count I take as the synonym. If there is a tie, I do the same thing, only checking the synsets that have the highest (tied) score, going through the lemma counts for the most frequent synset, choosing the max as the synonym. I think there may be a problem with how Im handling no overlap/ties, but Im not sure. I didnt try to implement the professors suggested algorithm. Question:  If the scores for this part are low/incorrect, will we be graded on our code/algorithm process and be given partial credit?"
        ]
    },
    "232": {
        "question": "I have seen several posts about \"Illegal division by zero at score. pl line 109\" on Windows but I haven't seen many solutions. I tried to use the lines written in #476 but I still get the same error when I run perl score. pl wn_frequency_predictor gold. trial. I would appreciate any guidance from people who have been able to solve this issue!",
        "answers": [
            "I got this error when the file name was incorrect.  Did you mean wn_frequency_predictor. predict?",
            "I think first check your file name and whether you change the main when you test for each part. And did you do \"python lexsub_main. py lexsub_trial. xml &gt; wn_frequency_predictor. predict \"  before you run score. pl? (I had the same error and it turns out to be this issue. )"
        ]
    },
    "233": {
        "question": "In Part 3, we need to find the synset with the greatest overlap between the context and the definition. When we get this synset, how should I decide which word to pull out? Because a synset is a collection of lemmas, most of these synsets will have numerous words. Am I supposed to use the p2 results to return the most common lemma in that synset? Thank you!",
        "answers": [
            "Yes you would then choose the lemma with the highest . count() value. Take a look at #477 for an optional alternative approach to selecting a lemma and breaking ties. "
        ]
    },
    "234": {
        "question": "Hi Prof. Bauer and TAs, When getting quota for GPU, I am facing 2 problems:There is no link that takes me to the GPU Quota page when the initial VM creation failed. The \"Learn More\" takes me to a documentation page, not a place to request for GPU quota. I am able to find the page to request GPU quota but no quota can be selected (but can be seen)Any suggestions on how to set up the VM instance?",
        "answers": [
            "So that error message indicates that no GPUs are available in your availability zone. Please try a different one. One the Quota page, filter for the \"NVIDIA T4 GPUs\" quota. You should see a list of all availability zones and a Limit of 0 on each. Select the zone you need (I had luck with us-east4). Then select the checkbox and click \"Edit Quotas\" on top of the page. "
        ]
    },
    "235": {
        "question": "hi! i am running into a really weird problem when trying to download add-on packages. .. the window that pops up looks like this lol",
        "answers": [
            "nevermind i think it's because i set up my virtual environment wrong again whoops"
        ]
    },
    "236": {
        "question": "Hey Staff, I don't think I am understanding what I am supposed to do in part 5:The instructions are:Obtain a set of candidate synonyms (for example, by calling get_candidates). Convert the information in context into a suitable masked input representation for the DistilBERT model (see example above). Make sure you store the index of the masked target word (the position of the [MASK] token). Run the DistilBERT model on the input representation. Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target position (i. e. the position of the masked word). Return this word. So firstly, call get_candidates and store it in a list. Secondly, Take the context, find the index with the context word and swap the word with [mask]. Thirdly, run the model on the context and get back the 10 best words based on the model. Lastly, -here is where I don't fully understand- from the set of candidate synonyms in part 1 choose the highest scoring word for the masked word. But then where does the BERT model come in? Here is my code:def predict(self, context : Context) -&gt; str:\n\n    # 1. TODO Obtain a set of candidate synonyms (for example, by calling get_candidates).    cs_list = set(get_candidates(context. lemma, context. pos))\n\n    # 2. TODO Convert the information in context into a suitable masked input representation for the DistilBERT model (see example above).    #. .. TODO Make sure you store the index of the masked target word (the position of the [MASK] token).    index = 0\n    context_list = context\n    for i in range(0, len(context_list)):\n        if context_list[i] == context. lemma:\n            index = i\n            context_list[i] = '[MASK]'\n\n    input_toks = self. tokenizer. encode(context_list)\n    input_mat = np. array(input_toks). reshape((1, -1))\n\n    # 3. TODO Run the DistilBERT model on the input representation.    outputs = self. model. predict(input_mat)\n    predictions = outputs[0]\n    best_words = np. argsort(predictions[0][5])[::-1]  # Sort in increasing order\n    best_tokens = self. tokenizer. convert_ids_to_tokens(best_words[:10])\n\n    # 4. TODO Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target    #. .. TODO position (i. e. the position of the masked word). Return this word. What exactly am I supposed to do in the last part? Thank you!",
        "answers": [
            "I would iterate through the predicted tokens suggested by BERT (in decreasing order of their probability). For each, you check if it appears in the candidate list. If it does, return the candidate. If not continue to the next predicted token. "
        ]
    },
    "237": {
        "question": "Hi, I noticed another Ed Post that indicated Part 5 should not take a long time. However, it seems to be taking at least 5 to 6 minutes for me, and I'm not sure if this is normal? Thanks so much! ",
        "answers": [
            "That seems fine to me. There may be a bottle-neck in how you sort the predictions. "
        ]
    },
    "238": {
        "question": "When I try to test my genism download, I had the same issue as #502, so I edit my embeddings import statement to not include the . gz extension. When I do that, I don't get an error, but when I try to run v1 = model. wv['computer'], I get the below errorAttributeError: 'KeyedVectors' object has no attribute 'wv'Do you know why this could be happening?",
        "answers": [
            "I saw that the workaround for this is just below on the instructions, please disregard this"
        ]
    },
    "239": {
        "question": "Dear Teaching Team, Just out of curiosity, when we it's said that HW 5 \"requires a GPU with sufficient memory\", what magnitude are talking about? Is it completely unfeasible to run this sort of projects on a personal computer? I'm curious because I want to try to make some projects over winter break. Thank you!",
        "answers": [
            "You will need about 16GB of GPU memory to fine-tune BERT.  "
        ]
    },
    "240": {
        "question": "In part2, if there is a tie, I just choose the word appears first (I made a dictionary to record all lemmas in all sysnets and call item() ). I got 0. 074 and 0. 107. If I choose the word appears last, I got 0. 035 and 0. 01. Why it differs so much? Is this score acceptable?",
        "answers": [
            "I'm actually surprised it makes such a huge difference. But then again, there aren't that many test instances. "
        ]
    },
    "241": {
        "question": "Hi, Are we allowed to import extra packages? I imported string, re, and defaultdict",
        "answers": [
            "Yes, as long as they are part of the python standard library. "
        ]
    },
    "242": {
        "question": "Hello TAs and Professor, The score I am getting for my Lesk predictor is lower than expected:I have updated my code using the Professor's tie-breaking method but I am still dealing with many &lt;synset, word&gt; ties. Is there anything I am doing incorrectly in the following? def wn_simple_lesk_predictor(context : Context) -&gt; str:\n    target = context. lemma\n    overlap, lemma_freq = defaultdict(int), defaultdict(int)\n    stop_words = set(stopwords. words('english'))\n    context_words = [word. lower() for word in (context. left_context + context. right_context) if word. lower() not in stop_words]\n\n    for synset in wn. synsets(target, pos=context. pos):\n        syn_def = set(tokenize(synset. definition(). lower()))\n\n        for example in synset. examples():\n            syn_def. update(tokenize(example. lower()))\n\n        for hypernym in synset. hypernyms():\n            syn_def. update(tokenize(hypernym. definition(). lower()))\n            for hypernym_example in hypernym. examples():\n                syn_def. update(tokenize(hypernym_example. lower()))\n\n        overlap[synset] += len(syn_def. intersection(context_words))\n        \n        for lemma in synset. lemmas():\n            lemma_freq[(synset, lemma. name())] += 1\n\n    score = {}\n    for pair, freq in lemma_freq. items():\n        synset, word = pair\n        if word ! = target:\n            score[pair] = 1000*overlap[synset] + 100*lemma_freq. get((synset, target), 0) + freq\n\n    return max(score, key=score. get)[1]Thank you!",
        "answers": []
    },
    "243": {
        "question": "In part5, do we need to consider the case where none of the prediction word is in the candidates(getting from part1)? Thanks!",
        "answers": [
            "Yes, I don't think this happens with this data, but it's a good idea to have a way of handling it. Maybe simply select the most likely BERT prediction :)? "
        ]
    },
    "244": {
        "question": "Hi, I'm getting \"Total = 298, attempted = 298precision = 0. 111, recall = 0. 111Total with mode 206 attempted 206precision = 0. 150, recall = 0. 150\"for part 3, which I believe is so much higher because simple lesk algorithm should not outperform the WordNet frequency baseline. . Is this expected? I post the output file just for reference.",
        "answers": [
            "Professor Bauer responded to another student about part 3 outperforming part 2! #510 #585 "
        ]
    },
    "245": {
        "question": "Hi, When I run python lexsub_main. py lexsub_trial. xml  &gt; part5. predict i get this message printed over and over below. However, my code doesn't error out (is this just a warning? )Then when I do perl score. pl part5. predict gold. trialI get results that are similar to what other students are getting for part 5.",
        "answers": [
            "This is just a warning that you can safely ignore. "
        ]
    },
    "246": {
        "question": "I am little confused that, set and list, which data type we should use for calculate overlap score. For example, the context is [\"insitution\"] and gloss of a \"bank\" sense is depository financial institution, bank, banking company\n\na financial institution that accepts deposits and channels the money\nThere are two \"institution\" in the gloss. In this case, the overlap score is 1 or 2?",
        "answers": [
            "I recommend just using a set. The overlap score would then be 1. You could make an argument that counting duplicates accounts for some words being more important, but I think you would have to control for the varying lengths of the glosses. "
        ]
    },
    "247": {
        "question": "Hi, For my part 5 output, it prints the process to the . predict file. Do I need to remove it from the . predict file? Thank you",
        "answers": [
            "For me, adding 'verbose=0' inside model. predict() helped remove these extra lines from the . predict file. So, the line of code would look something like:outputs = self. model. predict(input_mat, verbose=0)"
        ]
    },
    "248": {
        "question": "Hello! At the moment for HW 4 part 4, I'm getting a prediction and accuracy of 0. 102. Does anyone have any suggestions on slightly increasing these values? Thank you!",
        "answers": [
            "#495"
        ]
    },
    "249": {
        "question": "Hello, I wanted to sanity check some behavior I noticed. Should the tokenizer. encode add unnecessary #'s and split the sentence somewhat incorrectly in some cases such as the below? The third line starting with ['[CLS]', .. .. ] is what I get when I print the result of calling tokens. convert_ids_tokens on tokenizer. encode(first sentence shown below). I believe the extraneous #'s might be affecting how my model performs but I'm not sure if it's expected.",
        "answers": [
            "This is the subword wordpiece tokenization. Bert is trained on this type of tokenization, so the input is necessary for the encoder to work properly. "
        ]
    },
    "250": {
        "question": "Hi, I am working on part 3 right now and I have a question. I went into office hours early which helped me get started and get a basic idea of what we are doing for this part, but I'm bit confused. I've pasted my code below. For the code under my part B comment, shouldn't it go under the \"for lemma in lemmas\" loop from part A? Or am I understanding something wrong. Also for the tie breaker stuff that was mentioned in #477, where would this fit into my code? Like should I be handling the tie breaker stuff after my part C? Thanks. def wn_simple_lesk_predictor(context : Context) -&gt; str:\n    stop_words = stopwords. words('english')\n    # For part a, tokenize the context and finding the overlap between the synset + synset hypernyms definitions\n    # and examples\n    tokens = set()\n    context_lr = context. left_context + context. right_context\n    context_string = \" \". join(context_lr)\n    for token in tokenize(context_string):\n        if token not in stop_words:\n            tokens. add(token)\n    lemmas = wn. lemmas(context. lemma, context. pos)\n    for lemma in lemmas:\n        syn = lemma. synset()\n        definition = syn. definition() + \" \" + \" \". join(syn. examples())\n        for hypernym in syn. hypernyms():\n            definition += hypernym. definition() + \" \" + \" \". join(syn. examples())\n        definition_set = set(word. lower() for word in tokenize(definition) if word not in stop_words and word. isalpha())\n        overlap = tokens. intersection(definition_set)\n\n    # For part b, loop through the synset get the count go synset. lemma. count\n    for synset in overlap:\n        for lemmas in synset. lemma:\n\n\n\n    # Part c loop through synset. lemma, get l. count for l in synset. lemma\n    for l in synset. lemma:\n        count = 0\n\n\n    return None #replace for part 3",
        "answers": [
            "Im not sure what the loop under your part b comment I supposed to do. Once you have the overlap (in the for lemma in lemmas loop) you can compute the score for the current lexeme (the lemma variable) as described in #477. Then store the lemma and the score in a dictionary. Finally (after the main for loop) select the candidate with the max score. "
        ]
    },
    "251": {
        "question": "Hi I hope you are great. I have beeen testing my files non stop for days. I'm not sure what i did but i just got the errorpython: can't open file 'perl': [Errno 2] No such file or directorythe perl command has been working for days soi am not sure what is going onthank you",
        "answers": [
            "I suspect you are trying to run Perl from inside the Python interpreter. "
        ]
    },
    "252": {
        "question": "Hi, What is the syllabus for the final exam? Is the Quiz - 1 syllabus also included?",
        "answers": []
    },
    "253": {
        "question": "Hi, Is it possible to get some info on the contents of the finals and some prep guidance Thanks",
        "answers": []
    },
    "254": {
        "question": "Hi Professor and TAs, I tested my algorithm for pt 5 with Professor's test codes (in main). However, there's always an extra word at the end of my outputs (for example, it's supposed to be['tight', 'close', 'loose', 'deep', 'big', 'tighter', 'tied', 'firm', 'hard']\ninstead of['tight', 'close', 'loose', 'deep', 'big', 'tighter', 'tied', 'firm', 'hard', 'down']\nI am also confused about why I have to manually add the add_special_tokens = True in my test code input_toks = tokenizer. encode(\"If your money is tight, don't cut corners\", add_special_tokens=True)\nTo make it print the above outputs, if I don't add it, then the output is [', ', '. ', '-', ';', 'and', '/', '. .. ', '! ', ':', 'to']. Any hint or direction would be greatly appreciated, thank you in advance.",
        "answers": [
            "Hm, my example code suggests that there should be 10 tokens predicted, not 9 (as shown in the example). So your output looks good. I think with the special tokens here is what's going on: With special tokens, the tokenizer will automatically insert [CLS] and [SEP]. Then the the target word \"tight\" would be at index 5. Without special tokens it would be at index 4, and then the \", \" would be at index 5. Daniel "
        ]
    },
    "255": {
        "question": "Hi, Total = 298, attempted = 298precision = 0. 114, recall = 0. 114Total with mode 206 attempted 206precision = 0. 155, recall = 0. 155Is this output reasonable for part 4 Bert Model? Thank you!",
        "answers": []
    },
    "256": {
        "question": "Hi, I know other students also asked about this but I still struggle with it. I got \"(base) . .. .. .. hw4_files % perl score. pl smurf. predict gold. trialIllegal division by zero at score. pl line 109. \"even when trying with smurf. predict. The main method now is BTW I'm using Mac. I also tried import io but it still not workingThanks a lot!",
        "answers": [
            "hi did you do the piping step before creating smurf. predict, i think that might be the problem"
        ]
    },
    "257": {
        "question": "Hi, Total = 298, attempted = 298precision = 0. 083, recall = 0. 083Total with mode 206 attempted 206precision = 0. 121, recall = 0. 121Is this a reasonable output for part 3? Thank you!",
        "answers": []
    },
    "258": {
        "question": "For HW 4, is our goal to have the scores be as close to the expected values or do we want to have the scores for each part to be as high as possible on each individual task? I believe my scores have typically been around the estimates given in the HW4 documentation, but just want to understand whether we should prioritize increasing the score for each part or getting the \"right\" score",
        "answers": [
            "My advise is to not obsess about the score  -- use the provided scores as a reference to check if your code is working correctly in general. The evaluation for hw4 is relatively objective because we are using the \"official\" evaluation script provided for the lexical substitution task. So you can trust that better results are better :)"
        ]
    },
    "259": {
        "question": "Hi! I am not sure why my score. pl examines 300 lines while the previous postings examined 298 lines. The gold. trial has 300 lines, so I am quite confused. The number of modes differs (208 vs. 206), too. Here is the screenshot.",
        "answers": []
    },
    "260": {
        "question": "Hi, I just want to confirm that my results for HW-4 part 5 and 6 look reasonable.",
        "answers": []
    },
    "261": {
        "question": "Hi, I just want to confirm that my results for hw 4 part 2, 3 and 4 look reasonable.",
        "answers": []
    },
    "262": {
        "question": "Hi, I ran the evaluation code on the part5. predict file. I got the following error.",
        "answers": [
            "Please take a look at the actual output file and see if anything looks off. If you are running this on windows, you may be encountering an issue with the line endings. #476 "
        ]
    },
    "263": {
        "question": "As we talked about in class, GPT only has transformer decoder layers. How should we use GPT model for other classification test?  I see that slide Representing tasks as GPT input in lecture 14 and it shows it's the Transformer that used in other tasks. However, GPT and transformer are not the same thing, right? I'm just getting confused on this.",
        "answers": []
    },
    "264": {
        "question": "If we create a conceptually different approach for Part 6, however, the performance is worse, should we include part 5 or Part 6 in the main part of our final submission? Thank you so much! !!",
        "answers": [
            "It doesn't really matter. Let's say you should run the part that gives you the best performance -- in your case part 5. "
        ]
    },
    "265": {
        "question": "Hi, This is my output when I run part 5. It prints some \"error in line\" from lines 1 - 600 before printing the scores. However, my precision seems to be slightly better than the word2vec approach as mentioned in the assignment. Can you please confirm if I should be worried about this \"error in line\" part or is it fine as long as my precision is reasonable. Thank you!",
        "answers": [
            "Same problem here. Did you solve it?",
            "Hello! Looks like this can be prevented using what someone else mentioned above, but I'd also point out that it looks like these extra lines don't effect the actual effectiveness of the scoring script"
        ]
    },
    "266": {
        "question": "Hi I hope you are great. For this part of part 5:Obtain a set of candidate synonyms (for example, by calling get_candidates). can i just copy over the logic, or do i have to actaully call the get_candidates function.  Thank you!",
        "answers": [
            "It's actually easier to copy the logic because you can then score the candidates on-line instead of having to store them all in a list. "
        ]
    },
    "267": {
        "question": "Hello! I have been trying to \"pip install gensim\" into my Python environment, but I keep getting this error. Does anyone happen to have encountered this/resolved it? Thank you so much for any help!",
        "answers": [
            "Can you copy/paste the entire output? It looks cut-off to the right and top."
        ]
    },
    "268": {
        "question": "Hi I am hope you are doing great. I think in part3 I am still quite confused about the association between all of these:wn. lemmas('break', pos='n') # Retrieve all lexemes for the noun 'break'\n[Lemma('interruption. n. 02. break'), Lemma('break. n. 02. break'), Lemma('fault. n. 04. break'), Lemma('rupture. n. 02. break'), Lemma('respite. n. 02. break'), Lemma('breakage. n. 03. break'), Lemma('pause. n. 01. break'), Lemma('fracture. n. 01. break'), Lemma('break. n. 09. break'), Lemma('break. n. 10. break'), Lemma('break. n. 11. break'), Lemma('break. n. 12. break'), Lemma('break. n. 13. break'), Lemma('break. n. 14. break'), Lemma('open_frame. n. 01. break'), Lemma('break. n. 16. break')]How should I interpret this return. l1 = wn. lemmas('break', pos='n')[0]\n&gt;&gt;&gt; s1 = l1. synset() # get the synset for the first lexeme\n&gt;&gt;&gt; s1Synset('interruption. n. 02')\n&gt;&gt;&gt; s1. lemmas() # Get all lexemes in that synset\n[Lemma('interruption. n. 02. interruption'), Lemma('interruption. n. 02. break')]\n&gt;&gt;&gt; s1. lemmas()[0]. name() # Get the word of the first lexeme\n'interruption'In my code I used synset_list = wn. synsets(context. lemma, context. pos)to get a list of sysnets.  And from there I looped through this listfor syn in synset_list:   #wn. lemmas() changed\n        overlap_set = set()\n        #overlap = 0\n        s1 = syn #syn. synset() chnaged\n        count = 0and I calculated overlap as :for word in tokenize(s1. definition()):\n            if word in final_context_words_list:\n                overlap_set. add(word)\n\n        for hyper in s1. hypernyms():  #changed: 1. sysnet(). hypernyms() hypernyms()\n            if hyper. name() in final_context_words_list:  # added the . names\n                overlap_set. add(hyper. name())\n\n            for example in hyper. examples():\n                if example in final_context_words_list:\n                    overlap_set. add(example)\n\n        overlap = len(overlap_set)I am not sure if Iam thinking about all these associations correctly.  Does this seem like the right steps? In talking about the tie breaking algo Dr. Bauer gave me this guidance:t stands for the target word that you get from the context object. &lt;s, t&gt; is the lexeme that the target word t forms with each synset. I when you look up the possible synsets for t, I recommend using the wn. lemmas() function (see example in the setting up wordnet section). This way this know what the target lexemes are right away. isn't t a target word?  if I do something like wn. lemmas()ex. wn. lemmas('break', pos='n')how would I now what the arguments to pass in are, and wouldn't it return a list? Sorry for all the questions and thank you so much for all your help ",
        "answers": []
    },
    "269": {
        "question": "Hi, Was wondering how grading would work for HW4. All of my functions work/compile and get close to 0. 1. How will we be deducted points? Is it based on distance to expected values? Also for part6, the HW said we could focus on a conceptual approach -- I focused on something that could be interesting but it did not really improve, is that fine? Thanks!",
        "answers": []
    },
    "270": {
        "question": "Hi I hope you are great. I was wondering for the part 5 example, to go through the example, do I just create a random . py file and run the file? thank you!",
        "answers": [
            "Correct me if I'm misunderstanding your question, to do the introductory lines of code that introduce questions 5 you should be able to use your terminal (at least for mac). If you open a terminal and type python then hit enter, you should be able to type in the lines and get the expected returns. Anything with &gt;&gt;&gt; is something you type in, the next line is the return you should get in the terminal, if any. This would be the easier option, but yes you could also creat a . pay and run the file in your ide."
        ]
    },
    "271": {
        "question": "Hi  I hope you are doing great.  My part 3 results are above.  I am not sure how to improve this.  Is there anything you can suggest?  Thank you!",
        "answers": [
            "I would double check that you are tokenizing the text groupings properly for the intersection check. I think it was left+right context vs definition+examples+hypernyms examples+hypernyms definitions"
        ]
    },
    "272": {
        "question": "Does my following scores for part 3&amp;4&amp;5 looks reasonable? My Bert score is much lower than the word2vec model. So I am not sure is something that I did wrong. Thank you! perl score. pl wn_simple_lesk. predict gold. trialTotal = 298, attempted = 298 precision = 0. 039, recall = 0. 039 Total with mode 206 attempted 206 precision = 0. 068, recall = 0. 068perl score. pl word2vec. predict gold. trialTotal = 298, attempted = 298precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206precision = 0. 170, recall = 0. 170perl score. pl bert. predict gold. trialTotal = 298, attempted = 298precision = 0. 098, recall = 0. 098Total with mode 206 attempted 206precision = 0. 126, recall = 0. 126Then as a result, I tried to use Bert Large for my part 6, which also have a lower score. Is this fine? Do we need to actually have a model 6 that has the highest score over all models? perl score. pl bertlarge. predict gold. trialTotal = 298, attempted = 298precision = 0. 099, recall = 0. 099Total with mode 206 attempted 206precision = 0. 146, recall = 0. 146",
        "answers": [
            "My word2vec scores were better than my bert scores too. So for part 6 I decided to work off the word2vec method and make some small improvements to push the score up. I would imagine they expect your part 6 to be able to at least marginally beat your part 3, 4, and 5 scores since you have the flexibility to mix and match the techniques.",
            "Even my Word2Vec scores were better than the BERT scores. Infact, the Word2Vec scores were higher than the scores produced by any of the other three techniques. ",
            "Just adding on to the others aboveI had the same word2vec performance, and Bert exactly tied that. The simple lesk performance you have looks a bit low though, I'd make sure you try out Dr. Bauer's recommendation about breaking ties in #477 "
        ]
    },
    "273": {
        "question": "Hi,  I hope you are doing great. I accidentally exported and converted my main file to a txt.  I brought it back to the correct folder and changed the extension back to . py. It seems to be functioing normally, but when I look at the file in the hw4 folder it no longer shows, the . py extension, but it shows the exstension in my ide.  Is this okay?  Thank you!",
        "answers": [
            "The file you submit on Courseworks should have a . py extension. "
        ]
    },
    "274": {
        "question": "Hi, when we are returning values in Q3 predictions, should they be strings or synsets? for example should I return the string \"streak\" or \"Synset('streak. n. 01')\"? Here is an example of it returning the string \"go\" and the synset \"streak. n. 01\".",
        "answers": [
            "I believe the return should just be a string, e. g. \"streak\". so when you check your . predict file it will look like:run. v 300 :: streak"
        ]
    },
    "275": {
        "question": "Hello! I hope you are doing well. I'm currently trying to work on the assignment, but I'm having issues with my tensorflow. I'm getting this reccuring message, and I'm not sure what commands I have to run to get my tensorflow working on my Mac. Any feedback would be greatly appreciated!",
        "answers": [
            "It looks like you are using the tensorflow-metal accelerated version. For this assignment you should be good with the CPU version. $ pip uninstall tensorflow-metal\n$ pip install tensorflow\n"
        ]
    },
    "276": {
        "question": "Hello Professor, Is it possible for Part 4 and Part 5 to have the same score, even if their predictions are not the same? Thank you for your help.",
        "answers": [
            "I got the same result, see #520 "
        ]
    },
    "277": {
        "question": "Dear Prof. and TAs, Im wondering whether we can get sample paper of previous final exam or not? And Im curious about the question patterns, will it be the same as the midterm exam? Thanks!",
        "answers": [
            "Unfortunately I don't have a large enough trove of exam problems to be able to release a sample exam. I will try to share some individual example problems (as ungraded exercises)."
        ]
    },
    "278": {
        "question": "Hi, I just want to make sure whether my results for part 2 are okay, it says we get nearly 10 percent for recall and precision.",
        "answers": []
    },
    "279": {
        "question": "I have completed my assignment 4 on Windows pc and my perl score. pl script outputs precision results that I am satisfied with. However, when I try to run the same script on my mac book the script is not working. I am aware that this difference is due to Unix file endings as explained in one of the previous posts. What machine will be grading our . predict files? Can I submit my . predict files as output by my Windows pc?",
        "answers": [
            "Yes that should be okay -- worst case we can convert the file endings. "
        ]
    },
    "280": {
        "question": "Hello, I was wondering if for part 4 these results were too high. Thanks! perl score. pl part4. predict gold. trialTotal = 298, attempted = 298precision = 0. 136, recall = 0. 136Total with mode 206 attempted 206precision = 0. 218, recall = 0. 218",
        "answers": [
            "Hu, the results with mode are really quite good. Unless you somehow modified the evaluation script, it's difficult to \"cheat\" on this problem, so I would trust the good results. "
        ]
    },
    "281": {
        "question": "Hello Professor and TA, Is partx. predict or precision results more important? Are there any expectations for the results of prediction for each part? Thank you for your help.",
        "answers": []
    },
    "282": {
        "question": "Hello Professor and TA, It this score good for part 2 and part 3? ",
        "answers": [
            "These look reasonable. "
        ]
    },
    "283": {
        "question": "In regards to #476, I implemented the solution suggested here in the main function and I am still met with the error of illegal division by 0 on line 109. I previously tried to include the io. open line within the functions and was met with the same error. Here is a snippet of my code. Am I doing this correctly? Thanks!",
        "answers": [
            "Nevermind, I got it"
        ]
    },
    "284": {
        "question": "Hi NLP teaching staffs - Are these score okay for Part 4 and part 5? My bert model does not perform better than word2vec unfortunately, and I am not quite sure how I can improve it. Best,",
        "answers": [
            "That's expected. My theory here is that both w2v and Bert are really good at selecting a candidate, but that unfortunately the candidate set obtained from wordnet is too limited. "
        ]
    },
    "285": {
        "question": "Hi NLP teaching staffs - For hw4 part 5, I have to create a virtual python environment and reinstall all the dependencies to run Bert Predictor (otherwise I would run into some Keras package/ dependency issues in my local main machine). When I run the code in the virtual environment, it works. I want to make sure that this is okay and it is not something with my code that cause the issue. Thank you so much! ",
        "answers": [
            "Sure, you can assume the TAs environment when grading the assignment is set up properly. "
        ]
    },
    "286": {
        "question": "Hi! I see on courseworks that part 3 isn't expected to outperform part 2. However, based on what I've seen from other students' posts on Ed, I'm worried my output is too low. Do these scores seem acceptable? Thank you!  ",
        "answers": [
            "It's difficult to say based on the scores themselves. They don't look suspiciously low to me. "
        ]
    },
    "287": {
        "question": "Hi, I am working on part 4. I used get_candidates to get all the synonyms, but I am getting a KeyError: \"Key 'moving picture' not present. \"it seems that Word2Vec does not accept words with a space in between? What is the best way to fix this? Thanks!",
        "answers": [
            "There may be a way to fix this, but when I chose to skip those words with a space in between, I got the following results:Total = 298, attempted = 298; precision = 0. 115, recall = 0. 115Total with mode 206 attempted 206; precision = 0. 170, recall = 0. 170",
            "Note that in part 1 instruction, \"WordNet will represent such lemmas as \"turn_around\", so you need to remove the . \". Hence, from my understanding, you should update part 1 to convert word with space in between into _ instead. After that, the error should be fixed.",
            "I think you can try: if word not in self. model. key_to_index: proceed to next wordto skip the word that is not in the pre-trained model.",
            "For skipping, depending on the version I think the syntax I found worked was if word not in self. model. FWIW for part 6 I tried some more creative approaches for handling compound words that aren't in the model, but didn't find anything that improved (or even matched) the performance when just skipping them."
        ]
    },
    "288": {
        "question": "Hi, will there be any practice problems or ungraded exercises before Exam 2? Thank you!",
        "answers": []
    },
    "289": {
        "question": "Hi, For the simple less predictor, when using the shortcut in #477 to break ties, should we still expect there to be some ties between lexemes? When I implemented the shortcut, there were about 41/300 instances where there were still ties. For example, when the target word was 'cross' in context 51, the lexemes (Synset('crisscross. n. 01'), Lemma('crisscross. n. 01. crisscross')) and (Synset('crisscross. n. 01'), Lemma('crisscross. n. 01. mark')) were tied. I randomly picked crisscross. I think this tie is occurring because when I used count() to calculate their frequencies, I got 0, and since they are in the same synset, they had the same aggregate score. These are my results for part 3:Total = 298, attempted = 298; precision = 0. 109, recall = 0. 109Total with mode 206 attempted 206; precision = 0. 150, recall = 0. 150Also, I was surprised to find that when I always used the tie breaking shortcut on all lexemes, it slightly outperformed my original results:Total = 298, attempted = 298; precision = 0. 112, recall = 0. 112Total with mode 206 attempted 206; precision = 0. 155, recall = 0. 155",
        "answers": [
            "I didn't actually check how many remaining ties there are. But yes, I would expect there are some. "
        ]
    },
    "290": {
        "question": "Hello, I don't see the slides for today's lecture (Text Summarization) on courseworks. When can we expect to see them published? Thank you!",
        "answers": [
            "Theyre up now in the Files section on courseworks. "
        ]
    },
    "291": {
        "question": "Would it be possible to upload the lecture 18 slides as a PDF to Courseworks?",
        "answers": [
            "Theyre up now (in the files section. Ill link them from modules later) "
        ]
    },
    "292": {
        "question": "Hi, I am wondering if hw4 will be our last coding assignment. The syllabus says the course grade will be based on 5 programming assignments and the lowest grade will be dropped. ",
        "answers": [
            "In lecture, I believe the professor mentioned that there will be a fifth coding assignment, but that this will be due after the final. Also, since the lowest grade will be dropped, I think this assignment will be optional."
        ]
    },
    "293": {
        "question": "How long should it take to run part 5 of hw4? My laptop is taking upwards of 20 minutes and I wanted to know if that was normal or if there are some issues within my code.",
        "answers": [
            "That seems rather slow. It probably has to do with how you sort the BERT predictions. ",
            "I think you may load the word2vec = Word2VecSubst('GoogleNews-vectors-negative300. bin')\n in your loop function. I did not notice this for the first time I ran this. Move it outside and just call the predict_nearest in your loop"
        ]
    },
    "294": {
        "question": "Hi NLP teaching staffs - Is this score good for Part 2 and part 3. My lesk model (part3) slightly outperformed wn_frequency model (part2), and I want to make sure it is okay. Thank you so much!",
        "answers": [
            "yes, that looks good ."
        ]
    },
    "295": {
        "question": "Hi NLP teaching staffs - For HW4, can I update tokenize function and create helper method for my code? Tysm!",
        "answers": [
            "Yes. "
        ]
    },
    "296": {
        "question": "Hi NLP teaching staffs -Can I get some clarification on how to find \"the possible synonym with the highest total occurence frequency (according to WordNet)\". Does highest total occurence frequency mean occurence frequency of this sense of 'synonym' in the SemCor corpus? Can I simply get this frequency by using the . count() function? Tysm! !! !Best",
        "answers": [
            "Yes, you can simply get this using . count()The only thing to watch out for is that a synonym may occur in two different synsets."
        ]
    },
    "297": {
        "question": "Hi, I'm struggling to break the tie in part 3. I believe I have the correct overlap score, but I think I'm making errors in determining the values for 'b' and 'c' in @477. My logic is as follows:1. Create tokens of the contexts2. Find synsets using context3. For each synset in the synsets:     3. a. Calculate overlap_score for the synset     3. b. Retrieve synset lemmas.     3. c. For each lemma in the synset:          3. a. i. if this lemma is the same as the target(b case) -&gt; add it to b variable          3. a. 2. all the other lemmas will fall under the c case. -&gt; Increase c with each count     3. d. Calculate the total score and select the highest. However, this approach yields a low output (approximately 0. 06). Could you let me know where I might be making mistakes? ",
        "answers": [
            "Hi, from my understanding of @477, it seems that your logic is correct, but just want to clarify some things in your logic. In steps 3c-3d, are you calculating an aggregate score for each lexeme? or each synset? And in step 3. a. 2, when you calculate 'c' for \"all other lemmas, \" are you calculating a separate 'c' for each lemma?    "
        ]
    },
    "298": {
        "question": "For part 6, the precision and recall improve in the total = 298, but in mode 206, they decrease. Can this result be considered an improvement?",
        "answers": [
            "I'd consider it an improvement. But more importantly, we will not grade part 6 according to absolute improvement as long as you tried a different approach (one that is qualitatively different than the other parts). "
        ]
    },
    "299": {
        "question": "Hi this is a general question about the varying scores that people are posting for this section. For part2, aren't the predictions deterministic, since we are just outputting the most frequently occurring possible substitute? So why are people getting different scores for this section? Or am I misunderstanding the prompt. .. Much thanks!",
        "answers": [
            "There are a few implementation decisions that may lead to different scores:  Are you implementing the scoring method proposed in #477 or did you implement tie breaking differently? How did you preprocess the definitions and example sentences? "
        ]
    },
    "300": {
        "question": "Dear Prof. and TAs, I'm wondering how this graph was derived, I can't find any definition about the dependency structure or replacement rules on the PPT, could you please explain how the graph was derived to me? Thanks! (starting from this:)",
        "answers": [
            "The next step here would be to use Rule 5 (the full grammar is a few slides past this one) to convert the S2 edge into a VP2 edge. Then replace the VP2 edge using rule 6. "
        ]
    },
    "301": {
        "question": "Hi, When I downloaded the file, it was named 'GoogleNews-vectors-negative300. bin', not 'GoogleNews-vectors-negative300. bin. gz'. I attempted to convert it to the . gz format but encountered an error: gzip. BadGzipFile: Not a gzipped file (b'30'). Is it possible to use the 'GoogleNews-vectors-negative300. bin' file directly in lexsub_main. py?",
        "answers": [
            "Just imported it as the '. bin' file, it is a trained model.",
            "Yes, I think on some systems / browsers the file is automatically unzipped. Using the . bin file is fine. "
        ]
    },
    "302": {
        "question": "Hi all! Just saw this and thought that some folks here could be interested. Seems like a fun way to apply some of what we learned in class. The deadline for submissions is on January 2nd. Contest link: https://www. kaggle. com/competitions/linking-writing-processes-to-writing-qualityBest wishes",
        "answers": []
    },
    "303": {
        "question": "Hello, When I try running nltk. download(), I encounter the following pop-up:This is preventing me from being able to download the necessary packages. Does anyone have any idea on how to resolve this issue? Thank you!",
        "answers": [
            "Found a solution for anyone else that might encounter this:Install the following:pip install --upgrade certifi\nUpdate your python interpreter to contain the following:import nltk\nimport ssl\n\ntry:\n    _create_unverified_https_context = ssl. _create_unverified_context\nexcept AttributeError:\n    pass\nelse:\n    ssl. _create_default_https_context = _create_unverified_https_context\n\nnltk. data. path. append(\"/path/to/certifi/certifi/cacert. pem\")\nnltk. download('wordnet')\nnltk. download('stopwords')\n"
        ]
    },
    "304": {
        "question": "May I ask if the results in the picture are reasonable results for frequency and simple leak predictor?",
        "answers": [
            "These look reasonable to me!"
        ]
    },
    "305": {
        "question": "Hi, when trying to use the tokenize method, I get an error saying it doesn't recognize the item \"string\". Did anyone else encounter this problem? Would it be wrong to simply tokenize by using . split(\" \")?",
        "answers": [
            "feel free to import string. import string\nYou could simply split by whitespaces, but that would not handle punctuation at all, such as commas and periods. "
        ]
    },
    "306": {
        "question": "Would the final have the same format as the midterm? Would there be some guidelines or review sessions to help us review the content? What is the proportion of the later part of the semester contents taking in the exam?",
        "answers": []
    },
    "307": {
        "question": "Hi there, I am doing the part5 and I am not sure about how to handle the case when the candidate from WordNet is a multi-word like \"at long last\", in this case, how can we compare the candidate with the output tokens of the DistillBert model, as I think the prediction result from the DistillBert model is just a single token and doesn't include the multi-word case. Thanks a lot in advance!",
        "answers": [
            "In my implementation I skipped these candidates. "
        ]
    },
    "308": {
        "question": "For part 1, shall we return a list or a set?",
        "answers": [
            "Return a set, as shown in the example. Sorry for the misleading description. "
        ]
    },
    "309": {
        "question": "Hi, I am also confused what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". In particular I am confused what it means by \"if the word and the target appear together in multiple synsets\" Is it simply counting frequency of synonym occurrence in the list from part 1? Thanks!",
        "answers": [
            "Lets say the target word is bright and both bright and light appear in synset s1 and s2. To get the score for the candidate light you need to sum the count for &lt;light, s1&gt; and &lt;light, s2&gt;. "
        ]
    },
    "310": {
        "question": "Hi dear prof. and TAs, After finishing part2, I got this result, it seemed that it was lower than 10%, is it OK?",
        "answers": [
            "Hm that looks a little low from what I remember. Maybe take a look at the predict file and see if you notice anything suspicious. "
        ]
    },
    "311": {
        "question": "Hi, For part 6, I tried to use a larger version of the BERT model to increase the prediction \"accuracy\" of the replacement for [MASK] label. The version I used is BERT-large, also included in the transformer package. After making this replacement, the predictions for the [MASK] label all becomes labels like [unused818]. Why would this happen?",
        "answers": [
            "Are you also using the corresponding tokenizer? Its important to use the same tokenizer that the model was pretrained on. "
        ]
    },
    "312": {
        "question": "Hi! I'm currently working on part 2 and had a clarification question: am I meant to be looking at the word_form or the lemma of the given context? It seems to make more sense to me to use the lemma, but I'm getting the following results that don't seem quite right:My results when using the word_form are even further off, so I'm a bit confused about where I'm going wrong. Thank you!",
        "answers": [
            "You should be using the lemma. Look at the output file and see if you notice anything suspicious. "
        ]
    },
    "313": {
        "question": "When I evaluate both implementations, I get the following results:(tf) sameerkhanna@Sameers-MacBook-Pro hw4_files % perl score. pl word_vec. predict gold. trial Total = 298, attempted = 298 precision = 0. 115, recall = 0. 115 Total with mode 206 attempted 206 precision = 0. 170, recall = 0. 170 (tf) sameerkhanna@Sameers-MacBook-Pro hw4_files % perl score. pl bert. predict gold. trial Total = 298, attempted = 298 precision = 0. 115, recall = 0. 115 Total with mode 206 attempted 206 precision = 0. 170, recall = 0. 170I checked and confirmed the prediction files are indeed predicting different things. Is this result ok, or are we supposed to get better performance using BERT? ",
        "answers": [
            "Yes this is expected."
        ]
    },
    "314": {
        "question": "I want to confirm that my results for hw 4 part 2 wn_frequency test look reasonable. The assignment says they should both be around 10% and mine is 9% and 13%. ",
        "answers": [
            "I think this is fine. "
        ]
    },
    "315": {
        "question": "Hi, I'm getting an error \"Illegal Instruction: 4\" when I try to run the line python lexsub_main. py lexsub_trial. xml  &gt; smurf. predict. Everything else in the instructions has worked up until this point, so not entirely sure what could be happening. Thanks so much! ",
        "answers": [
            "Can you post the entire error message you are getting? "
        ]
    },
    "316": {
        "question": "The documentation mentions removing the _ in some of the words. By removing, does that mean delete the underscore (which combines the words) or replace the underscore with a space? Ex: CanisFamiliaris or Canis Familiaris",
        "answers": [
            "In the test data, multiword expressions show up with white spaces \" \". But note that this won't help you for the gensim and Bert approach (because multiword expressions are not in the lexicon for these models). "
        ]
    },
    "317": {
        "question": "As mentioned earlier in #476, Windows laptops have an issue with the newlines. To circumvent this, I made it so that my python file directly opens and writes a file. For the final submission, is it fine to keep my code written in that manner so it automatically creates a part6. predict file when we run this linepython lexsub_main. py lexsub_trial. xmlor does it need to be runnable in the following format (as in the HW documentation)? python lexsub_main. py lexsub_trial. xml  &gt; part6. predict\nI would prefer to have it so that my python file is able to open and write the file itself instead of storing the printed lines in a file, because the newlines in Windows do not behave appropriately the second way.",
        "answers": [
            "Its fine to leave it the way you changed it, with the output being written directly to a file. "
        ]
    },
    "318": {
        "question": "Hi I was running the python file for assignment 4 and I got an unexpected error. I always ran python file using pycharm but this time I got an pop up window that says python quited unexpected. I retried a few times but this window appears every time I try to run. I am using python 3. 9. 7 and mac. I am not very familiar with python, can anyone give me some suggestion about how to solve this? any advice helps, Thank you so much.",
        "answers": [
            "Not sure what's going on -- the issue seems to be the pycharm IDE. Maybe try a different IDE or editor? You could also try to install a separate Python environment, rather than the one that comes preinstalled with macOS. I recommend looking into Anaconda (or miniconda). "
        ]
    },
    "319": {
        "question": "Hello, For part 3, what is an acceptable precision/recall score? My implementation produces the following result: I was wondering whether this is acceptable or not. Thanks!",
        "answers": [
            "Yes its expected that the scores for part 4 and 5 produce very similar results. The score for part 3 also looks fine to me, but maybe post publicly for comparison? "
        ]
    },
    "320": {
        "question": "Hi, I was reviewing the slides and I have a question about biLM embeddings. From my understanding, biLM generates different embeddings for the same word if it has different senses. How does biLM differentiate the two senses? I would guess that even if two words have the same sense but appear in slightly different contexts, their embeddings would be slightly different, but not enough to consider them to have different senses. Does biLM store embeddings for each of the instances of a word and then classify them into groups depending on their similarities? Thanks in advance!",
        "answers": [
            "The representations computed by a bidirectional language model do not differentiate between discrete word senses. Rather, each token representation will be unique based on the actual sentence context that the word appears in. Tokens that appear in similar contexts will have similar senses. The premise is that there are no discrete senses and that contextualized embeddings capture fine-grained sense distinctions more accurately. After all, the sense annotations in WordNet (and other resources, like VerbNet, PropBank and FrameNet) are coarse and somewhat artificial -- they are created by human annotators. If you wanted to use contextualized word representations to obtain representations for discrete senses (as annotated in WordNet), you could do the following. For each lexeme, obtain contextualized embeddings for all annotations of the lexeme in SemCor. Then average these embeddings (which should, after all, be reasonably similar anyway). "
        ]
    },
    "321": {
        "question": "Are we allowed to use cheat sheets for Final Exam?",
        "answers": [
            "\"One double-sided letter-sized \"cheat sheet\" will be allowed\" (on Courseworks)"
        ]
    },
    "322": {
        "question": "When working on part 4, I noticed that some candidate synonyms (obtained from the get_candidates method) does not exist in the word2vec model. For example, for the word film. n, I got the following error:KeyError: \"Key 'moving picture' not present\"\nI'm wondering that when processing multi-word expressions in part 1, by removing '_', should we change (for example) the word turn_around to turn around or turnaround? If it's the first case, when computing the embedding of this phrase, should the embedding be the average of the embedding of each word? If it's the second case, this word will still be shown as not present. How should we process this case? Also, I noticed that words like of cannot be found in the KeyedVector. Should we consider using the stopword to remove words like this?",
        "answers": [
            "Check out this thread: #517"
        ]
    },
    "323": {
        "question": "I was able to get the score. pl to work with our smurf. predict, and it correctly gives 0 for everything. However, in part 2, we implemented a version that should be better. I checked my output part2. predict file and it did look like the words seemed to line up occasionally with the gold. trial. However, I still receive 0 for both the precision and recall (even though my part2. predict file looks more accurate) when I run the following command:perl score. pl part2. predict gold. trialThe only parts that I updated were the part1 and part2 code so far, and I changed the prediction to use part 2's wn_frequency_predictor method to generate the part2. predict file. Can anyone help me understand where I am going wrong and how to move forward?",
        "answers": [
            "I suspect this is an issue of file endings. The Perl script expects Unix file endings, but you may be running your code on windows, in which case Python produces windows file endings by default.  You a modify the code to write to a file directly rather than printing the predictions to console. That would allow you to control the formatting and encoding of the file. Try something like this:import iof = io. open('file. presidict, 'w', newline='\\n')",
            "Thank you so much professor! !!"
        ]
    },
    "324": {
        "question": "Hello, Every time I try to run score. pl using the following command:perl score. pl smurf. predict gold. trial\nI get the following output:Error in smurf. predict on line 1Error in smurf. predict on line 2Error in smurf. predict on line 3Error in smurf. predict on line 4Error in smurf. predict on line 5Error in smurf. predict on line 6Error in smurf. predict on line 7Error in smurf. predict on line 8Error in smurf. predict on line 9Error in smurf. predict on line 10. .. Error in smurf. predict on line 298Error in smurf. predict on line 299Error in smurf. predict on line 300Illegal division by zero at score. pl line 109,  line 300. I haven't modified any of the starter files. This also occurs when I try to use my own predictors (i. e. the predictor you make in part 2). The score. pl script is just erroring on every input. Is there a way to resolve this? I'm using a computer running windows 10.",
        "answers": [
            "I met the same issue when I ran the code on Windows, but I never met it on my Ubuntu WSL. I guess that the code is not compatible with Windows because Windows uses /r/n instead of /n for newlines or due to different encodings. ",
            "#476"
        ]
    },
    "325": {
        "question": "Hi, I am a bit confused about the following statements in the description for part 3If this is the case (or if there is a tie), you should select the most frequent synset (i. e. the Synset with which the target word forms the most frequent lexeme, according to WordNet). Then select the most frequent lexeme from that synset as the result. How do we define the most frequent lexeme and the most frequent synset? As far as I understand, WordNet does not contain any information on the frequency of each synset. Do we need to rely on any external corpus to get this frequency? I would really appreciate some explanations on this, preferably with an example. Thanks!",
        "answers": [
            "Wordnet contains frequency information about individual lexemes (in the SemCor corpus). So if your candidate lemma is bright. an and you  find three lexemes for synset 1, 2, and 3, lets call them &lt;bright-s1&gt; &lt;bright-s2&gt; &lt;bright-s3&gt;, then the most frequent synset is the one for the most frequent of these lexemes. There is a shortcut/trick for doing the tie-braking. I will post in another thread. "
        ]
    },
    "326": {
        "question": "Hi! Hope everyone is having a good break, just wondering when the solutions to the ungraded AMR exercise will become available :)",
        "answers": [
            "I will post them early this week. "
        ]
    },
    "327": {
        "question": "Hi, I try to run the Bert model by using &gt;&gt;&gt; import transformers \n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; model = transformers. TFDistilBertForMaskedLM. from_pretrained('distilbert-base-uncased')and encounter this error:File \"/Users/. .. /anaconda3/lib/python3. 11/site-packages/transformers/modeling_tf_utils. py\", line 70, in  from keras. engine import data_adapter ModuleNotFoundError: No module named 'keras. engine'RuntimeError: Failed to import transformers. models. distilbert. modeling_tf_distilbert because of the following error (look up to see its traceback):No module named 'keras. engine'How to resolve this error? I tried to update my tensorflow and transformers, and keras is also installed in my computer, but none of them worked. Thanks!",
        "answers": [
            "Sorry for the slow response. This looks like a version incompatibility between tensorflow and transformers. Make sure you have the latest version of each. pip install --upgrade keraspip install --upgrade transformersIs anyone else encountering the same issue? "
        ]
    },
    "328": {
        "question": "For Part 6, should we be creating another function separate from the functions from the first 5 parts? For instance, if I want to refine the model in Part 4, could I just define another function inside the Word2VecSubst class? Also, do we only need to refine one of the 5 models? Thanks!",
        "answers": [
            "Yes this should be a new function (though it can duplicate code or call other functions. The refinement soils be to one of the 5 models or a combination of multiple models. We are more interested in new/creative ways of attempting this task rather than just getting the best performance. "
        ]
    },
    "329": {
        "question": "ML for Dependency Parsing: Max of 3 untyped choices: $|R| \\times 2 + 1$ ? I didnt understand what R is and why is it multiplied by 2 and added by 1? What does it mean by Left arc transition focus on relating head to adjacent, while right arc focusing on relating the head? So is buffer typically considered head? GPT/BERT: does auto-regressor and auto-encoder classification suggest how the models are originally trained or the type of tasks they can do? PropBank: how to differentiate between Arg2 and Arg3? Seems quite similar to each other. Span-graph based SRL predictions: what is p, a, l in this case? can you give a more detailed explaination of the Unary scores, label score, and combined score and how they relate and interact with each other? For the PropBank SRL Results: the last paper on swapping out ELMo, its still based on RNN network or Span-graph? If span-graph is it the Word &amp; character representation layer (x) that is being replaced? Thanks in advance!",
        "answers": [
            "Sorry for the slow response. Let me address these one-by-oneR is the set of relation labels. Each label can appear with two transitions, arc_left and arc_right. The +1 is for the shift transition which does not require a label. Not sure I understand the question. With arc_left the head is the first word on the buffer and the dependent is the first word on the stack. With arc_right, it's the other way around. Yes, the terms auto-regressor and auto-encoder refer to the pre-training objective. Arg2 - Arg4 have to be interpreted differently for each verb sense). They do not generalize across different predicates. The definition for what the Arg2-Arg4 roles mean is part of the propbank annotation (in the \"frame file\" for that verb). p, a, l stand for predicate, argument, and label. We didn't go too much into detail for this specific model -- if you are interested in the detail I suggest looking at the He et al. 2018 paper. https://aclanthology. org/P18-2058/The last row is the span-graph based approach, but using pre-trained ELMo to compute contextualized word representations. And yes, this would replace the \"Word &amp; Character representation layer\" in the figure in lecture 15 slide 48. "
        ]
    },
    "330": {
        "question": "Hi, I'm just wondering where can i find the class distribution of midterm. Thank you very much",
        "answers": [
            "Hi this should be available on Gradescope, but was also included in the Announcement I sent out when the grades were released. "
        ]
    },
    "331": {
        "question": "Hi, I'm a bit unsure of what the instructions mean by \"sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets\". Would you be able to give an example of this?",
        "answers": [
            "I am also struggling to understand what is meant here.",
            "Similar to the get_candidates method, as you iterate through all the lemmas, store the counts for each word encountered to get the most frequent synonym."
        ]
    },
    "332": {
        "question": "Really insignificant but just wanted to make sure I didn't set up something incorrectly but when I run :l1 = wn. lemmas('break', pos='n')[0]\ns1 = l1. synset()\ns1. hypernyms()I should get: \"[Synset('happening. v. 01')]\"but instead, get:\"[Synset('happening. n. 01')]\"Everything else looks okay but just wanted to make sure ",
        "answers": [
            "\"[Synset('happening. n. 01')]\" seems good to me actually because your initial search was for a noun. "
        ]
    },
    "333": {
        "question": "Will the final be cumulative or will it be the part covered only after the midterm.  ",
        "answers": [
            "(not a TA) They mentioned the final will be cumulative"
        ]
    },
    "334": {
        "question": "Hi - I noticed that the final exam schedule date published on SSOL is different from the one noted on Canvas. I want to confirm which one is the correct schedule:Canvas/ syllabus: 12/11 (class time)SSOL: 12/18/23 Best,",
        "answers": [
            "The syllabus is correct -- exam 2 will be on the last day of class (12/11, 1:10pm). The registrar automatically schedules a final exam slot, but we are not using it. "
        ]
    },
    "335": {
        "question": "hi, I still don't understand the multi-head attention in lec14, can anyone plz explain it, thanks in advance!",
        "answers": []
    },
    "336": {
        "question": "Hi NLP teaching staffs -I am wondering when will HW4 be released so I can plan my Thanksgiving plan accordingly? Thank you so much!",
        "answers": [
            "(Not a TA) I think in lecture he said it would be released on Friday ",
            "I'm still working on it, apologies. But it should come out today or maybe tomorrow. "
        ]
    },
    "337": {
        "question": "Hi, I was wondering when the slides for Lecture 16 (the slide deck for the second half of today's lecture) would be uploaded to courseworks? I don't see them currently under lecture notes. Thank you very much!",
        "answers": [
            "Lecture 16 is now available on Courseworks. "
        ]
    },
    "338": {
        "question": "Dear TAs and Professor, I noticed that my exam grade on canvas is different from that is on the grade scope. Will the grade on canvas be changed later? Thanks!",
        "answers": [
            "Yes I will synchronize these once all regrades have been processed. "
        ]
    },
    "339": {
        "question": "Hi, A few of us are on the zoom link sent via the announcement, wondering if we are in the right place or is there a change in OH? Thanks",
        "answers": [
            "I was running late, but I think I got to talk to you eventually (as well as everyone else who was waiting). Sorry for that. "
        ]
    },
    "340": {
        "question": "Hi i hope you are doing great.  I think I remember from class discussion that on the final we are allowed to have a cheat/crib sheet.  If so, how long can the sheet be and are there any rules about  what can and can't be on it, if it has to written or typed etc.  Thank you!",
        "answers": [
            "#384"
        ]
    },
    "341": {
        "question": "Hi I'm sorry if this is an inappropriate  use of ED, but I am trying to get into the NLP field and I was wondering if anyone knew of any NLP internships (particularly with mitigating bias) that are specifically  for undergrads?",
        "answers": [
            "Hi there! A little late, but I believe looking into an Ontology/Knowledge Engineering internship at Amazon could be helpful. I was an intern at Alexa AI last year and while not directly related to NLP, my work was somewhat adjacent to it. I believe one of the other interns in our team was working directly with NLP, so if you show interest in the topic maybe an NLP project can be assigned to you. In any case, it could be a nice way to get some exposure to it. Hope this helps"
        ]
    },
    "342": {
        "question": "Hello Teaching staff, Just letting you know that my courseworks grade is not yet updated after the regrade is reflected on gradescope.",
        "answers": [
            "Yes, these don't automatically synchronize. I will update them after the end of the regrade period. "
        ]
    },
    "343": {
        "question": "Hi TAs and Prof. Bauer, This is Millie and I know I asked for a lot of help on HW3. The code seems all correct but was just not running properly. Turns out, there may be problems with my machines. I transported everything onto Google Colab and finally got very reasonable results. Training Model Loss: Done loading data. Epoch 1/5 18996/18996 [==============================] - 80s 4ms/step - loss: 0. 4917 Epoch 2/5 18996/18996 [==============================] - 79s 4ms/step - loss: 0. 3149 Epoch 3/5 18996/18996 [==============================] - 78s 4ms/step - loss: 0. 2796 Epoch 4/5 18996/18996 [==============================] - 78s 4ms/step - loss: 0. 2597 Epoch 5/5 18996/18996 [==============================] - 80s 4ms/step - loss: 0. 2462Evaluation: 5039 sentence. Micro Avg. Labeled Attachment Score: 0. 7465165107845009 Micro Avg. Unlabeled Attachment Score: 0. 7942181887591316 Macro Avg. Labeled Attachment Score: 0. 7558937632762938 Macro Avg. Unlabeled Attachment Score: 0. 8047567682996984So happy to finally get the program to work, but perhaps suggest issues with my machine I may need to look into for future HWs. Overall, thank you again for all your patience and help throughout the process! ! ",
        "answers": [
            "How odd! No other change? Id be very curious to learn what the issue was in the end. "
        ]
    },
    "344": {
        "question": "Hi, I hope you are doing great. I was wondering if I could ask if students could be given the option to weigh the final more heavily in the grade calculations.  I was wondering if I could make the final worth 50% of my grade and drop my midterm grade. I  understand if this is not possible.  I am not trying to make excuses or to get special treatment.  I am not yet sure what I misunderstood with the midterm material, but I think I can get a lot better on the final. Thank you for your consideration!",
        "answers": [
            "Unfortunately this is not really an option, unless there is a significant reason for a lower midterm performance (which would typically mean health-related -- and even then I have entertained options like this one only in extreme cases). The final exam is curved, so everyone is affected by the midterm exam in the same way. "
        ]
    },
    "345": {
        "question": "Hi, Will midterm solutions be posted? Would love to learn how to improve for the final. Thanks",
        "answers": [
            "See #446"
        ]
    },
    "346": {
        "question": "Hi there, Just a quick question, if the answer to Question 1 mid-exam is \"v-&gt;d-&gt;adj-&gt;n\"? Thanks a lot!",
        "answers": [
            "Yes, the POS sequence is V-&gt;D-&gt;Adj-&gt;N."
        ]
    },
    "347": {
        "question": "Incredibly embarrassed that I have to ask, but what is the likelihood of failing the class if I got a 38 on the midterm :( ",
        "answers": [
            "My experience is that people only fail this class if they stop submitting work. The homework problems alone are worth 40% of the grade. A 38% is not great, but I would think of it as a diagnostic tool to see how you can improve for exam 2. I will release a sample solution for the exam today. Maybe take a look and compare to your exam. If there are any remaining questions, we can go through your exam together in office hours and analyze which concepts you should revise and how to prepare better for exam 2. "
        ]
    },
    "348": {
        "question": "Hi I hope your are doing great.  Are the correct answers for the exam available somewhere?  I see the deductions, but I am trying to figure out where I went wrong. Thank you",
        "answers": [
            "Yes, I will release a sample solution. I just didnt get a chance to type it up. "
        ]
    },
    "349": {
        "question": "Hello, I think my exam for question 4 was correct, but the text was blurred. It only scanned part of my answer on it. Is there any way to deal with this issue?",
        "answers": [
            "Yea please submit a regrade request through Gradescope so we can process these more efficiently. Worst case, I can try to chase down the physical copy. "
        ]
    },
    "350": {
        "question": "Dharnna moved OH to today in CSTA room 4:30PM - 6PM. A few students are here and not seeing her. ",
        "answers": []
    },
    "351": {
        "question": "Hi! I totally missed the part of the assignment instructions for HW 3 where we were supposed to name our file &lt;uni&gt;_homework3 -- is this big enough of an issue to resubmit or is there any way to resubmit without losing credit?",
        "answers": [
            "No, not worth resubmitting. We will figure it out. "
        ]
    },
    "352": {
        "question": "Hello! I just realized I submitted my code with a line I meant to delete that was leftover from testing. It was a single line in the train_model. py file. Is there any way for me to resubmit without the 20 point late penalty? Sorry for the late post!",
        "answers": [
            "Yes, please go ahead and resubmit today. "
        ]
    },
    "353": {
        "question": "I first used Keras 2. 12 on my local Mac and got a loss of around 0. 41 after 5 epochs. After updating to Keras 2. 14, the loss finally decreased to 0. 28 after 5 epochs and performance improved.",
        "answers": [
            "That makes some sense. I think there is a better version of the optimizer in recent versions.  "
        ]
    },
    "354": {
        "question": "Should every py file, h5 file, and vocab file be in the same directory in the zip file? or should there be a data folder as well?",
        "answers": [
            "Not a TA. I just included all 6 . py, 1 h5 file, and 2 vocab file into one folder and zip it."
        ]
    },
    "355": {
        "question": "I get a score of . 76 and . 77 for my micro and macro unlabeled attachment scores, however, my labeled attachment score is 0. Actually, I get a division by zero error without adjusting the main. py in evaluate. py. This appears to be because the program doesn't recognize any of my dependency relations in each edge. What should the argument for left and right arc look like exactly? I'm currently keying into the output label dictionary, but this doesn't seem to be working.",
        "answers": [
            "Nevermind, found the answer, it turns out that if you just print the target_labeled list you can check to see the correct format for the labeled dependency relations."
        ]
    },
    "356": {
        "question": "For this particular HW, make sure you have python version 3. 9. 6 and tensorflow version 2. 14. 0. Some of you are facing the issue that there are different results on Google Colab vs running it on local. Using these versions gave me the same results, might help you too.",
        "answers": []
    },
    "357": {
        "question": "I let my part 4 run around 45 minutes and it's not finished so I cancelled it. When I try running it on smaller data set that I made, it produces 0. 5 -&gt; 0. 6 accuracy (around 100 sentences). Took it to an OH, and the TA was kind to walked through my code but we still coulnd't figure out what's wrong. (My loss value for part 3 is around 0. 3 after 5th epoch). I also ran out of GPU so testing is gonna be slower, should I just submit ? Appreciate any advice.",
        "answers": [
            "Sounds like its working  there are some reason why part 4 may be slow. Check that you have eager execution turned off. Another possible issue might be that the sorting of transitions is slow. Make sure you use np. argsort. "
        ]
    },
    "358": {
        "question": "Has anyone got this message in google colab and know how to get around it?  ",
        "answers": [
            "maybe try another Google account where you still have credits?"
        ]
    },
    "359": {
        "question": "When checking the legality for arc-right, the condition requires that the stack is not empty, does this mean that the stack contains no root node as well or would it be empty (not including root) i. e. its length equals 1? Thank you!",
        "answers": [
            "right_arc is permitted if the stack contains root. It is not permitted if the stack is empty. Left_arc however is not permitted if the top of the stack is root. "
        ]
    },
    "360": {
        "question": "I keep getting this error while running the decoder and then Python crashes:UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown\n\nwarnings. warn('resource_tracker: There appear to be %d 'For reference, I am on an M1 Mac but am using CPU TensorFlow. Any ideas what could be causing this? I've checked my Part 2 and Part 3 and they look mostly right. Not sure what's going on here.",
        "answers": [
            "This is just a warning you get because Python didnt shut down properly. Its not the reason for why it crashes. Are there any other error messages?  This happens in part 4 only? Its odd because if it was a tensorflow issue it should have also shown up in part 3. "
        ]
    },
    "361": {
        "question": "Does anyone know if Dhaarna OH is supposed to be in the CS TA room today? I've been here for 10 minutes but the only person here is the whiteboard with NLP written on it",
        "answers": [
            "I think there was an announcement on canvas they got moved to tmrw :("
        ]
    },
    "362": {
        "question": "Hi all, If you're working on a Linux machine and don't want to install specific CUDA and cuDNN libraries system-wide, you might find figuring out your TensorFlow installation a bit tricky like I did. I am sharing the installation method that I have found most desirable for my needs (on Linux with just conda + NVIDIA drivers and no cuda installation system-wide) in hopes of it being useful. You most likely have NVIDIA drivers installed, you can just verify it via nvidia-smiIf the above command does not display a nice output of your GPU processes, then you should figure out installing NVIDIA drivers first. Setup a new conda environment with Python 3. 10 (due to TF compatibility, we will not use newest Python versions):conda create -n tf python=3. 10Next, review the TF compatibility website as well as the conda-forge packages for cudatoolkit and cudnn to find the newest version of TF that you can install via conda alone. As of today, this is 2. 11, which came out about a year ago. The tradeoff with this installation method is that you won't be able to use the shiniest TF release but you will have the ease of installation that comes with conda. https://www. tensorflow. org/install/source#gpuhttps://anaconda. org/conda-forge/cudatoolkit/files? version=https://anaconda. org/conda-forge/cudnn/files? version=Install the cudatoolkit and cudnn packages first:conda install -c conda-forge cudatoolkit=11. 2. 2 cudnn=8. 1. 0Configure conda activate scripts to set your $LD_LIBRARY_PATH (which is where TF looks for your cuda/cudnn libraries) whenever this environment is activated: mkdir -p $CONDA_PREFIX/etc/conda/activate. d\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate. d/env_vars. shNow you need reload the conda environment for the above changes to go in effect. Easiest way is to close your terminal and reopening&amp;reactivating the environment. Next, install the latest TF version that you've identified that can be installed via this method (2. 11) as of today:pip install tensorflow==2. 11This is it! You can verify by running:python -c \"import tensorflow as tf; print('GPUs Available: ', tf. config. list_physical_devices('GPU'))\"which should print a ton of TF warnings/messages but also:GPUs Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nwhich confirms the installation is successful! Hope it's useful!",
        "answers": [
            "Nice, thanks for the detailed instructions. "
        ]
    },
    "363": {
        "question": "Hi Everyone! I'm having a problem running my code for part 3 on Colab. When I run it on my computer (Mac) it works fine, but when I try to run it on Colab, I get this error:ValueError: cannot reshape array of size 33816560 into shape (1899519, 91)From this line:outputs = np. load(sys. argv[2])Does anyone know if there are extra steps you need to take to run on Colab? Thanks!",
        "answers": [
            "Did you rerun the feature extractor on colab or upload the . npy file from your local machine? It could be an encoding issue. "
        ]
    },
    "364": {
        "question": "If we used it for part 4, should we include tf. compat. v1. disable_eager_execution() in our submission? Or should that be left out in the submission and only used when we are testing our own code?",
        "answers": [
            "To my knowledge, it shouldn't matter that you have tf. compat. v1. disable_eager_execution() in your code because you only need to submit the . py files, . vocab files, and the . h5 model file. Unless this line is in one of these files (which it probably wouldn't be since you would be using an external file such as a . ipynb notebook file ffor testing), this shouldn't matter or cause any issues.",
            "You can leave that line in your code when you submit. "
        ]
    },
    "365": {
        "question": "Hello, I've been working on this part (part 2) for a day now, and I thought I had it so I moved onto part 3. However, when I run python train_model. py data/input_train. npy data/target_train. npy data/model. h5I get loss values that are quite strange. Within the first epoch they decrease to 0, then they increase back up for the rest of epoch 1 and in subsequent epochs.  I am almost positive that this is because of my get_input_representation method, but many hours of re-debugging later I have no results :(My implementation of get_input_representation is below. I know I can't ask for someone to fully debug, but I am wondering if the bug is in this function or elsewhere. If it is wrong, I would appreciate some pointers in the right direction. def get_input_representation(self, words, pos, state):\n        # TODO: Write this method for Part 2\n        # initially, fill the vector with the index for \"&lt;NULL&gt;\"\n        rep = np. full((6, ), self. word_vocab[\"&lt;NULL&gt;\"])\n\n        # first, stack\n        for i in range(1, min(4, 1 + len(state. stack))):\n            put_in_rep = None\n            tok_idx = state. stack[-i]\n            if tok_idx == 0:\n                put_in_rep = self. word_vocab[\"&lt;ROOT&gt;\"]\n            elif words[tok_idx] in self. word_vocab:\n                # token on stack is a word\n                put_in_rep = self. word_vocab[words[tok_idx]]\n            elif pos[tok_idx] in self. pos_vocab:\n                # token on the stack is a POS tag\n                put_in_rep = self. pos_vocab[pos[tok_idx]]\n\n            rep[i - 1] = put_in_rep\n\n        # now, buffer\n        for i in range(1, min(4, 1 + len(state. buffer))):\n            put_in_rep = None\n            tok_idx = state. buffer[-i]\n            if tok_idx == 0:\n                put_in_rep = self. word_vocab[\"&lt;ROOT&gt;\"]\n            elif words[tok_idx] in self. word_vocab:\n                put_in_rep = self. word_vocab[words[tok_idx]]\n                print(words[tok_idx], \"is in words\")\n            elif pos[tok_idx] in self. pos_vocab:\n                # token on the stack is a POS tag\n                put_in_rep = self. pos_vocab[pos[tok_idx]]\n                print(pos[tok_idx], \"is in pos\")\n\n            rep[2 + i] = put_in_rep\n        return rep",
        "answers": [
            "I'm on an M1 mac, and I think this is the problem. I am trying colab right now."
        ]
    },
    "366": {
        "question": "Hello, has anyone else run into a problem where they get stuck in an infinite loop of shifting and right-arcing? I am not sure why this is, I'm thinking perhaps my checks are incorrect but I'm not sure where to start with debugging.",
        "answers": [
            "(Not a TA) I have run into a similar issue. I recommend you check your right-arc implementation to see if the correct word is removed from the buffer.",
            "Isolate the sentence for which this happens, then run the decoder on this sentence only and trace the steps. ",
            "I would also try to check that you properly mapped the actions to their probabilities before sorting."
        ]
    },
    "367": {
        "question": "Hi, I believe i found an issue in my code for get_input_representation() method. My old program had the representation as [ stack[-3], stack[-2], stack[-1] buffer[-3], buffer[-2], buffer[-1] ]. so I fixed it to be:  [ stack[-1], stack[-2], stack[-3], buffer[-1], buffer[-2], buffer[-3] ]However, even with the incorrect representation, my old program was giving me loss values of 0. 5 (epoch 1) down to 0. 28 (epoch 5). and attachment scores between 0. 75-0. 8My fixed program is giving me loss values of 0. 55 (epoch 1) down to 0. 38 (epoch 5) and attachment scores of around 0. 67-0. 74. So it seems like my model got worse even though I fixed the input representation. Can someone explain how this is possible? ",
        "answers": [
            "The specific representation shouldnt matter as long as its consistent between training and deciding. Theres some randomness in the training process which might explain this difference. "
        ]
    },
    "368": {
        "question": "Hi all, I'm having this problem with model loss rate (decreasing and increasing). I've went over the input and output representation with the TA and professor with many hours of debugging. I'm still getting an increasing loss rate in the 6 figures range. Please help! from conll_reader import DependencyStructure, conll_readerfrom collections import defaultdictimport copyimport sysimport kerasimport numpy as npclass State(object): def __init__(self, sentence = []): self. stack = [] self. buffer = [] if sentence:  self. buffer = list(reversed(sentence)) self. deps = set()  def shift(self): self. stack. append(self. buffer. pop()) def left_arc(self, label): self. deps. add( (self. buffer[-1], self. stack. pop(), label) ) def right_arc(self, label): parent = self. stack. pop() self. deps. add( (parent, self. buffer. pop(), label) ) self. buffer. append(parent) def __repr__(self): return \"{}, {}, {}\". format(self. stack, self. buffer, self. deps)def apply_sequence(seq, sentence): state = State(sentence) for rel, label in seq: if rel == \"shift\": state. shift() elif rel == \"left_arc\": state. left_arc(label)  elif rel == \"right_arc\": state. right_arc(label)  return state. depsclass RootDummy(object): def __init__(self): self. head = None self. id = 0 self. deprel = None  def __repr__(self): return \"&lt;ROOT&gt;\"def get_training_instances(dep_structure): deprels = dep_structure. deprels sorted_nodes = [k for k, v in sorted(deprels. items())] state = State(sorted_nodes) state. stack. append(0) childcount = defaultdict(int) for ident, node in deprels. items(): childcount[node. head] += 1 seq = [] while state. buffer:  if not state. stack: seq. append((copy. deepcopy(state), (\"shift\", None))) state. shift() continue if state. stack[-1] == 0: stackword = RootDummy()  else: stackword = deprels[state. stack[-1]] bufferword = deprels[state. buffer[-1]] if stackword. head == bufferword. id: childcount[bufferword. id]-=1 seq. append((copy. deepcopy(state), (\"left_arc\", stackword. deprel))) state. left_arc(stackword. deprel) elif bufferword. head == stackword. id and childcount[bufferword. id] == 0: childcount[stackword. id]-=1 seq. append((copy. deepcopy(state), (\"right_arc\", bufferword. deprel))) state. right_arc(bufferword. deprel) else:  seq. append((copy. deepcopy(state), (\"shift\", None))) state. shift() return seq dep_relations = ['tmod', 'vmod', 'csubjpass', 'rcmod', 'ccomp', 'poss', 'parataxis', 'appos', 'dep', 'iobj', 'pobj', 'mwe', 'quantmod', 'acomp', 'number', 'csubj', 'root', 'auxpass', 'prep', 'mark', 'expl', 'cc', 'npadvmod', 'prt', 'nsubj', 'advmod', 'conj', 'advcl', 'punct', 'aux', 'pcomp', 'discourse', 'nsubjpass', 'predet', 'cop', 'possessive', 'nn', 'xcomp', 'preconj', 'num', 'amod', 'dobj', 'neg', 'dt', 'det']class FeatureExtractor(object): def __init__(self, word_vocab_file, pos_vocab_file): self. word_vocab = self. read_vocab(word_vocab_file)  self. pos_vocab = self. read_vocab(pos_vocab_file)  self. output_labels = self. make_output_labels() def make_output_labels(self): labels = [] labels. append(('shift', None)) for rel in dep_relations: labels. append((\"left_arc\", rel)) labels. append((\"right_arc\", rel)) return dict((label, index) for (index, label) in enumerate(labels)) def read_vocab(self, vocab_file): vocab = {} for line in vocab_file:  try: word, index_s = line. strip(). split() index = int(index_s) vocab[word] = index except ValueError: print(f\"Problem with line: '{line}'\") return vocab  def get_word_index(self, idx, words, pos): \"\"\"Utility function to convert word index to its actual index, handling special tokens. \"\"\" if idx == -1:  return self. word_vocab[\"&lt;NULL&gt;\"] word = words[idx] if idx == 0:  word = \"&lt;ROOT&gt;\" #print(\"word\", word) pos_tag = pos[idx] if word == \"&lt;UNK&gt;\": return self. word_vocab[\"&lt;UNK&gt;\"] elif word == \"&lt;ROOT&gt;\": return self. word_vocab[\"&lt;ROOT&gt;\"] elif pos_tag == \"CD\": # Check POS tag for numbers return self. word_vocab[\"&lt;CD&gt;\"] elif pos_tag == \"NNP\": # Check POS tag for proper nouns return self. word_vocab[\"&lt;NNP&gt;\"] else: return self. word_vocab. get(word. lower(), self. word_vocab[\"&lt;UNK&gt;\"]) def get_input_representation(self, words, pos, state): # TODO: Write this method for Part 2 # Fetch the top three items from the buffer and stack print(state) buffer_top3 = [state. buffer[-(i+1)] if i &lt; len(state. buffer) else -1 for i in range(3)]  print(\"buffer:\", buffer_top3) stack_top3 = [state. stack[-(i+1)] if i &lt; len(state. stack) else -1 for i in range(3)] print(\"stack:\", stack_top3) # Convert words to indices using the utility function buffer_indices = [self. get_word_index(idx, words, pos) for idx in buffer_top3] stack_indices = [self. get_word_index(idx, words, pos) for idx in stack_top3] # Concatenate to get the final representation representation = stack_indices + buffer_indices print(\"representations\", representation) return np. array(representation) def get_output_representation(self, output_pair):  # TODO: Write this method for Part 2 idx = self. output_labels. get(output_pair, None) if idx is None: raise ValueError(f\"Invalid output pair: {output_pair}\") # Create a one-hot vector of length 91 one_hot_vector = [0] * 91 one_hot_vector[idx] = 1 #print(one_hot_vector) return np. array(one_hot_vector)def get_training_matrices(extractor, in_file): inputs = [] outputs = [] count = 0  for dtree in conll_reader(in_file):  words = dtree. words() pos = dtree. pos() for state, output_pair in get_training_instances(dtree): inputs. append(extractor. get_input_representation(words, pos, state)) outputs. append(extractor. get_output_representation(output_pair)) if count%100 == 0: sys. stdout. write(\". \") sys. stdout. flush() count += 1 sys. stdout. write(\"\\n\") print(np. vstack(inputs), np. vstack(outputs)) return np. vstack(inputs), np. vstack(outputs)if __name__ == \"__main__\": if len(sys. argv) &gt; 1 and sys. argv[1] == \"test\": test_get_output_representation() else: WORD_VOCAB_FILE = 'data/words. vocab' POS_VOCAB_FILE = 'data/pos. vocab' try: word_vocab_f = open(WORD_VOCAB_FILE, 'r') pos_vocab_f = open(POS_VOCAB_FILE, 'r')  except FileNotFoundError: print(\"Could not find vocabulary files {} and {}\". format(WORD_VOCAB_FILE, POS_VOCAB_FILE)) sys. exit(1)  with open(sys. argv[1], 'r') as in_file:  extractor = FeatureExtractor(word_vocab_f, pos_vocab_f) print(\"Starting feature extraction. .. (each . represents 100 sentences)\") inputs, outputs = get_training_matrices(extractor, in_file) print(\"Writing output. .. \") np. save(sys. argv[2], inputs) np. save(sys. argv[3], outputs)Output: [0, 3, 7, 8, 11, 12], [61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13], {(11, 10, 'amod'), (2, 1, 'nn'), (7, 5, 'nsubjpass'), (3, 2, 'nsubj'), (5, 4, 'poss'), (7, 6, 'auxpass'), (11, 9, 'amod')}buffer: [13, 14, 15]stack: [12, 11, 8]representations [7507, 2239, 7507, 1, 12237, 8230][0, 3, 7, 8, 11], [61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 12], {(12, 13, 'pobj'), (11, 10, 'amod'), (2, 1, 'nn'), (7, 5, 'nsubjpass'), (3, 2, 'nsubj'), (5, 4, 'poss'), (7, 6, 'auxpass'), (11, 9, 'amod')}buffer: [12, 14, 15]stack: [11, 8, 7]representations [2239, 7507, 6921, 7507, 12237, 8230][0, 3, 7, 8], [61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 11], {(12, 13, 'pobj'), (11, 10, 'amod'), (2, 1, 'nn'), (7, 5, 'nsubjpass'), (3, 2, 'nsubj'), (11, 12, 'prep'), (5, 4, 'poss'), (7, 6, 'auxpass'), (11, 9, 'amod')}buffer: [11, 14, 15]stack: [8, 7, 3]representations [7507, 6921, 8546, 2239, 12237, 8230][0, 3, 7, 8, 11], [61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14], {(12, 13, 'pobj'), (11, 10, 'amod'), (2, 1, 'nn'), (7, 5, 'nsubjpass'), (3, 2, 'nsubj'), (11, 12, 'prep'), (5, 4, 'poss'), (7, 6, 'auxpass'), (11, 9, 'amod')}buffer: [14, 15, 16]stack: [11, 8, 7]representations [2239, 7507, 6921, 12237, 8230, 5548][0, 3, 7, 8], [61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 11], {(12, 13, 'pobj'), (11, 10, 'amod'), (2, 1, 'nn'), (11, 14, 'punct'), (7, 5, 'nsubjpass'), (3, 2, 'nsubj'), (11, 12, 'prep'), (5, 4, 'poss'), (7, 6, 'auxpass'), (11, 9, 'amod')}buffer: [11, 15, 16]stack: [8, 7, 3]representations [7507, 6921, 8546, 2239, 8230, 5548]",
        "answers": []
    },
    "369": {
        "question": "Hi! I know there have been several posts about loss values, but I haven't seen anyone else mention loss values of 0. Whenever I run my code, the loss value only shows as 0. All my code from part 2 is running as predicted, and I don't see any obvious errors in the build_model function. Does anyone have any idea what might be causing this behavior? Thanks!",
        "answers": [
            "Hey Anonymous, If you are getting loss values of 0 it is most likely because your input feature vector is not in the correct format and doesn't match up at all with how you are training it. I would recommend looking specifically at the example the professor gives you in the spec with \"dog eats a\" and \"the\" and &lt;ROOT&gt; are on the stack. Pay attention to the order of how he mentions it and then also how it is put in root and whatnot. IE: it is stack for the first 3 and then buff for the last 3. Also, make sure it is numpy of length (6, )Your Favorite Nguyen, Nguyen Quoc Tran &lt;3"
        ]
    },
    "370": {
        "question": "Hi, I'm consistently getting a loss of 1. 6 and can't figure out why. Anything stick out here as wrong? I've pasted the relevant code segments below. Thanks. def get_input_representation(self, words, pos, state):\n    # TODO: Write this method for Part 2    val = self. word_vocab['&lt;NULL&gt;']\n    sol = np. array([val, val, val, val, val, val])\n\n    for i in range(min(len(state. stack), 3)):\n        if pos[state. stack[-(i+1)]] == 'CD':\n            sol[i] = self. word_vocab['&lt;CD&gt;']\n        elif pos[state. stack[-(i+1)]] == 'NNP':\n            sol[i] = self. word_vocab['&lt;NNP&gt;']\n        elif pos[state. stack[-(i+1)]] == 'UNK':\n            sol[i] = self. word_vocab['&lt;UNK&gt;']\n        elif pos[state. stack[-(i+1)]] is None:\n            sol[i] = self. word_vocab['&lt;ROOT&gt;']\n        elif pos[state. stack[-(i+1)]] == 'NULL':\n            sol[i] = self. word_vocab['&lt;NULL&gt;']\n        else:\n            if words[state. stack[-(i+1)]] not in self. word_vocab:\n                sol[i] = self. word_vocab['&lt;UNK&gt;']\n            else:\n                sol[i] = self. word_vocab[words[state. stack[-(i+1)]]]\n\n    for i in range(min(len(state. buffer), 3)):\n        if pos[state. buffer[-(i+1)]] == 'CD':\n            sol[i + 2] = self. word_vocab['&lt;CD&gt;']\n        elif pos[state. buffer[-(i+1)]] == 'NNP':\n            sol[i + 2] = self. word_vocab['&lt;NNP&gt;']\n        elif pos[state. buffer[-(i+1)]] == 'UNK':\n            sol[i + 2] = self. word_vocab['&lt;UNK&gt;']\n        elif pos[state. buffer[-(i+1)]] is None:\n            sol[i + 2] = self. word_vocab['&lt;ROOT&gt;']\n        elif pos[state. buffer[-(i+1)]] == 'NULL':\n            sol[i + 2] = self. word_vocab['&lt;NULL&gt;']\n        else:\n            if words[state. buffer[-(i+1)]] not in self. word_vocab:\n                sol[i + 2] = self. word_vocab['&lt;UNK&gt;']\n            else:\n                sol[i + 2] = self. word_vocab[words[state. buffer[-(i+1)]]]\n\n    return sol\n\ndef get_output_representation(self, output_pair):\n    # TODO: Write this method for Part 2    sol = np. zeros(91)\n    sol[self. output_labels[output_pair]] = 1\n    return sol\n\n\ndef build_model(word_types, pos_types, outputs):\r\n    # TODO: Write this function for part 3\r\n    model = Sequential()\r\n    model. add(keras. layers. Embedding(input_dim=word_types, output_dim=32, input_length=6))\r\n    model. add(keras. layers. Flatten())\r\n    model. add(keras. layers. Dense(100, activation='relu'))\r\n    model. add(keras. layers. Dense(10, activation='relu'))\r\n    model. add(keras. layers. Dense(91, activation='softmax'))\r\n    model. compile(keras. optimizers. Adam(lr=0. 01), loss=\"categorical_crossentropy\")\r\n    return model",
        "answers": []
    },
    "371": {
        "question": "Hi, My current build for tensorflow on WSL for windows 11 successfully runs on Nvidia GPU, but fails to support cuDNN, cuFFT, cuBLAS, and NUMA (see image below), despite numerous attempts to fix these issues. In this homework my runtime is acceptable (slightly over 10 mins on part 4 without eager execution) and the score is greater than 0. 7, but I wonder if in future assignments these libraries would be necessary to ensure performance. Thank you!",
        "answers": [
            "No I dont think you will need those. "
        ]
    },
    "372": {
        "question": "Hi, Once I add tf. compat. v1. disable_eager_execution(), the runtime for part 4 is reduced to less than a minute when using just the CPU. In contrast, the runtime is approximately 2 hours without tf. compat. v1. disable_eager_execution(). Does this speed improvement seem plausible? The accuracy appears fine, but I'm concerned about whether such a significant runtime reduction does not negatively affect the implementation.",
        "answers": [
            "Yes that seems reasonable. :) Essentially with eager execution, rather than using the compiled computation graph, the model is evaluated layer by layer. This is good for debugging and implementing certain control flow mechanism, but it slows things down considerably. "
        ]
    },
    "373": {
        "question": "Hi, I'm using Anaconda's virtual environment, and on that environment, I installed Tensorflow. The installation of Tensorflow was successful, but I could not use it and got the same error \"illegal hardware instruction\" in post #343. I tried both Python 3. 9 and 3. 11, and neither works. I wonder how I can solve this issue.",
        "answers": [
            "Hmm thats different from the increasing loss issue other people have encountered. Which version of tensorflow is this? You could always switch to colab. "
        ]
    },
    "374": {
        "question": "Question to understand things better: I ran my model locally (M2 Max Ventura) and it gave me a loss that was increasing in value and way greater than 1. When I ran the same code on Google Colab, the model trained as expected and gave me a more reasonable loss that got less than 0. 2956 after the 3rd epoch. I am curious why the training loss varies based on what device we train the model on even though the training data and model structure stays the same. Is it that tensorflow computes calculations differently across different device packages? ",
        "answers": [
            "To be honest I dont quite understand whats going on there. Its definitely an issue in the M1/M2 version of tensorflow. I initially thought only the GPU (tensorflow-Metal) version was affected, but it seems to actually affect the CPU version as well. I think the most recent version of the Adam optimizer is at fault. There is a way to use a legacy version  maybe something to look up? "
        ]
    },
    "375": {
        "question": "I was wondering if I should be concerned with my model loss (around 0. 39 after epoch 5) as it's a bit higher than what other people have got. Despite this, I get a Labeled Attachment Score of around 0. 7 and an Unlabeled Attachment Score of around 0. 75. Should I still look at my model more closely? Thanks!",
        "answers": [
            "That seems okay. "
        ]
    },
    "376": {
        "question": "hi wondering if the OH will be in the CS TA room or online? Thanks! ",
        "answers": [
            "It will be in the CS TA room."
        ]
    },
    "377": {
        "question": "Hello, I have two questions regarding part 4:1) When looking for legal transitions, are we to check if state. stack is empty, or if the stack that is presented in the 1x6 input representation vector is empty (or are both of these stacks the same structure)? 2) How are we to update the current state after we find a legal transition? My current implementation  uses conditional statements to apply a transition on the state given a legal action, but my output is returning None.",
        "answers": [
            "1) The 1x6 input representation vector should include the top three elements of the stack (or stack[-1], stack[-2], stack[-3]) in the first three indices. Thus, if the first three elements of the vector are all equal to the index for &lt;NULL&gt;, the state. stack would be empty, but I think it's more straightforward to just check if state. stack is empty. 2) The State class comes with functions that allow you to perform shift, left_arc, and right_arc, so calling those functions with the dependency label should update your state."
        ]
    },
    "378": {
        "question": "Hi, I get the following outputs in pt3 and pt4, not sure if they are okay or should I update my code to improve the result?",
        "answers": [
            "This looks great! Better than some other results I have seen. "
        ]
    },
    "379": {
        "question": "Hi! I was wondering what a good time/step would be for part 4 to make sure it won't take forever to run. Thanks!",
        "answers": [
            "(Not a TA)You could use the trick in #361. And it would take several minutes. ",
            "I'd say 5-10 minutes if you turn off eager-executions "
        ]
    },
    "380": {
        "question": "This is more of a conceptual question, but how were the densities and number of layers chosen here? For example, why 100 and then 10? And why only 2 hidden layers? ",
        "answers": [
            "I am also curious about the conceptual basis of this model as well, can you please explain how the \"Dense\" layers work? Based on the documentation, it appears that they apply a weight and an activation function to the previous layer's output, but I am not certain my understanding is correct.",
            "My understanding is that the number of hidden layers is up to personal choice and what you think is best for the problem you are trying to solve; basically it is arbitrary, three hidden layers could have probably done the job or even one, but finding a good balance between accuracy and training time is the main goal, and whenever you add hidden layers the training time goes up, especially for larger datasets like the one we are working with. A \"Dense\" layer is basically a normal neural network layer, similar to what we have seen in lecture. The values for these layers (such as 100 or 10) correspond to the number of nodes/input size of the layer, which depends on the value of the output of the previous layer/the general input. For \"Dense\" layers, you tend to have a lot more freedom on these values, as they usually can be any value, but remember, the more nodes you have, the more the training time increases, albeit while also most likely increasing accuracy."
        ]
    },
    "381": {
        "question": "When I'm trying to train the model, I get the Value Error above for the data shapes. In my part 2, I return the np arrays of shape (6, ) and (91, ) which I think is right. These get put into the npy files by the code provided. Do you know why these shapes could be incorrect and what I can do to fix this? ? ",
        "answers": [
            "I see this was answered with #368, disregard! https://edstem. org/us/courses/46417/discussion/3797602 "
        ]
    },
    "382": {
        "question": "Hi everybody, I know there have been a couple of posts about the loss values already but I think all of them have been about getting extremely large values or showing values as low as . 2. My loss values just steadily stay around . 43 which is neither extremely high nor as low as many people are getting. Does anyone have any ideas on how to get it lower or are these values also ok? (For reference I'm running on CPU on my local Windows machine). Thanks!",
        "answers": [
            "That seems okay to me. I would try to see what happens when you continue with this model in part 4. ",
            "I was also getting similar pattern and loss values when running locally on CPU. (I'm on a Macbook whose OS is quite old). I was able to get ~0. 25 loss value by the end of 5 epoch when running the same code on Google Colab, so I was suspecting that there may be potential issues with the Keras or tensorflow installed on my local computer. "
        ]
    },
    "383": {
        "question": "Hello TAs, I am getting low accuracy scores for part 4 and can't seem to figure out why.  I'm posting my code from the decoder with explanations here to see if anyone can find where my process is wrong. First, I am getting the feature input, then I define which actions are permissible.  If the stack if empty, the only action permissible is shift.  If the buffer has only one item on it and the stack is not empty, left_arc and right_arc are permissible but shift is not.  If the top of the stack contains root, only shift and right_arc are possible, not left_arc.  Otherwise, all 3 actions are permissible.  Next, I get the predicted outputs using the feature input.  Then, I iterate through output_labels and make a list of indices that only correspond to permissible actions (basically creating a list of transitions that are possible). For all of these possible indices, I create a list of tuples (index, probability) that correspond to the probabilities of those transitions.  This gives me a list of permissible transition indices in output_labels along with the predicted probability associated with that transition.  I sort this list by probability in reverse order so the largest probability is at the front of the list.  Finally, I take the first item in the list and use it to obtain the best transition along with its label and call the appropriate function on the state (shift, left_arc, or right_arc). Doing this results in:5039 sentence. Micro Avg. Labeled Attachment Score: 0. 19105660721540924Micro Avg. Unlabeled Attachment Score: 0. 45831364062253843Macro Avg. Labeled Attachment Score: 0. 1977219277981588Macro Avg. Unlabeled Attachment Score: 0. 4580644216796739These values are clearly too low, but I'm not sure where I'm making a mistake.  My loss value when training the model was ~0. 28 after 5 epochs.     def parse_sentence(self, words, pos):\n        state = State(range(1, len(words)))\n        state. stack. append(0)    \n\n        while state. buffer: \n            # pass\n            # TODO: Write the body of this loop for part 4\n            input = self. extractor. get_input_representation(words, pos, state)\n\n            # get permitted actions\n            permitted_actions = []\n\n            if (len(state. stack) == 0):\n                permitted_actions. append('shift')\n            elif (len(state. buffer) == 1 and len(state. stack) ! = 0):\n                permitted_actions. append('left_arc')\n                permitted_actions. append('right_arc')\n            elif (state. stack[-1] == 3):\n                permitted_actions. append('shift')\n                permitted_actions. append('right_arc')\n            else:\n                permitted_actions. append('shift')\n                permitted_actions. append('left_arc')\n                permitted_actions. append('right_arc')\n\n            # predict actions from model\n            input_reshaped = input. reshape(1, -1)\n            all_actions = self. model. predict(input_reshaped)\n\n            # retain the indices of only those that are possible\n            possible_action_indices = []\n\n            for k, v in self. output_labels. items():\n                if (v[0] in permitted_actions):\n                    possible_action_indices. append(k)\n\n            # create list of probabilities of possible actions\n            possible_actions = []\n\n            for i in possible_action_indices:\n                tup = (i, all_actions[0][i])\n                possible_actions. append(tup)\n\n            # sort the list of possible actions\n            sorted_possible_actions = sorted(possible_actions, key=lambda x: x[1], reverse=True)\n\n            # select best predicted transition/label output\n            transition = self. output_labels[sorted_possible_actions[0][0]][0]\n            label = self. output_labels[sorted_possible_actions[0][0]][1]\n\n            # update state\n            if (transition == 'shift'):\n                state. shift()\n            if (transition == 'left_arc'):\n                state. left_arc(label)\n            if (transition == 'right_arc'):\n                state. right_arc(label)\n                \n        result = DependencyStructure()\n        for p, c, r in state. deps: \n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return result \n        \n",
        "answers": [
            "Okay, so I think I figured out what was wrong.  I was storing my output in a different representation than how it is ordered in output_values in the decoder.  I modified my get_output_representation function, re-extracted the input/output, retrained the model, and re-ran the evaluator.  I am now getting:Micro Avg. Labeled Attachment Score: 0. 6425544969859788Micro Avg. Unlabeled Attachment Score: 0. 6912846280772452Macro Avg. Labeled Attachment Score: 0. 6320441928046332Macro Avg. Unlabeled Attachment Score: 0. 6815717757362703These seem more reasonable, but still slightly low?  Would these values be sufficient for the assignment?"
        ]
    },
    "384": {
        "question": "Hello, I just wanted to ensure that my part 2 code is working as expected. After completing get_input_representation, my vector of length 6 is outputted as:[4. 4. 4. 2. 4. 4. ]Indicating that the stack is empty, and the top (only) item in the buffer is an unknown token. My intuition makes me believe that the top item in the buffer should be \"root\", not an unknown token, can you please correct my intuition if it its wrong?",
        "answers": [
            "Yeah. If there is only one token left, that one token should be root. "
        ]
    },
    "385": {
        "question": "Hello, Every time I run the command for part 0, I get a different words. vocab file. I have not been able to get it to match the spec (the first few lines of mine look like this): &lt;CD&gt;    0\n&lt;NNP&gt;    1\n&lt;UNK&gt;    2\n&lt;ROOT&gt;    3\n&lt;NULL&gt;    4\nsilly    5\nvaluing    6\nembezzling    7\nIs this intended behavior?",
        "answers": [
            "Hey Anonymous, That is the intended behavior. If you look at the spec it actually directly says \"(Your indices may differ from this example because the get_vocab script outputs a different index mapping each time it is run. )\". You can also look into how it is getting made and you will see that it will be a little diff each time. Because of the iteration. Happy to help, Nguyen Quoc Tran &lt;3"
        ]
    },
    "386": {
        "question": "For the files we submit, should there/can there be a subfolder \"data\" containing the model and vocab files?",
        "answers": [
            "Yes that would be a good idea. "
        ]
    },
    "387": {
        "question": "Hi, I'm unable to figure out the reason for my low scores in Part 4. My model in Part 3 had a loss of 0. 28, which I believe is within the valid range; I've thoroughly checked my parts 2-4 several times (including printing debug statements to check values) and reran my code a few times as well, but my score still remains the same. Does anyone have any idea what could be causing this? Thank you!",
        "answers": [
            "Hi! I had a similar issue and resolved it by recloning hw3 and running everything from scratch and then running the evaluate/decode commands. I'm not sure if I accidentally corrupted a data file or something or if this is the same issue as what I had, but hopefully this helps!",
            "If your loss values look normal and only the attachment scores look abnormally low, this might be an indication that something is off with your one-hot encoding. The mapping of indices to output actions in self. output_labels (inside of decoder. py) performs the one-hot encoding in a very particular way, so I would make sure that it's consistent with how you performed one-hot encoding in get_output_representation() of extract_training_data. py.",
            "Its also possible that there is an issue with the decoder. Even though your transitions are predicted correctly, the way you select the best transition from the predicted probabilities may not be (make sure you are sorting in reverse order). ",
            "I also had that extremely low score. I ran my decoder evaluation function on colab and the numbers were fixed!"
        ]
    },
    "388": {
        "question": "Hi, I'm trying to test my part 3 but I can't figure out how to run the given command on Colab since input_train. npy, target_train. npy, and model. h5 don't exist in data. ! python '/content/drive/MyDrive/hw3_files/train_model. py' '/content/drive/MyDrive/hw3_files/data/input_train. npy' '/content/drive/MyDrive/hw3_files/data/target_train. npy' '/content/drive/MyDrive/hw3_files/data/model. h5' is the code I'm using but I get this error and I'm not sure how to fix it. ",
        "answers": [
            "input_train. npy and target_train. npy should exist from p2 -- I ran part 2 locally and then uploaded those files into my folder on Drive. It's okay that model. h5 doesn't exist yet in the Drive when running p3 since it will be created after training.",
            "You can use import os\nprint(os. getcwd())\n! ls . /drive/MyDrive/. ..\nto check the path."
        ]
    },
    "389": {
        "question": "Hello, I am using Colab w GPU so I assume it's not a hardware issues. My model is just like what's laid out in the description, and I even changed it according to other posts on Edstem, so can it be because of part2 ? (I am not sure how to test this). I don't know why the loss value is 9 digits and gets worse after each iteration. Any pointers on this would be so appreciated. Thanks all.",
        "answers": [
            "Yes I suspect there is something wrong with your training data, unfortunately. I would recommend trying it out on a single training sentence so that you can verify that the resulting input and output representations are correct. Just copy the sec0. conll file and delete everything except for one sentence. "
        ]
    },
    "390": {
        "question": "Hi, does anyone know what the zoom passcode is for Vedangi's OH right now? Thanks",
        "answers": []
    },
    "391": {
        "question": "Hi, If this is what my build_model is looking like But my loss values are very large (example: 9645028344. 0000). I've been modifying input/output representations, but am unsure what the root cause of this is. Am I right in assuming that this is an input/output problem given my build_model?",
        "answers": []
    },
    "392": {
        "question": "How are we supposed to deal with \"None\" in the get_input_representation function? Should we assign it the POS &lt;NULL&gt; or is there another value that would make more sense here? Thanks!",
        "answers": [
            "None represents the ROOT POS. If you see the code, you will observe that it is added to the front of each sentence to serve as the Root."
        ]
    },
    "393": {
        "question": "This isn't about the inconsistency between mac and windows that other people have mentioned. But, when i run the final evaluate script, roughly half the time there's some error in the execution (tensorflow or threading probably) and it either fails or doesn't terminate. When it does work, I get acceptable results, but I'm concerned that I might be graded on a failing execution. Is that okay as long as it eventually works?",
        "answers": [
            "Whats the error message you get? The results you posted look good. "
        ]
    },
    "394": {
        "question": "Hi, Part 4 is timing out even when I disable eager execution. When attempting to improve my model, I was wondering if there should be an order of precedence between the different cases in get_input_representation. For instance, if a certain word is unknown and it has a special tag, which special case should it fall under? Thanks so much!",
        "answers": [
            "The special tag should take priority in that case. "
        ]
    },
    "395": {
        "question": "In the build_model function, what is the pos_types argument used for?",
        "answers": [
            "It is not used at all. Refer to this Ed post - https://edstem. org/us/courses/46417/discussion/3712855"
        ]
    },
    "396": {
        "question": "Hi, I just wanna check that we should submit 6 . py files2 . vocab files1 . h5 fileright?",
        "answers": [
            "Yes. "
        ]
    },
    "397": {
        "question": "Hello Professor and TA. I ran this part of my code and got two different results on Mac and Windows computers, I would like to ask if this result is acceptable? Thank you.",
        "answers": [
            "Hm people have reported similar observations. I dont really have any idea why. Did you train to the same loss on both machines? Either way, the results look good.  "
        ]
    },
    "398": {
        "question": "I just wanted to confirm, do these scores look reasonable?",
        "answers": [
            "The labeled scores seem a little low. Take a look at the output of decoder. py (the actual dependency trees) to see if there is any systematic issue you see with the labels. "
        ]
    },
    "399": {
        "question": "Hi Professor and TAs, The accuracy of my decoder seems to be extremely low (~0. 03). I've checked my methods carefully but am struggling to figure out where I went wrong. Any hint/tip would be greatly appreciated! Thank you.",
        "answers": [
            "Check the actual output of the decoder (the dependency trees printed by decoder. py). You may be able to see some systematic issue. "
        ]
    },
    "400": {
        "question": "Hi, For part 4, when I make a prediction, can I use predict_on_batch instead of predict?",
        "answers": [
            "You will have to make predictions for one input at a time, unfortunately. "
        ]
    },
    "401": {
        "question": "Hi, my decoder. py doesn't stop running, it keeps outputting:. .. .\n1/1 [==============================] - 0s 9ms/step\n1/1 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 8ms/step\n1/1 [==============================] - 0s 8ms/step\n. .. .\n\nAnd when I use:import tensorflow as tf\ntf. compat. v1. disable_eager_execution()\nI get this message in my terminal:zsh: suspended  python evaluate. py data/model. h5 data/dev. conll\n(base) omernauer@macbook-pro-689 hw3_files % python evaluate. py data/model. h5 data/dev. conll\n2023-11-06 13:59:37. 137752: I tensorflow/compiler/mlir/mlir_graph_optimization_pass. cc:382] MLIR V1 optimization pass is not enabled\n2023-11-06 13:59:37. 145617: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_2/kernel/Assign' id:82 op device:{requested: '', assigned: ''} def:{{{node dense_2/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/kernel, dense_2/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n2023-11-06 13:59:37. 211473: W tensorflow/c/c_api. cc:305] Operation '{name:'embedding/embeddings/m/Assign' id:200 op device:{requested: '', assigned: ''} def:{{{node embedding/embeddings/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](embedding/embeddings/m, embedding/embeddings/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\nEvaluating. (Each . represents 100 test dependency trees)\n/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training_v1. py:2359: UserWarning: `Model. state_updates` will be removed in a future version. This property should not be used in TensorFlow 2. 0, as `updates` are applied automatically.\n  updates=self. state_updates,\n2023-11-06 13:59:37. 260652: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_2/Softmax' id:93 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_2/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\nAnd it doesn't stop running either. When I look at the ConLL sentences, I see that it stops at the sentence:1   In _  IN IN _  5  prep   _  _\n2  other  _  JJ JJ _  4  amod   _  _\n3  commodity  _  NN NN _  4  nn _  _\n4  markets    _  NNS    NNS    _  1  pobj   _  _\n5  yesterday  _  NN NN _  0  root   _  _\n6  :  _  :  :  _  5  punct  _  _and I'm assuming it gets stuck on the next sentence:1   ENERGY _  NN NN _  0  root   _  _\n2  :  _  :  :  _  1  punct  _  _\n3  Crude  _  JJ JJ _  6  amod   _  _\n4  oil    _  NN NN _  6  nn _  _\n5  futures    _  NNS    NNS    _  6  nn _  _\n6  prices _  NNS    NNS    _  7  nsubj  _  _\n7  increased  _  VBD    VBD    _  1  dep    _  _\n8  in _  IN IN _  7  prep   _  _\n9  moderate   _  JJ JJ _  10 amod   _  _\n10 trading    _  NN NN _  8  pobj   _  _\n11 ,  _  ,  ,  _  7  punct  _  _\n12 but    _  CC CC _  7  cc _  _\n13 much   _  JJ JJ _  17 nsubj  _  _\n14 of _  IN IN _  13 prep   _  _\n15 the    _  DT DT _  16 det    _  _\n16 action _  NN NN _  14 pobj   _  _\n17 was    _  VBD    VBD    _  7  conj   _  _\n18 in _  IN IN _  17 prep   _  _\n19 heating    _  NN NN _  20 nn _  _\n20 oil    _  NN NN _  18 pobj   _  _\n21 .  _  .  .  _  1  punct  _  _Extract_training_data. py and model_training. py work for me. Here is their code:Extract_training_data. py:from conll_reader import DependencyStructure, conll_reader\nfrom collections import defaultdict\nimport copy\nimport sys\nimport keras\nimport numpy as np\nimport tensorflow as tf\n\nclass State(object):\n    def __init__(self, sentence = []):\n        self. stack = []\n        self. buffer = []\n        if sentence:\n            self. buffer = list(reversed(sentence))\n        self. deps = set()\n\n    def shift(self):\n        self. stack. append(self. buffer. pop())\n\n    def left_arc(self, label):\n        self. deps. add( (self. buffer[-1], self. stack. pop(), label) )\n\n    def right_arc(self, label):\n        parent = self. stack. pop()\n        self. deps. add( (parent, self. buffer. pop(), label) )\n        self. buffer. append(parent)\n\n    def __repr__(self):\n        return \"{}, {}, {}\". format(self. stack, self. buffer, self. deps)\n\n\n\ndef apply_sequence(seq, sentence):\n    state = State(sentence)\n    for rel, label in seq:\n        if rel == \"shift\":\n            state. shift()\n        elif rel == \"left_arc\":\n            state. left_arc(label)\n        elif rel == \"right_arc\":\n            state. right_arc(label)\n\n    return state. deps\n\nclass RootDummy(object):\n    def __init__(self):\n        self. head = None\n        self. id = 0\n        self. deprel = None\n    def __repr__(self):\n        return \"&lt;ROOT&gt;\"\n\n\ndef get_training_instances(dep_structure):\n\n    deprels = dep_structure. deprels\n\n    sorted_nodes = [k for k, v in sorted(deprels. items())]\n    state = State(sorted_nodes)\n    state. stack. append(0)\n\n    childcount = defaultdict(int)\n    for ident, node in deprels. items():\n        childcount[node. head] += 1\n\n    seq = []\n    while state. buffer:\n        if not state. stack:\n            seq. append((copy. deepcopy(state), (\"shift\", None)))\n            state. shift()\n            continue\n        if state. stack[-1] == 0:\n            stackword = RootDummy()\n        else:\n            stackword = deprels[state. stack[-1]]\n        bufferword = deprels[state. buffer[-1]]\n        if stackword. head == bufferword. id:\n            childcount[bufferword. id]-=1\n            seq. append((copy. deepcopy(state), (\"left_arc\", stackword. deprel)))\n            state. left_arc(stackword. deprel)\n        elif bufferword. head == stackword. id and childcount[bufferword. id] == 0:\n            childcount[stackword. id]-=1\n            seq. append((copy. deepcopy(state), (\"right_arc\", bufferword. deprel)))\n            state. right_arc(bufferword. deprel)\n        else:\n            seq. append((copy. deepcopy(state), (\"shift\", None)))\n            state. shift()\n    return seq\n\n\ndep_relations = ['tmod', 'vmod', 'csubjpass', 'rcmod', 'ccomp', 'poss', 'parataxis', 'appos', 'dep', 'iobj', 'pobj', 'mwe', 'quantmod', 'acomp', 'number', 'csubj', 'root', 'auxpass', 'prep', 'mark', 'expl', 'cc', 'npadvmod', 'prt', 'nsubj', 'advmod', 'conj', 'advcl', 'punct', 'aux', 'pcomp', 'discourse', 'nsubjpass', 'predet', 'cop', 'possessive', 'nn', 'xcomp', 'preconj', 'num', 'amod', 'dobj', 'neg', 'dt', 'det']\n\n\nclass FeatureExtractor(object):\n\n    def __init__(self, word_vocab_file, pos_vocab_file):\n        self. word_vocab = self. read_vocab(word_vocab_file)\n        self. pos_vocab = self. read_vocab(pos_vocab_file)\n        self. output_labels = self. make_output_labels()\n        self. count = 0\n\n    def make_output_labels(self):\n        labels = []\n        labels. append(('shift', None))\n\n        for rel in dep_relations:\n            labels. append((\"left_arc\", rel))\n            labels. append((\"right_arc\", rel))\n        return dict((label, index) for (index, label) in enumerate(labels))\n\n    def read_vocab(self, vocab_file):\n        vocab = {}\n        for line in vocab_file:\n            word, index_s = line. strip(). split()\n            index = int(index_s)\n            vocab[word] = index\n        return vocab\n\n    def get_input_representation(self, words, pos, state):\n\n        self. count += 1\n\n        # TODO: Write this method for Part 2\n\n        word_list = []\n        word_dict = {index: word for index, word in enumerate(words)}  # creating an index dict\n        pos_dict = {index: word for index, word in enumerate(pos)}  # creating an index dict\n\n        # building list (4)\n\n        if len(state. stack) == 0:\n            word_list. append(int(4))\n            word_list. append(int(4))\n            word_list. append(int(4))\n        else:\n            i = 1\n            while i &lt;= len(state. stack) and i &lt; 4:\n                word_list. append(str(state. stack[-i]))\n                i += 1\n            if len(state. stack) &lt; 3:\n                for x in range(0, 3-len(state. stack)):\n                    word_list. append(int(4))\n\n        if len(state. buffer) == 0:\n            word_list. append(int(4))\n            word_list. append(int(4))\n            word_list. append(int(4))\n        else:\n            j = 1\n            while j &lt;= len(state. buffer) and j &lt; 4:\n                word_list. append(str(state. buffer[-j]))\n                j += 1\n            if len(state. buffer) &lt; 3:\n                for x in range(0, 3-len(state. buffer)):\n                    word_list. append(int(4))\n\n        # Check ROOT (3)\n        for i in range(0, len(word_list)):\n            if word_list[i] == '0':   # if it's position is 0, it's the root\n                word_list[i] = int(3)\n\n        # Check NNP (1) and CD (0)\n        t = 0\n        for w in word_list:\n            if isinstance(w, str) and (pos_dict[int(word_list[t])] == 'NNP'):\n                word_list[t] = int(1)\n            elif isinstance(w, str) and (pos_dict[int(word_list[t])] == 'CD'):\n                word_list[t] = int(0)\n            t += 1\n\n        # Check UNK (2)\n        t = 0\n        for w in word_list:\n            if isinstance(w, str) and word_dict[int(word_list[t])] is not None:\n                if word_dict[int(word_list[t])] not in self. word_vocab. keys():\n                    word_list[t] = int(2)\n            t += 1\n\n        # Swap word ids with indices\n        t = 0\n        for w in word_list:\n            if isinstance(w, str):\n                word_list[t] = int(self. word_vocab[word_dict[int(word_list[t])]])\n            t += 1\n\n        word_emb = np. array([word_list])\n        return word_emb\n\n    def get_output_representation(self, output_pair):\n        # TODO: Write this method for Part 2\n        dep_dict = {rel: index for index, rel in enumerate(dep_relations)}  # creating an index dict\n        oh_vector = np. zeros(91)\n        transition, label = output_pair\n        if transition == 'left_arc':\n            oh_vector[1+2*dep_dict[str(label)]] = int(1)\n        elif transition == 'right_arc':\n            oh_vector[2+2*dep_dict[str(label)]] = int(1)\n        elif transition == 'shift':\n            oh_vector[0] = int(1)\n        return oh_vector\n\n\ndef get_training_matrices(extractor, in_file):\n    inputs = []\n    outputs = []\n    count = 0\n    for dtree in conll_reader(in_file):\n        words = dtree. words()\n        pos = dtree. pos()\n        for state, output_pair in get_training_instances(dtree):\n            inputs. append(extractor. get_input_representation(words, pos, state))\n            outputs. append(extractor. get_output_representation(output_pair))\n        if count%100 == 0:\n            sys. stdout. write(\". \")\n            sys. stdout. flush()\n        count += 1\n    sys. stdout. write(\"\\n\")\n    return np. vstack(inputs), np. vstack(outputs)\n\n\n\nif __name__ == \"__main__\":\n\n    WORD_VOCAB_FILE = 'data/words. vocab'\n    POS_VOCAB_FILE = 'data/pos. vocab'\n\n    try:\n        word_vocab_f = open(WORD_VOCAB_FILE, 'r')\n        pos_vocab_f = open(POS_VOCAB_FILE, 'r')\n    except FileNotFoundError:\n        print(\"Could not find vocabulary files {} and {}\". format(WORD_VOCAB_FILE, POS_VOCAB_FILE))\n        sys. exit(1)\n\n\n    with open(sys. argv[1], 'r') as in_file:\n\n        extractor = FeatureExtractor(word_vocab_f, pos_vocab_f)\n        print(\"Starting feature extraction. .. (each . represents 100 sentences)\")\n        inputs, outputs = get_training_matrices(extractor, in_file)\n        print(\"Writing output. .. \")\n        np. save(sys. argv[2], inputs)\n        np. save(sys. argv[3], outputs)\n\n\nmodel_training. py:from extract_training_data import FeatureExtractor\nimport sys\nimport numpy as np\nimport keras\nimport tensorflow as tf\nfrom keras import Sequential\nfrom keras. layers import Flatten, Embedding, Dense\n\n\ndef build_model(word_types, pos_types, outputs):\n    # TODO: Write this function for part 3\n    model = Sequential()\n    print(\"word types\", word_types)\n\n    # Embedding Layer\n    model. add(Embedding(input_dim=word_types, output_dim=32, input_length=6))\n\n    # Flatten Layer\n    model. add(Flatten())\n\n    # 2 Layers, 100, 10\n    model. add(Dense(100, activation='relu'))\n    model. add(Dense(10, activation='relu'))\n\n    # Output Layer with softmax\n    model. add(Dense(91, activation=keras. activations. softmax))\n\n    model. compile(keras. optimizers. legacy. Adam(learning_rate=0. 01), loss=\"categorical_crossentropy\")\n    model. summary()\n    return model\n\n\nif __name__ == \"__main__\":\n\n    WORD_VOCAB_FILE = 'data/words. vocab'\n    POS_VOCAB_FILE = 'data/pos. vocab'\n\n    try:\n        word_vocab_f = open(WORD_VOCAB_FILE, 'r')\n        pos_vocab_f = open(POS_VOCAB_FILE, 'r')\n    except FileNotFoundError:\n        print(\"Could not find vocabulary files {} and {}\". format(WORD_VOCAB_FILE, POS_VOCAB_FILE))\n        sys. exit(1)\n\n    extractor = FeatureExtractor(word_vocab_f, pos_vocab_f)\n    print(\"Compiling model. \")\n    model = build_model(len(extractor. word_vocab), len(extractor. pos_vocab), len(extractor. output_labels))\n    inputs = np. load(sys. argv[1])\n    outputs = np. load(sys. argv[2])\n    print(\"Done loading data. \")\n\n    # Now train the model\n    model. fit(inputs, outputs, epochs=5, batch_size=100)\n\n    model. save(sys. argv[3])\nHow long should this process take? What can I do to speed it up or at least know that I'm on the right track?",
        "answers": []
    },
    "402": {
        "question": "Hi, I want to double-check that whether the last day of this course is Mon 12/11 for exam 2? Thanks.",
        "answers": [
            "That's correct. "
        ]
    },
    "403": {
        "question": "Hello TAs and Professor, Is the following output acceptable? Thank you, Shimon",
        "answers": [
            "Yes, that looks okay to me. "
        ]
    },
    "404": {
        "question": "Hi, My losses are EXTREMELY large. I think my build_model is fine because I compared it to a previous post (which you had told them that theirs looked fine). Ive pretty much boiled it down to how I am handling input/output representation. The vectors are both correctly formatted with the correct lengths respectively and everything compiles, but the loss results are clearly terrible. Wsa just wondering if my thought process of focusing on input/output was on the right track and if there are any insights/tips anyone has. Below is the summary of the losses (2020 Macbook Pro on Google Colab):Epoch 1/5 18996/18996 [==============================] - 146s 8ms/step - loss: 86050283520. 0000 Epoch 2/5 18996/18996 [==============================] - 143s 8ms/step - loss: 1137886625792. 0000 Epoch 3/5 18996/18996 [==============================] - 151s 8ms/step - loss: 4919651205120. 0000 Epoch 4/5 18996/18996 [==============================] - 147s 8ms/step - loss: 14147200745472. 0000 Epoch 5/5 18996/18996 [==============================] - 146s 8ms/step - loss: 32414466310144. 0000Thanks!",
        "answers": [
            "Hm, that almost looks to me like an issue with the model. The predicted vector will all have values between 0 and 1, and the target should be 1-hot. The reported loss is actually the average loss of the batches, not the sum of losses of the epoch. ",
            "I am facing the same issue, was anyone able to solve it?"
        ]
    },
    "405": {
        "question": "Hi! I get the following error when trying to run $ python -m pip install tensorflow. I have deduced that it is an issue with Windows 11. Is there an easier workaround for this besides importing the code into Colab when I want to train the model?",
        "answers": [
            "Hi, this is due to window limiting its path to &lt;260 characters iirc. What you have to do is moving tensorflow folder to another directory with shorter path or enable long-path. Step: Open \"Run\" (The window app), you should be able to look it up w window search bar. Type \"%systemroot%\\syswow64\\regedit\". This should pops up sth called \"Registry Editor\", which resembles a big directory. You should follow this path into the subfolders: Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem. You can either paste the path or just manually click on them until you get to  \"FileSystem\". At this point, you should see \"LongPathEnabled\", click on it and set value from 0-&gt;1. If this works for you, lmk so I can put it on my academic highlight mixtapes for gradschool"
        ]
    },
    "406": {
        "question": "Hello, I was curious as to why I have a different output when running with eager execution disabled. When it is disabled, there is no constant stream of outputted text, as opposed to when it is enabled. Is this to be expected? ",
        "answers": [
            "You mean the progress-bar information isn't showing up? "
        ]
    },
    "407": {
        "question": "I'm a little confused on the input being used in this part.  When it comes to parsing sentences and making predictions on the transitions, should we be extracting the input representation of a given state of the sentence using get_input_representation that we created in part 2?",
        "answers": [
            "That's right, you'll be using get_input_representation on the current state of the parser. "
        ]
    },
    "408": {
        "question": "I'm using Colab for this assignment. For part 4, I already turned off \"eager execution\" for TensorFlow, but it has been running for 40 minutes and still has not finished. I wonder approximately how long it takes to run each of these two executions on Colab:1. python decoder. py data/model. h5 data/dev. conll2. python evaluate. py data/model. h5 data/dev. conllWill the evaluation step take a shorter amount of time? If it's running too long, does it indicate there is an error? Thanks in advance! \"",
        "answers": [
            "That seems a bit long. Make sure you are correctly updating the parser state after each prediction, otherwise you may end up in an infinite loop. You could try to print out the current state and see if it repeats. "
        ]
    },
    "409": {
        "question": "Hello Professor and TA. I am running the same code and getting different loss results on Windows and Mac computers. windows computer has a lower loss. what should I do in this case? Thank you for being so helpful and understand. ",
        "answers": [
            "It may based on the calculation ability about the chip? I guess.",
            "Assuming that the second screenshot is from mac, I have very similar results. Also very interested in knowing the answer. ",
            "Hu, I assume that is running on the CPU in both cases (I. e. not GPU) "
        ]
    },
    "410": {
        "question": "Hi everyone, To run the program on Colab, do we have to create a notebook and then upload all the files into it? Thanks!",
        "answers": [
            "I don't know if I'm doing it the most efficient way, but I put all my files into a Google Drive folder and then am accessing them through my notebook."
        ]
    },
    "411": {
        "question": "Hi, When running:python train_model. py data/input_train. npy data/target_train. npy data/model. h5\nI get the error:Error: \"Cast string to float not supported\"\nThis is my code:def build_model(word_types, pos_types, outputs):\n    # TODO: Write this function for part 3\n    model = Sequential()\n\n    # Embedding Layer\n    model. add(Embedding(input_dim=word_types, output_dim=32, input_length=6))\n\n    # Flatten Layer\n    model. add(Flatten())\n\n    # 2 Layers, 100, 10\n    model. add(Dense(100, activation='relu'))\n    model. add(Dense(10, activation='relu'))\n\n    # Output Layer with softmax\n    model. add(Dense(91, activation=keras. activations. softmax))\n\n    model. compile(keras. optimizers. legacy. Adam(learning_rate=0. 01), loss=\"categorical_crossentropy\")\n    model. summary()\n    return model\nThis is the full error:Compiling model.\n/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/optimizers/legacy/adam. py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n  super(). __init__(name, **kwargs)\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 6, 32)             484896    \n                                                                 \n flatten (Flatten)           (None, 192)               0         \n                                                                 \n dense (Dense)               (None, 100)               19300     \n                                                                 \n dense_1 (Dense)             (None, 10)                1010      \n                                                                 \n dense_2 (Dense)             (None, 91)                1001      \n                                                                 \n=================================================================\nTotal params: 506207 (1. 93 MB)\nTrainable params: 506207 (1. 93 MB)\nNon-trainable params: 0 (0. 00 Byte)\n_________________________________________________________________\nDone loading data.\nEpoch 1/5\n2023-11-05 12:36:07. 078331: W tensorflow/core/framework/op_kernel. cc:1816] OP_REQUIRES failed at cast_op. cc:121 : UNIMPLEMENTED: Cast string to float is not supported\nTraceback (most recent call last):\n  File \"/Users/omernauer/Desktop/Columbia Studies/Fall 2023/NLP/hw3_files/train_model. py\", line 52, in &lt;module&gt;\n    model. fit(inputs, outputs, epochs=5, batch_size=100)\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 70, in error_handler\n    raise e. with_traceback(filtered_tb) from None\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/tensorflow/python/eager/execute. py\", line 60, in quick_execute\n    tensors = pywrap_tfe. TFE_Py_Execute(ctx. _handle, device_name, op_name,\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntensorflow. python. framework. errors_impl. UnimplementedError: Graph execution error:\n\nDetected at node sequential/Cast defined at (most recent call last):\n  File \"/Users/omernauer/Desktop/Columbia Studies/Fall 2023/NLP/hw3_files/train_model. py\", line 52, in &lt;module&gt;\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1783, in fit\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1377, in train_function\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1360, in step_function\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1349, in run_step\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 1126, in train_step\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/training. py\", line 589, in __call__\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 65, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/base_layer. py\", line 1149, in __call__\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/utils/traceback_utils. py\", line 96, in error_handler\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/sequential. py\", line 398, in call\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/functional. py\", line 515, in call\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/functional. py\", line 654, in _run_internal_graph\n\n  File \"/Users/omernauer/miniconda3/lib/python3. 11/site-packages/keras/src/engine/functional. py\", line 751, in _conform_to_reference_input\n\nCast string to float is not supported\n         [[{{node sequential/Cast}}]] [Op:__inference_train_function_760]\n\nAny help is appreciated!",
        "answers": [
            "I suspect theirs is an issue in your input matrices. Are you correctly replacing the tokens from the input string with integer indices? "
        ]
    },
    "412": {
        "question": "Hi, From what I understand, in part 2, the task is to represent the top 3 words from both the stack and the buffer in a single array. ['the', 'root', 'null', 'dogs', 'eats', 'a']. What if the buffer has less than 3 items? For instance, if we only have two words, 'dog', and 'eats', should it be ['the', 'root', 'null', 'dog', 'eats', 'null'] or ['the', 'root', 'null', 'null', 'dog', 'eats']? Thanks!",
        "answers": [
            "I believe the array should essentially be [stack[-1], stack[-2], stack[-3], buffer[-1], buffer[-2], buffer[-3]]. But you need to make sure those indices exist. If they don't, you should replace any non-existent ones with &lt;NULL&gt;'s representation (i. e. , 4 from the words. vocab output in Part 1). You also have to check for the special symbols (e. g. , &lt;CD&gt;) by looking at the part of speech tag. Finally, you want to map the string words to their index value from the word-to-index dictionary in the attribute word_vocab. The actual state. stack and state. buffer values are just a word's index in the sentence (e. g. , 3 to mean the third word in the sentence)."
        ]
    },
    "413": {
        "question": "Hi , Does my output needs to be under 70? Or is this fine.  Thank you! Best, Adrian",
        "answers": [
            "These look great! "
        ]
    },
    "414": {
        "question": "Hi Professor and TAs, I created my model with the following code:def build_model(word_types, pos_types, outputs):\n    # TODO: Write this function for part 3\n    model = Sequential()\n    #model. add(. .. )\n    model. add(Embedding(input_dim=word_types, input_length=6, output_dim=32))\n    model. add(Flatten())\n    model. add(Dense(100, activation='relu'))\n    model. add(Dense(10, activation='relu'))\n    model. add(Dense(outputs, activation='softmax'))\n    model. compile(keras. optimizers. Adam(learning_rate=0. 01), loss=\"categorical_crossentropy\")\n    return modelThe following command runs without error:python train_model. py data/input_train. npy data/target_train. npy data/model. h5However, in Part 4 when I try to run model. predict(), I get an incompatibility error. Here is my code:    def parse_sentence(self, words, pos):\n        state = State(range(1, len(words)))\n        state. stack. append(0)    \n\n        while state. buffer: \n            # TODO: Write the body of this loop for part 4 \n            input = extractor. get_input_representation(words, pos, state)\n            probs = self. model. predict(input) # Causes Error\n\n        result = DependencyStructure()\n        for p, c, r in state. deps: \n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return result And here is the error:    ValueError: Exception encountered when calling layer 'sequential' (type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 192, but received input with shape (None, 32)\n    \n    Call arguments received by layer 'sequential' (type Sequential):\n       inputs=tf. Tensor(shape=(None, ), dtype=int64)\n       training=False\n       mask=NoneI would appreciate any form of guidance or direction on how to begin debugging this issue. Thank you, Shimon",
        "answers": [
            "Actually, I may have solved the issue. "
        ]
    },
    "415": {
        "question": "Hi, Like a few others have mentioned, I'm running into large loss values when attempting to train the model in Part 3. During the first epoch, the loss value appears to continue reducing until around 1. 95, at which point it begins to steadily increase. What could be causing this behavior? Thanks so much!",
        "answers": [
            "Hi, I had a similar issue training my model locally on my M1 Macbook Air where the number went down to 0. 6 before it exploded into a really large value (~200). I trained my model on Google colab and it seems to fix my problem though. My loss values are now more reasonable, it was around 0. 6 at the end of epoch 1 and I ended with 0. 29 at the end of epoch 5. Hope this helps!",
            "If you are not trying to use GPU acceleration on an M1/M2 unit, maybe there is something wrong with your training data?"
        ]
    },
    "416": {
        "question": "Hi! I'm getting this error from the \"model. fit(inputs, outputs, epochs=5, batch_size=100)\" line in train_model. py and am not sure where it may be coming from (probably from my 6 inputs and 91 outputs dimensions), but am not sure how to resolve it-- I would appreciate any insight on this, thank you! !",
        "answers": [
            "You can also try feature_vector = feature_vector. reshape(1, -1). I threw this into my code and it seems to have fixed something for now.",
            "Yes, the first dimension required is generally the batch size, so instead of feeding an input of dimensionality (, 6) you should feed an input of (1, 6). You can get this using . reshape(1, -1). Similarly, the output will be an array of size (1, 91). "
        ]
    },
    "417": {
        "question": "Dear TAs, When I was working on hw3 part4, I met this problem when running evaluation program: I'm wondering what will cause this error? Thank you!",
        "answers": [
            "I just modified my codes and I noticed that the scores seem to be too low:I'm wondering where should I modify my codes? Thanks!",
            "Hi dear TAs and professor:After modification, I finally got this output, does it reach the assignment's requirement? Thank you!"
        ]
    },
    "418": {
        "question": "When I tried installing the tensorflow 1 GPU version on my windows 11 laptop that has a nvidia GPU, I get prompted with the following errorAnd because the latest version of tensorflow requires linux environment on windows, I installed WSL and minicando3, then installing tensorflow ontop of that, but it's not recognizing my GPU, or skipping the process of checking all together ",
        "answers": [
            "Hm, sorry I don't really have a good answer to this as a non-windows user. If you can't figure it out, you could try Google Colab. It's actually possible to run training on the CPU as well, so if it runs without GPU support that's okay for now. For homework 4 you will likely use GCP anyway. "
        ]
    },
    "419": {
        "question": "Hello! I've been working on part 4 for a while now and I can't seem to solve the following errorThis is my model:And when I try to call model. predict:I get the following error:I double-checked the output of currentState, and it is indeed an np array of integers of length 6. I don't understand what is going on with the dimensions (192) and (32). Could you please help me understand what's going on? Thanks in advance for your help",
        "answers": [
            "Hi the predict method expects the input to be of shape (1, 6), that is a 2D matrix (the first dim is the batch size), instead of (6, ). Try to reshape the result if get_input_representation using . reshape(1, -1). "
        ]
    },
    "420": {
        "question": "",
        "answers": [
            "Solved in OH!"
        ]
    },
    "421": {
        "question": "Hi Prof. Bauer and the TAs, Is it normal to have decoder. py to run for more than 30min? I can't remember exactly how long but somewhere in 30-40min range. I was using the accelerator on M2 chip. I also tried on Colab with T4 GPU. The step time seems to be longer on Colab (~20ms/step) than locally on M2 chip (~10ms/step). Best, Ansen Gong",
        "answers": [
            "I recommend turning off eager execution.  tf. compat. v1. disable_eager_execution()"
        ]
    },
    "422": {
        "question": "Hello, I would like to confirm whether I understood the task for get_output_representation. It's asking us to make a one-hot representation of the transition. Does that mean that we should make a function that maps each of the 91 possible transitions to a unique index in the output array, leaving the rest as 0? If this is right, I am guessing there are multiple ways of doing this, and it won't matter as long as we're consistent. Thanks for your time and the clarificationsEDIT: Now I'm realizing that function make_output_labels already creates a dictionary of all possible transitions. Maybe we can take each value in the dictionary (which represents the index and goes from 0 to 90)?",
        "answers": [
            "Thats correct. You can use the output labels dictionary to find the numeric representation for the transition. Then turn it into a one-hot vector. "
        ]
    },
    "423": {
        "question": "Hi, sorry to ask a silly question, I can't type \"\\$ python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\"in the terminal in my VScode, I can only type \"python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\". All command contain \"$\" is unable to run on my terminal. Btw, I find every time I run \"python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\", I get a different words. vocab, is it normal?",
        "answers": [
            "The $ is a common prompt character, not part of the actual command. :)Getting different words. vocab files is expected. The types are collected in a set, which is unordered so when they are written to the file, the order is arbitrary. "
        ]
    },
    "424": {
        "question": "The current state of the art for dependency parsing is ~97 ( http://nlpprogress. com/english/dependency_parsing. html). Feel free to experiment with additional features to improve the parser. Dear Prof. Bauer and TAs, Does this experimentation part hold any marks? I would like to do it but probably later sometime due to the number of assignments lined up this week. Thank you, Shreyas.",
        "answers": [
            "I feel like if you could build state of the art in a week or two you'd have a future in NLP research"
        ]
    },
    "425": {
        "question": "Hi Prof. Bauer and TAs, As I am trying to migrate the code to Colab, I have encountered this issue on the optimizer Adam below: My Colab uses Tensorflow version 2. 14. 0. Have there been similar issues? And if so, what were the solutions? Best, Ansen",
        "answers": [
            "Hm -- probably a version incompatibility. Try using tf. keras. optimizers. legacy. Adam.",
            "I met the same issue when I disabled eager execution before fitting. I guess that after disabling it the backpropagation is also blocked."
        ]
    },
    "426": {
        "question": "Hello Professor and TA, I'm curious about the output result of part 2. Below is what I've obtained. Is it the expected result? Thank you for your help and response. [None, 'BUSH', 'AND', 'GORBACHEV', 'WILL', 'HOLD', 'two', 'days', 'of', 'informal', 'talks', 'next', 'month', '. '][None, 'NNP', 'CC', 'NNP', 'MD', 'VB', 'CD', 'NNS', 'IN', 'JJ', 'NNS', 'JJ', 'NN', '. '][None, '. ', '. ', 'GORBACHEV', 'AND', 'BUSH'][3, 4, 4, 4, 5106, 4]",
        "answers": [
            "The first line are the words, second line are the POS -- what is the third line? And the last line is the actual input representation? For which state?"
        ]
    },
    "427": {
        "question": "Hi, I hope you are doing well. My output looks like:I see that the scores are very bad, but i also see that it is printing twice.  Is that supposed to happen? Thank you so much",
        "answers": [
            "The scores are different though. First one is Micro, the other is macro LAS. They are calculated differently in the code.",
            "I think it is supposed to happen because I am also getting two separate sets of scores, one for the micro averages and other for the macro averages of the labeled and unlabeled attachments. Also, Micro LAS and Macro LAS are calculated and printed separately in the evaluate. py file."
        ]
    },
    "428": {
        "question": "I am trying to join Andrew Stein's Zoom OH, but am facing invalid meeting ID error. ",
        "answers": []
    },
    "429": {
        "question": "Hi, I am trying to join the OH with link in the canvas announcement, but it says meeting ID is not valid.  Does anyone have the valid link? Thank you",
        "answers": []
    },
    "430": {
        "question": "Hello, For the get_input_representation method, if our stack/buffer is shorter than 3 words, should we replace the corresponding elements in the array we'll return with the index of \"&lt;NULL&gt;\" in the word vocab?",
        "answers": [
            "Yes, that is what I did, and it worked for me."
        ]
    },
    "431": {
        "question": "Hi i hope you are greatI am having issues with my code so I printed out words in extract training data. This is what printed:[None, '``', 'Feeding', 'Frenzy', \"''\", '-LRB-', 'Henry', 'Holt', ', ', '326', 'pages', ', ', '$', '19. 95', '-RRB-', ', ', 'a', 'highly', 'detailed', 'account', 'of', 'the', 'Wedtech', 'scandal', ', ', 'begins', 'on', 'a', 'reassuring', 'note', '. ']I have a few questions about this parameter, is None the root? what are -LRB- and  -RBR-thank you so much!",
        "answers": [
            "Yes, None will always appear in position 0 and represents the root symbol. -LRB- stands for the symbol (  left round bracket. "
        ]
    },
    "432": {
        "question": "Hi after running $python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\n\n\ni do not get the screenshot on Courseworks, instead i get the following: Is there something I did wrong with uploading the information? ",
        "answers": [
            "The order of tokens in the vocabulary may come out differently since we are keeping track of words in a set, which is unordered. "
        ]
    },
    "433": {
        "question": "Hello, I had different results when I ran part 3 locally vs Google Colab. I am using MacBook Pro (M1 chip) , macOS : Sonoma and I got a very high loss value which is &gt;700 and it kept increasing on every epoch. However, when I ran my part 3 at Google Colab not only its significantly faster in time but my loss value is also significantly lower, which is &lt;0. 5 and decreasing on every epoch. I am curious on why this happened, and since I get a very drastic output when running on both platforms which one is more reliable? Thank you ",
        "answers": [
            "Same questions^"
        ]
    },
    "434": {
        "question": "Hi, when trying to run:python extract_training_data. py data/dev. conll data/input_dev. npy data/target_dev. npy\nI get the error: zsh: illegal hardware instruction  python extract_training_data. py data/dev. conll data/input_dev. npy \nIs anyone else getting this error or knows how to solve this? Thanks",
        "answers": [
            "What architecture are you running this on, and are you using the CPU or GPU version of tensorflow? Is this really an error or just a warning?"
        ]
    },
    "435": {
        "question": "Hi NLP teaching staffs - I am having trouble understanding the output of model. predict. When I print predict out, the result is a 2D array consisting of numbers. How can I tell 1) which transition the model predict 2) the probability of the prediction? Appreciate any guidance/ pointer. Thank you so much!",
        "answers": [
            "The output is a (batch_size, vocab_size) array. Since you are just passing a single input you can just take the first entry from the array in the batch dimension (index 0). Then you end up with a 1D array of length vocab_size. The probability values represent the probability of each word.  For each dimension you get a probability value. You need to sort the dimensions according to the probability, which should result in an array of integer dimensions sorted in decreasing order of the probability. This can be done using np. argsort"
        ]
    },
    "436": {
        "question": "Hi, I'm still a little confused here, just wanna check I correctly understanding this. For part2:the word and pos contains every possible word and corresponding tags from the input file and state can be any middle transition state for one sentence?",
        "answers": [
            "In get_input_representation(self, words, pos, state), words and pos refer to the actual input sentence and corresponding pos sequence. State is the current (intermediate) state in the transition sequence. state. stack and state. buffer contain token positions (that is, you can treat them as indices in the words and pos list). "
        ]
    },
    "437": {
        "question": "For labeled attachment score, I'm getting around 0. 68 and 0. 75 for unlabeled attachment score. Are these scores sufficient for the assignment? If they're not sufficient, what's the best way to go about testing the code to find places to improve?",
        "answers": [
            "That sounds pretty good to me!"
        ]
    },
    "438": {
        "question": "Hello, I just ran evaluate. py on my decoder, but my accuracy is super low, as copied below:Micro Avg. Labeled Attachment Score: 0. 04646650580831323Micro Avg. Unlabeled Attachment Score: 0. 20907440113430015Macro Avg. Labeled Attachment Score: 0. 052479354272750564Macro Avg. Unlabeled Attachment Score: 0. 21695312916782164I think there's something really wrong in my code for it to be this low, so does anyone have any suggestions on where to even start debugging or has anyone come across this? Totally understand this is hard to help with without seeing my code, but I'd appreciate any pointers. Thanks!",
        "answers": [
            "I would look at the output produced by decoder to see if there are any obvious systematic mistakes (such as disconnected trees, or everything depending directly on 0-root). With low scores like these, I think something systematic is likely. At the very least, you should be able to identify if the issue is the model or the decoder. Then you can try to find possible causes in your code. ",
            "I had the same issue and to fix it I printed out the state. stack and the state. buffer and every action I was checking the validity of in the while loop. I found a lot of implementation issues when I did this where certain arcs or shifts weren't done properly and once I fixed these, the accuracy increased."
        ]
    },
    "439": {
        "question": "In the instructions it says: Take a look at the file extract_training_data. py States: The input will be an instance of the class State, which represents a parser state. The attributes of this class consist of a stack, buffer, and partially built dependency structure deps. --stack and buffer are lists of word ids (integers). --\"Based on this it means that the values on the stack and buffer are the word positions in the sentence, and I don't see if there is a way to reverse engineer it to find the word itself. I have been at a few OHs and no one can figure out and TAs think it could be a wording mistake because it would make more sense that the values on the stack and buffer are the words themselves. Can you please clarify if there is a mistake or not? Thank you!",
        "answers": [
            "Hi Omer, Responding here -- I also saw your email. The description is correct. Let's say the input sentence is 0-root 1-the 2-cat 3-sleeps. The initial configuration would look like this stack: [0]  buffer: [3, 2, 1]  To find the specific tokens you need to do two lookup steps. First, find the word at the specific index, then lookup the integer representation of that word in the vocabulary dictionary.  For example, for token 1, you would use word_vocab[words[1]] to get the integer representation of \"the\". Does that help? Daniel "
        ]
    },
    "440": {
        "question": "Hi, Can I label my outputs in a different one-hot format? Thank you ",
        "answers": [
            "Hi, I initially tried to use my own scheme, but I ran into an issue because of this during the decoder and evaluation stage since I utilize self. output_labels from the Parser class which is defined as dict([(index, action) for (action, index) in extractor. output_labels. items()]) and imposes a certain structure on the output. It looks like this: \"{0: ('shift', None), 1: ('left_arc', 'tmod'), 2: ('right_arc', 'tmod'), 3: ('left_arc', 'vmod'), 4: ('right_arc', 'vmod'). .. }\". My code ended up working after following the same scheme as shown here. It may be possible to do it with your own output ordering scheme, but alternating left and right arcs like done in self. output_labels worked best in my case. Hope this helps!"
        ]
    },
    "441": {
        "question": "How do I find root? \"So for example, if the next words on the buffer are \"dog eats a\" and \"the\" and &lt;ROOT&gt; are on the stack , the return value should be a numpy array numpy. array([4047, 3, 4, 8346, 8995, 14774]). Here 4 is the index for the &lt;NULL&gt; symbol, 3 is the index for the &lt;ROOT&gt; symbol, 4047 is the index for \"the\" and 8346, 8995, 14774 are the indices for \"dog\", \"eats\" and \"a\". \"Based on this quote, if root is sitting on the stack, and I call state. stack[root index] would it return the string \"&lt;ROOT&gt;\"?",
        "answers": [
            "Both the stack and the buffer contain word ids. You'd want to extract the word (string) using its id. Then, think about how to incorporate word_vocab in order to switch between the word and the mapped index!"
        ]
    },
    "442": {
        "question": "Hello! I just wanted to confirm if and which office hours will be available during fall break since the homework is due the day after? Thank you!",
        "answers": [
            "Yes, I will hold office hours on Monday and Tuesday on Zoom. I may have to schedule a different time slot than usual on Monday -- I will send an announcement through Courseworks. I don't think the TAs will hold office hours Mon/Tue."
        ]
    },
    "443": {
        "question": "Hi I hope you are doing great.  When I run python evaluate. py data/model. h5 data/dev. conll, I get the warning below:(base) venkatramamoorthi@dyn-160-39-55-55 hw3_files % python evaluate. py data/model. h5 data/dev. conll 2023-10-31 09:18:52. 036024: I tensorflow/core/platform/cpu_feature_guard. cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropri ate compiler flags. 2023-10-31 09:18:57. 104034: I tensorflow/compiler/mlir/mlir_graph_optimization_pass. cc:375] MLIR V1 optimization pass is not enabled 2023-10-31 09:18:57. 227919: W tensorflow/c/c_api. cc:304] Operation '{name:'dense_2/bias/Assign' id:87 op device:{requested: ' ', assigned: ''} def:{{{node dense_2/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/bias, dense_2/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. Evaluating. (Each . represents 100 test dependency trees) /Users/venkatramamoorthi/opt/anaconda3/lib/python3. 8/site-packages/keras/src/engine/training_v1. py:2359: UserWarning: `Model. state_updateswill be removed in a future version. This property should not be used in TensorFlow 2. 0, asupdates` are appl ied automatically. updates=self. state_updates, 2023-10-31 09:18:57. 453587: W tensorflow/c/c_api. cc:304] Operation '{name:'dense_2/Softmax' id:93 op device:{requested: '', a ssigned: ''} def:{{{node dense_2/Softmax}} = SoftmaxT=DT_FLOAT, _has_manual_control_dependencies=true}}' w as changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error i n the future. Either don't modify nodes after running them or create a new session. I let this run for 4  hours and it stayed like this.  I worked with a TA and we moved:import tensorflow as tftf. compat. v1. disable_eager_execution()between files.  If I dont have the lines above, it just keeps outputting lines like:1/1 [==============================] - 0s 15ms/stepIs there anything else I can try? Thank you so much! Based on another question in the ed, i think something larger is wrong with my code. it seems someone had this error but their code compiled.  I am unsure of how to troubleshoot this.  thank you!",
        "answers": [
            "Hm, so the warnings won't affect your code, as far as I can tell. The output is expected. 15 ms/step seems still very slow, but should allow you to evaluate the model. After the progress bars, does it fail to compute the actual scores? "
        ]
    },
    "444": {
        "question": "Hi, I'm running the decoder and evaluator on an M1-chip computer, and I'm encountering numerous warnings in part 4. While the decoder and evaluator functions execute correctly and produce results, I'm noticing that the accuracy is significantly low. Due to the warnings, I'm uncertain about whether the issues mentioned in the warnings may be having some impact on my performance that may be attributed to my M1 and some error in my initial setup. Here are some examples of the warnings I get: \"2023-10-30 23:08:58. 971460: I tensorflow/compiler/mlir/mlir_graph_optimization_pass. cc:382] MLIR V1 optimization pass is not enabled 2023-10-30 23:08:59. 002409: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_2/kernel/Assign' id:82 op device:{requested: '', assigned: ''} def:{{{node dense_2/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/kernel, dense_2/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. 2023-10-30 23:08:59. 094512: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_1/bias/v/Assign' id:265 op device:{requested: '', assigned: ''} def:{{{node dense_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/bias/v, dense_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. Evaluating. (Each . represents 100 test dependency trees) /Users/anaconda3/lib/python3. 11/site-packages/keras/src/engine/training_v1. py:2359: UserWarning: Model. state_updates will be removed in a future version. This property should not be used in TensorFlow 2. 0, as updates are applied automatically. updates=self. state_updates, 2023-10-30 23:08:59. 187791: W tensorflow/c/c_api. cc:305] Operation '{name:'dense_2/Softmax' id:93 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = SoftmaxT=DT_FLOAT, _has_manual_control_dependencies=true}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session. \"Thanks!",
        "answers": [
            "Are you just running the CPU version on an M1, or are you attempting to use the tensorflow-metal GPU acceleration? There are known issues with tensorflow-metal, so I would recommend to just use the CPU version for now. The warnings should not affect performance in that case. "
        ]
    },
    "445": {
        "question": "Hi! Thank you so much for a great course so far! I was wondering if the second exam includes what was on the first exam or if the scope is just what was covered after? Thank you!",
        "answers": [
            "I think it will be cumulative, based on this comment - https://edstem. org/us/courses/46417/discussion/3807069? comment=8777621"
        ]
    },
    "446": {
        "question": "Hi I hope you are great. I asked this question on ED:Once we figure out the best transition in Part 4, how should we store it? I'm unsure of how finding the best transition connects with the given part:        result = DependencyStructure()\n        for p, c, r in state. deps:\n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return resultThe answer was:The best transition is found and used to update the parser's state within the while loop of the parse_sentence method. There's no need to separately store the best transition because it is immediately applied to the parser's state. So the result here is already the best transitionI have my while loop with my logic to see if its a legal transition.  I'm not sure how to update the parser's state once i get the best transitionI think i should call the legal transition' function from the state class, prior to          result = DependencyStructure()\n        for p, c, r in state. deps:\n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return resultthank you again!",
        "answers": [
            " Hm, I'm not sure I understand your follow-up question. Once you have the best transition, you can call the corresponding method on the parser state. for example, result. shift() or result. left_arc(\"nsubj\"). This will change the state of result -- then you proceed with the next prediction based on the new state. "
        ]
    },
    "447": {
        "question": "Hi, I would like some clarification on get_input_representation(self, words, pos, state). First, are words and pos basically used to check if any entry in the buffer/stack needs to be replaced by a tag? Secondly, are words and pos structured such that pos[i] is the pos tag of word[i]? If there is no tag, does pos[i] equal to None? Also, regarding to the comment in post #315, can we assume if we see None(or 0, as described in the instructions) in the stack or buffer, we can replace it with &lt;ROOT&gt;? Thank you",
        "answers": [
            "You're given a list of words that are just words, prepended with NoneYou're given a list of POS that correspond 1:1 with the wordsStack and buffer contain a list of indexes where 0 is root, but these index the word/POS list. I think the wording of the assignment is awkward for thatYou need to identify which words are being referenced and when, but also handle finding their actual IDs to create the vectors. you can use the POS to check if some words need to be converted to a special ID corresponding to a category of words, like NNP and CD."
        ]
    },
    "448": {
        "question": "Hi I hope you are great. Once we figure out the best transition in Part 4, how should we store it? I'm unsure of how finding the best transition connects with the given part:result = DependencyStructure()\n        for p, c, r in state. deps:\n            result. add_deprel(DependencyEdge(c, words[c], pos[c], p, r))\n        return resultthank you so much!",
        "answers": [
            "The best transition is found and used to update the parser's state within the while loop of the parse_sentence method. There's no need to separately store the best transition because it is immediately applied to the parser's state. So the result here is already the best transition"
        ]
    },
    "449": {
        "question": "Hello, I am getting a strange error when attempting to run:python evaluate. py data/model. h5 data/dev. conll\nWhen I try and run it on Google Colab, I get the following output:Evaluating. (Each . represents 100 test dependency trees)\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n/content/evaluate. py in &lt;module&gt;\r\n     45             words = dtree. words()\r\n     46             pos = dtree. pos()\r\n---&gt; 47             predict = parser. parse_sentence(words, pos)\r\n     48             labeled_correct, unlabeled_correct, num_words = compare_parser(dtree, predict)\r\n     49             las_s = labeled_correct / float(num_words)\r\n\r\n/content/decoder. py in parse_sentence(self, words, pos)\r\n     38 \r\n     39             transition = False\r\n---&gt; 40             features = self. extractor. get_input_representation(words, pos, state). reshape(1, -1)\r\n     41             status = self. model. predict(features)\r\n     42             while transition == False:\r\n\r\nNameError: name 'extractor' is not defined\nThis is very confusing, since running decoder. py does not bring up any errors at all. I suspected that it might be my indentation, but I went over it and it seems fine to me. I have not touched evaluate. py. I even redownloaded it just to make sure. What could be causing this? Any help would be greatly appreciated. Thank you.",
        "answers": [
            "I assume this might be an import problem. Make sure your working directory on your google colab is correct. It's hard to solve this kind of problem on ed discussion. Could you go to one of the OH and ask the TA for help?"
        ]
    },
    "450": {
        "question": "Hello, I am getting a error I do not understand when attempting to predict using extractor. get_input_representation(). Right now, I have it so I read it in through:status = self. model. predict(extractor. get_input_representation(words, pos, state))\nHowever, when this line is run, I receive the following error:ValueError: Exception encountered when calling layer 'sequential_3' (type Sequential).\r\n    \r\n    Input 0 of layer \"1stHidden\" is incompatible with the layer: expected axis -1 of input shape to have value 192, but received input with shape (None, 32)\nI do not understand what might be causing this. extractor. get_input_representation() returns a numpy array of shape (6, ), which is what I expect. Here is how my model is set up for reference:def build_model(word_types, pos_types, outputs):\r\n    m = Sequential()\r\n    m. add(Embedding(word_types, 32, input_length=6, name=\"Embedding\"))\r\n    m. add(Flatten(name=\"FlattenEmbed\"))\r\n    m. add(Dense(100, activation=\"relu\", name=\"1stHidden\"))\r\n    m. add(Dense(10, activation=\"relu\", name=\"2ndHidden\"))\r\n    m. add(Dense(outputs, activation=\"softmax\", name=\"Output\"))\r\n\r\n    m. compile(keras. optimizers. Adam(lr=0. 01), loss=\"categorical_crossentropy\")\r\n    return m\nAny help regarding what might be causing this would be greatly appreciated. Thank you.",
        "answers": [
            "Hi the predict method expects the input to be of shape (1, 6), that is a 2D matrix (the first dim is the batch size), instead of (6, ). Try to reshape the result if get_input_representation using . reshape(1, -1). "
        ]
    },
    "451": {
        "question": "For the part 4, I want to get the input representation, so I wrote something like input_representation = self. extractor. get_input_representation(words, pos, state) but it throws an error Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 192, but received input with shape (None, 32) So I change my code to input_representation = self. extractor. get_input_representation(words, pos, state). reshape(1, -1) and it works. But I just want to make sure this is a necessary step to do to reshape or it indicates something went wrong with my previous code? ",
        "answers": [
            "Yes thats fine. By reshaping this way you are turning the d-dim vector (i. e a 1-D Matrix) into a (1, d) 2-D matrix. This allows the training data to be stacked together in batches of 192 of shape (192, d). "
        ]
    },
    "452": {
        "question": "1. To get output representation, can I use return keras. utils. to_categorical(self. output_labels[output_pair], num_classes=91) to get the output matrix? Or I have to implement this manually? 2. To get input representation, when we considering the special case of root, I am confused about do we compare the word with None or 'None'? i. e. with quotation mark or without? So, I am confused about how the stack is initialized for the special case &lt;ROOT&gt;?",
        "answers": [
            "1. Yes that sounds okay. 2. I think the entry in the word list is just the value None (not a string). Note that the stack contains word positions, so for the root it will contain the value 0. In the initial state, it will be [0]. "
        ]
    },
    "453": {
        "question": "Hello, Is it possible to know (or share) the approximate loss values people are getting for their training? ",
        "answers": [
            "Hi, When I was training I started at like a loss of 2. 5 and it went down to about 0. 25 by the end of the 5 epochs. Hope this helps.",
            "Hi, I got: 0. 4751 0. 3072 0. 2737 0. 2544 0. 2413I'm not sure whether this is the expected answer]",
            "Thanks all, at time of writing my model wasn't converging so I asked but now it's consistent."
        ]
    },
    "454": {
        "question": "Hi! I am trying to run the evaluation (python evaluate. py data/model. h5 data/dev. conll) for the model and I found it took quite a long time. On average it takes 9ms / step on a A6000 gpu and it went for nearly two hours. Is this normal evaluation time or something is wrong in my setup? For reference my final results are as follows:5039 sentence. Micro Avg. Labeled Attachment Score: 0. 7391242832055795Micro Avg. Unlabeled Attachment Score: 0. 786863415811507Macro Avg. Labeled Attachment Score: 0. 7480423144749794Macro Avg. Unlabeled Attachment Score: 0. 7964001075386273Thanks!",
        "answers": [
            "Did you disable eager execution? I found that it speeds up the last step significantly"
        ]
    },
    "455": {
        "question": "Hi! In HW3 part 2 get_input_representation(self, words, pos, state), I am not sure what the pos variable can be used for, is it supposed to be used in the function? Also, I found that the list of words for a sentence always start with none, for example:words: [None, '``', 'Feeding', 'Frenzy', \"''\", '-LRB-', 'Henry'. .. ]. Is this normal and we leave it as it is? Thanks!",
        "answers": [
            "For the input representation you need the POS to replace some of the input tokens with special symbols. Note that you need to account for the special symbols (&lt;CD&gt;, &lt;NNP&gt;, &lt;UNK&gt;, &lt;ROOT&gt;, &lt;NULL&gt;) in creating the input representation. Make sure you take into account states in which there are less than 3 words on the stack or buffer. The first token (position 0) corresponds to the \"root\" symbol. None is included in the list of input tokens so that the first \"actual\" token has the index 1."
        ]
    },
    "456": {
        "question": "Hello, When starting HW 3, I wanted to make sure I had a solid understanding of Dependency Parsing. To clarify, we manually create an annotated dependency tree that is the gold tree from a treebank. I am confused as to how this dependency tree is different from the one we are trying to construct. If we already have the gold tree, why are we training this model to construct transitions if the transitions are already from the gold tree? Are we using the model to create transitions on unseen inputs that are not from the gold tree?",
        "answers": [
            "I believe you are training it to construct dependency trees for any input, not just the training set.",
            "I think your understanding is correct here. We have the manually annotated treebank. We obtain transition sequences for these trees using the oracle algorithm. The transition sequence for each training sentence make up the training data. We are training the model to predict good transition sequences for unseen sentences. "
        ]
    },
    "457": {
        "question": "Hi I hope you are doing great! I think I have completed part 2. I just ran these lines:python extract_training_data. py data/train. conll data/input_train. npy data/target_train. npypython extract_training_data. py data/dev. conll data/input_dev. npy data/target_dev. npySo now I have target_dev. npy and target_train. py as files in my data folder. I am using atom and when I clicked on the files, I got a message that: 'Atom will be unresponsive while loading such large files\"I procceded and target_dev. npy has a bunch of lines like:  ? ?and target_traim. np seems to just be blank. Is this normal? Thank you so much!",
        "answers": [
            "Yes. These are binary files, so you cant just open them in a text editor. However target_train. npy should not be empty  I dont know if there is an issue with your code or if it simply doesnt show the content in Atom. Whats the actual file size? "
        ]
    },
    "458": {
        "question": "Hello, I had a quick clarification question regarding what \"teacher forcing\"  is. In the slides today, we saw the outputs for an RNN LM being fed independently into each other, as well as outputs being pipelined together in a Generator. My question is in which application of RNNs would we consider the inputs to be \"teacher-forced? \" I am under the assumption that \"teacher-forcing\" means to over-write an incorrect model prediction output with the correct training data as an input in pipelining, but we mentioned teacher-forcing in the RNN LM context as well.",
        "answers": [
            "So the generator and RNN language model is trained exactly the same way. But you are right that you cant really speak of teacher forcing in the context of the language model. "
        ]
    },
    "459": {
        "question": "Hi there, when I ran the same code in both Colab and PyCharm on the \"dev. conll\" set to evaluate the model performance, it gave me very low accuracy on PyCharm (roughly Micro Avg. Unlabeled Attachment Score 0. 05), but the code run in Colab gave me an accuracy of \"Micro Avg. Unlabeled Attachment Score: 0. 77, \" so I am not sure what happened here. Also, I further trained the model on the \"input_dev\" and \"target_dev\" sets and named the model \"model_dev. h5\". However, when I ran the original model which is not further trained on the \"input_dev\" and \"target_dev\" sets on \"test. conll\" file, it returned the \"ZeroDivisionError: float division by zero\", but if I ran the evaluation by using the \"model_dev. h5\" model, it worked just fine.  I am also unsure what is wrong here. Thanks a lot in advance.",
        "answers": [
            "It was solved by reinstalling all libraries."
        ]
    },
    "460": {
        "question": "Hi i hope you are doing great.  I get that the words map to indicies.  But do the particular indicies have a meaning, or are they just a mapping of a word to an arbitrary numeric value. Thank you! ",
        "answers": [
            "They are just arbitrary numeric values (since you need to represent the input to the neural network numerically). "
        ]
    },
    "461": {
        "question": "Hi there, I am doing the HW3 part 3, the input parameter \"pos_types\" for function \"build_model\" seems a bit redundant and is not used in constructing the neural network. Just want to confirm if it is right or if I missed something. Thanks a lot in advance!",
        "answers": [
            "That's correct, that parameter is not used. "
        ]
    },
    "462": {
        "question": "for c, wouldn't the tri-grams be on applied to the transition probabilities rather than the emission probabilities? Meaning it should be p(V|N, start) rather than the example with P(START time flies)? In that case, would it be P(V|N, start) = P(V, N, start) / sum of P(x, N, start)and we should do it for as the full set? P(N| N, start)P(Prep| N, start)P(N| V, start)P(Prep| V, start)P(Prep|V, N)P(N|V, N)P(Prep|N, V)P(N|N, V)P(V|N, V)P(N|Prep, V)P(N|Prep, N)For part (d), I don't really understand why don't the solution use a count variable but instead uses: pi[k, t] += pi[k-1][s]is this assuming the first p[0, start] is initialized as 1 and that 1s will keep accumulating? what is sum_s pi[n, s]  ? is sum_s a function or variable here? ",
        "answers": [
            "Hi Millie, I think we discussed this during office hours. Please ask again if I misremember. Daniel "
        ]
    },
    "463": {
        "question": "In step 11, one might think that a RIGHT-ARC is possible. However, since there is no subsequent connection between root and today, we cannot actually make that move. This makes this example unique. Am I interpreting this correctly?",
        "answers": [
            "I think we cannot perform right arc when sent still have a child (today), therefore we have to shift first). Once the child (today) is processed, we can right arc root to sent"
        ]
    },
    "464": {
        "question": "Given that this is our data in Ungraded Exercise: Naive BayesEmail1 (spam): buy car Nigeria profitEmail2 (spam): money profit home bankEmail3 (spam): Nigeria bank check wireEmail4 (ham): money bank home carEmail5 (ham): home Nigeria flyI want to calculate simple P(buy). One way I go is P(buy) = count(buy)/total_no_words = 1/19The other way to go is using total probability theorem, P(buy) = Summation P(buy|x)P(x) = P(buy|spam) * P(spam) + P(buy|ham) * P(ham) = 1/12 (considering the answer is right) * 3/5 + 0/7 * 2/5 = 1/20Where am I going wrong here? I definitely know that total probability theorem is not wrong.",
        "answers": [
            "It's a bit late in the night, so I can't really think this through fully, but consider this:Email1 (spam): buy * infinityEmail2 (ham): somethingIn this case, using the first method to calculate P(buy), we get 1, because inf/(1+inf) goes to 1. But it can be seen that P(buy) should be 1/2, because if only half of all emails are spam (in this data), then it should not be guaranteed that we see \"buy\". So it seems like the count(buy)/total_words method of calculation is invalid, although I can't think of why right now - maybe that method requires P(buy) to be independent of P(class = x)?",
            "I think that P(HAM) and P(SPAM) need to be considered on a per word basis to use the law of total probability in this way. Thus, in this case, P(HAM) = 7/19 and P(SPAM) = 12/19, P(buy) = 1/19, P(buy|HAM)P(HAM) + P(buy|SPAM)*P(SPAM) = (1/12)*(12/19) = 1/19. I believe this is because the definition of the Law of Total Probability states the events you are conditioning over must partition the same sample space as the event you're trying to find the probability of, and that the law of total probability seems to fail in the original example because the sample space over documents and the sample space over words are being conflated. If all documents contained the same number of words, I don't think this would have been an issue."
        ]
    },
    "465": {
        "question": "I am confused about how to tackle this subproblem:b) Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus. Can anyone provide a hint? The solution didn't offer me any clue. ",
        "answers": [
            "Hello! My understanding when I was going through it is that we have a really small lexicon and set of possible labels, so our rules can be super specific. Because our rules are so specific, we can make it so they don't have any false positives, hence we probably don't even need to try to weight them differently. ",
            "Not sure if my approach is correct, so take it with a huge pinch of salt. Basically I try to approach it like a super simplify Softmax. It would great that TA can point out where Im doing wrong. Based on how the feature function is set up in the solution, as long as the weights are all greater than 0, then all then all POS should correctly assigned.",
            "My intuition is that since the feature functions are defined based on what has appeared in the training corpus, the POS tag/token combination that matches what appeared in the training corpus will always have more 1s in the features comparing to another POS tag/token that hasn't appeared. Therefore to make sure that the POS tag/token combination gets the highest score among all possible POS tag choices, we should set all positive numbers for the weights. "
        ]
    },
    "466": {
        "question": "Hi, I'm studying the different smoothing methods and wondering what value we should be using for the unigram probabilities in linear interpolation. Should we use  count(word_i)/(number of tokens in the lexicon including END) or count(word_i)/(number of tokens in the lexicon not including END)",
        "answers": [
            "I believe that we typically include the END token but do not include the START token, similar to how we calculated it for HW 1. Hope that helps!"
        ]
    },
    "467": {
        "question": "Hi, I just wanted to clarify a few things about the slide on constituent labels. Since the slide states that constituents should have one non-bracketed word, would this mean the constituents in the example are the NP consisting of DetP and noun \"girl\", VP consisting of verb \"likes\" and NP, and the NP consisting of AdjP and noun \"boys\"? Or is the logic behind this that all phrases XP containing X (regardless of how many non-bracketed words there are) would be a constituent with X as the head?",
        "answers": [
            "One assumption here is that the splits are binary and are chose in such a way that the single non-bracketed word is the head. This approach (called X-bar theory) is not always consistently used. The S constituent, for example, does not have an un-bracketed head. Instead, the head of that constituent is the verb of the VP. "
        ]
    },
    "468": {
        "question": "for part a, why we have 2 routes of red back pointer, one start from \"leaves\" and one start from \"like\"",
        "answers": [
            "This may be an oversimplification but, if you check the lecture slides for 05 hmms, toward the middle it goes over the Viterbi algorithm and explains it as d^n paths for n words and d tags. Essentially a matrix of possibilities to work out all possible paths forward then the most probable paths backward. For the red paths I believe it shows multiple because you want to see the most probable path from node to node where one exists &gt;0. Not until youve moved front to back then back to front do you pick the most probable singular path. Until then you want to keep your options on the table, per say."
        ]
    },
    "469": {
        "question": "What's the process of knowing how many features to list out for the feature function assignment? The solution has 8, but I'm assuming that depends based on what our features are. ",
        "answers": [
            "Not a TA, but in lecture the instructor made it sound like it was kind of an art form of picking the right features that capture the right aspects and perform well. I'm pretty sure the dimensionality is just based on however many features we determine are sufficient for the given task. "
        ]
    },
    "470": {
        "question": "Dear Professor and TA, Unfortunately, I've been falling sick and have decided to opt for the Zoom alternative for tomorrow's exam after discussing with the professor. Could you provide any instructions or guidelines on how this will be conducted? I greatly appreciate your assistance. Thank you, Xinyue",
        "answers": [
            "Please log into the regular classroom Zoom meeting. I will have the exam set up in Gradescope for you. You will have to either print it and then complete the template, or respond on separate sheets of paper. One you are done, please scan your work (I recommend using a smartphone scanner app, such as GeniusScan), and upload to Gradescope. During the exam, please keep your camera turned on. One of the TAs will proctor the exams on Zoom. "
        ]
    },
    "471": {
        "question": "In lecture07, it says that the complexity of retrieving any single parse tree is O(N^2). Why is it not O(2*N)?",
        "answers": [
            "You start at [0, n, S] and then follow the backpointers deterministically. The table has n^2 entries for the spans, and at most you will touch each cell once. "
        ]
    },
    "472": {
        "question": "Hello, I wonder how could we get sum_z P(u, v, z) in this specific case? What other z tokens can it take except \"flies\"? Thanks in advance!",
        "answers": [
            "In principle, the z can range over all words in the vocabulary. Some words may not be possible at all in the context u, v thought.  Note that the computation for P(START time flies) is just an example. You would have to do this for all trigrams that the model could generate. "
        ]
    },
    "473": {
        "question": "Hi, I'm looking at the formula for Katz' backoff for bigram models. and I was wondering if there was a typo in the formula? should it be SUM v: count(v, w) Pmle(v) in the denominator of the backoff? I don't understand what the variable u is here.",
        "answers": [
            "In my opinion, the variable 'u' here is different from 'w', it is used to describe the condition : 'count(v, u) = 0'. P_mle is used to describe the viewed variable 'w' and 'u' is the hidden(not viewed) variable."
        ]
    },
    "474": {
        "question": "Do we include the counts for the start and end markers when determining M for perplexity calculations using the formula given in the lecture notes? ",
        "answers": [
            "Correct me if I'm wrong, but I believe you count the end markers and not the start markers (at least this is what I remember doing in the homework).",
            "I agree with Alexandra, the logic was that we count everything that can be generated by our model. In our ngram model, the START token were never generated therefore we don't include it in the perplexity calculations. ",
            "The student answers are correct. You count the end markers but not the start markers. "
        ]
    },
    "475": {
        "question": "Hi there, I am a bit confused about the answer to the feature functions to this question. It defines eight feature functions, does it mean that for each:\\left(w_{i-1}, \\ w_i, \\ t_{i-1}, \\ t_i\\right), should we compute all the eight feature functions for the single input? If this is the case, for the input (time, flies, N, V) from sentence 1, the resulting feature vector would be (0, 0, 0, 0, 0, 0, 0, 1), which only has one dimension to be 1. So I am a bit confused about what the \"specifying the conditions under which each dimension of the feature vector is 1. \" in the question means. Thanks a lot in advance!",
        "answers": [
            "Hello! Obligatory not a TA but was just thinking through the same questionthis is the formula from the ungraded exercise, plus a few details I added when I was going through it to make it clearer for myself. We have a set of possible labels T = {N, V, P}, a d-dimensional vector of weights v (d being 8 for the feature function here), and our feature function converts its inputs w_i, w_i-1, t_i-1, and t_i to a d-dimensional feature vector. t_i^*=\\arg\\max_{t_i\\in T}\\sum_{j=1}^d\\Phi_j(w_{i-1}, \\ w_i, \\ t_{i-1}, \\ t_i)\\cdot v_jSo the argmax is basically a for loop for t_i in T , the sum is basically sum(dimension for dimension in feature_function), so for each context, we consider each label, then sum up the result of applying each dimension of the feature function on it.  Basically, we sum up all the weighted values from each dimension, so the more dimensions that are set to one, the higher our unweighted chance to pick that label. Based on the way the individual functions that build up our feature function are written, (time, flies, N, N) and (time, flies, N, P) would both be (0, 0, 0, 0, 0, 0, 0, 0). If we sum up the values from each dimension, we get 1 when we try V and 0 when we try N or P, so our model picks t_i* = V. This ended up being a bit long winded but hope that helps! And teaching staff please correct anything I got wrong here. "
        ]
    },
    "476": {
        "question": "In the lecture 3 slides, a token is defined as \"total number of word occurrences\", but during lecture, when we went over the exam material, I believe it was defined as \"an instance of word in particular context\". To clarify, is a token a number or an instance of a word?",
        "answers": [
            "I believe usually when we refer to tokens we are referring to the instance of a word in a particular context. If you look at the definition of \"tokenization\" presented in lecture three, it is the process of segmenting text (a sequence of characters) into a sequence of tokens (words). From my understanding, the number of tokens is the number of unique words in a particular context. "
        ]
    },
    "477": {
        "question": "I did assignment 2 this week and submitted it on Monday. Now when I check canvas, it's listed as a past assignment and doesn't show that I submitted. I was wondering if someone could check Courseworks from the instructor side and verify that you can see my submission? My name is Sean Harrison and my uni is sph2146I really appreciate your help!",
        "answers": [
            "Hi Sean, I can see your submission on Courseworks. Daniel "
        ]
    },
    "478": {
        "question": "Dear Professor and TA, may I ask if there is a full version of the Linear Model exercise answer? I cannot see the answer for b) except the first line, and all for c) is missing. Many thanks!",
        "answers": [
            "The answer for b) is just the weight vector itself. For c) I dont have a written up answer, and I didnt have time to write it up today. However, its not critical in terms of exam prep. "
        ]
    },
    "479": {
        "question": "I was doing n-gram ungraded exercise. I was wondering why vocabulary size of a bigram is all the types and END. Why is END included and not START? Why don't we calculate the vocabulary size by the number of every bigram seen, but instead look at individual words? ",
        "answers": [
            "professor has answered a similar question, just see #51 ",
            "Hi, I am not a TA but I believe that END is included because you will need to predict and include END in order to mark the sentence boundaries. For instance, consider the sentence \"time flies like flies END\". You will actually have to deliberately make the decision of END in that sentence. As opposed to START which might not be necessarily needed because you are looking at sets of words. As for your second question, I believe that we look at individual words + END because the individual words could result in more bigrams than what is seen Blike pairings and stuff. Bigrams are from individual words."
        ]
    },
    "480": {
        "question": "Hi I hope you are doing great. part b states:Use the Forward algorithm to compute the probability of the surface sequence \"time flies like leaves\". I get how that works in general, but the table ended with this:[4, N] = [3, V] P(N|V)  P(leaves|N) + [3, Prep]  P(N|Prep)  P(leaves|N)) = (0. 004918  2/3  1/4), (0. 021267  1  1/4 )) = 0. 00614[4, V] = [3, V] P(V|V)  P(leaves|V) + [3, Prep]  P(V|Prep)  P(leaves|V) = 0 // because of the 0 transition probabilitiesso if 4, V was a possibility, we would just pick whichever had the higher probability between [4, N] and [4, V] as our final answer, right? Thank you!",
        "answers": [
            "Hi, Opps, I thought you were talking about the part a Virterbi algo. where you would pick the higher probability between [4, N] and [4, V] if they were both non-zero. This is because you want to select the state of the position of the sequence that has the most likelihood. In this context, there is no transition probability for leaves to be a verb which means it is most likely a noun in that contextI think you can look from slide 37 onwards in the slide deck lecture05-hmm_pos_tagging. pdf for more information, or you can look on page 4 of this page: https://web. stanford. edu/~jurafsky/slp3/A. pdf",
            "I might be mistaken, but I think you actually sum them instead of taking the max, at least for the forward algorithm. As seen in the forward algorithm, you loop through each word in the sentence, summing probabilities of each incoming sequence of POS, until reaching the final word. However, after the loop, you can see that we return the sum of the probabilities. \\sum_s^{ }\\pi\\left[n, s\\right]"
        ]
    },
    "481": {
        "question": "Hi TAs and Professor, As shown below, how do we decide on the number of feature functions that we should use? As long as we are good to cover all different cases in the training data or is there a underlying rule that sets a specific number of feature functions we should use? Thank you, Lixuan Yang",
        "answers": []
    },
    "482": {
        "question": "I'm having some trouble understanding the solution for ungraded exercise #6. What's the logic behind specifying conditions under which each dimension of the feature vector is 1?",
        "answers": [
            "From my understanding, feature value = 1 when the feature is true for all training data in the corpus, and 0 otherwise. For example, for all training data, start are always followed by N. Therefore, when we set feature factor N follow start, the feature function will = 1. "
        ]
    },
    "483": {
        "question": "Hi, I am a bit confused about using \"reduce\" in arc-eager transition, are we only allowed to use it  at the end or we can use it anytime once we want to drop a token in the stack?",
        "answers": [
            "It can be used anytime (but reducing too early may lead to incorrect results / bad parse trees, of course). "
        ]
    },
    "484": {
        "question": "Do we need to memorize the formule of the partial derivative of the Log Likelihood to prepare for the mid-term exam? Thank you.",
        "answers": [
            "No calculus will be required on the exam. You certainly don't have to know the formula for the partial derivatives of the log likelihood function. You should know the formulas for ngram probabilities, smoothing, HMMs, etc. My advise would be to focus on how these models work, rather than specifically memorizing the formulas. You should then be able to reconstruct the formulas if you need them. "
        ]
    },
    "485": {
        "question": "Does anyone know the transitions different than the one on questino a? I have been thinking for a long time about the solution to this question. Is there any other arc-standrard transitions that can generate the same dependency tree?",
        "answers": [
            "It's very possible that the dependency tree in part a) has a unique transition sequence. Try to construct a minimal example for which there are multiple sequences. "
        ]
    },
    "486": {
        "question": "Just to double check, are derivation trees and parse trees  essentially the same thing? ",
        "answers": [
            "Essentially :) The derivation tree specifies the order of production application. The nodes in the derivation tree (including the terminals) correspond to specific productions. In the parse tree, the internal nodes correspond to phrase labels, and the terminal nodes correspond to  tokens in the sentence. Because each production introduces a nonterminal (phrase label) derivation trees and parse trees are essentially identical (except for the terminals, which the derivation tree does not have). "
        ]
    },
    "487": {
        "question": "It states that \"You should bring a calculator\" on the exam 1 topics. How necessary is this? Are we going to answer some hard calculation question?",
        "answers": [
            "In lecture he mentioned it's helpful for computing the probabilities to arrive at an answer. I believe he also mentioned something along the lines of, if you don't have access to a calculator you would need to write out the calculations to show you would otherwise know how to do the multiplication if you had a calculator."
        ]
    },
    "488": {
        "question": "Hi there, In lecture 11 slide 2, it describes that the log-linear models can be considered as a neural network with one hidden layer. But it says the input of the neural network is:X\\ =\\ \\left[\\phi\\left(x, y\\right)_1, \\phi\\left(x, y\\right)_2, \\ . .. ,\\ \\phi\\left(x, y\\right)_d\\right]I am not sure why the input is this form. I can understand that for the log-linear model, we take the original input and each potential class label y  as the inputs to the feature function and generate a new d-dimensional input. But in the neural network scenario, since the output contains the probabilities for each class label y , so why do we still need to take the output of the feature function as the input of the neural network? Or does it mean that we should input each class label y  to the feature function and generate a corresponding feature vector to the input of the neural network? I might make the question a bit hard to understand, sorry about that, and I attached a screenshot of the slide as well. ",
        "answers": [
            "Thanks, this is not a great slide and potentially misleading. The analogy is a bit more loose than the slide makes it appear. In the log-linear model, we have the same weight vector for all classes. In the neural network, you can have different weights for each output unit. Ordinarily in a neural network, you would not use the feature function approach. "
        ]
    },
    "489": {
        "question": "Hi, Why can transition based parsing with arc-standard or arc-eager only produce projective dependency structures? Thank you! ",
        "answers": [
            "I don't know the details specifically but the intuition I observed is that these algos essentially looks at stuff on your immediate left and right and form dependency between them (this is why we have Maxent tagger function, which can establish dependency anywhere in the setence). For example, normally you have NP (5-&gt;7) -&gt; N (5-&gt;6), VP (6-&gt;7); this means that NP (which spans word_5 -&gt; word_7) can be broken down into N and VP which spans word 6 and 7 respectively. What a non-projective algo would do is that it will still see NP but now NP is (5-&gt;8), and NP is made up of N(5-&gt;6) and VP (7-&gt;8). There would be sth else in between them (6-&gt;7) but the algo somehow skipped over that to connect N and VP together into NP despite them being separated by sth in the middle. In your questions, arc-standard and arc-eager only looks at the immediate left/right neighbor, they can't \"skip\" a word like I mention =&gt; they are projective. Lmk if this helps"
        ]
    },
    "490": {
        "question": "I understand that the idea behind regularization is to subtact spikey parameter vectors from the log likelihood function, but can someone explain why we are multiplying by lamba/2 (what is lambda here? )",
        "answers": [
            "objective_function = min(loss) + lambda * max(regularization)lambda is one of those common parameters along with the learning rate alpha that you tuned usually via cross-validation. it balances the trade off between minimizing loss and maximizing the margin. using a boundary maximization model like SVM as example, the purpose of a high lambda value put more emphasis on the margin boundaries in an attempt to make the model more generalizable to unseen data. could however potentially lead to more errors if it is too large. conversely, a small lambda value will de-emphasize the regularization (thereby the margin-boundaries), thus more emphasis on the loss-function, which could make the model overfits to the training data. less generalizable to unseen data. lambda / 2 could just be it will simplify the derivative of the regularization term. when differentiating theta^2 using the power rule, the 2 being pull down will nicely cancel off with the 1/2, thereby leaving the derivative of a clean lambda * theta for the purpose of computing the gradient descent."
        ]
    },
    "491": {
        "question": "Hi I hope you are doing great. In a previous post about dependcy parcing an answer was:The bar | separates the first or last symbol from the remainder of the stack or buffer. w  stands for a buffer whose first token is w (read left-to-right) and w stands for a stack with top (the top is conventionally shown on the right). Is it just convention that the first element of the buffer is on the left of the bar. while the first element of the stack is on the right of bar?  So, they will always be depicted this way? Since they both operate as stacks, I am a little confesued as to why they are depicted differentlyThank you so much!",
        "answers": [
            "Yes, thats the convention. For the buffer, it makes sense to have the first word on the left, as the buffer initially contains all tokens in the sentence from left to right. For the stack, I think the idea is that you can easily visualize the shift operation if the top of the stack is on the right, and the tokens involved in a dependency edge will be adjancent. "
        ]
    },
    "492": {
        "question": "I was reviewing the lecture 4 notes for ngram language models and have a couple of questions regarding the terminology. 1. When we used N for the number of tokens, are tokens just the unique set of all unigrams excluding START? 2. For the tokens, do we include STOP, and why is it that we choose to exclude start/stop/both? 3. When we are talking about the size of the lexicon, are we referring to the number of tokens or the total number of words present? 4. When we did additive smoothing, for constant V (size of vocabularly), is that just the total number of words in the file? Thank you for the clarifications!",
        "answers": [
            "I could be wrong on these, so take them with a grain of salt. 1. N is the total number of words in the training data, not unique words but the total number of words. 2. I believe you include STOP and exclude START.  See #51. 3. The size of the lexicon is the number of unique words (or types--the number of distinct words). 4. V is the number of types (unique words) in the corpus.  Since you are adding one to each count in the numerator in Laplacian smoothing (over-counting every word by 1), you need to do the same for the denominator, which is why you add the size of the lexicon."
        ]
    },
    "493": {
        "question": "Hi, I just had a question about overfitting - the definition/when it occurs wasn't really clear to me from the lecture/slides, so i was wondering if someone could explain it. Thanks!",
        "answers": [
            "Overfitting means when your model fits too well that it loses generalization ability. Say you have a bunch of (x, y)s and you want to fit a model to predict y from x. You chose a very complicated model with tons of parameters and it worked very well: it reached 100% accuracy rate on this training set of (x, y). Now you receive another set of (x_new, y_new) and you are using it as your test set, your model is likely not going to do well on this new set of data because it's very specially designed for (x, y). This problem is overfiting: the model tried so hard on the training dataset that it can't generalize to other circumstances. Let me know if this make sense. "
        ]
    },
    "494": {
        "question": "Hi, I'm re-watching the lectures right now and I'm on the part where we talk about how Chinese is arguably more difficult to tokenize because several textual elements need to be grouped together to form one \"piece of meaning\" more often than that happens in English, making the anatomy of written English a little easier to parse than written Chinese, to my understanding. I'm curious to know if there exists a language with a very consistent grammar, very few colloquial deviations, and has an easily-parsable written form that's the ideal language for NLP? We're going over a bunch of edge-cases in English in this course, and I wanted to know if there's a natural language out there that's already an NLP dream, or at least easier to process than English? ",
        "answers": []
    },
    "495": {
        "question": "Will any of the content from Wednesday's lecture on Lexical Semantics be on the midterm? I don't see it explicitly mentioned on the topics sheet, but just wanted to confirm since we did cover the material in class.",
        "answers": [
            "The last topic covered on the midterm will be lecture 11. "
        ]
    },
    "496": {
        "question": "The size of the vocabulary (excluding the [START] and [END] markers) is 6. Was wondering why the denominator of the laplace bigram probability has 7 in it? Why is the [END] marker considered in the count of the vocabulary?",
        "answers": [
            "The probability distribution needs to assign a probability over all possible events so that they sum to 1. 0. The possible events are the possible next words given the left context. END is one of these possibilities, so it should receive some probability. Essentially we are treating it as just another possible event. "
        ]
    },
    "497": {
        "question": "Hi, I do not fully understand why (1, 1, 1, 1, 1, 1, 1, 1) is the answer for part bThank you",
        "answers": [
            "The features are designed in such a way that only features with the correct tag will be activated for any of the tokens in the data. So there is never any ambiguity between features that would encoder a preference for different tags. "
        ]
    },
    "498": {
        "question": "Hi there, I am reviewing the lecture notes and lecture 05 page 40, asks us what the advantage of using HMM as a language model over the plain word n-gram model. I am actually not sure about the answer to this question, any hints would be very appreciated. Thanks a lot in advance.",
        "answers": [
            "HMMs explicitly model a limited amount of syntactic structure (verbs typically follow nouns, etc. ). HMMs dont assume direct dependencies between words which makes the model generalize better. For example, it may have never seen the sequence the green cheese during training, but knows that green is an adjective and cheese is a noun. During training, the model has learned that nouns can follow adjectives, so this sentence should receive a high probability. "
        ]
    },
    "499": {
        "question": "Hi TAs and Professor, It seems that the part b and part c in the solutions of 'Linear models and feature functions' are missing, would you please provide a full edition? And I'm wondering whether there will be a piece of exam paper for reference, thank you! Sincerely, Yuning",
        "answers": [
            "What do you mean by a \"piece of exam paper\"? I don't know if i will be able to provide answers to the last part by tonight, unfortunately. Sorry I missed this one. "
        ]
    },
    "500": {
        "question": "Hello, I hope you are all doing well! I had a question regarding the probability calculations for Naive Bayes. In the exercise, I noticed that the probabilities don't divide by P(B), and I was wondering why that was the case? For example, this calculation (P(class=ham, Nigeria) = P(class=ham) * P(Nigeria | class=ham) = 2/5 * 1/7 = 2/35) doesn't divide by P(Nigeria). Should we always do this in Naive Bayes? How should we approach problems like this in the midterm? Best, Aishwarya",
        "answers": [
            "Because we are usually only interested in selecting one of the classes, not computing the exact conditional probability, its sufficient to compute the joint probability. The denominator for computing the conditional probability is the same for all classes, so you can think of this as a constant factor (in the slides, this was called alpha). "
        ]
    },
    "501": {
        "question": "Hi Professor, I'm wondering whether can I take the exam on zoom? And what is the time for us to finish the exam? Do I need to open the camera or download any software to monitor my exam? Thanks! Sincerely, Yuning",
        "answers": [
            "In general, all regular (non-CVN) students are expected to take the exam in-person. If you need an exception, for example because you are sick, please reach out to me by email. The exam will be in 309 Havemeyer at 1:10pm and will take 65min. "
        ]
    },
    "502": {
        "question": "want to make sure if the exam place is in the 309 HAVEMEYER ",
        "answers": [
            "Yes correct. "
        ]
    },
    "503": {
        "question": "Hello everyone, For the solution to part d of the question as in the title, \"s\" is a bit unclear in this context. So I added an extra line to the solution like the below, please correct me if I'm wrong:Input: An input sequence w, consisting of tokens w[1] . .. w[n]\n//initialization\nInitialize pi[0, start] = 1\nFor each tag t in T, initialize pi[0, t] = 0\n\n//main loop\nfor k = 1 . .. n:\n    for each tag t in T:\n      for each tag s in T:\n        if pi[k-1, s] &gt; 0 and  P(t|s) * P(w[k] | t) &gt; 0:\n          pi[k, t] += pi[k-1][s]   \nreturn sum_s pi[n, s]\nThank you :)",
        "answers": [
            "Yes, you're right. This looks more complete. "
        ]
    },
    "504": {
        "question": "Hi, I was hoping to get some clarification on the possible feature function given in the solution. Is the reason 3 has 3 conditions (wi = time and ti = V and ti-1 = N) instead of 2 conditions like 2 because there are two possible POS (time/V and time/N) defined for time in the sample sentences?  ",
        "answers": []
    },
    "505": {
        "question": "Hi, I have few questions of this ungraded assignment:1). For question a, do we must follow the format write in the answer, or it's okay that we write in other way, i. e. : {shift, left-arc_{nsubj}, shift, right-arc_{jobj}, shift, shift, shift, . .. .}, or just draw the graph like it shows in lecturenote. 2) For question a again, since it's an \"arc-standard\" transition, so if I remembered correctly, we should finish left-arc then use right-arc, but in this example, we do use a right-arc before we finish left-arc, does that mean it's not a strict rule? 3) For question b, can we use \"arc-eager\" transition or we still have to use \"arc-standard\" transition? If we have to use \"arc-standard\" transition, I got no idea how to do write a different one. Thanks in advance! !!",
        "answers": [
            "1) as long as you write down the required sequence of transitions, that would be okay on the exam. But note that its difficult to give partial credit if we dont have intermediate dependency structures (drawing the graph for those is fine as long as the transitions are also listed).  2) thats not a strict rule  in fact this relates to your question about part b). In some cases you can reach the correct final dependency tree by creating the left and right children in either order. 3) for part b) you should still use arc standard. I dont remember if the example sentence in a) can be created in different ways, but you can try to come up with a new (minimal) dependency structure that has an ambiguous transition sequence. "
        ]
    },
    "506": {
        "question": "Weight bias didnt understand when the professor was explaining the difference between when setting j = 0 to d vs 1 to d? For feature function: Input: n-dimensional vector where x is an input object and y is a possible outputOutput: n-dimensional vector feeds into another indicator function, where value is 0 or 1? Ungraded exercise: Can you please upload the solution for the last portion \"More advanced\"? For (a), is the answer here pretty open-ended, for instance, if I were to add a condition where wi = Fruit, Ti =N, Ti-1 = V, it would be correct as well? For (b), the vectors are all 1s because all the conditions are true based on the example? Thank you! ",
        "answers": [
            "1. Often, linear models are formulate like this: (\\sum_{i=1}^dx_iw_i) + bWhere b is a bias. Instead, it's convenient to just set w_0 = b and then add x_0 =1, so you get \\sum_{i=0}^dx_iw_i = (\\sum_{i=1}^d x_i w_i) + 1 \\cdot w_02. Not sure what you mean here. The feature function maps an (x, y) pair to a feature vector. x is the original input object, and y is one possible prediction candidate. The value of the feature function is a vector. The individual values are typically (but not always) binary indicator features. 3. Not sure I will get to the writeup for the last part before the exam, but it's not really relevant. For a) and b) Correct, there are many different feature functions. The weight vector can be all 1 because only features for the correct output (POS) will fire."
        ]
    },
    "507": {
        "question": "Why does the divide line mean? e. g. $(\\sigma, w_i | \\beta, A) =&gt; (\\sigma | w_i, \\beta, A)$? Why during the arc left operation word deleted from the stack side but not the buffer side? Why during the arc right operation, $w_i$ moved from top of the stack to the buffer? What is r and w in this case? difference between Ad and A? does it has something to do with not moving root to the right until everythings been parsed essentially? does it mean this rule doesnt apply to the arc eager example? What is lambda in the score equation for the graph-based approach?",
        "answers": [
            "1. The bar | separates the first or last symbol from the remainder of the stack or buffer. w  stands for a buffer whose first token is w (read left-to-right) and w stands for a stack with top (the top is conventionally shown on the right). 2. You only delete the dependent but not the head! If you delete the head, then neither of the tokens would be able to participate in further dependency relations. You would never be able to create more than one edge. 3. The reason the head shifts back to the buffer is that there is no other operation to shift it back.  You dont yet know if you may want to attach additional children to the right or to the left of that token, or if you want the token to receive a head to the left or to the right. You can always shift it back to the stack, but you can only move it to the buffer using the right-arc operation. 4. A is the partially built dependency structure that is part of this state. Ad is the gold dependency tree. w ans r are variables referring to the different tokens and relation labels in Ad. What the condition says is that, before you are allowed to do a right-arc with beta[0] as a dependent, all tokens w that are dependents of beta[0] in the gold tree Ad must have already been introduced in A. 5. Lambda is a scoring function computed by some machine learning model, for example a log linear model or a neural net. "
        ]
    },
    "508": {
        "question": "Hi! I'd like to make sure that there is a typo in the following slide from lecture 8:Arc-right should build an edge from the top word on the stack to the next word on the buffer; while the arc-left builds an edge from the next word on the buffer to the top word on the stack. Thank you!",
        "answers": [
            "Yes, good catch. Thank you. "
        ]
    },
    "509": {
        "question": "Hello, I hope you are doing well. When will the CVN instructions for the midterm be released? I am mostly curious about the time frame we have to take the exam within. Most classes I have taken so far provide a 48 hour time period within which we need to take the exam via Proctorio (~ same amount of time as in person students). Is it the same for this course? Knowing this will be immensely helpful in planning the next couple of days out. Thanks in advance! ",
        "answers": [
            "I will send out an email in a few minutes. You have the option of either taking the exam synchronously on Zoom, or using the CVN Proctorio system (like you have done in the past). For the asynchronous option, the exam will be open until Wednesday night. "
        ]
    },
    "510": {
        "question": "Hi, I had a question about the relationship between the logit and sigmoid function presented in lecture. I am a bit confused as to why we are able to interpret z\\ =\\ \\sum_{ }^{ }x\\cdot was the log-odds. I understand how logit and sigmoid are inverses of each other, however this fact seemed like it is making an assumption. Thanks!",
        "answers": [
            "Yes, we are assuming that the log-odds are a linear function of the x values. Logistic regression is sort of the simplest model you can assume if you want the output to be a probability, but there are other options. "
        ]
    },
    "511": {
        "question": "Question b asked us to show that the arc standard sequences are not unique, and there is no answer to this question on the posted solution page. Could you explain the way to answer this question? Thank you!",
        "answers": [
            "You would have to provide two different transition sequences resulting in the same dependency tree. ",
            "If it helps, looking specifically at transitions we can do to build the relations between he, sent and sent, her made things clearer for me."
        ]
    },
    "512": {
        "question": "In this question, Why do we need to compute the conditional probabilities?",
        "answers": [
            "Because the parameters of the trigram model are specified as conditional probabilities. In other words, you want P(flies | START, time) instead of P(START time flies). "
        ]
    },
    "513": {
        "question": "Hi NLP teaching staffs - I am wondering if the order of the right hand side matter here. For example:S -&gt; NP VVP -&gt; V NPWe record S -&gt; NP (cell 1-2) and V (cell 2-3) in the solution, however we do not record rule VP -&gt; V (cell 2 - 3) NP (cell - 2). Just want to make sure that VP -&gt; V NP # VP -&gt; NP V. Thank you,",
        "answers": [
            "Yes, the order matters! Otherwise, you would be able to swap the left and right constituent and that would give you \"ungrammatical\" sentences. Instead of \"the dog eats the bone\" you would be able to do things like \"the dog the bone eats\". "
        ]
    },
    "514": {
        "question": "Hi Professor and TAs, I just tested positive for COVID and have been running a fever and experiencing symptoms since yesterday. I am not sure if I will be able to take Monday's exam in-person due to the isolation protocol and my current symptoms - I was wondering how or when I can take the exam in this case? I apologize if this is the wrong place for this question, and I am happy to follow up through email and copy my class dean/advisor and provide a doctor's note if needed. Thank you for any help.",
        "answers": [
            "Hi sorry for the slow response. Typically email is better for questions like this one. If you feel well enough to take the exam, you can take it on Zoom. Otherwise, we can try to schedule a make-up exam once you have recovered. "
        ]
    },
    "515": {
        "question": "In Monday's lecture, Professor went through a list of topics to know for the midterm but I wasn't able to find the file. Where can I find that document?",
        "answers": [
            "This is listed under the Pages section in Courseworks."
        ]
    },
    "516": {
        "question": "Hi - I am at the CS TA room but it's very crowed and I cannot find the NLP TAs. Are the TAs in the room or are they somewhere else? Best,",
        "answers": []
    },
    "517": {
        "question": "Hi, I just realized that in my verify_grammar code for part 1 of the coding assignment, I return 2 things - a boolean true or false and a sentence that pinpoints exactly what causes the PCFG to not be in CNF because the assignment said to print out an error or confirmation message. Depending on how this code will be tested, would it be better to resubmit and take the 20 point deduction (same worth as the part) or is it possible to get partial credit on this? Thanks!",
        "answers": [
            "This is certainly not worth a 20 point deduction. I dont recommend resubmitting."
        ]
    },
    "518": {
        "question": "Hi, I made two submissions for hw2 as I realized that I didn't upload the most updated zip file for my first submission, which was on time. The second submission was 40 min late. I made some minor updates to my codes and did more testing in my second submission, but there shouldn't be that much of a difference. Is it OK to grade whichever submission that gives me the most points? If late penalty was gonna apply to my second submission, could you grade my first submission instead? Thanks!",
        "answers": [
            "No worries. Thats well within the grace period. "
        ]
    },
    "519": {
        "question": "Hi NLP teaching staffs - I am wondering why END symbol is included in the calculation of V, but not START sumbol? Tysm! !!",
        "answers": [
            "The START symbol is never predicted by the model and should therefore not have a probability. Its therefore not a word in the lexicon. In other words, the START symbol is not one of the possibilities for the next word that the model would consider. ",
            "I would also like to share that It is essential for the model to predict END symbol too, because the model must know how significant it is for END token to occur with other tokens (certain tokens, such as . (Full stop)).  Thats why it can be considered just as any other token, as it helps with the predictions. "
        ]
    },
    "520": {
        "question": "Hello NLP Teaching Team. I am encountering this SyntaxError when I attempt to evaluate my parser, although am unsure why it is incorrect? I am using Spyder (Python 3. 10).",
        "answers": [
            "This happens because you are running the command in the Python console, not the terminal. One way of running it from within Python is to use! python evaluate_parser. py . .. The ! runs a shell command in the Python console. "
        ]
    },
    "521": {
        "question": "Hi, Sorry, just to check, should we submit the assignment as 3 separate zip files, or put them all into a folder and then compress the entire folder? Thanks in advance! ",
        "answers": [
            "I believe we are to take those 3 files and put them into a singular . zip folder:\"Pack these files together in a zip or tgz file as described on top of this page. Please follow the submission instructions precisely! \"",
            "Put them all in a folder and then zip and submit the folder. "
        ]
    },
    "522": {
        "question": "Hello, In reading over the ed posts, such as #208, I saw that we need to multiply the probabilities in the PCFG algorithm, then take the natural log of this product. I implemented that as shown below, but I get the following \"out of domain\" error. Can you please provide some guidance as to whether my equation was implemented properly to calculate the log probabilities? I tried to use conditional statements to avoid ln(0), but since I am also getting the natural log of negative numbers, I must be doing something else incorrectly. prob= rule_prob * (probs[i, k][B]) * (probs[k, j][C])\nprint(rule_prob, probs[i, k][B], probs[k, j][C])\nprint(prob)\nif prob == -0. 0 or prob==0. 0:\n    prob = float(\"-inf\")\nelse:\n    prob=math. log(prob)\n#print(prob)\nif prob &gt; max_prob:\n    max_prob=prob\n    table[i, j][X]= ((i, k, B), (k, j, C))\n0. 285714285714 0. 0 0. 0\n0. 0\n0. 0588235294118 0. 0 -inf\nnan\n0. 0588235294118 0. 0 -inf\nnan\n0. 117647058824 0. 0 -inf\nnan\n0. 0384615384615 0. 0 -inf\nnan\n0. 153846153846 -3. 195065176174159 -inf\ninf\n0. 0384615384615 0. 0 -inf\nnan\n0. 0344827586207 0. 0 -inf\nnan\n0. 206896551724 -3. 195065176174159 1. 5670520399415253\n-1. 0358965659447945\n\nline 261, in parse_with_backpointers\n    prob=math. log(prob)\n         ^^^^^^^^^^^^^^\nValueError: math domain error\n",
        "answers": [
            "I would recommend using log probabilities consistently, that is, during initialization use the log probability and then sun the various log probs together (rather than multiplying).  "
        ]
    },
    "523": {
        "question": "Hi everyone, I see the assignment says to try and create small toy grammars to error check with and I was just wondering if there are any good examples or tips when it comes to making these grammars (i. e. covering all the potential edge cases). Thank you!",
        "answers": [
            "Not sure about edge cases, but you want some limited amount of ambiguity in the grammar for your test sentence. Three example from the lecture notes is a good starting point. "
        ]
    },
    "524": {
        "question": "    def parse_with_backpointers(self, tokens):\n        \"\"\"\n        Parse the input tokens and return a parse table and a probability table.\n        \"\"\"\n        # TODO, part 3\n        n = len(tokens)\n\n        # Create parse table and probability table as dictionaries\n        table = {}\n        probs = {}\n\n        # Initialize the tables\n        for i in range(n):\n            for j in range(i, n):\n                table[(i, j)] = {}\n                probs[(i, j)] = {}\n\n        # Fill in the diagonal of the parse table with nonterminals that can produce terminals\n        for i in range(n):\n            word = tokens[i]\n            for lhs, rhs, prob in self. grammar. rhs_to_rules. get((word, ), []):\n                table[(i, i)][lhs] = word\n                probs[(i, i)][lhs] = math. log(prob)\n\n        # Fill in the upper triangle of the parse table\n        for length in range(2, n + 1):\n            for i in range(n - length + 1):\n                j = i + length -1\n#                 j = i +length\n                for k in range(i, j):# i\n                    for lhs1 in table[(i, k)]:\n                        for lhs2 in table[(k +1, j)]: #k+1\n                            # Check if there are rules that can combine lhs1 and lhs2\n                            for lhs, rhs, prob in self. grammar. rhs_to_rules. get((lhs1, lhs2), []):\n                                new_prob = math. log(prob) + probs[(i, k)][lhs1] + probs[(k+1 , j)][lhs2] #k+1\n                                if lhs not in table[(i, j)] or new_prob &gt; probs[(i, j)][lhs]:\n                                    table[(i, j)][lhs] = ((lhs1, i, k), (lhs2, k+1 , j)) #k+1\n                                    probs[(i, j)][lhs] = new_prob\n        \n        return table, probs\n\ndef get_tree(chart, i, j, nt): \n    \"\"\"\n    Return the parse-tree rooted in non-terminal nt and covering span i, j.\n    \"\"\"\n    # Check if the span is valid and if the non-terminal exists in the chart\n    if (i, j) not in chart:\n        print(f\"Chart at ({i}, {j}) does not exist. \")\n        return None\n\n    if nt not in chart[(i, j)]:\n        print(f\"Chart at ({i}, {j}) does not contain non-terminal {nt}\")\n        return None\n\n    # Retrieve the backpointer for the non-terminal\n    backpointer = chart[(i, j - 1)][nt]\n\n    # If the backpointer is a string (terminal), return it as a terminal node\n    if isinstance(backpointer, str):\n        return (nt, backpointer)\n\n    # If the backpointer is a pair of backpointers, recursively construct the tree\n    left_child, right_child = backpointer\n    left_tree = get_tree(chart, left_child[1], left_child[2], left_child[0])\n    right_tree = get_tree(chart, right_child[1], right_child[2], right_child[0])\n\n    # Return a tuple representing the parse tree\n    return (nt, left_tree, right_tree)\n \n \n for the backpointers function, I'm getting \n \n True for both check table and check probs \n \n output: \n ({(0, 0): {'NP': 'miami'},\n  (0, 1): {'NP': (('NP', 0, 0), ('FLIGHTS', 1, 1)),\n   'FRAG': (('NP', 0, 0), ('NP', 1, 1)),\n   'FRAGBAR': (('NP', 0, 0), ('NP', 1, 1)),\n   'SQBAR': (('NP', 0, 0), ('NP', 1, 1)),\n   'VPBAR': (('NP', 0, 0), ('NP', 1, 1))},\n  (0, 2): {'FRAG': (('NP', 0, 0), ('FRAGBAR', 1, 2)),\n   'FRAGBAR': (('NP', 0, 0), ('FRAGBAR', 1, 2)),\n   'NP': (('NP', 0, 1), ('NP', 2, 2)),\n   'SQBAR': (('NP', 0, 0), ('SQBAR', 1, 2)),\n   'VPBAR': (('NP', 0, 0), ('VPBAR', 1, 2))},\n  (0, 3): {'FRAG': (('NP', 0, 0), ('FRAGBAR', 1, 3)),\n   'FRAGBAR': (('NP', 0, 0), ('FRAGBAR', 1, 3)),\n   'NP': (('NP', 0, 1), ('NP', 2, 3)),\n   'SQBAR': (('NP', 0, 0), ('SQBAR', 1, 3)),\n   'VPBAR': (('NP', 0, 0), ('VPBAR', 1, 3))},\n  (0, 4): {},\n  (0, 5): {},\n  (1, 1): {'FLIGHTS': 'flights', 'NP': 'flights'},\n  (1, 2): {'FRAG': (('NP', 1, 1), ('NP', 2, 2)),\n   'FRAGBAR': (('NP', 1, 1), ('NP', 2, 2)),\n   'NP': (('NP', 1, 1), ('NP', 2, 2)),\n   'SQBAR': (('NP', 1, 1), ('NP', 2, 2)),\n   'VPBAR': (('NP', 1, 1), ('NP', 2, 2))},\n  (1, 3): {'FRAG': (('NP', 1, 1), ('NP', 2, 3)),\n   'FRAGBAR': (('NP', 1, 1), ('NP', 2, 3)),\n   'NP': (('NP', 1, 1), ('NP', 2, 3)),\n   'SQBAR': (('NP', 1, 1), ('SQBAR', 2, 3)),\n   'VPBAR': (('NP', 1, 1), ('NP', 2, 3))},\n  (1, 4): {},\n  (1, 5): {},\n  (2, 2): {'CLEVELAND': 'cleveland', 'NP': 'cleveland'},\n  (2, 3): {'FRAG': (('NP', 2, 2), ('PP', 3, 3)),\n   'NP': (('NP', 2, 2), ('PP', 3, 3)),\n   'SQBAR': (('NP', 2, 2), ('PP', 3, 3)),\n   'VPBAR': (('NP', 2, 2), ('PP', 3, 3))},\n  (2, 4): {},\n  (2, 5): {},\n  (3, 3): {'FROM': 'from', 'PP': 'from'},\n  (3, 4): {},\n  (3, 5): {},\n  (4, 4): {'TO': 'to', 'X': 'to'},\n  (4, 5): {},\n  (5, 5): {'PUN': '. '}},\n {(0, 0): {'NP': -4. 706522680248739},\n  (0, 1): {'NP': -12. 121095561599688,\n   'FRAG': -10. 17027139774193,\n   'FRAGBAR': -10. 466537213884134,\n   'SQBAR': -10. 240986922539397,\n   'VPBAR': -8. 54766752612518},\n  (0, 2): {'FRAG': -16. 02265925150763,\n   'FRAGBAR': -16. 318925067651136,\n   'NP': -20. 683040775760592,\n   'SQBAR': -15. 549370753841544,\n   'VPBAR': -16. 030803078912392},\n  (0, 3): {'FRAG': -26. 066987069809162,\n   'FRAGBAR': -26. 36325288595267,\n   'NP': -30. 727368594062128,\n   'SQBAR': -24. 882202252912503,\n   'VPBAR': -26. 075130897213928},\n  (0, 4): {},\n  (0, 5): {},\n  (1, 1): {'FLIGHTS': 0. 0, 'NP': -3. 195065176174159},\n  (1, 2): {'FRAG': -9. 4443343943576,\n   'FRAGBAR': -9. 740600210499807,\n   'NP': -11. 757010390335061,\n   'SQBAR': -9. 515049919155068,\n   'VPBAR': -7. 82173052274085},\n  (1, 3): {'FRAG': -19. 488662212659136,\n   'FRAGBAR': -19. 78492802880134,\n   'NP': -21. 801338208636594,\n   'SQBAR': -18. 847881418226027,\n   'VPBAR': -17. 866058341042386},\n  (1, 4): {},\n  (1, 5): {},\n  (2, 2): {'CLEVELAND': 0. 0, 'NP': -3. 9805856768644103},\n  (2, 3): {'FRAG': -12. 020710341309226,\n   'NP': -14. 024913495165945,\n   'SQBAR': -14. 325018087614133,\n   'VPBAR': -13. 631870907056262},\n  (2, 4): {},\n  (2, 5): {},\n  (3, 3): {'FROM': 0. 0, 'PP': -6. 618738983514369},\n  (3, 4): {},\n  (3, 5): {},\n  (4, 4): {'TO': 0. 0, 'X': -1. 2527629684963681},\n  (4, 5): {},\n  (5, 5): {'PUN': 0. 0}})\n  \n  But for get trees, I keep getting: \n  print(get_tree(table, 0, len(toks) - 1, grammar. startsymbol))\n  Chart at (0, 5) does not contain non-terminal TOP\nNone",
        "answers": [
            "We discussed this during my office hours. Id be interested to hear what the issue was eventually. "
        ]
    },
    "525": {
        "question": "Been in the waiting room for a while, anyone know how to get in? ",
        "answers": [
            "Hi, There is waitlist. Will try to get to each one asap. But there are many people in the waiting room just fyi."
        ]
    },
    "526": {
        "question": "Hi, I just wanted to clarify what the late policy for this class is? On the syllabus it says we can submit a maximum of 4 days late with a 20 point deduction, does that mean 5 points are deducted each day, or the deduction happens as soon as it becomes late?",
        "answers": [
            "There is a one-time deduction of 20 points for any late submission. Please reach out to me if you think your circumstances merit an extension. "
        ]
    },
    "527": {
        "question": "I was wondering if someone could clarify why table[(2, 3)][\"FLIGHTS\"] should be flights? Using my understanding, using the hw example with tokens of ['flights', 'from', 'miami', 'to', 'cleveland', '. '] , table[0][1][\"FLIGHTS\"] would be flights but table[(2, 3)] should only have the rules that have miami  on the right side? Additionally, is there any particular way we should map the \"parent\" indices for terminals? Ex: for the case of table[(0, 1)'][\"Flights\"], what should we set this equal to? Thanks so much!",
        "answers": [
            "So, some of the examples in the assignment description exist only to illustrate the format of the table, not the specific values in a specific example. I don't quite understand what you mean by \"parent indices\". table[(0, 1)'][\"Flights\"] should be \"flights\". "
        ]
    },
    "528": {
        "question": "Hi NLP teaching staffs - I am wondering if there will be ungraded/ practice assignment for lecture 11. Thank you so much!",
        "answers": [
            "Ill try to get one up, but it doesnt currently exist. "
        ]
    },
    "529": {
        "question": "For checking the lhs_to_rules and rhs_to_rules, I use the same helper function to reduce the redundancy in code. I just want to double make sure that helper function is allowed for this assignment :)",
        "answers": [
            "Thats fine. "
        ]
    },
    "530": {
        "question": "What are the acceptable ranges for Coverage, Average F-score (parsed sentences), Average F-score (all sentences) in part 5 of hw2? Is there any way to improve the Average F-score (parsed sentences)?",
        "answers": [
            "What is it now? There shouldnt really be too much variation on these. "
        ]
    },
    "531": {
        "question": "Hi, I wonder if anyone got:Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9526771952649747, Average F-score (all sentences): 0. 6405932864712761Is it considered as a acceptable result? Thank you so much!",
        "answers": [
            "Got the same, pretty sure this matches the performance of Dr. Bauers solution "
        ]
    },
    "532": {
        "question": "For assigning the inner dictionary values of span length 1, since it still needs to be a pair would I make the two instances in the pair the same since it is only based off the one word. For example, when assigning the inner dictionary values for the first word 'flights' in a sentence, the inner dictionary would look like {'flights': ((0, 1, 'NP'), (0, 1, 'NP'))}.",
        "answers": [
            "flights should never be a key in the dictionary. Instead, the dictionary might look something like this {(0, 1): {NP: flights}}"
        ]
    },
    "533": {
        "question": "Hi! While working on part 3, my visual studio code stopped processing any changes I made. When I closed and reopened the window, I got the below error. What should I do to fix this? Thanks!",
        "answers": [
            "Make sure grammar. py is in the same directory.  You probably done any to edit this code in your Downloads directory because thats prone to being deleted. "
        ]
    },
    "534": {
        "question": "Hi! Will something similar to the inductive proof in n-Gram Language Models be tested in midterm or final exam? Thanks!",
        "answers": [
            "No there will not be any proofs. "
        ]
    },
    "535": {
        "question": "Hello, apologies if this was already answered somewhere. Let us consider the following situation:If there was such a rule in the form \"S --&gt; NP\", would an S be added to pi[0, 3]? Or would nothing be added, since the Cartesian product of a set with the empty set is always the empty set?",
        "answers": [
            "S --&gt; NP would not be a valid rule according to chomsky normal form."
        ]
    },
    "536": {
        "question": "Upon running evaluate_parser, I'm getting values that are somewhat lower than expected:Coverage: 62. 07%, Average F-score (parsed sentences): 0. 876237979991289, Average F-score (all sentences): 0. 5438718496497656Does anyone know where I should look to correct this?  Will I need to trace out the parses to see what's going wrong?",
        "answers": [
            "I suspect that you only keep the highest scoring nonterminal for each span, non the highest-scoring split for each nonterminal.  "
        ]
    },
    "537": {
        "question": "Hi, In HW2 Q1, suppose S is the start symbol, and A, B are nonliterals, are rules such as A -&gt; SB or A -&gt; BS allowed? These rules do not seem to make sense, but they do not seem to contradict the rules of CFG.",
        "answers": [
            "These rules would be permitted. The start symbol is just another nonterminal. In fact, it's okay for the start symbol to appear on the right hand side of a rule. "
        ]
    },
    "538": {
        "question": "Hi, just want to confirm whether should we print all results of functions in HW2, like shown in the assignment example code chunks (also for part 5), or just complete the functions and essential output?",
        "answers": [
            "The intermediate results on the assignment description are just shown for illustration purposes. They do not have to be part of the output of your submission. "
        ]
    },
    "539": {
        "question": "Why does transition-based dependency parsing guarantee a projective parse tree? I remember it has something to do with the bottom-up approach that the procedure takes, that a node collects its children before its parent. That intuitively makes sense but is there a more rigorous explanation? Appreciate any input :)",
        "answers": [
            "Following up on this procedure, how do we know that it's complete? It seems to me that the algorithm halts when one particular parse tree is generated due to its greedy nature. If that's true, do we run the procedure multiple times to generate the entire collection of parse trees, or do we use different oracles for this purpose?"
        ]
    },
    "540": {
        "question": "Hi! I was wondering if the teaching team (or anyone else) had any good tips to prepare for the midterm next week? Would it be most helpful to review previous lectures or HW assignments? Are there any other resources you recommend we look at? Thanks!",
        "answers": [
            " I think ungraded exercises are helpful to practice for the exam. Also, seek advice/helpful resources."
        ]
    },
    "541": {
        "question": "Hello! I hope you are all doing well. I had a question regarding the first part on the homework. Would zeros and punctuation be counted as terminals? For example, if I had PP-&gt; '0' or NP-&gt; '. ', would this be considered valid CNF Form? Thank you so much for your help!",
        "answers": [
            "Yes, these would be counted as terminals. Anything that never appears on the LHS of any rule is a terminal. "
        ]
    },
    "542": {
        "question": "Hi, I am a bit confused on the  difference between Part 2 and Part 3 and wanted to verify my understanding was correct. From my understanding, our Part 3 runs the CKY algorithm to generate the parse table and probabilities (according to the specified format) while then our part 2 verifies if the given sentence can be generated?",
        "answers": [
            "Correct!"
        ]
    },
    "543": {
        "question": "According to the HW spec:\"Because of numeric inaccuracies the sum may not be exactly 1. 0. \"When using the isclose method, can we leave the maximum allowed difference as default, or what should be the accepted difference from 1. 0?",
        "answers": [
            "You can use the default tolerance. "
        ]
    },
    "544": {
        "question": "Hello, May I ask when will the solutions for ungraded exercises in dependency parsing and linear model &amp; feature functions be released? Thank you :)",
        "answers": [
            "Here is the solution for the dependency parsing problem:https://courseworks2. columbia. edu/courses/179333/pages/ungraded-exercise-solution-transition-based-dependency-parsingI will post the feature function solution early this week. Sorry for the delay. "
        ]
    },
    "545": {
        "question": "In the main loop of the algorithm for PCFG parsing, we have to iterate through all X that belongs to N and all X-&gt;BC that belongs to R. Why can't we do something similar to the original CKY algorithm where we only calculate the probabilities where span(i, k) can form B and span(k, j) can form C? Since for the ones that do not meet this criteria, the probability will be 0 anyways. ",
        "answers": [
            "Im not sure I fully understand the question. There is a difference between how the algorithm is conceptually defined in pseudocode and how it is implemented. You could implement the rule lookup by taking all nonterminals with &gt;0 probability in (i, k), and and all nonterminals with &gt;0 probability in (k, j),  then go through all combinations B C and check it there is a matching A -&gt; B C rule. "
        ]
    },
    "546": {
        "question": "If get_tree is called with a non-terminal thats not in the tree (for example, calling get_tree(0, len(toks), grammar. startsymbol) on a table of a sentence that is not in the language (and therefore does not contain TOP), what should be returned? Is it an empty tuple? Thanks in advance! ",
        "answers": [
            "Correct :) the evaluation script catches the KeyError and counts the sentence as not parseable. "
        ]
    },
    "547": {
        "question": "Hi, I am wondering what is the coverage for our upcoming midterm? Which materials would be included and tested? Thank you.",
        "answers": [
            "#184"
        ]
    },
    "548": {
        "question": "When calculating probabilities in part 3, Lecture 7 seems to suggest multiplication of the probabilities. However, should we add the log probs instead of multiplying because they are log probs? Also, is it enough to multiply by the backpointers and assume that that will propagate all the multiplying across the probabilities?",
        "answers": [
            "Hello! Not a TA but based on my understanding, yes and yes. Because of the base case initialization, you can think of the probabilities getting multiplied (aka log probs getting added) recursively on each step."
        ]
    },
    "549": {
        "question": "I'm a little confused by this. In question 3 we are asked to make a data structure that has a dictionary in a dictionary in order to store back pointers and probabilities. My question is about whether we can store all the back pointer possibilities or just the one best backpointer. In other words, should each entry in the dictionary be a dictionary with only one key? In the problem it is states \"This value represents the log probability of the best parse tree (according to the grammar) for the span 0, 3 that results in an NP. \" This seems to suggest as long as we store the best tree for NP mapped to NP we can still have other mappings. For instance{Np : {NP : (npsource, num, num), P: ((source, i, k), (source2, k, j)})}Or would one need to REMOVE either NP or P from this list in order to only have the biggest one? This seems to be what is suggested here, but because of the structure of the dictionary is very unintuitive to me. Why store objects in a dictionary when we would only every have one key in every dictionary? Plus most parses have 2 things they point to.",
        "answers": [
            "I think I understand the wording a bit more now. Am I correct in understanding we ONLY replace those with a better probability that line up the same coordinates AND values. For example if there is already a an NP in (0, 2) and we find an NP with a larger probability then we switch them out? But if there is an NP in (0, 2) and we found a P with a smaller probability than the NP we could keep both because they are different keys?"
        ]
    },
    "550": {
        "question": "Curious if the answer we should be getting is a single output stating that the grammar is not valid? Are we expected to return additional outputs?",
        "answers": [
            "The main method in part 1 simply needs to print if the grammar is valid or not. The verify_grammar method simply needs to return True or False. No other output is needed. "
        ]
    },
    "551": {
        "question": "Not a question, but here's an issue I ran into in Part 5 that will hopefully help some other people as well. For context, I ran the command provided in the instructions python evaluate_parser. py atis3. pcfg atis3_test. ptb , but was getting the following output: Coverage: 0. 00%, Average F-score (parsed sentences): 0. 950447540861, Average F-score (all sentences): 0. 639094036096\nThe 0. 00% coverage was confusing, especially since the F-scores matched those in the instructions. After looking into the code for evaluate_parser. py, I realized that the code for computing coverage coverage = (parsed / total) *100 was operating on two ints (parsed and total), which defaulted to integer division (since my python command still points Python 2). Long story short, use python3 for testing evaluate_parser. py , though the regular python command might've been updated to point to Python 3 in newer systems.",
        "answers": [
            "Thanks. Good point in general, we assume Python 3 for all assignments. "
        ]
    },
    "552": {
        "question": "Hello, For Part 3 of HW2, when it says to use \"log probabilities\", are we expected to just use the natural log? Or should we use log to a specific base?",
        "answers": [
            "Hello! On the previous hw we used math. log2 ",
            "It doesnt really matter. In my implementation i just used math. log which is the natural logarithm (base e). "
        ]
    },
    "553": {
        "question": "Hello! I'm doing the assignment and I am trying to understand what 'TOP' means. In this case, is 'TOP' what we want to find in our table[0, len(tokens)] at the end of the CKY algorithm? Meaning, is 'TOP' the non-terminal that originates a sentence within the language? I'm confused because we also have 'S' as a non-terminal in the rules. Thank you!",
        "answers": [
            "TOP is the startsymbol in this particular grammar. So unless you find TOP in the span for the entire input, the sentence cannot be parsed. You should probably use grammar. startsymbol instead of hardcoding a specific startsymbol. "
        ]
    },
    "554": {
        "question": "Hi, Would HW3 be release this week or will be release during the week of Midterm 1? Thank you! Best, Adrian Harischand ",
        "answers": [
            "Im planning to release homework 3 toward the end of this week, after the due date for homework 2. "
        ]
    },
    "555": {
        "question": "In the description of part 3, it is given that \"the value of table[(0, 3)]['NP'] could be ((\"NP\", 0, 2), (\"FLIGHTS\", 2, 3))\", and \"the table entry for table[(2, 3)][\"FLIGHTS\"] should be \"flights. \" Are these table entries for the tokens ['flights', 'from', 'miami', 'to', 'cleveland', '. ']? If we run parse_with_backpointers on the above list of tokens, should we get the same entries in the table?",
        "answers": [
            "No the table entries do not match that example sentence. Its just an illustration of what the table format should look like. "
        ]
    },
    "556": {
        "question": "Hello, I am currently debugging my Code for part 2 of HW 2. I am getting an output of \"False\" for the test string, so I believe I am implementing the CKY algorithm incorrectly. To be clear, are the dimensions of the parser table pi as follows:                     [the length of tokens -1] x [the length of tokens]? If that is correct, there must be another bug somewhere in my code that I will look for.    def is_in_language(self, tokens):\n        \"\"\"\n        Membership checking. Parse the input tokens and return True if \n        the sentence is in the language described by the grammar. Otherwise\n        return False\n        \"\"\"\n        # TODO, part 2\n\n        #NOTE: CKY Psuedocode below\n        '''\n        Grammer G = {nonterminals, terminals, rules, start symbol}\n        input string tokens of len n\n        '''\n        nonterminals= grammar. lhs_to_rules. keys()\n        #array of rules without their probabilities\n        rules=[]\n        lhs=grammar. lhs_to_rules\n        for nt in lhs:\n            for i in range(len(lhs[nt])):\n                nt, rule=lhs[nt][i][0], lhs[nt][i][1]\n                rules. append((nt, rule))\n                #print((nt, rule))\n        S= grammar. startsymbol\n\n        n= len(tokens)\n        #create 2d parse table of non-terminals as a dictionary\n        #    keys: span (i, j)\n        #    values: list of non terminal for span\n        pi = {}\n        for i in range (0, n):\n            for j in range(0, n+1):\n                pi[i, j]= []\n        #INITIALIZATION\n        #initialize parse table with NT rules\n        #find all Non-terminals that generate token\n        for i in range (0, n):\n            for nt in nonterminals:\n                #production is the rule itself\n                production= (nt, tuple(tokens[i:i+1]))\n                if production in rules:\n                    #nt is A here\n                    pi[i, i+1]. append(nt)\n\n        #MAIN LOOP\n        for length in range(2, n+1):\n            print(length)\n            for i in range (0, n - length +1):\n                j= i + length\n                M=[]\n                for k in range (i+1, j):\n                    #again, go through all possible productions and try to find one in rules\n                    #difference, this time its not just any terminal in tokens\n                    #B and C come from existing parts of array, must go through all \n                    #  non-terminals in B and C, as they are in pi\n                    for nt in nonterminals:\n                        for B in pi[i, k]:   \n                            for C in pi[k, j]:\n                                production= (nt, (B , C)) \n                                if production in rules:\n                                    print(production)\n                                    M. append(nt)\n                pi[i, j]. append(M)\n        if S in pi[0, n]:\n            return True\n        else:\n            return False \n",
        "answers": [
            "I figured it out, I was using M. append(nt) instead of M. extend(nt)!"
        ]
    },
    "557": {
        "question": "Dear Professor, I am getting the following scores. On rounding them the scores are roughly matching. Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9504475408614075, Average F-score (all sentences): 0. 6390940360964636Just wanted to confirm if these are fine.",
        "answers": [
            "Hi, we have the same scores, refer to here: https://edstem. org/us/courses/46417/discussion/3606779? answer=8303893"
        ]
    },
    "558": {
        "question": "For the verify grammar problem, will there ever be a scenario where the inputted lhs has multiple words?  If so, do we just consider it as one word (since it is one string) and it is valid, or is it considered as multiple words and it is invalid?",
        "answers": [
            "No that cant happen. There will always be a single LHS symbol. "
        ]
    },
    "559": {
        "question": "1. is it okay if there are many for loops in verify grammar and running it on a bit dataset takes some time? I'm talking like 10 seconds2. In order for the grammar to be verified, all these following the only things we need to check? A) from the slides lecture 7 page 4, the node goes to either non-terminals or a terminal   (also does the amount of terminals/non-terminals matter? Should it be going to ONLY 2 non-terminals or 1 terminal? Or  could it go to 4 non-terminals or 1 non-terminal or 3 terminals? The 2 and 1 rules seems to be true for the grammar we are getting and the rules on the slides I just want to check)B) all of the probabilities when going to non-terminal nodes add up to 1if A) and B) are true can we assume this is a good grammar? Thanks",
        "answers": [
            "Hello! Not a TA but my understanding is we're checking if it's in Chomsky Normal Form, meaning every rule maps from a non-terminal to a single terminal, or from a non-terminal to two nonterminals. As far as I know, if every rule is in the form A -&gt; B C or A -&gt; b and the probabilities add up it should be good by the homework specs. Also, if all we're checking is the above conditions I'm pretty sure verify_grammar() can be O(n) for n production rules. I ran my implementation on my pretty old laptop on atis3. pcfg and it was quite a bit less than a second, maybe make sure you're using the provided dictionaries correctly?",
            "Im not a TA, but heres my understanding. Chomsky Normal Form allows production rules only of the form where there are two non-terminals or one terminal on the right hand side. The left hand side is always a Non-terminal. The probabilities for each of the non-terminals should add up to 1. Sometimes they might not add up exactly to 1, so the assignment suggests using math. isclose() for verification. "
        ]
    },
    "560": {
        "question": "This isn't a question, just wanted to say thanks! The feedback you gave me was helpful and I got everything working.",
        "answers": [
            "Glad that everything works!"
        ]
    },
    "561": {
        "question": "Hi, I'm getting all zeros. Coverage: 0. 00%, Average F-score (parsed sentences): 0. 0, Average F-score (all sentences): 0. 0I'm trying to find what's wrong with my setting but have no clue. Does anyone have the same problem and can give a hint about which part might have gone wrong?",
        "answers": [
            "I'd look into evaluate_parser. py in the actual evaluate_parser(), and maybe add some logging? There's a chance you're throwing a keyerror or something similar every time",
            "Its possible to at the output format is correct. Make sure each tree returned is a nested tuple. "
        ]
    },
    "562": {
        "question": "Is a TA able to endorse the answer to #175 please? I have the same question of can you assume all lhs are in the right format of only one nonterminal since you assume lhs make up inventory of nonterminals ",
        "answers": [
            "I just responded to that thread. The student response is correct.  "
        ]
    },
    "563": {
        "question": "Hi, As title, I'm getting ~0. 63 as the both F-scores, and they're exactly the same. The scores for every sentence are the same too. Any idea why? Thanks!",
        "answers": [
            "I suspect that the evaluation script is not registering when a sentence cannot be parsed. It expects that in such cases, the get_tree method throws a KeyError (because the start symbol will not be in the table for the full span). It likely also accepts None for unparseable sentences, but I'm not quite sure I remember correctly. "
        ]
    },
    "564": {
        "question": "Hi Andrew, Are your office hours still going on today? Thanks.",
        "answers": []
    },
    "565": {
        "question": "Hi, Correct me if I misunderstood, but according to #173, as long as our get_tree consistently works does it matter if we continue to receive the len(bps) ! = 2 error? My get_tree continues to work despite me receiving this error",
        "answers": [
            "The problem is likely in the way you represent terminal symbols in the table. The scripts expects that these are plain strings -- not, for example, single element tuples with a string in it, such as ('flights', ). I would recommend you try to fix this before submitting, but as I mentioned in #173 it doesn't really matter. If you choose a different representation for terminals, I recommend you adapt the checker function, to accept the new format. "
        ]
    },
    "566": {
        "question": "I have 3 questions regarding our hw1 solution. 1. In part 3) raw_bigram_probability function, Is the expected return value for input bigram ('START', 'START') 0? 2. In part 3) raw_bigram_probability function, why are we returning bigramcounts/total#ofwords like below? Aren't we supposed to return unigram probability for v (which in this example is bigram[1]) so P(v | u, w) = P(v)? if self. unigramcounts[(bigram[0], )] == 0: return self. bigramcounts[bigram] / float(self. total_num_words)3. In part 3)  raw_trigram_probability function, why are we handling unseen token by replacing its probability with uniform distribution if we have already replaced all the unseen tokens with UNK? I feel like its a redundant step. if trigram_count == 0: # We don't know anything about the distribution of the trigram[2]# in the context of trigram[0], trigram[1] -- assume# uniform distributionreturn 1 / self. total_num_words",
        "answers": [
            "1. That's correct! 2. I'm not sure I understand your question. When self. unigramcounts[(bigram[0], )] == 0, for instance when we have ('START', . . ), we want to ensure the denominator isn't 0 and thus we use self. bigramcounts[bigram] / float(self. total_num_words) (just like we use self. trigramcounts[trigram] for trigrams or self. unigramcounts[unigram] for unigrams in the numerator)3. There's a difference between unseen tokens and unseen context. You have to handle that edge case by returning  1 / self. total_num_words. Please see #15 as the difference was explained well there! Hope that helps!"
        ]
    },
    "567": {
        "question": "Hello! I'm currently running into slightly-lower-than-expected F-scores, does anyone have any tips for areas to debug? I'm going to check things like &gt; vs &gt;= but other than that I'm not sure what to look for. Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9248795500760255, Average F-score (all sentences): 0. 6219017664304309",
        "answers": [
            "I managed to match my score with the assignment by going through those tokens where didnt match 100% running evaluate_parser. py. Taking those non 100% matches tokens, using a debugger and check the backtracking and prob tables generated to see why your get_parse_tree is not returning the expected tree. Check to see is the divergence caused by the data or the code. ",
            "If anyone ends up with similar numbers to mine here, make sure you're checking the probabilities from the left and right symbols when picking the new parse, not just the probability for the new parsed nonterminal generating the left and right symbols. I missed that somehow and was just checking the probability of new parse -&gt; left_symbol right_symbol."
        ]
    },
    "568": {
        "question": "Hi, Are we suppose to utilize the probabilities in building our parse tree for part 4 or should  the table from part 3  contain the most probable parse for the given sentence which means that all we have to do in part 4 is just build that parse tree without having to worry about checking the probability of the subtrees? Thank you!",
        "answers": [
            "The table in part 3 will only contain the single most-probably expansion for each span and nonterminal. So in part 4 you do not have to consider the probabilities. You can simply traverse the table and assemble the tree, which is unique. "
        ]
    },
    "569": {
        "question": "Hi NLP teaching staffs - As exam 1 come close, I am wondering what is the material cutoff for review? Thank you so much",
        "answers": [
            "Good question. I will upload a topic overview sheet and we will go through this in class next week. We will cover everything up to and including the material of this week (the last topic word2vec). The material from next week will not be on the midterm. "
        ]
    },
    "570": {
        "question": "Hi NLP teaching staffs -I am running cky. py main and ran into an error: \"name chart is not definied\". Upon inspecting the pre-populated code for cky. py, I noticed that chart is not defined anywhere. I updated the given code to check_table_format(table) and it works. I am wondering if this is the correct fix or if there is anything wrong with my code? Thank you so much! cky. py main code:#print(parser. is_in_language(toks))#table, probs = parser. parse_with_backpointers(toks)#assert check_table_format(chart) - should this be assert check_table_format(table) instead? #assert check_probs_format(probs)",
        "answers": [
            "Hu, odd this wasn't noticed before -- looks like just a typo in the template. Please change \"chart\" to \"table\". "
        ]
    },
    "571": {
        "question": "I know I am supposed to email the TA who graded my assignment for a regrade request, but on courseworks it does not say a TA. It says graded by Professor Bauer. ",
        "answers": [
            "That's because I uploaded the grades. But if you look at the bottom of the comment field, it will say \"Grader: Shreyas\" "
        ]
    },
    "572": {
        "question": "Is there any way to test verify grammar other than running on atis3 and getting true in response? ",
        "answers": [
            "Sorry, not that I'm aware. ",
            "I would say it could be helpful to try out other grammars: make a toy grammar like the one from lecture (someone posted one on Ed), then break it in different ways to make it not in Chomsky Normal Form and verify that your check fails (don't think we're supposed to log the reason, but you could add logging temporarily, make sure the reason it fails is correct, then delete it later).",
            "You could try constructing a simple toy grammar, and then use grammary. py over it. Just remember that the response to whether your grammar is in Chomsky Normal Form or not depends on how you define the production rules of your grammar."
        ]
    },
    "573": {
        "question": "Hi, After implemented part 2, 3, 4, my get_tree function printed out this value: ('TOP', ('NP', ('NP', ('NP', ('NP', 'flights'), ('PP', 'from')), ('NP', 'miami')), ('PP', ('TO', 'to'), ('NP', 'cleveland'))), ('PUN', '. '))instead of the one in homework description like this:('TOP', ('NP', ('NP', 'flights'), ('NPBAR', ('PP', ('FROM', 'from'), ('NP', 'miami')), ('PP', ('TO', 'to'), ('NP', 'cleveland')))), ('PUN', '. '))\nI'm just wondering, is this an accepted format? Or, is there something wrong with my implementation of constructing the table of backpointers? I kind of have no clue about finding the bug. Thanks very much for any hint!",
        "answers": [
            "Bug resolved! Just for reference, I saved the wrong parse tree because of wrong probabilities."
        ]
    },
    "574": {
        "question": "To get the inventory of non-terminals, I use set(self. lhs_to_rules. keys()) Is this the correct way to approach it? If I add the rule ('NON_EXISTENT', ('AT', 'NP'), 1. 0) to the grammar, should it return True? This pertains to how I manage the list of non-terminals. After adding this rule, 'NON_EXISTENT' becomes a potential non-terminal symbol, given that 'AT' and 'NP' are already valid non-terminals. This rule follows the A -&gt; B C structure. My uncertainty lies in whether we should consider 'NON_EXISTENT' as a legitimate non-terminal symbol. In other words, should our list of non-terminals remain fixed, thereby deeming any new, previously non-existent terminals as invalid? Should we assume that all grammar rules are triple? Should we consider the edge case that the rule is not a triple, for example, do not have a probability? ",
        "answers": []
    },
    "575": {
        "question": "I saw the email that the grade has been released, but I checked my grading, it said 0 out of 0 points and a few comments. I downloaded the file and pretty sure that I did every part of the homework. So I am a little confused. ",
        "answers": [
            "That's surprising. Can you send email to May and cc me so we can look into that for you?"
        ]
    },
    "576": {
        "question": "1. In the last if statement of check_table_format, I noticed that the first element of the tuple is a string, while the second and third elements are integers. This seems to be in line with the assignment's instructions. However, the error message states: \"It must be a pair ((i, k, A), (k, j, B)). \" I believe it should be corrected to: \"it must be a pair ((A, i, k), (B, k, j)). \"to avoid confusion. 2. I tried to test my get_tree function by implementing Part 5, but there was an issue when I used the test sentence:toks = ['with', 'the', 'least', 'expensive', 'fare', '. ']I realised that this sentence cannot produce a valid parse tree. How should I handle such exceptions? Initially, I had it return None, but the coverage rate was still 100%, which doesn't seem correct. Do we need to check if it can generate a parse tree before we get it?",
        "answers": [
            "1. Thank you. I will make sure to correct this in future iterations if this assignment.  2. I think the evaluation script is looking for an exception, such as a KeyError when the sentence cannot be parsed. "
        ]
    },
    "577": {
        "question": "Hi Prof. Bauer and TAs, I am curious to see whether the rule ('NP', ('NP', 'VP'), xxx) is the same as ('NP', ('VP', 'NP'), xxx). Note, the order of non-terminal changed. In my implementation, recognizing them as different rules helped to increase the coverage by appx. 4% and the overall F-score by 2%, but the parsed F-score decreased by 2. 5%. My original implementation was very close to Prof. Bauer's performance. ",
        "answers": [
            "They are two different rules. The order of the nonterminal matters. "
        ]
    },
    "578": {
        "question": "Hi, For Part 1:We need to check if there are any terminals in lhs_to_rules, right? If so, when I reviewed the pcfg file, all lhs keys were uppercase strings. Can we assume an lhs key is a non-terminal if it is composed of uppercase strings? Do we just need to print a statement indicating whether the pcfg file is valid (e. g. , print \"Valid\" if the above method returns true, and \"Invalid\" if not)? Thanks for your help!",
        "answers": [
            "I believe in the assignment, it has been stated that all the symbols present on the LHS are assumed as Non-terminals. "
        ]
    },
    "579": {
        "question": "Hi, I am not sure when I should right-arc instead of left-arc in arc standard transition. Thank you",
        "answers": [
            "Look at the first word on the buffer and the top word on the stack. If the target dependency tree has a left edge between these tokens (buffer[0] to stack[0]), do a left arc. If it has a right edge between the tokens, do a right arc  but only if all dependents of buffer[0] that appear in the target have already been constructed. "
        ]
    },
    "580": {
        "question": "Hi, I had problems meeting the table checking where error comes from \"len(bps) ! = 2\". However, there is also \"Terminal symbols in the table could just be represented as strings. For example the table entry for table[(2, 3)][\"FLIGHTS\"] should be \"flights\". \" The statement from the homework handout is different from the checking criteria, what should we do? Thanks!",
        "answers": [
            "Hmm odd. Implement it according to the checking criteria when in doubt. But you can really do it either way as long as you are consistent and your get_tree function works. "
        ]
    },
    "581": {
        "question": "Hi, For Part 3, should we be calling is_in_language within parse_with_backpointers? Currently, my probs table comes out right but the backpointer table does not. The get_tree functions also outputs the correct solution despite this. Was just wondering if I was missing something. Thanks!",
        "answers": [
            "No is_in_language uses a different table format, so you do not need to call it in parse_with_backpointers. Is the problem that the check_table method reports and error but your get_tree method works? Whats the specific error in that case?"
        ]
    },
    "582": {
        "question": "Hi, In part 3, I am not sure how does the algorithm work to obtain the most probable parse tree. For example when I am focusing on the cell x03, do I check all combinations from x01, x13 and x02, x23 and just pick the combination that results in the highest probability to put into x03, discarding the other options? Therefore, when I move on to cell x04, the previously chosen combination stored in x03 will be used? Alternatively, is it that for example when I am focusing on the cell x03, for every type of nonterminals, I check all combinations from x01, x13 and x02, x23 and pick the combination that results in the highest probability to put into x03, discarding the other options of the same type of terminals? Such that I result in a set of unique non-terminals that have the highest probability from its own respective types. With this second approach, I am concerned that the table will discard potential combinations that may become more optimal later in building the table. Thanks!",
        "answers": [
            "Piggy back onto this, as I am dealing with the same exact question. Alternative to the greedy approach you mentioned would be to build for all possible parse trees and find the single max before returning from part 3 function. But the assignment explicitly mentioned that we should build the backtrack, and prob tables for the best parse tree during the parsing process. I still couldnt figure out how can this be done on the fly greedily. "
        ]
    },
    "583": {
        "question": "Hi, In HW2 part one, the structure of the dataset we received doesn't strictly follow CNF. For example we have  ('PP', ('ABOUT', 'NP'), 0. 00133511348465) seems to have a terminal ('ABOUT') and a nonterminal ('NP') on the right-hand side, which doesn't match the CNF format. Is this expected and I am not sure what we need to actually check for in part 1 to ensure match to CNF. Thanks!",
        "answers": [
            "That's incorrect. ABOUT is a nonterminal because it appears on the LHS of a rule. Note that there is also a terminal 'about'.  "
        ]
    },
    "584": {
        "question": "Hi Prof. Bauer and TAs, As of now, my parsed average F-score is ~82% and overall at 55%, which are lower than Prof. Bauer's scores. As I am reviewing the function evaluate_parser, it seems like there should be some checks for language attribution in the function parser_with_backpointers. However, I am a bit unsure about how we treat the return value in this case. Also, in the last paragraph of Part 2, you mentioned that the \"this method works for grammar with different start symbols. \" Would you mind elaborate on what will be the start symbols? I am using the grammar. startsymbol attribute in my implementation. I am not sure whether this is what you mean and whether changing this value assignment would increase F-score. My implementation of functions is_in_language and parse_with_backpointers are very similar. Looking forward to hearing back. ",
        "answers": [
            "Can you clarify what you mean by language attribution? Your choice of startsymbol is correct. This is essentially just a warning to not hardcode the startsymbol and instead use the one provided by the grammar. "
        ]
    },
    "585": {
        "question": "Hi! I was wondering what the exam format will be, is a pen and paper and theory kind or more coding based. ",
        "answers": [
            "There wont really be any coding on the midterm, except for possibly a modify this pseudocode question, or this is broken, can you find the mistake and fix it question.  Most of the midterm will be constructive problems, working with the algorithms we discussed and performing the steps of some algorithm on paper, similar to the ungraded exercises. There may be some factual short answer questions as well. "
        ]
    },
    "586": {
        "question": "Hi, For HW2, are there specific answers we should consistently be getting or is it like HW1 where answers vary?  I saw the HW said its graded on functionality, but I just wanted to receive some clarification. Thanks!",
        "answers": [
            "There is less flexibility in homework 2, except there may be some small variation based on how you break ties when two splits have the same probability (updating on &gt; or on &gt;=). I dont have the exact numbers ready right now, unfortunately, but you are welcome to post your result so you can compare with other students. It should be more or less consistent. "
        ]
    },
    "587": {
        "question": "Hi everyone, For HW2 Part2, the instruction said \"The ATIS grammar actually overgenerates a lot, so many unintuitive sentences can be parsed\", and from what I observed in our atis grammar file, there're more than one non-terminals generate the same terminal. I wonder how do we take care of that cases - for example, should we check all combinations of possible non-terminals existed in the rule during the parsing? Thank you so much!",
        "answers": [
            "The CKY algorithm already takes care of this. During initialization, if there are two rules for the same nonterminal, both nonterminals are included in the table.  For example if the input token s[1, 2] is flights, and there are rules NP -&gt; flights and FLIGHTS -&gt; flights, both NP and FLIGHTS cover the span [1, 2] and should be added to the table entry for this span. "
        ]
    },
    "588": {
        "question": "Hi, If there are multiple nonterminals in one box(e. g. the NP, N box), do we try all combinations available (e. g. P NP and P N)? If multiple combinations work, do we list out all possible nonterminals (e. g. if NP -&gt; P N is valid, will the red box become PP, NP? )Thank you",
        "answers": [
            "yes, you want to consider all feasible combinations"
        ]
    },
    "589": {
        "question": "Hi, I am wondering if there is a typo in the solution to ungraded exercise. Why the two terms look exactly the same but have different calculation results?",
        "answers": [
            "Yes, you are right. The second entry should be pi[2, V] = max( pi[1, N] * P(VIN) * P(flies | V), pi[1, V] * P(V|V) * P(flies|V)) :max (1/4 * 2/4 * 2/6), (1/12 * 0 * 2/6)) = 1/24"
        ]
    },
    "590": {
        "question": "Hi, Can we add \"import math\" to the grammar. py file? Thank you",
        "answers": [
            "Im not a TA, but I believe we probably can. Because the assignment even suggests using math. isclose(), which is basically a function present in the math library of python.",
            "Yes, importing math is okay. "
        ]
    },
    "591": {
        "question": "grammar. py1. do we need to explicitly check if a word exist in the non-terminal set ?      a. or can we assume that any word that is not part of the terminal set a valid non-terminal for the CNF ? cky. pt1. can we assume all words in the given input string are in the non-terminal set of the grammar ? 2. Part 3, the desire structure based on what is required by the check_table_format function for the backtrack table does not allow for multiple paths for the same constituency.    a. for example, what happen if cell (i, j) has two different k-splits that resulted in VP like the toy cat with glasses example.    b. the structure below will have the latter split overrides the earlier VP's tuple.    c. so, shouldn't the the bps be a LIST instead of a TUPLE of exactly length 2 or  str ?    d. ```json{      (i, j): {          VP -&gt; ((i, k_0, V), (k_0, j, VP),          ### what happen when k_1 is also another VP ? ??          ### VP -&gt; ((i, k_1, VP), (k_1, j, PP),          ### like above          NP -&gt; ((i, k_n, D), (k_n, j, N)),          . ..      }}```",
        "answers": [
            "1. For each symbol on the right hand side of a production, you can assume its a non terminal if it is ever used on the left hand side of any production. Otherwise, assume its a terminal. 2. Thats correct. The table only keeps track of one split per nonterminal. Thats okay because we are only interested in membership checking and then later obtaining the single highest probability parse tree. "
        ]
    },
    "592": {
        "question": "In the HW including Viterbi, I am confused about why we do not include [2, Prep] in our second step? How do we know to include it in the 3rd with [3, Prep]? Computing the DP value makes sense but I don't fully understand how the choice is being made. [2, N] = max( [1, N]  P(N|N)  P(flies|N), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 1/4  1/4), (1/12 2/3  1/4)) = 1/64[2, V] = max( [1, N]  P(V|N)  P(flies |V), [1, V]  P(N|V)  P(flies|N)) = max( (1/4 2/4  2/6), (1/12  0  2/6)) = 1/24[3, V] = max( [2, N] P(V|N)  P(like|V), [2, V]  P(V|V)  P(like|V)) = max( (1/64  2/4  2/6), (1/24  0  2/6)) = 1/384[3, Prep] = max( [2, N] P(Prep|N)  P(like|Prep), [2, V]  P(Prep|V)  P(like|Prep)) = max( (1/64  1/4  1), (1/24  1/3  1)) = 1/72",
        "answers": [
            "The emission probability P(flies| Prep) = 0.  In other words, we already know that flies cannot be a preposition, according to the lexicon. "
        ]
    },
    "593": {
        "question": "I could be wrong, but I believe there is a typo in the Ungraded HW Solutions for Viterbi for pi[2, V]. It says . .. , pi[1, v] * p(N | V) * P(flies | N)) but shouldn't it say . .. , pi[1, v] * p(V | V) * P(flies | V)) since we are solving for pi[2, V]? ",
        "answers": []
    },
    "594": {
        "question": "Follow up #51, (not sure if anyone else asked before) so for questions in later exam, we also follow this rule: to add END as a word type but not count START, so |V| = #. of different word +1. Or will we be given other specific rules in exam?",
        "answers": [
            "No, thats the general rule. Any other choice will give you incorrect/inaccurate distributions. "
        ]
    },
    "595": {
        "question": "courseworks mentions -Pack these files together in a zip or tgz file as described on top of this page. Please follow the submission instructions precisely! I can't seem to find the submission instructions referenced. thanks! ",
        "answers": [
            "Ah. Just zip the files and then upload the zip file as you did for homework 1. What you need to submitcky. pyevaluate_parser. pygrammar. pyDo not submit the data files. "
        ]
    },
    "596": {
        "question": "get_tree(table, 0, len(toks), grammar. startsymbol)is given in the written example in hw, and is used in the evaluator. I'm assuming that the ranges indicated in the CKY algo are inclusive [i, j], which means my final parse chart would have a entry at (0, len(toks)-1), 'TOP', but not at (0, len(toks)), since len(toks) is one outside the range of the string. Is it acceptable for evaluation purposes for the get_tree method to work with inclusive indexing for ease of recursion? ",
        "answers": [
            "The indices specify the left and right boundaries of the tokens. So [0, 1] would be the single token at index 0, and [0, N] would be the entire sentence of length N. Your code should follow this convention. "
        ]
    },
    "597": {
        "question": "Hi I hope you are doing great! I worked through  the ungraded ckyand I was a liitle unsure of something  in part b.  so  am i right about thinking that backpointers essentially tell us if there was multiple ways to build up to so block in the sentence. so if there was no point where there were different ways to get to the same box, then there would only be one parse tree? am i right in think that parse trees only come down to the last box we fill? If there is more than one parse tree does that only show up in the last square we fill where we derive the final S?  Like if there were 3 options for that final S then there would be 3 parse trees? thank you so much!",
        "answers": [
            "In the example we saw in class, there is only a single S entry in [0, 6] (with backpointers to NP in [0, 1] and VP in [1, 6]). However, there are ultimately two different parse trees because there are two backpointers for VP in [1, 6]. So it doesnt just come down to the last cell in the table. Each time you find a new way of constructing a non terminal, you are creating a new backpointer. "
        ]
    },
    "598": {
        "question": "from the textbook (17. 2. 1), the start symbol 'S' is considered a member of the non-terminal symbol set. 1. Given that, does that mean we can expect to see 'S' in right-hand-side of CKY's constituency ruie. Is something below legitimate: NP -&gt; S VPthe form above look weird as it doesn't make much sense. A NP can be a constituency describing start -&gt; verb phrase. Basically implying NP -&gt; VP ? 2. If we can't do what above, then how are we suppose to treat 'S' symbols that appear in any other cells not (0, n) memoization matrix during  the CKY algorithm.",
        "answers": [
            "The startsymbol is a nonterminal and can appear on the right hand side of productions. An example for this are sentences like he said that S "
        ]
    },
    "599": {
        "question": "Hi curious if Vedangi Kishor Wagh still holding OH in room 122 Mudd today? Not seeing her. Thanks!",
        "answers": [
            "Hi Vedangi sent out a message earlier today to move her office hours this week to Thursday from 12:00 noon to 1:30 pm on Zoom. https://columbiauniversity. zoom. us/j/6826910890? pwd=RVJLeURDemRYSDloZDdSNzQxakhGdz09Make sure you receive notifications sent through courseworks. We will generally use courseworks announcement for communication. "
        ]
    },
    "600": {
        "question": "In the Ungraded assignments regarding HMMs, Viterbi and Forward Algorithms, In question d, is the solution simply turning the Viterbi algorithm into the Forward algorithm? Thanks!",
        "answers": []
    },
    "601": {
        "question": "Hi everyone. In case anyone is interested in the \"toy grammar\" for HW2, here is the one I made (based on lecture 7) and used for myself. Please let me know if I need to correct anything. Best.",
        "answers": [
            "Do you not need a set of rules that define what the TOP rule can create, or to redefine the start symbol to one or more of the non-terminals? Otherwise, there is no way to get from TOP to anywhere else when parsing.",
            "minor typo in the provided pcfg file. The start symbol need to match up with S -&gt; NP VP in order for CKY to able to work when checking the [0, n] cell. Or change all instance S to TOP to match the given atis3 test file. S ; 1. 0\n#TOP ; 1. 0\n\nS -&gt; NP VP ; 1. 0\nVP -&gt; V NP ; 0. 6\nVP -&gt; VP PP ; 0. 4\nPP -&gt; P NP ; 1. 0\nNP -&gt; D N ; 0. 7\nNP -&gt; NP PP ; 0. 2\n\nNP -&gt; she ; 0. 05\nNP -&gt; glasses ; 0. 05\nD -&gt; the ; 1. 0\nN -&gt; cat ; 0. 3\nN -&gt; glasses ; 0. 7\nV -&gt; saw ; 1. 0\nP -&gt; with ; 1. 0\n"
        ]
    },
    "602": {
        "question": "Hi, love this class and want to make sure I do enough to secure a solid participation grade. Schedule's been busy, however, so in-lecture might be hard for me. What are some other good ways to make sure I participate enough? ",
        "answers": [
            "I'm not a TA so take this with a grain of salt but I recall the professor mentioned asking or participating on Edstem + OH could count towards your participation grade!",
            "Correct, participation in Ed courts for participation. Note that this is technically an in person class, so the expectation is that you attend most lectures (although Im not taking attendance). ",
            "Not a TA, but I recall that participating in the Ed discussion should count toward the participation grade.",
            "Can we have a more clear expectation for participation on Ed + OH? Like is it posting/replying how many times vs. attending OH how often, etc?"
        ]
    },
    "603": {
        "question": "Hi, when do the solution will be released for ungraded exercises for HMM? ",
        "answers": [
            "Sorry I thought I had uploaded this already but must have forgotten to release it :) Ill post it later today. "
        ]
    },
    "604": {
        "question": "Hello, When calculating my log probabilities, I keep getting a math domain error. Stack overflow points to errors with numbers close to 0 triggering this exception. I am not sure if I'm the only running into this error but a suggested fix according to the internet is to calculate the log probability of the probability + epsilon. I wanted to confirm if this is acceptable or if there are better alternatives. Thank you! Best,",
        "answers": [
            "This is because you have some trigram_prob = 0. You might want to double check your smoothed probability to avoid this case"
        ]
    },
    "605": {
        "question": "I just went through all the unread discussions on ed and realized that somewhere it was mentioned that we need to comment out the tests we conducted in main. I did not do that and I am sorry for any inconvenience caused. I cannot resubmit now as well. Should I do something about it? I apologize again and will make sure to do so for the next assignment.",
        "answers": [
            "Don't worry about it. We will not actually run the program itself, but rather import your class and then call the individual methods. "
        ]
    },
    "606": {
        "question": "Dear Professor Bauer, I hope this message finds you well. My name is Yolanda Zhu, and I am in your Natural Language Processing class. I regret to inform you that I tested positive for COVID-19 earlier this week and was feeling extremely unwell. While I initially tried to push through and complete the homework due tonight, my symptoms made it particularly challenging. Therefore, Im wondering if its possible for me to request a 1-2 day extension for HW1. I understand the importance of meeting deadlines, but under these circumstances, I hope you can consider my request. I truly apologize for the inconvenience and appreciate your understanding during this challenging time. Thank you for considering my situation. I look forward to your response. Warm regards, Yolanda Zhu",
        "answers": [
            "Responded by email. Closing this thread. "
        ]
    },
    "607": {
        "question": "Going off of some previous posts regarding accuracy, I was wondering at what point does the accuracy start to become suspiciously high? Would accuracy of 0. 90 or 0. 925 or even 0. 95 seem off? Thanks!",
        "answers": [
            "Yes, that's too high. Sorry for the late response. I suspect you didn't set up the experiment in part 7 correctly (you are probably testing one of the models against itself). "
        ]
    },
    "608": {
        "question": "Not sure if this was already addressed, but are comments in the code read and taken into consideration, or should we just not bother writing them? Thank you!",
        "answers": [
            "The way we grade assignments in general is to test the individual methods first using an autograding script. If they dont work as expected, the TAs look at your code in more detail. So comments would be read and taken into account at that stage. Comments are not required however. More generally, think of comments as improving readability for your own sake. Its good to get into the habit of writing meaningful comments. "
        ]
    },
    "609": {
        "question": "Sorry these questions have been asked a bunch, but does a perplexity score of ~85 for the test data tell me something is not right? For the training data I get a perplexity score of ~11  and my accuracy score comes out to about 87%",
        "answers": [
            "Hm, the perplexity scores seem somewhat low, did you maybe include the start symbol in the denominator somewhere?"
        ]
    },
    "610": {
        "question": "Hi Prof. Bauer and TAs, In lecture 4 slides, I am a bit unclear as to exactly what we should use for N in the unigram probability calculation below. Take the example of ungraded exercise 2 - part 1. START dog bites human ENDSTART dog bites duck ENDSTART dog eats duck ENDSTART duck bites human ENDSTART human eats duck ENDSTART human eats salad ENDWhat value should we use for N? ",
        "answers": [
            "N in this case would be 24 (the sum of length of all the sentences, including END, but excluding START). V would be {dog, bites, human, duck, eats, salad, END}  so |V| = 7"
        ]
    },
    "611": {
        "question": "For part 7 is it normal to get 100% accuracy?",
        "answers": [
            "No, there is likely a bug in your experimental setup, Im afraid. "
        ]
    },
    "612": {
        "question": "Hi, According to post #55 , we need to handle bigram edge cases of [\"START\", x]. What are we supposed to do in this case exactly? Thanks.",
        "answers": [
            "The reason why you need to handle this case separately is that count(START) is 0. My suggestion would be to keep track of the number of sentences in the training data in a separate instance variable, and then use that value for the denominator when computing P(x |start). "
        ]
    },
    "613": {
        "question": "raw_bigram_probability('START', 'START') raw_trigram_probability ('START', 'START', 'START')",
        "answers": [
            "These should be 0. "
        ]
    },
    "614": {
        "question": "For Q6 I'm getting:\n(base) omernauer@MacBook-Pro-689 HW1 % python -i trigram_model. py hw1_data/brown_train. txt hw1_data/brown_test. txt\nM: 98203\nl -4. 488916578819041\npp = 22. 454249203523293\n&gt;&gt;&gt; \n(base) omernauer@MacBook-Pro-689 HW1 % python -i trigram_model. py hw1_data/brown_train. txt hw1_data/brown_train. txt\nM: 1042565\nl -2. 1932556839359254\npp = 4. 573363791549528\nHow can I understand where my mistake is?\n\n\nimport sys\nfrom collections import defaultdict\nimport math\nimport random\nimport os\nimport os. path\n\"\"\"\nCOMS W4705 - Natural Language Processing - Fall 2023 \nProgramming Homework 1 - Trigram Language Models\nDaniel Bauer\n\"\"\"\n\n\ndef corpus_reader(corpusfile, lexicon=None): \n    with open(corpusfile, 'r') as corpus:\n        for line in corpus: \n            if line. strip():\n                sequence = line. lower(). strip(). split()\n                if lexicon: \n                    yield [word if word in lexicon else \"UNK\" for word in sequence]\n                else: \n                    yield sequence\n\n\ndef get_lexicon(corpus):\n    word_counts = defaultdict(int)\n    for sentence in corpus:\n        for word in sentence: \n            word_counts[word] += 1\n    return set(word for word in word_counts if word_counts[word] &gt; 1)  \n\n\ndef get_ngrams(sequence, n):\n    \"\"\"    COMPLETE THIS FUNCTION (PART 1)    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.    This should work for arbitrary values of n &gt;= 1     \"\"\"    ngrams = []\n\n    if n == 1:\n        init_tuple = 'START',\n        ngrams. append(init_tuple)\n        for word in sequence:\n            ngrams. append(tuple([word]))\n        ngrams. append(tuple(['STOP']))\n    elif n == 2:\n        bituple = ['START'] * 2\n        for word in sequence:\n            bituple. pop(0)\n            bituple. append(word)\n            ngrams. append(tuple(bituple))\n        bituple. pop(0)\n        bituple. append('STOP')\n        ngrams. append(tuple(bituple))\n    elif n == 3:\n        trituple = ['START'] * 3\n        for word in sequence:\n            trituple. pop(0)\n            trituple. append(word)\n            ngrams. append(tuple(trituple))\n        trituple. pop(0)\n        trituple. append('STOP')\n        ngrams. append(tuple(trituple))\n    return ngrams\n\n\nclass TrigramModel(object):\n    \n    def __init__(self, corpusfile):\n    \n        # Iterate through the corpus once to build a lexicon \n        generator = corpus_reader(corpusfile)\n        self. lexicon = get_lexicon(generator)\n        self. lexicon. add(\"UNK\")\n        self. lexicon. add(\"START\")\n        self. lexicon. add(\"STOP\")\n    \n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self. lexicon)\n        self. count_ngrams(generator)\n\n\n        # number of words = wordcount\n        generator = corpus_reader(corpusfile)\n        unigramcount = defaultdict(int)\n        uc = []\n        for sentence in generator:\n            uc. extend(get_ngrams(sentence, 1))\n        for word in uc:\n            unigramcount[word] += 1\n        self. wordcount = len(unigramcount)\n\n\n    def cut_trigram(self, trigram):  # returns a bigram\n        a = list(trigram)\n        a. pop(-1)\n        b = tuple(a)\n        return b\n\n    def cut_bigram(self, bigram):  # returns a unigram\n        a = list(bigram)\n        a. pop(-1)\n        b = tuple(a)\n        return b\n\n    def count_ngrams(self, corpus):\n        \"\"\"        COMPLETE THIS METHOD (PART 2)        Given a corpus iterator, populate dictionaries of unigram, bigram,        and trigram counts.         \"\"\"        self. unigramcounts = defaultdict(int)  # might want to use defaultdict or Counter instead\n        self. bigramcounts = defaultdict(int)\n        self. trigramcounts = defaultdict(int)\n\n        unigram_corpus = []\n        bigram_corpus = []\n        trigram_corpus = []\n\n        for sentence in corpus:\n            unigram_corpus. extend(get_ngrams(sentence, 1))\n            bigram_corpus. extend(get_ngrams(sentence, 2))\n            trigram_corpus. extend(get_ngrams(sentence, 3))\n\n        for word in unigram_corpus:\n            self. unigramcounts[word] += 1\n\n        for word in bigram_corpus:\n            self. bigramcounts[word] += 1\n\n        for word in trigram_corpus:\n            self. trigramcounts[word] += 1\n\n        #print(\"unigram count:\", self. unigramcounts)\n        #print(\"bigram count:\", self. bigramcounts)\n        #print(\"trigram count:\", self. trigramcounts)\n        return\n\n    def raw_trigram_probability(self, trigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 3)        Returns the raw (unsmoothed) trigram probability        \"\"\"        bigram = self. cut_trigram(trigram)\n\n        if self. bigramcounts[bigram] == 0:  # If we haven't seen the preceeding bigram\n            # Technique A\n            #return 1/(len(self. lexicon)-1)  # Taking out start\n\n            # Technique B\n            unigram = self. cut_bigram(bigram)\n            return self. raw_unigram_probability(unigram)\n        else:\n            return self. trigramcounts[trigram]/self. bigramcounts[bigram]\n\n\n\n    def raw_bigram_probability(self, bigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 3)        Returns the raw (unsmoothed) bigram probability        \"\"\"        unigram = self. cut_bigram(bigram)\n        if self. unigramcounts[unigram] ! = 0:\n            return self. bigramcounts[bigram]/self. unigramcounts[unigram]\n        else:\n            return 0\n    \n    def raw_unigram_probability(self, unigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 3)        Returns the raw (unsmoothed) unigram probability.        \"\"\"        return self. unigramcounts[unigram]/self. wordcount\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.\n\n\n    def generate_sentence(self, t=20): \n        \"\"\"        COMPLETE THIS METHOD (OPTIONAL)        Generate a random sentence from the trigram model. t specifies the        max length, but the sentence may be shorter if STOP is reached.        \"\"\"        return result            \n\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"        COMPLETE THIS METHOD (PART 4)        Returns the smoothed trigram probability (using linear interpolation).         \"\"\"        lambda1 = 1/3. 0\n        lambda2 = 1/3. 0\n        lambda3 = 1/3. 0\n        bigram = self. cut_trigram(trigram)\n        unigram = self. cut_bigram(bigram)\n        return (lambda1*self. raw_trigram_probability(trigram) +\n                lambda2*self. raw_bigram_probability(bigram) +\n                lambda3*self. raw_unigram_probability(unigram))\n\n\n        \n    def sentence_logprob(self, sentence):\n        \"\"\"        COMPLETE THIS METHOD (PART 5)        Returns the log probability of an entire sequence.        \"\"\"        probability = 0\n        trigrams = get_ngrams(sentence, 3)\n        for trigram in trigrams:\n            probability += math. log2(self. smoothed_trigram_probability(trigram))\n        return probability\n\n    def perplexity(self, corpus):\n        \"\"\"        COMPLETE THIS METHOD (PART 6)         Returns the log probability of an entire sequence.        \"\"\"        # Count the amount of words = M\n        counter = 0\n        uni = []\n        m_sum = 0\n        for s in corpus:\n            uni. extend(get_ngrams(s, 1))\n            m_sum += self. sentence_logprob(s)\n\n        for word in uni:\n            if word ! = (\"START\", ):\n                counter += 1\n        M = counter\n        print(\"M:\", M)\n\n\n\n        l = m_sum/M\n        print(\"l\", l)\n        return math. pow(2, -l)\n\n\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n\n        model1 = TrigramModel(training_file1)\n        model2 = TrigramModel(training_file2)\n\n        total = 0\n        correct = 0       \n \n        for f in os. listdir(testdir1):\n            pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n            if pp &lt; pp2:\n                correct += 1\n            total += 1\n\n\n            # . .\n    \n        for f in os. listdir(testdir2):\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\n            pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n            if pp &gt; pp2:\n                correct += 1\n            total += 1\n        print(\"hi\")\n        return correct/total\n\nif __name__ == \"__main__\":\n\n    model = TrigramModel(sys. argv[1])\n\n    # put test code here. ..\n    # or run the script from the command line with \n    # $ python -i trigram_model. py [corpus_file]\n    # &gt;&gt;&gt; \n    #\n    # you can then call methods on the model instance in the interactive \n    # Python prompt. \n\n    \n    # Testing perplexity:\n    dev_corpus = corpus_reader(sys. argv[2], model. lexicon)\n\n    pp = model. perplexity(dev_corpus)\n    print(pp)\n\n\n    # Essay scoring experiment: \n    acc = essay_scoring_experiment('train_high. txt', '/train_low. txt', \"test_high\", \"test_low\")\n    print(acc)\n\n",
        "answers": [
            "Hi, one obvious bug is that in your code, you have  self. wordcount = len(unigramcount)-1 \nThis should beself. wordcount = sum(unigramcount. values()) \nBy modifying this I was able to get:M: 98203\nl -6. 8231568321629466\n113. 23348441315729\nYou might also want to consider modifying your calculations for raw probabilities to better handle edge cases. For ex, I modified your smoothed_trigram_probability and raw_unigram_probability and obtained:M: 98203\nl -7. 697321492396698\n207. 5509165052973\nLet me know if you have any other questions."
        ]
    },
    "615": {
        "question": "I am having the same issue as #93 but I re-downloaded the zip file twice and still get the following error:FileNotFoundError: [Errno 2] No such file or directory: 'hw1_data/ets_toefl_data/test_low/1443872. txt'when I run: python3 trigram_model. py brown_train. txt brown_test. txtI have checked that I am in the correct directory.",
        "answers": [
            "I ran into the same issue actually about 10 minutes ago, and I ended up finding what my error was. Check your model1 and model2 testdir's in essay_scoring_experiment! That solved my error on my end! And also, part 7 doesn't take arguments, rather it uses the last few lines of code in the main method!"
        ]
    },
    "616": {
        "question": "Hello, I just wanted to check that my perplexity of 132 is a reasonable output to have. Thank you for your time and help, the TAs have been extremely helpful during OH and in answering discussion posts! My TrigramModel class is below:class TrigramModel(object):\n    \n    def __init__(self, corpusfile):\n    \n        # Iterate through the corpus once to build a lexicon \n        generator = corpus_reader(corpusfile)\n        self. lexicon = get_lexicon(generator)\n        self. lexicon. add(\"UNK\")\n        self. lexicon. add(\"START\")\n        self. lexicon. add(\"STOP\")\n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self. lexicon)\n        self. count_ngrams(generator)\n\n        #keep track of the number of tokens, and then number of sentences\n        self. total_tokens=0\n        self. num_sents=0\n        for sentence in corpus_reader(corpusfile, self. lexicon):\n            self. num_sents+=1\n            self. total_tokens+= len(sentence)\n            #must count number of STOP tokens too\n            self. total_tokens+=1\n        print(self. total_tokens)\n\n    #store stop in gram count, do not store start\n    #trigram, wouldn't store start start, only ever appears in context: never predicting start\n    #no triple start in tri, double in bi, single in uni\n    #if see start in unigram, produce 0; need stop\n\n    def count_ngrams(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts. \n        \"\"\"\n        self. unigramcounts = defaultdict(int) # might want to use defaultdict or Counter instead\n        self. bigramcounts = defaultdict(int)\n        self. trigramcounts = defaultdict(int)\n        self. total_tokens = 0\n        self. num_sents = 0\n        ##Your code here\n        #Call get ngrams, if already in dict, increment current one, if not add to dictionary and count to 1\n        #call get_ngrams, each sentence is a sequence\n        #Go through each sentence, pad with starts and stops\n        \n        for sentence in corpus:\n            unigrams= get_ngrams(sentence, 1)\n            for unigram in unigrams:\n                if (unigram in self. unigramcounts):\n                    self. unigramcounts[unigram] +=1\n                else:\n                    self. unigramcounts[unigram] =1\n\n        #because of the way I implemented get_ngrams, I had to create new generators,\n        # each time I call get_ngrams I change the sentence, and I want it reset for each n-gram\n        corpus = corpus_reader(sys. argv[1])\n        for sentence in corpus:\n            bigrams= get_ngrams(sentence, 2)\n            for bigram in bigrams:\n                if (bigram in self. bigramcounts):\n                    self. bigramcounts[bigram] +=1\n                else:\n                    self. bigramcounts[bigram] =1\n        \n        corpus = corpus_reader(sys. argv[1])\n        for sentence in corpus:\n            trigrams=get_ngrams(sentence, 3)\n            for trigram in trigrams:\n                if (trigram in self. trigramcounts):\n                    self. trigramcounts[trigram] +=1\n                else:\n                    self. trigramcounts[trigram] =1\n\n#number of starts that appears is exactly number of sentences\n\n    def raw_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n        #trigram is (u, v, w)\n        tri_counts=self. trigramcounts[trigram]\n        #want bigram (u, v)\n        bigram= (trigram[0:2])\n       \n        bi_counts= self. bigramcounts[bigram]\n        \n        #lexicon does not contain START\n        V= len(self. lexicon) - 1\n        #handle edge cases\n        if(trigram[0] == \"START\" and trigram[1]== \"START\"):\n            tri_prob= self. raw_bigram_probability((trigram[1:3]))\n           \n        #all w are equally likely\n        elif (bi_counts==0):\n            tri_prob= 1 / V\n        else:\n            tri_prob= tri_counts/bi_counts\n        return tri_prob  \n\n    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        #want bigram (u, w)\n        bi_counts=self. bigramcounts[bigram]\n        \n        #want unigram (u)\n        unigram= (bigram[0:1])\n        uni_counts= self. unigramcounts[unigram]\n        V= len(self. lexicon) - 1\n        \n        #all u are equally likely\n        if (uni_counts==0):\n            bi_prob= 1 / V\n        else:\n            bi_prob= bi_counts/uni_counts\n        return bi_prob\n    \n    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n        uni_counts=self. unigramcounts[unigram]\n        total_toks=self. total_tokens\n        #by virtue of how I created the unigrams, there will never be a \"START\" unigram\n        #just in case:\n        if (unigram == \"START\"):\n            uni_prob= 0\n        uni_prob= uni_counts/total_toks\n\n\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.  \n        return uni_prob\n\n    def generate_sentence(self, t=20): \n        \"\"\"\n        COMPLETE THIS METHOD (OPTIONAL)\n        Generate a random sentence from the trigram model. t specifies the\n        max length, but the sentence may be shorter if STOP is reached.\n        \"\"\"\n        return result            \n\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation). \n        \"\"\"\n        lambda1 = 1/3. 0\n        lambda2 = 1/3. 0\n        lambda3 = 1/3. 0\n        \n       #print(trigram[2:3])\n\n        #trigram (u, v, w)\n        tri_mle= self. raw_trigram_probability(trigram)\n        #want bigram (v, w)\n        bi_mle= self. raw_bigram_probability(trigram [1:3])\n        #want unigam (w)\n        uni_mle= self. raw_unigram_probability(trigram [2:3])\n\n        lin_int_prob= (lambda1*tri_mle) + (lambda2*bi_mle) + (lambda3*uni_mle)\n\n        return lin_int_prob\n        \n    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        n=len(sentence)\n        log_prob=0\n\n        #want to get range 0 to n, but since need 2 previous each time, must start at 2, and go backwards each time\n        for i in range (2, n):\n            prob=self. smoothed_trigram_probability((sentence[i-2], sentence[i-1], sentence[i]))\n            \n            log_prob+=math. log2(prob)\n        return log_prob\n\n#get a perplexity of 130\n    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6) \n        Returns the log probability of an entire sequence.\n        \"\"\"\n        l=0\n        M=0\n\n        #need to recount M, for a different corpus\n        #self. total_tokens was for training corpus\n        #need number of tokens for test tokens\n        for sentence in corpus:\n            M+= len(sentence)\n            #must add STOP tokens too\n            M+=1\n\n        m=self. num_sents\n        print(M)\n        corpus = corpus_reader(sys. argv[2], self. lexicon)\n        for sentence in corpus:\n            l+=self. sentence_logprob(sentence)\n        l= l/M\n        perplex= 2 ** -l\n        return perplex \n",
        "answers": [
            "The lowercase m in the perplexity formula should be the number of sentences in the test data, not the training data. "
        ]
    },
    "617": {
        "question": "While I am able to attain adequate perplexity and accuracy scores through my methods, when I try to run the following test on sentence_logprob, I am getting a math error due the smoothed probability of one of the trigrams being zero. Is this to be expected? test_sentence = [\"This\", \"is\", \"a\", \"test\", \"sentence\"]log_prob = model. sentence_logprob(test_sentence)print(\"Log Probability:\", log_prob)",
        "answers": [
            "Yes -- the tokens int he corpus are all lower-cased. "
        ]
    },
    "618": {
        "question": "Hi, I'm a bit confused on what essay_scoring_experiment() is supposed to do. I followed what Dr. Bauer said in post #58 but my code isn't working. This is what I have right now:def essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n    model1 = TrigramModel(training_file1)\n    model2 = TrigramModel(training_file2)\n\n    total = 0\n    correct = 0\n\n    for f in os. listdir(testdir1):\n        pp_high = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n        pp_low = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n        if pp_high &lt; pp_low:\n            correct += 1\n        total += 1\n\n    for f in os. listdir(testdir2):\n        pp_low = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\n        pp_high = model1. perplexity(corpus_reader(os. path. join(testdir2, f), model1. lexicon))\n        if pp_low &lt; pp_high:\n            correct += 1\n        total += 1\n\n    return correct/total",
        "answers": [
            "That looks okay at first glance. Are you getting an error or incorrect results?"
        ]
    },
    "619": {
        "question": "Hey, I just wanted to clarify, for q6 (Here M is the total number of word tokens. .. ), to calculate M, are we supposed to  count the worlds in the corpus, or  count the unigrams we have and omit 'start'? For example if our corpus was: ['to', 'be', 'or', 'not', 'to', 'be']:and my unigrams: 'START', 'to', 'be', 'or', 'not', 'to', 'be', 'STOP'Then would my M be 7 (by adding STOP) or 6 (by just iterating over the corpus)? Or a different answer? Thanks for your help!",
        "answers": [
            "M would be 7! you want to account for the STOP but not for the START :)"
        ]
    },
    "620": {
        "question": "Hi, When I run my code I get the following output. Is my perplexity supposed to be this low or is this not actually the perplexity value that I'm returning or something? Edit: Here's my perplexity func",
        "answers": [
            "The corpus passed into perplexity() is not necessarily the train corpus and you cannot simply divide by self. total_words. In your case, the input corpus is brown_test. txt. Thus you need to calculate M in the perplexity formula by summing the total number of word tokens in the input corpus. "
        ]
    },
    "621": {
        "question": "Hello TA's! I'm having some trouble getting my model to achieve an acceptable perplexity/accuracy score. Right now I am getting Perplexity = 343 and Accuracy = 72%. I implemented error handling for edge cases and I've also checked that my bigram and trigram probabilities add up to 1, so I'm a bit unsure as to where my error is. If anyone could take a quick look and provide some guidance I would be forever grateful! (': def get_ngrams(sequence, n):\r\n \r\n    new = sequence. copy()\r\n    for i in range(n-1):\r\n        new. insert(0, \"START\")\r\n    new. append(\"STOP\")\r\n\r\n    ngrams = []\r\n    for i in range(len(new)-n+1):\r\n        gram = []\r\n        for j in range(n):\r\n            gram. append(new[i+j])\r\n        ngrams. append(tuple(gram))\r\n\r\n    return ngrams\r\n\r\n\r\nclass TrigramModel(object):\r\n    \r\n    def __init__(self, corpusfile):\r\n    \r\n        # Iterate through the corpus once to build a lexicon \r\n        self. total_sentences = None\r\n        generator = corpus_reader(corpusfile)\r\n        self. lexicon = get_lexicon(generator)\r\n        self. lexicon. add(\"UNK\")\r\n        self. lexicon. add(\"START\")\r\n        self. lexicon. add(\"STOP\")\r\n    \r\n        # Now iterate through the corpus again and count ngrams\r\n        generator = corpus_reader(corpusfile, self. lexicon)\r\n        self. count_ngrams(generator)\r\n\r\n        self. total_words = 0\r\n        for key in self. unigramcounts:\r\n            self. total_words += self. unigramcounts[key]\r\n\r\n\r\n\r\n    def count_ngrams(self, corpus):\r\n     \r\n        self. unigramcounts = {} # might want to use defaultdict or Counter instead\r\n        self. bigramcounts = {} \r\n        self. trigramcounts = {}\r\n        corpus_list = []\r\n        unigrams = []\r\n        bigrams = []\r\n        trigrams = []\r\n\r\n        for sentence in corpus:\r\n            unigrams. append(get_ngrams(sentence, 1))\r\n            bigrams. append(get_ngrams(sentence, 2))\r\n            trigrams. append(get_ngrams(sentence, 3))\r\n\r\n        self. total_sentences = len(unigrams)\r\n\r\n        for sentence in range(len(unigrams)):\r\n            for unigram in unigrams[sentence]:\r\n                if unigram in self. unigramcounts:\r\n                    self. unigramcounts[unigram] += 1\r\n                else:\r\n                    self. unigramcounts[unigram] = 1\r\n\r\n            for bigram in bigrams[sentence]:\r\n                if bigram in self. bigramcounts:\r\n                    self. bigramcounts[bigram] += 1\r\n                else:\r\n                    self. bigramcounts[bigram] = 1\r\n\r\n            for trigram in trigrams[sentence]:\r\n                if trigram in self. trigramcounts:\r\n                    self. trigramcounts[trigram] += 1\r\n                else:\r\n                    self. trigramcounts[trigram] = 1\r\n\r\n\r\n\r\n    def raw_trigram_probability(self, trigram):\r\n       \r\n        uvw = 0\r\n        uv = 0\r\n\r\n        if trigram in self. trigramcounts:\r\n            uvw = self. trigramcounts[trigram]\r\n        if trigram[0] == \"START\" and trigram[1] == \"START\":\r\n            uv = self. total_sentences\r\n        elif (trigram[0], trigram[1]) in self. bigramcounts:\r\n            uv = self. bigramcounts[(trigram[0], trigram[1])]\r\n\r\n        if uv == 0:\r\n            return 1 / (len(self. lexicon) - 1)\r\n\r\n        return uvw / uv\r\n\r\n    def raw_bigram_probability(self, bigram):\r\n       \r\n        uv = 0\r\n\r\n        if bigram in self. bigramcounts:\r\n            uv = self. bigramcounts[bigram]\r\n        if bigram[0] == \"START\":\r\n            u = self. total_sentences\r\n        else:\r\n            u = self. unigramcounts[(bigram[0], )]\r\n        return uv / u\r\n    \r\n    def raw_unigram_probability(self, unigram):\r\n    \r\n        if unigram == (\"START\", ):\r\n            return 0\r\n\r\n        return 1 / self. total_words\r\n\r\n    def smoothed_trigram_probability(self, trigram):\r\n       \r\n        lambda1 = 1/3. 0\r\n        lambda2 = 1/3. 0\r\n        lambda3 = 1/3. 0\r\n\r\n        return lambda1 * self. raw_trigram_probability(trigram) + \\\r\n            lambda2 * self. raw_bigram_probability((trigram[0], trigram[1])) + \\\r\n            lambda3 * self. raw_unigram_probability((trigram[1]))\r\n        \r\n    def sentence_logprob(self, sentence):\r\n       \r\n        ngrams = get_ngrams(sentence, 3)\r\n        prob = 0\r\n        for gram in ngrams:\r\n            prob += math. log2(self. smoothed_trigram_probability(gram))\r\n\r\n        return prob\r\n\r\n    def perplexity(self, corpus):\r\n       \r\n        perplexity = 0\r\n        tokens = 0\r\n        for sentence in corpus:\r\n            tokens += self. count_words(sentence) + 1\r\n            perplexity += self. sentence_logprob(sentence)\r\n        perplexity = perplexity / tokens\r\n        perplexity = 1 / math. pow(2, perplexity)\r\n        return perplexity\r\n\r\n    def count_words(self, sentence):\r\n        count = 0\r\n        for word in sentence:\r\n            count += 1\r\n\r\n        return count\r\n\r\n\r\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\r\n\r\n        model1 = TrigramModel(training_file1)\r\n        model2 = TrigramModel(training_file2)\r\n\r\n        total = 0\r\n        correct = 0       \r\n \r\n        for f in os. listdir(testdir1):\r\n            pp1 = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\r\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\r\n            total += 1\r\n            if 'high' in training_file1:\r\n                if pp1 &lt; pp2:\r\n                    correct += 1\r\n            else:\r\n                if pp2 &lt; pp1:\r\n                    correct += 1\r\n\r\n    \r\n        for f in os. listdir(testdir2):\r\n            pp1 = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\r\n            pp2 = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\r\n            total += 1\r\n            if 'high' in training_file1:\r\n                if pp1 &lt; pp2:\r\n                    correct += 1\r\n            else:\r\n                if pp2 &lt; pp1:\r\n                    correct += 1\r\n        \r\n        return correct / total\n",
        "answers": [
            "Your raw unigram probability is incorrect. You are always returning 1/self. total_words rather than count(unigram)/self. total_words. "
        ]
    },
    "622": {
        "question": "When I test my part 6 &amp; 7, I am getting value &gt;400 and &lt;80% (specifically 457 and 76%). I tried testing parts 3-5 using:print(model. raw_trigram_probability((\"START\", \"START\", \"the\"))) # Part 3 print(model. raw_bigram_probability((\"START\", \"the\"))) # Part 3 print(model. raw_unigram_probability((\"the\", ))) # Part 3 print(model. smoothed_trigram_probability((\"START\", \"START\", \"the\"))) # Part 4 print(model. sentence_logprob([\"natural\", \"language\", \"processing\"])) # Part 5The output makes sense to me:4. 0436716538617066e-054. 0436716538617066e-050. 0589200673339312160. 019666980255669483-49. 039356515775864I have attended a few OHs but still no luck. I've included my current code in this post. Thank you in advance for any advice/feedback! import sys\nfrom collections import defaultdict\nimport math\nimport random\nimport os\nimport os. path\n\n\"\"\"\nCOMS W4705 - Natural Language Processing - Fall 2023 \nProgramming Homework 1 - Trigram Language Models\nDaniel Bauer\n\"\"\"\n\n\ndef corpus_reader(corpusfile, lexicon=None):\n    with open(corpusfile, \"r\") as corpus:\n        for line in corpus:\n            if line. strip():\n                sequence = line. lower(). strip(). split()\n                if lexicon:\n                    yield [word if word in lexicon else \"UNK\" for word in sequence]\n                else:\n                    yield sequence\n\n\ndef get_lexicon(corpus):\n    word_counts = defaultdict(int)\n    for sentence in corpus:\n        for word in sentence:\n            word_counts[word] += 1\n    return set(word for word in word_counts if word_counts[word] &gt; 1)\n\n\ndef get_ngrams(sequence, n):\n    \"\"\"\n    COMPLETE THIS FUNCTION (PART 1)\n    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n    This should work for arbitrary values of n &gt;= 1\n    \"\"\"\n    if n &lt; 1:\n        raise ValueError(\"n must be greater than or equal to 1\")\n\n    ngrams = []\n\n    # Pad the list of words with 'START' and 'STOP' tokens\n    padded_words = [\"START\"] * (n - 1) + sequence + [\"STOP\"]\n\n    for i in range(len(padded_words) - n + 1):\n        ngram = tuple(padded_words[i : i + n])\n        ngrams. append(ngram)\n\n    return ngrams\n\n\nclass TrigramModel(object):\n    def __init__(self, corpusfile):\n        # Iterate through the corpus once to build a lexicon\n        generator = corpus_reader(corpusfile)\n        self. lexicon = get_lexicon(generator)\n        self. lexicon. add(\"UNK\")\n        # self. lexicon. add(\"START\")\n        self. lexicon. add(\"STOP\")\n\n        self. total_words = 0\n\n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self. lexicon)\n        self. count_ngrams(generator)\n\n    def count_ngrams(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts.\n        \"\"\"\n\n        self. unigramcounts = defaultdict(int)  # Using defaultdict to initialize counts\n        self. bigramcounts = defaultdict(int)\n        self. trigramcounts = defaultdict(int)\n\n        for item in corpus:\n            # Count the number of word tokens in the corpus\n            self. total_words += len(item) + 1\n\n            unigrams = get_ngrams(item, 1)\n            bigrams = get_ngrams(item, 2)\n            trigrams = get_ngrams(item, 3)\n\n            for unigram in unigrams:\n                self. unigramcounts[unigram] += 1\n\n            for bigram in bigrams:\n                self. bigramcounts[bigram] += 1\n\n            for trigram in trigrams:\n                self. trigramcounts[trigram] += 1\n\n        return\n\n    def raw_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n        u, w, v = trigram\n\n        # Check if the trigram includes two \"START\" tokens at the beginning\n        if u == \"START\" and w == \"START\":\n            bigram = (w, v)\n            return self. raw_bigram_probability(bigram)\n        # Check if count(u, w) is 0\n        elif self. bigramcounts[(u, w)] &gt; 0:\n            return self. trigramcounts. get(trigram, 0) / self. bigramcounts[(u, w)]\n        else:\n            # Use uniform distribution over vocabulary size if unseen\n            return 1 / len(self. lexicon)\n            # return self. raw_unigram_probability((v, ))\n\n    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        u, w = bigram\n\n        # Check if count(u) is 0\n        if self. unigramcounts[u] &gt; 0:\n            return self. bigramcounts. get(bigram, 0) / self. unigramcounts[u]\n        else:\n            # Use uniform distribution over vocabulary size if unseen\n            return 1 / len(self. lexicon)\n\n    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n\n        # hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once,\n        # store in the TrigramModel instance, and then re-use it.\n        w = unigram[0]\n\n        return self. unigramcounts. get(unigram, 0) / self. total_words\n\n    def generate_sentence(self, t=20):\n        \"\"\"\n        COMPLETE THIS METHOD (OPTIONAL)\n        Generate a random sentence from the trigram model. t specifies the\n        max length, but the sentence may be shorter if STOP is reached.\n        \"\"\"\n        return result\n\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation).\n        \"\"\"\n        u, w, v = trigram\n\n        lambda1 = 1 / 3. 0\n        lambda2 = 1 / 3. 0\n        lambda3 = 1 / 3. 0\n\n        # Calculate the individual probabilities using raw methods\n        P_trigram = self. raw_trigram_probability(trigram)\n        P_bigram = self. raw_bigram_probability((w, v))\n        P_unigram = self. raw_unigram_probability((v, ))\n\n        # Compute the smoothed probability using linear interpolation\n        smoothed_prob = lambda1 * P_trigram + lambda2 * P_bigram + lambda3 * P_unigram\n\n        return smoothed_prob\n\n    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n\n        # Initialize the log probability to 0\n        log_prob = 0. 0\n\n        # Calculate the log probability of each trigram in the sentence\n        trigrams = get_ngrams(sentence, 3)\n        for trigram in trigrams:\n            log_prob_trigram = math. log2(self. smoothed_trigram_probability(trigram))\n            log_prob += log_prob_trigram\n\n        return log_prob\n\n    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        log_prob_total = 0. 0\n        total_tokens = 0\n\n        for sentence in corpus:\n            # Calculate the log probability of the sentence\n            log_prob_total += self. sentence_logprob(sentence)\n\n            # Count the number of word tokens in the sentence\n            total_tokens += len(sentence) + 1\n\n        # Calculate the perplexity using the formula\n        perplexity = math. pow(2, -log_prob_total / total_tokens)\n\n        return perplexity\n\n\ndef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n    model1 = TrigramModel(training_file1)\n    model2 = TrigramModel(training_file2)\n\n    total = 0\n    correct = 0\n\n    for f in os. listdir(testdir1):\n        pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n        pp2 = model2. perplexity(\n            corpus_reader(os. path. join(testdir1, f), model2. lexicon)\n        )\n\n        # If model1 has a lower (better) perplexity, then it's a correct prediction for testdir1\n        if pp &lt; pp2:\n            correct += 1\n        total += 1\n\n    for f in os. listdir(testdir2):\n        pp1 = model1. perplexity(\n            corpus_reader(os. path. join(testdir2, f), model1. lexicon)\n        )\n        pp2 = model2. perplexity(\n            corpus_reader(os. path. join(testdir2, f), model2. lexicon)\n        )\n\n        # If model2 has a lower (better) perplexity, then it's a correct prediction for testdir2\n        if pp2 &lt; pp1:\n            correct += 1\n        total += 1\n\n    return correct / total\n\n\nif __name__ == \"__main__\":\n    model = TrigramModel(sys. argv[1])\n\n    print(model. raw_trigram_probability((\"START\", \"START\", \"the\")))  # Part 3\n    print(model. raw_bigram_probability((\"START\", \"the\")))  # Part 3\n    print(model. raw_unigram_probability((\"the\", )))  # Part 3\n    print(model. smoothed_trigram_probability((\"START\", \"START\", \"the\")))  # Part 4\n    print(model. sentence_logprob([\"natural\", \"language\", \"processing\"]))  # Part 5\n\n    # put test code here. ..\n    # or run the script from the command line with\n    # $ python -i trigram_model. py [corpus_file]\n    # &gt;&gt;&gt;\n    #\n    # you can then call methods on the model instance in the interactive\n    # Python prompt.\n\n    # Testing perplexity:\n    # dev_corpus = corpus_reader(sys. argv[2], model. lexicon)\n    # pp = model. perplexity(dev_corpus)\n    # print(pp)\n\n    # Essay scoring experiment:\n    # acc = essay_scoring_experiment(\n    #     \"hw1_data/ets_toefl_data/train_high. txt\",\n    #     \"hw1_data/ets_toefl_data/train_low. txt\",\n    #     \"hw1_data/ets_toefl_data/test_high\",\n    #     \"hw1_data/ets_toefl_data/test_low\",\n    # )\n    # print(acc)\n",
        "answers": [
            "Just an idea: In line 127, you check self. unigramcounts[u], but I think the keys in that dictionary are in a tuple format, so self. unigramcount[(u, )]. "
        ]
    },
    "623": {
        "question": "Hello, I'm running into an issue with my sentence_logprob() function where if I run this test code: where 'sample. txt' is a very small corpus, it would give me a 'math domain error' due to the fact that I'm taking the log of 0 because none of the words in \"natural language processing\" are in the corpus. For example, the first trigram created is ('START', 'START', 'natural) which I believe should return 0 for all three unigram, bigram, and trigram probabilities because the model has not seen \"natural. \" How should I take this into account?  When I try to implement a check to see if the smoothed_probability() return 0, I run into a division by 0 error with my bigram probability because it will divide by the unigram probability of \"natural\" --&gt; which will be 0. ",
        "answers": [
            "This is not something you need to account for. The smoothed trigram probability will eventually just use the unigram probability. There are no unseen tokens in the test data, because the corpus reader replaces them ass with UNK. So, in a sense, your test case is invalid. The token natural would never be encountered during testing. "
        ]
    },
    "624": {
        "question": "Hello I am getting a very low perplexity score (0. 0026658676711989126) This is my homework file, but I will add screenshots of specific methods. My perplexity method looks like this I saw an earlier post on ed where the TA said check the log prob method. This is mine I'm not sure where I am going wrong. My count_ngrams and get_ngrams show the expected output. ",
        "answers": [
            "In your perplexity function why are you using generator? I'd try to incorporate your calculation of M from the \"for x in corpus\" loop. Don't forget to account for the STOP symbol. In your sentence_logprob function try to cast your result with float(). Hope this helps!"
        ]
    },
    "625": {
        "question": "If the bigram or trigram is not in the count dictionary, should we simply return 0 or should we turn it into UNK and calculate the probability of it?",
        "answers": [
            "in the raw probability methods you should just return 0 for unseen bigrams and trigrams. "
        ]
    },
    "626": {
        "question": "Good morning, I've been trying to test my code, and am unsure if my understanding of the concepts is correct. Given these inputs and the respective results on \"brown_train. txt\", can anyone help me verify if these values make sense? Thank you in advance! a) print(model. raw_trigram_probability(('START', 'START', 'the'))) # Part 3  b) print(model. raw_bigram_probability(('START', 'the'))) # Part 3 c) print(model. raw_unigram_probability(('the', ))) # Part 3 d) print(model. smoothed_trigram_probability(('START', 'START', 'the'))) # Part 4 e) print(model. sentence_logprob([\"natural\", \"language\", \"processing\"])) # Part 5a) 0. 13163839092613064b) 0. 0c) 0. 0613696374747615d) 0. 06433600946696405e) -51. 488849275796895",
        "answers": []
    },
    "627": {
        "question": "Hi all, When I'm testing perplexity against the training data, I get a score of 19. xxx (which seems okay? ) but ~4. xxx-5. xxx when running the test data. I saw #57 but I checked in OH that I'm getting the correct values for token/sentence counts so I don't think that is the issue. Anyone else having this problem?",
        "answers": [
            "This seems very low. are you maybe using the tokens in the training data for M, instead of the test data?"
        ]
    },
    "628": {
        "question": "Hi, I'm wondering what $M$ and $m$ are in the perplexity equation:$$\\begin{align*}2^{-l}&amp; &amp;\\text{where} \\ \\ \\ \\ \\ \\ \\ \\ &amp;l = \\frac{1}{M}\\sum_{i=1}^{m}\\log_2p(s_{i})\\end{align*}$$Right now, I'm taking $M$ to be the sum of the frequency of all unigrams (not including the (START, ) unigram). Then I multiply $M$ by the sum of the log probability for each sentence in the corpus. My perplexity is too low (less than 10) so I'm wondering if this is the source.",
        "answers": [
            "Update: I found the issue! M is the number of tokens in the corpus (test file). Perplexity is normal now."
        ]
    },
    "629": {
        "question": "For other people wondering, Zoom OHs people are taken one-by-one so you might be in waiting for a while. If the Zoom says \"Host has joined\" you should be set with some patience. ",
        "answers": []
    },
    "630": {
        "question": "Hi Daniel and TAs, I would like to resubmit my assignment but I could not find where to upload it. I submitted my HW1 at Canvas earlier using the upload box at the end of the instruction page but it disappeared after the first submission. I also could not find the course at Gradescope if we are using that. Thank you very much!",
        "answers": [
            "I can see your submission on Courseworks. You should be able to resubmit a new version. Did the entire submission box disappear? We will only use Gradescope for providing feedback/grades on the midterm and final. A few students asked about the Gradescope link though -- did I accidentally specify Gradescope as the submission method for assignments somewhere? Thanks ."
        ]
    },
    "631": {
        "question": "Hello, I just wanted to clarify that I understand the difference between the regular probability equation, and the MLE equation for getting the probability of n-grams. E. g. for a bigram:raw probability:P\\left(w\\ \\ \\left|u\\right|\\right)=\\frac{count\\left(u, w\\right)}{count\\left(u\\right)}mle probability:\\left(P\\ \\left(w\\right|u\\right)=\\frac{count\\left(u, w\\right)}{\\sum_{u'\\ \\in u}^{ }count\\left(u', w\\right)}Can you explain what u' means here? Is the count(u', w) the count of the total number of bigrams that contain a bigram? I am confused how the denominator of the MLE probability works, how can you sum of the counts if there is only one count of the bigram (u, w)?",
        "answers": [
            "Hm, the \"raw\" probabilities are the maximum likelihood estimates for the training data. Where did you get the second formula from? Did you mean? P(w | u ) = \\frac{count(u, w)}{\\sum_{w' \\in V} count(u, w')}That's equivalent to your first formula because \\sum_{_{w'\\ \\in V\\ }}^{ }count\\left(u, w'\\right)  = count(u)"
        ]
    },
    "632": {
        "question": "Hi, I have 2 questions regarding the interlude:1. When handling the edge case of UNK being the only possible next word, are we allowed to use any methods other than beam search? For instance, if we're stuck on UNK after multiple resamples, can we prematurely end the sentence? 2. The two given examples all end in periods. Is this a requirement at the end of the generated sentence? Thank you!",
        "answers": [
            "Since the interlude is ungraded you can solve the UNK edge case either way. The sentence is not required to end in a period. "
        ]
    },
    "633": {
        "question": "Hello, I'm currently testing my get_ngrams() function and noticed that it produces extraneous ngrams like (token, STOP, STOP) or (STOP, STOP, STOP) when I print out the ngrams within count_ngrams(). However, when I test it with a sequence I pass in, it produces the correct ngrams. &gt;&gt;&gt; print(get_ngrams([\"natural\", \"language\", \"processing\", \"is\", \"fun\"], 3)[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'fun'), ('is', 'fun', 'STOP')]My current get_ngrams() iterates over each token in the sequence and builds the n-grams at each index starting from the (n-1) to current index. I think it might be a problem with my implementation but I'm not sure since the rest of my functions seem to work okay (perplexity 208 and accuracy 84%).",
        "answers": [
            "When I run model = TrigramModel('sample. txt') #sample. txt is a truncated brown_train. txt\nprint(model. trigramcounts)\nThe resulting dict contains {('. ', 'STOP', 'STOP'): 13, ('STOP', 'STOP', 'STOP'): 13}"
        ]
    },
    "634": {
        "question": "Would it be possible to ask for reasonable inputs and outputs that I can run for parts 3-5? I'm testing my code and would appreciate any confirmation that I'm on the right track. Thank you!",
        "answers": [
            "I'd suggest adding something like the following in main: # print(model. raw_trigram_probability(('START', 'START', 'the'))) # Part 3\n # print(model. raw_bigram_probability(('START', 'the'))) # Part 3\n # print(model. raw_unigram_probability(('the', ))) # Part 3\n # print(model. smoothed_trigram_probability(('START', 'START', 'the'))) # Part 4\n # print(model. sentence_logprob([\"natural\", \"language\", \"processing\"])) # Part 5\nAnd see if the results make sense. Let us know if there's anything in particular you're unsure about!"
        ]
    },
    "635": {
        "question": "Dear teaching team, Just out of curiosity: I made a little change to my code to account for STOP words when calculating perplexity, and it reduced my overall perplexity (from ~290 to ~216 when running for the train file). However, my accuracy remained exactly the same. I wonder if there is any expected theoretical justification for this or if it should be just a random occurrence (I did some sensitivity analysis and didn't seem to be a bug at least). My guess is that it changes perplexity equally for both models when evaluating the essays, and therefore does not change how they are split into the two categories, but I'm not so confident about it. Thank you!",
        "answers": [
            "So since you are comparing the perplexity scores between the two models for the same data, the perplexity simply scales (and normalizes) the log probability for each test document. It won't change the relative order of the scores at all. ",
            "Adding on to the above with some anecdotal evidence: I noticed a small increase in accuracy when i got perplexity down from 400s to 200s, but tweaks beyond that didn't change anything."
        ]
    },
    "636": {
        "question": "Hello, quick question: do we have to comment out everything under the main function? Should we be leaving anything within the body of main?",
        "answers": [
            "(not a TA) I believe they said they would just be testing individual methods, nothing in main "
        ]
    },
    "637": {
        "question": "Hi Teaching Staff, 1) Can you elaborate on how periods/exclamation points/question marks (as end-of-sentence indicators) are supposed to be handled when we count_ngrams? Take for instance this processed sentence from brown_train. txt:['the', 'fulton', . .. , 'took', 'place', '. ']\nand accounting for all the edge cases mentioned on Ed:INCLUSIONS: [('STOP'), ] in unigramcounts |  [('word', 'STOP')] and [('START', 'word')] in bigramcounts | [('word', 'word', 'STOP')] and [('START', 'word', 'word') and [('START', 'START', 'word')]in trigramcountsEXCLUSIONS: [('START')] in unigramcounts | [('START', 'START')] in bigramcounts2) Would [('. '), ('STOP')] be a valid bigram? 3) How about [('place'), ('. '), ('STOP')] as a valid trigram? 4) If valid, why are these ngrams important to account for in our model? If not valid, do we implement code to ignore {. ?! }? Thank You.",
        "answers": [
            "Punctuation symbols should be handled like any other token. So [('. '), ('STOP')] and [('place'), ('. '), ('STOP')] would be a correct bigram / trigram. We want the model to learn that a sentence typically ends in a period. "
        ]
    },
    "638": {
        "question": "Hi I hope you are doing greatIn the original code we were given for the assignment:model = TrigramModel(sys. argv[1]) is not commented it out.  To test part 7, i believe it has to commented it out, and I submitted with this line being commented out. Is that okay?  Or would you like me to uncomment it and resubmit? Thank you so much!",
        "answers": [
            "Please see response in #111"
        ]
    },
    "639": {
        "question": "Hello, This is what I got for output:train: 16. 091318233161303test: 208. 49720370974467accuracy: 0. 8446215139442231I wonder if this perplexity is in the acceptable range?",
        "answers": [
            "Yes, that seems okay to me. "
        ]
    },
    "640": {
        "question": "Hello, I just wanted to confirm that we need to submit the assignment only on Courseworks.",
        "answers": [
            "Correct. "
        ]
    },
    "641": {
        "question": "Hi! I keep getting a key error \"KeyError: ('START', 'the', 'blade')\" when I test perplexity. This is the code I'm running in terminal \"python3 trigram_model. py . /brown_train. txt . /brown_test. txt\". Parts 1-5 work, but when it gets to running perplexity in part 6 I get an error that doesn't recognize the trigram. This is incredibly confusing because when I print the variable trigram right before the key error it prints as the correct trigram. I'm attaching the error message and my code. I would really appreciate any help/pointers! import sysfrom collections import defaultdictimport mathimport randomimport osimport os. path\"\"\"COMS W4705 - Natural Language Processing - Fall 2023 Programming Homework 1 - Trigram Language ModelsDaniel Bauer\"\"\"def corpus_reader(corpusfile, lexicon=None):  #print(corpusfile) with open(corpusfile, 'r') as corpus:  for line in corpus:  if line. strip(): sequence = line. lower(). strip(). split() if lexicon:  yield [word if word in lexicon else \"UNK\" for word in sequence] else:  yield sequencedef get_lexicon(corpus): word_counts = defaultdict(int) for sentence in corpus: for word in sentence:  word_counts[word] += 1 return set(word for word in word_counts if word_counts[word] &gt; 1) def get_ngrams(sequence, n): \"\"\" COMPLETE THIS FUNCTION (PART 1) Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple. This should work for arbitrary values of n &gt;= 1  \"\"\" output = [] sequence. append(\"STOP\") for i in range (n-1):  sequence. insert(0, \"START\") length = len(sequence) for word in range (length-n+1):  ngram = tuple(sequence[word:word+n]) output. append(ngram) return outputclass TrigramModel(object): def __init__(self, corpusfile): # Iterate through the corpus once to build a lexicon  generator = corpus_reader(corpusfile) self. lexicon = get_lexicon(generator) self. lexicon. add(\"UNK\") self. lexicon. add(\"START\") self. lexicon. add(\"STOP\") # Now iterate through the corpus again and count ngrams generator = corpus_reader(corpusfile, self. lexicon) self. count_ngrams(generator) self. word_count = sum(self. unigramcounts. values())  def count_ngrams(self, corpus): \"\"\" COMPLETE THIS METHOD (PART 2) Given a corpus iterator, populate dictionaries of unigram, bigram, and trigram counts.  \"\"\" self. unigramcounts = {} # might want to use defaultdict or Counter instead self. bigramcounts = {}  self. trigramcounts = {}  ##Your code here self. sentence_count = 0 for sentence in corpus:  self. sentence_count += 1 #print(sentence)  uni = get_ngrams(sentence, 1) bi = get_ngrams(sentence, 2) tri = get_ngrams(sentence, 3) for ngram in uni: if ngram in self. unigramcounts: self. unigramcounts[ngram] += 1 else: self. unigramcounts[ngram] = 1 #print(\"ngram\", ngram) #print(\"self. unigramcounts[ngram]\", self. unigramcounts[ngram]) for ngram in bi: if ngram in self. bigramcounts: self. bigramcounts[ngram] += 1 else: self. bigramcounts[ngram] = 1 #print(\"ngram\", ngram) #print(\"self. bigramcounts[ngram]\", self. bigramcounts[ngram]) for ngram in tri: if ngram in self. trigramcounts: self. trigramcounts[ngram] += 1 else: self. trigramcounts[ngram] = 1 # print(\"ngram\", ngram) #print(\"self. trigramcounts[ngram]\", self. trigramcounts[ngram]) return  def raw_trigram_probability(self, trigram): \"\"\" COMPLETE THIS METHOD (PART 3) Returns the raw (unsmoothed) trigram probability \"\"\" bigram = trigram[:2] if bigram == ('START', 'START'): return self. trigramcounts[trigram] / self. sentence_count #prob /= self. bigramcounts[trigram[1], trigram[2]] if self. bigramcounts[bigram] == 0: prob = (1/len(self. lexicon -1)) else: prob = self. trigramcounts[trigram] / self. bigramcounts[trigram[:2]] return prob def raw_bigram_probability(self, bigram): \"\"\" COMPLETE THIS METHOD (PART 3) Returns the raw (unsmoothed) bigram probability include edge case of if bigram = 0 and if unigram = 0  \"\"\" if bigram[0] == \"START\": prob = self. bigramcounts[bigram] /self. sentence_count else: prob = self. bigramcounts[bigram]/ self. unigramcounts[bigram[0], ] return prob def raw_unigram_probability(self, unigram): \"\"\" COMPLETE THIS METHOD (PART 3) Returns the raw (unsmoothed) unigram probability. include edge case for if uni = 0  \"\"\" #hint: recomputing the denominator every time the method is called # can be slow! You might want to compute the total number of words once,  # store in the TrigramModel instance, and then re-use it.  #print(unigram) if unigram[0] == \"START\": prob = 0. 0 else: prob = self. unigramcounts[(unigram[0], )] / self. word_count return prob def generate_sentence(self, t=20):  \"\"\" COMPLETE THIS METHOD (OPTIONAL) Generate a random sentence from the trigram model. t specifies the max length, but the sentence may be shorter if STOP is reached. \"\"\" return result  def smoothed_trigram_probability(self, trigram): \"\"\" COMPLETE THIS METHOD (PART 4) Returns the smoothed trigram probability (using linear interpolation).  \"\"\" lambda1 = 1/3. 0 lambda2 = 1/3. 0 lambda3 = 1/3. 0 bi = tuple(trigram[1:3]) uni = tuple(trigram[2:3], ) linear_interp_prob = (lambda1 * self. raw_trigram_probability(trigram)) + (lambda2 * self. raw_bigram_probability(bi)) + (lambda3 * self. raw_unigram_probability(uni)) return linear_interp_prob def sentence_logprob(self, sentence): \"\"\" COMPLETE THIS METHOD (PART 5) Returns the log probability of an entire sequence. Write the method sentence_logprob(sentence) which returns the log probability of an entire sequence (katz backoff).  Use the get_ngrams function to compute trigrams and the smoothed_trigram_probabilitymethod to obtain probabilities.  Convert each probability into logspace using math. log2.  \"\"\" trigrams = [] trigrams = get_ngrams(sentence, 3) logprob = 0. 0 for trigram in trigrams: logprob += math. log2(self. smoothed_trigram_probability(trigram)) return logprob def perplexity(self, corpus): \"\"\" COMPLETE THIS METHOD (PART 6)  Returns the log probability of an entire sequence. \"\"\" print(\"in perp\") perp = 0. 0 for sentence in corpus: perp += self. sentence_logprob(sentence) perp /= self. word_count return perp def essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2): \"\"\"The method already creates two trigram models,  reads in the test essays from each directory, and computes the perplexity for each essay. All you have to do is compare the perplexities and then return the accuracy (correct predictions / total predictions). \"\"\" model1 = TrigramModel(training_file1) model2 = TrigramModel(training_file2) total = 0 correct = 0  perp_list1 = [] perp_list2 = []  for f in os. listdir(testdir1): pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon)) perp_list1. append(pp) # . .  for f in os. listdir(testdir2): pp = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon)) perp_list2. append(pp) # . .  length = 0 if len(perp_list1) &gt; (perp_list2): length = len(perp_list1) else: length = len(perp_list2) for i in range(length): if perp_list1[i] &gt; perp_list2[i]: correct += 1 total += 1 accuracy = correct/total return accuracy if __name__ == \"__main__\": print(\"--------part one--------\") sequence = [\"hello\", \"my\", \"name\", \"is\", \"Danelle\"] print(get_ngrams(sequence, 3))  #model = TrigramModel(sys. argv[1])  model = TrigramModel(sys. argv[1]) #print(model. trigramcounts[('START', 'the', 'blade')]) print(\"--------part two--------\") print(\"trigram count\", model. trigramcounts[('START', 'START', 'the')]) print(\"bigram count\", model. bigramcounts[('START', 'the')]) print(\"unigram count\", model. unigramcounts[('the', )]) print(\"--------part three--------\") trigram = tuple() trigram = ('his', 'own', 'doctor') bigram = tuple() bigram = ('his', 'own') unigram = tuple() unigram = ('his', ) print(\"trigram prob\", model. raw_trigram_probability(trigram)) print(\"bigram prob\", model. raw_bigram_probability(bigram)) print(\"unigram prob\", model. raw_unigram_probability(unigram)) print(\"--------part four--------\") tri = ('his', 'own', 'doctor') print(\"raw\", model. raw_trigram_probability(tri)) print(\"smooth\", model. smoothed_trigram_probability(tri)) print(\"--------part five--------\") sentence = ['despite', 'his', 'yearning', ', ', 'the', 'colonel', 'would', 'not', 'go', 'down', 'to', 'see', 'the', 'men', 'come', 'through', 'the', 'lines', '. '] print(\"log prob\", model. sentence_logprob(sentence)) print(\"--------part six--------\") dev_corpus = corpus_reader(sys. argv[2], model. lexicon) #model. raw_trigram_probability(dev_corpus) pp = model. perplexity(dev_corpus) print(pp) \"\"\"print(\"--------part seven--------\") acc = essay_scoring_experiment(\"train_high. txt\", \"train_low. txt\", \"test_high\", \"test_low\") print(acc)\"\"\" \"\"\"trigram = tuple() trigram = (\"his\", \"own\", \"doctor\") print(model. raw_trigram_probability(trigram))\"\"\" # put test code here. .. # or run the script from the command line with  # $ python -i trigram_model. py [corpus_file] # &gt;&gt;&gt;  # # you can then call methods on the model instance in the interactive  # Python prompt.  # Testing perplexity:  #dev_corpus = corpus_reader(sys. argv[2], model. lexicon) #dev_corpus = corpus_reader(test, model. lexicon) #model. raw_trigram_probability(dev_corpus) # pp = model. perplexity(dev_corpus) # print(pp) # Essay scoring experiment:  # acc = essay_scoring_experiment('train_high. txt', 'train_low. txt\", \"test_high\", \"test_low\") # print(acc)",
        "answers": [
            "Hi, in your perplexity function you don't seem to use the formula of 2^{-l}You might want to start there :)"
        ]
    },
    "642": {
        "question": "Hi, I am getting this super weird division by zero error where my unigram count for a specific unigram for some reason is zero when running Part 7. I thought that if it was zero, it should be replaced by \"UNK\".  Would love any insight into why this could be happening my below code! It works for parts 1-6. For some reason, this happens with a lot of words such as: ('discovers', ). Any help is appreciated! Traceback (most recent call last):\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 273, in &lt;module&gt;\n    acc = essay_scoring_experiment('ets_toefl_data/train_high. txt', \"ets_toefl_data/train_low. txt\", \"ets_toefl_data/test_high\", \"ets_toefl_data/test_low\")\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 228, in essay_scoring_experiment\n    pp_2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 210, in perplexity\n    log_prob. append(self. sentence_logprob(sentence))\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 193, in sentence_logprob\n    nonlog_trigram_prob = self. smoothed_trigram_probability(trigram)\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 172, in smoothed_trigram_probability\n    bigram_probs = self. raw_bigram_probability(bigram)\n  File \"/Users/alfonsovelasco/Downloads/hw1_data/trigram_model. py\", line 138, in raw_bigram_probability\n    return self. bigramcounts. get(bigram, 0) / unigram_count\nZeroDivisionError: division by zero\nraw_bigram_probability():def raw_bigram_probability(self, bigram):\n    \"\"\"    COMPLETE THIS METHOD (PART 3)    Returns the raw (unsmoothed) bigram probability    \"\"\"    unigram = (bigram[0], )\n    unigram_count = self. unigramcounts[unigram]\n\n    if bigram[0] == \"START\":\n        unigram_count = self. sentence_count\n    if unigram_count == 0:\n        print(unigram)\n        return self. bigramcounts[bigram] / 1\n    return self. bigramcounts[bigram] / unigram_count\nessay_scoring_experimentdef essay_scoring_experiment(training_file1, training_file2, testdir1, testdir2):\n\n        model1 = TrigramModel(training_file1)\n        model2 = TrigramModel(training_file2)\n\n        total = 0\n        correct = 0       \n \n        for f in os. listdir(testdir1):\n            pp = model1. perplexity(corpus_reader(os. path. join(testdir1, f), model1. lexicon))\n            pp_2 = model2. perplexity(corpus_reader(os. path. join(testdir1, f), model2. lexicon))\n\n            if pp &lt; pp_2:\n                correct += 1\n            total += 1\n        for f in os. listdir(testdir2):\n            pp = model2. perplexity(corpus_reader(os. path. join(testdir2, f), model2. lexicon))\n            pp_2 = model1. perplexity(corpus_reader(os. path. join(testdir2, f), model1. lexicon))\n\n            if pp_2 &gt; pp:\n                correct += 1\n            total += 1\n        \n        return correct / total\n",
        "answers": [
            "Hm your version of raw_bigram_probability does not appear to match the error message. Did you maybe change it? "
        ]
    },
    "643": {
        "question": "Is there any suggestions on how to debug the code, Like any part need to pay attention on? I already go through my code several times but got the followings which I think are not that correct. ..",
        "answers": [
            "That looks pretty good to me. "
        ]
    },
    "644": {
        "question": "Do we need to count the word 1 more than each sentence when computing perplexity? Because I think there will be a hiding STOP for each sentence. So the total word tokens is the words in document plus number of sentences. Am I understanding this correctly?",
        "answers": [
            "Yes, you need to include the STOP occurrences -- there will be one per sentence. "
        ]
    },
    "645": {
        "question": "I have been trying to figure out why my perplexity is so high but do not have luck. It is around 6000. I seem to have every other part working (I think) but I wanted to get some input because it feels like I'm banging my head against the wall. I included my part 1-6 code here. I am printing as I go and it seems to all make sense except for my score. I would love some feedback - thank youPart 1 Code:Part 2 Code:Part 3 Code:Part 4Part 5Part 6",
        "answers": [
            "From what I can tell:1. You might want to check your count_ngrams as it seems you're not accounting for ('START', ) when populating the unigram dictionary. 2. You don't account for the STOP symbol when calculating M in your perplexity method, so maybe take a look at that too. 3. In your raw_probability methods, you want to try and account for cases of ('START', 'START', 'X'), ('START', 'X') and ('START') in the respective n-gram method. Hope this helps!"
        ]
    },
    "646": {
        "question": "Hello, Regarding the way in which we count the n-grams in \"count_ngrams()\", should we remove punctuation from the sentence before we begin to use them as the sequences for our ngrams? The corpus generator object that is given separates each sentence into tokens using lemmatization similar to how we saw in class:Mr. O'Neill thinks that the boys' stories about Chile's capital aren't\namusing.\n\nmr. | o'neill | think | that | the | boy | story | about | chile | 's| capital | are | n't\namusing | . |\nSo I am wondering if we need to change the way these words are processed, because having \" 's \" as a token may change the count of sequences of ngrams, e. g. ngram( \"chile's\", \"capital\") may not be found with the current corpus generator output.",
        "answers": [
            "You may use the corpus iterator without changing the processing of the words."
        ]
    },
    "647": {
        "question": "I found out that I am enrolled in the class but not the gradescope. Is there an entry code that I should use to enroll myself? Clicking the link on canvas didn't seem to help. My netid: zw2958",
        "answers": [
            "We wont be using Gradescope until the midterm. At that point we will enroll you automatically. For homework assignments we use Courseworks. "
        ]
    },
    "648": {
        "question": "Hello! I have a couple questions regarding the count_ngrams method. 1. For some reason, the counts return 0 for words starting with an uppercase letter (i. e. model. unigramcounts[('State', )] = 0) while returning some number for the same words starting with a lowercase letter ( i. e. model. unigramcounts[('state', )] = 830). Is this phenomenon supposed to happen? 2. To double check what previous people have asked regarding how to count the 'START' and 'STOP' tokens, we do not count the unigram 'START' but we do count the unigram 'STOP'.  (meaning model. unigramcounts[('START', )] = 0). Meanwhile, bigramcounts should count bigrams (START, w_1) and (w_n, STOP) and trigramcounts should include trigrams (START, START, w1), (START, w1, w2) and ( w_n-1, w_n, STOP). The bigram (START, START) should be ignored however in favor of finding the number of sentences.",
        "answers": [
            "1. The `corpus_reader` function provided automatically lowercase all incoming words. So all ngrams will be lowercase by default. Which explain why the vocab will not contain any uppercase words. 2. I believe you can either choose to not include it in the Counters, or another approach is to just ignore START and return 0 when raw_unigram_probability is given START as an argument. 3. bigram (START, START) should be ignored as it does not provide any context to the model similar to Unigram (START)"
        ]
    },
    "649": {
        "question": "Dear Professor and TAs, Hello, my name is Charles Yoon and I am currently at the hospital after being transported to the ER at around 4 AM. I began having intense back pain Tuesday that immobilized me completely, rendering me unable to walk, stand, or really do anything except lay down and use my phone. I thought it would get better if I rested in bed after taking some Tylenol, but it kept getting worse and once my legs started to lose feeling, I was immediately taken to the ER by CUEMS, where I was diagnosed with a herniated disc. The doctors want me to stay in the hospital for the time being to receive proper care, and unfortunately I will not be able to type or submit a coding assignment by Thursday. Would it be possible to be granted an emergency extension? Thank you. I am willing to provide any documentation that might be necessary.",
        "answers": [
            "Sorry to hear; that doesnt sound good. You can submit homework 1 until Monday October 2nd. Beyond that, it may be better to drop homework 1 from grading and reweigh the remaining deliverables. I hope you feel better soon. "
        ]
    },
    "650": {
        "question": "Hi! I currently have a low-300s perplexity score and was wondering where to start looking to get this score down? Thank you!",
        "answers": [
            "Hello! One thing that helped me was making sure to account for the edge cases mentioned in other discussions here related to 'START' tokens and word counts. For unigram ('START') and bigram ('START', 'START'), making sure that the count and probability of those was handled properly. I. e. , for counts, how many sentences would kind of implicitly begin with ('START') and ('START', 'START'), and for probabilities, what should the chances of our model generating a 'START' token be?"
        ]
    },
    "651": {
        "question": "Hi, I noticed that when I re-submit my code for HW1, Canvas automatically adds a number suffix to the file's name. For example, the first time I submit the code, its name will be trigram_model. py . When I resubmit an updated version, it becomes trigram_model-1. py . Would this naming issue pose any deduction to this assignment due to the names being inconsistent with what's mentioned in the requirements? Thanks!",
        "answers": [
            "This is fine, Courseworks automatically renames resubmissions this way"
        ]
    },
    "652": {
        "question": "Hi! When running part 7, I keep stumbling on an error: FileNotFoundError: [Errno 2] No such file or directory: 'hw1_data/ets_toefl_data/test_low/1443872. txt'I have set the file paths properly: Acc =essay_scoring_experiment(\"hw1_data/ets_toefl_data/train_high. txt\", \"hw1_data/ets_toefl_data/train_low. txt\", \"hw1_data/ets_toefl_data/test_high\", \"hw1_data/ets_toefl_data/test_low\") I was thinking maybe it had something to do with the command I was running so could you please provide me with clarification on what to follow with here \"python trigram_model. py ____\"",
        "answers": [
            "For part 7 the command line parameters shouldnt matter because the path names are all hardcoded. I wonder if you possible use the wrong directory name for one of the test batches in essay_scoring_experiment. "
        ]
    },
    "653": {
        "question": "My accuracy is surprisingly high (0. 958). My perplexity for Q6 was 196. 5. I've checked my previous methods so many times; not sure which step could've gone wrong. ",
        "answers": [
            "You seem to be using model 2 twice when computing perplexity for the \"low\" test data. So essentially all of the \"low\" documents are classified as correct :)"
        ]
    },
    "654": {
        "question": "Hello TAs, I seem to be having a problem with my perplexity calculations and I cannot figure out what the problem is.  I am including my code here along with my thinking process to see if anyone can pinpoint where I'm going wrong. My get_ngrams and count_ngrams functions appear to be working correctly (I get the correct counts for the example ngrams in the assignment prompt), so I don't think the problem is in part 1 or 2.  If you'd like to see my code for these parts anyway, let me know. For part 3, here is my code for the raw probabilities.  I am taking into account the unseen bigram context for trigrams, the case when a bigram begins with START in bigrams, and unseen words in unigrams:    def raw_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n\n        # unseen bigram context, return 1 / |V|\n        if (trigram not in self. trigramcounts and \n            (trigram[0], trigram[1], ) not in self. bigramcounts):\n            return (1 / len(self. lexicon))\n\n        # unseen bigram        \n        if (trigram not in self. trigramcounts):\n            return 0. 0\n        \n        # bigram prob\n        return(self. trigramcounts[trigram]\n               /self. bigramcounts[(trigram[0], trigram[1])])\n\n    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        if ((bigram[0], ) == ('START', )):\n            return (self. bigramcounts[bigram]/self. total_sentences)\n \n        if (bigram not in self. bigramcounts):\n            return 0. 0 \n        \n        return(self. bigramcounts[bigram]/self. unigramcounts[(bigram[0], )])\n    \n    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.  \n\n        # UNK unigram\n        if (unigram not in self. unigramcounts):\n            return (self. unigramcounts[('UNK', )]/self. total_words)\n\n        # unigram prob\n        return (self. unigramcounts[unigram]/self. total_words)\nFor part 4, I seem to be using linear interpolation correctly:    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation). \n        \"\"\"\n        lambda1 = 1/3. 0\n        lambda2 = 1/3. 0\n        lambda3 = 1/3. 0\n\n        return(lambda1*self. raw_trigram_probability(trigram) +\n        lambda2*self. raw_bigram_probability((trigram[1], trigram[2])) +\n        lambda3*self. raw_unigram_probability((trigram[2])))\nFor part 5, I get all trigrams in the sentence, find the smoothed probability for each trigram, convert it to log probability using math. log2(), then add that log probability to the list.  Finally, I sum up all the log probabilities in the list to find the total sentence log probability:    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        trigrams = get_ngrams(sentence, 3)\n        probs = []\n        for t in trigrams:\n            p = self. smoothed_trigram_probability(t)\n            log_p = math. log2(p)\n            probs. append(log_p)\n\n        logprob = 0. 0\n        # sum log probabilities:\n        for p in probs:\n            logprob += p\n        return float(logprob)\nFinally, I calculate the perplexity, keeping track of the number of tokens in the test corpus (adding 1 for each sentence to account for 'STOP'), summing the log probabilities of each sentence and dividing by the number of tokens in the test corpus, multiplying this result by -1 and raising 2 to this result per the perplexity formula:    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6) \n        Returns the log probability of an entire sequence.\n        \"\"\"\n        log_prob_sum = 0. 0\n        total_test_words = 0\n        for sentence in corpus:\n            total_test_words += len(sentence) + 1\n            log_prob_sum += self. sentence_logprob(sentence)\n\n        return float(pow(2, -1*(log_prob_sum/total_test_words)))\nWhen training the model on the train data and calculating the perplexity of the test data, I get a value of around 36.  If I run the model on the test data and calculate the perplexity of the test data, I get a perplexity of about 8.  If I run the model on the test data and calculate the perplexity of the train data, I get a perplexity of around 16.  Something is clearly off here, but I can't seem to figure it out.  Any help is appreciated.  Thank you for your time.",
        "answers": [
            "From what I can tell:1. Try adding ', ' to your smoothed_trigram_probability, so your return statement should be (see in bold)return(lambda1*self. raw_trigram_probability(trigram) +\n        lambda2*self. raw_bigram_probability((trigram[1], trigram[2])) +\n        lambda3*self. raw_unigram_probability((trigram[2], )))\n2. In your raw_trigram method you want to account for ('START', 'START', 'X') case. For your raw_unigram method you want to call self. unigramcount[unigram] (rather than self. unigramcount[('UNK', )]), and divide by the number of words (not sure where you calculated them prior but maybe check that too). Also, for each of the return statements of raw_probability methods, cast the result with float()Hope this helps!"
        ]
    },
    "655": {
        "question": "Hi, Not sure if I can ask this here therefore put it on private but for question 6 &amp; 7, at first for raw_trigram_probability I returned 1/word_count (where word_count is total number of words in training data) for when the count(u, w) would be 0 and got a perplexity of around 250 and accuracy of 73% for Q6 &amp; Q7. After I changed  the raw_trigram_probability for when the count(u, w) would be 0 to be 1/lexicon_size (number of unique unigrams except for START), I got a perplexity of around 28 and accuracy 99%. Is this possible or is this too good to be true and I accidentally made something wrong here ?",
        "answers": [
            "Accuracy of 99% is definitely not right -- I suspect there is something wrong with your experimental setup. "
        ]
    },
    "656": {
        "question": "Regarding the probability of the unigram ('START', ), would this be a special case where we need to divide the number of sentences in the corpus by the total unigram count?",
        "answers": [
            "The unigram ('START') should never be predicted by the language model and therefore it should have a probability of 0. "
        ]
    },
    "657": {
        "question": "Hi! I just submitted my lab and I want to double check that it's okay that the file is now called trigram_model-1. pyThanks! ",
        "answers": [
            "This is fine -- Courseworks automatically renames resubmissions this way. "
        ]
    },
    "658": {
        "question": "Hi, I am getting perplexity scores which are very high. 3. 6668123503602652e+50 Is there something I should check that maybe be throwing this number way off. Thanks!",
        "answers": [
            "I recalled having the same problem yesterday, several things to check:1. ensure your count_ngram is correct(as the example in the homework description page)2. special cases in raw_trigram_probability, raw_bigram_probability, raw_unigram_probability , I recommend checking #55, and that you are assigning zero to those bigrams/trigrams that didn't appear(thus numerator being zero)3. check the perplexity denominator of l(should be number of tokens appeared in testing)Hopefully these will help you debug this. I recalled not having any problem with sentence_logprob , but having problem mostly with the 4 functions mentioned above",
            "I also had this issue because I didn't realize there was a difference between m and M. m is the total number of sentences while M is the total number of word tokens"
        ]
    },
    "659": {
        "question": "Just to ensure my understanding of Part 3 is correct, is calculating the unigram probabilities just P(v) = count(v) / |V|? Meanwhile, is finding the bigram probability just P(w|u) = count(u and w)/count(u) while finding the trigram probability just P(w|u, v) = count(u, v, w)/count(u, v)? (not taking into account the edge cases). ",
        "answers": [
            "The unigram probability should be count(u) / N (where N is the total number of tokens in the corpus). "
        ]
    },
    "660": {
        "question": "Hello! For clarification purposes, if, for parts 6 &amp;7, our program takes a minute or two to display an output, will that negatively impact our grade? Or as long as an output is rendered, are we fine? Thank you in advance! ",
        "answers": [
            "Chances are that, in that case, you are not using dictionaries efficiently. We may take a small deduction if the program is inefficient. That being said, iterating through the training corpora may take a minute or so during training. "
        ]
    },
    "661": {
        "question": "when I run the \"generate_sentence\" function, I keep getting sentences that have no meaning at all such as the following, and the probability of the \"STOP\" token is very small and almost constant (of the order of e-5) everytime, does this make sense and is it the same for anyone else ? Example outputs with a t of 10:['distinguish', 'proteases', 'sell', 'neil', 'whichever', 'corner', 'dogmatically', 'hilum', 'talking', '%']['yearned', 'dominant', 'timeless', 'townsmen', 'mountains', 'interpreter', 'prize', 'assume', 'depressed', 'agreeing']['instantly', 'covering', 'enforcing', 'disrespect', 'peeled', 'blasphemous', 'analyzing', 'charming', 'wrath', \"o'dwyers\"]Not sure where I'm going wrong. Approach:1) for each word in the lexicon, generate a trigram probability2) Draw a random number r between 0 and 1. 3) Iterate through every word in the lexicon, keeping a sum of event probabilities. 4) Once the sum exceeds r, return the current event. is my step 1 correct? am I approaching it the wrong way?",
        "answers": [
            "Any clarification on this would be very much appreciated! Thank you!"
        ]
    },
    "662": {
        "question": "when I run the \"generate_sentence\" function, I keep getting sentences that have no meaning at all such as the following, and the probability of the \"STOP\" token is very small and almost constant (of the order of e-5) everytime, does this make sense and is it the same for anyone else ? Example outputs with a t of 10:['distinguish', 'proteases', 'sell', 'neil', 'whichever', 'corner', 'dogmatically', 'hilum', 'talking', '%']['yearned', 'dominant', 'timeless', 'townsmen', 'mountains', 'interpreter', 'prize', 'assume', 'depressed', 'agreeing']['instantly', 'covering', 'enforcing', 'disrespect', 'peeled', 'blasphemous', 'analyzing', 'charming', 'wrath', \"o'dwyers\"]Not sure where I'm going wrong.",
        "answers": [
            "Also getting this problem with responses like:['lilly', 'preferred', 'banks', 'the', 'banks', 'preferred', 'the', 'butcher', 'shop', 'shop', 'the', 'took', 'f or', 'acting', 'preferred', 'family', 'butcher', 'butcher', 'preferred', ', ', 'shop', ', ', 'but', 'will', 'took' , 'be', 'took', 'butcher', 'where', 'for', 'the', 'butcher', 'butcher', 'the', 'the', 'took', 'shop', 'chronicle s', 'students', 'help', 'of', 'family', 'leveling', 'relationships', 'took', 'their', 'shop', 'preferred', 'the', 'the', 'reports', 'students', 'privacy', 'her', '. ', 'loneliness', 'continue', 'STOP'] If you increase t you see STOP more frequently but I'm getting a lot of repeated words and nonsensical sentences. "
        ]
    },
    "663": {
        "question": "Hi TAs, I believe I have done parts 1-5 correctly; however, my perplexity score is still very high. I have made sure to not count ('START', ) in my unigramcounts, but I still achieve a 325. 7 as my score. I would appreciate any guidance on how I can reduce this number. Thanks! Here is my code for reference:def get_ngrams(sequence, n):\n    \"\"\"\n    COMPLETE THIS FUNCTION (PART 1)\n    Given a sequence, this function should return a list of n-grams, where each n-gram is a Python tuple.\n    This should work for arbitrary values of n &gt;= 1 \n    \"\"\"\n    temp = ((['START']*(n-1)) + sequence + ['STOP']) if n &gt;= 2 else (['START'] + sequence + ['STOP'])\n\n    res = []\n    for i in range(len(temp) - n + 1):\n        res. append(tuple(temp[i:i+n]))\n\n    return res\n    def count_ngrams(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 2)\n        Given a corpus iterator, populate dictionaries of unigram, bigram,\n        and trigram counts. \n        \"\"\"\n\n        self. unigramcounts = {} # might want to use defaultdict or Counter instead\n        self. bigramcounts = {} \n        self. trigramcounts = {} \n\n        ##Your code here\n\n        for sentence in corpus:\n            for gram in get_ngrams(sentence, 1):\n                if gram ! = ('START', ):\n                    self. unigramcounts[tuple(gram)] = 1 + self. unigramcounts. get(tuple(gram), 0)\n\n            for gram in get_ngrams(sentence, 2):\n                self. bigramcounts[tuple(gram)] = 1 + self. bigramcounts. get(tuple(gram), 0)\n            \n            for gram in get_ngrams(sentence, 3):\n                self. trigramcounts[tuple(gram)] = 1 + self. trigramcounts. get(tuple(gram), 0)\n\n        self. total_words = sum(self. unigramcounts. values())\n        print(self. total_words)\n\n        return\n    def raw_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) trigram probability\n        \"\"\"\n\n        numerator = self. trigramcounts. get(trigram, 0)\n        denominator = self. bigramcounts. get(trigram[0:2], 0)\n\n        if denominator == 0:\n            return 1/len(self. lexicon)\n        \n        return numerator/denominator\n    def raw_bigram_probability(self, bigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) bigram probability\n        \"\"\"\n        numerator = self. bigramcounts. get(bigram, 0)\n        denominator = self. unigramcounts. get(bigram[0:1], 0)\n\n        if denominator == 0:\n            return 1/len(self. lexicon)\n        \n        return numerator/denominator\n    def raw_unigram_probability(self, unigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 3)\n        Returns the raw (unsmoothed) unigram probability.\n        \"\"\"\n\n        #hint: recomputing the denominator every time the method is called\n        # can be slow! You might want to compute the total number of words once, \n        # store in the TrigramModel instance, and then re-use it.\n        \n        return self. unigramcounts. get(tuple(unigram), 0)/self. total_words\n    def smoothed_trigram_probability(self, trigram):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 4)\n        Returns the smoothed trigram probability (using linear interpolation). \n        \"\"\"\n        lambda1 = 1/3. 0\n        lambda2 = 1/3. 0\n        lambda3 = 1/3. 0\n\n        return lambda1 * self. raw_trigram_probability(trigram) + lambda2 * self. raw_bigram_probability(trigram[1:3]) + lambda3 * self. raw_unigram_probability(trigram[2:3])\n    def sentence_logprob(self, sentence):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 5)\n        Returns the log probability of an entire sequence.\n        \"\"\"\n        trigrams = get_ngrams(sentence, 3)\n        res = 0\n\n        for i in sentence:\n            self. total_test += 1\n            \n        for gram in trigrams:\n            res += math. log2(self. smoothed_trigram_probability(gram))\n\n        return res\n    def perplexity(self, corpus):\n        \"\"\"\n        COMPLETE THIS METHOD (PART 6) \n        Returns the log probability of an entire sequence.\n        \"\"\"\n        res = 0\n\n        for sentence in corpus:\n            res += self. sentence_logprob(sentence)\n\n        res /= self. total_test\n\n        return 2**(-1 * res)\n",
        "answers": [
            "My question has been answered!"
        ]
    },
    "664": {
        "question": "Please help, I have been struggling trying to get the accuracy up but it is staying at 0. 79 and the perplexity score at 162. Attached is the code, please help point me in the right direction. Thank you very much for your help, Andy",
        "answers": [
            "I'd try and add print statements. From what I can tell:1. You're not taking care of ('START', ) in your get_ngrams (or maybe you just commented what's below by accident? )2. In your count_ngrams you should handle the case of unigram being ('START', ) but not the cases of bigrams/trigrams with ('START', .. ) so maybe try changing that3. In your raw_probability methods, you should handle the cases of having 'START'Let me know if that helps :)"
        ]
    },
    "665": {
        "question": "Hi all, I am getting some strange behavior for my generate_sentences function. It is common for words like 'and' and 'the' to repeat themselves, as well as punctuation. i. e. : ['instinctively', ', ', ', ', 'aware', ', ', 'the', 'and', 'innocence', 'mrs', 'innocence', 'by', 'innocence', 'is', ', ', 'soldiers', 'far', 'innocence', 'or', 'is', 'he']\nAnother example with t = 100:['lilly', 'preferred', 'banks', 'the', 'banks', 'preferred', 'the', 'butcher', 'shop', 'shop', 'the', 'took', 'f\r\nor', 'acting', 'preferred', 'family', 'butcher', 'butcher', 'preferred', ', ', 'shop', ', ', 'but', 'will', 'took'\r\n, 'be', 'took', 'butcher', 'where', 'for', 'the', 'butcher', 'butcher', 'the', 'the', 'took', 'shop', 'chronicle\r\ns', 'students', 'help', 'of', 'family', 'leveling', 'relationships', 'took', 'their', 'shop', 'preferred', 'the', 'the', 'reports', 'students', 'privacy', 'her', '. ', 'loneliness', 'continue', 'STOP']\r\n\nWondering if anyone can offer some advice. I am using my raw_trigram_probabilities function to make a probability distribution for the possible words in the lexicon (excluding UNK and START), which I think is correct (seemed to work fine for the rest of the problems). Help would be appreciated! Thanks.",
        "answers": []
    },
    "666": {
        "question": "I'm getting a really, really high perplexity and am struggling to figure out why. I went step by step through 1-5 which all seemed to work smoothly, but for perplexity, I am getting around a 6000 score. Did anyone else get a score in this range and figure out why?",
        "answers": [
            "If your 1-5 worked smoothly, I would suggest you looking into the part where you calculate the log probabilities.  If you still have the problem, you can make a private post so the TA can take a look at your code."
        ]
    },
    "667": {
        "question": "Is it acceptable if I get an accuracy of 73. 7% for \"essay_scoring_experiment\"?",
        "answers": [
            "That seems a little low, unfortunately. People are reporting accuracies around 84% or 85%. "
        ]
    },
    "668": {
        "question": "HelloHow what command do I use to test part 6? When I run python trigram_model. py hw1_data/brown_test. txt I get an IndexError. ",
        "answers": [
            "You also need to provide the test corpus as a second command line parameter. "
        ]
    },
    "669": {
        "question": "Hi, I am confused on how to implement start. I currently have: In get_ngrams I add START as a unigram. I do not count start as one of the words in the lexicon. In raw_bigram_prob, if it is START, WORD i calculate probability by doing bigramcounts[bigram]/sentencecount. In raw_trigram. If I have START, START, WORD, then i get prob by saying trigramcounts[trigram]/sentencecount. (I did not include any special cases for START, WORD, WORD as that did not seem to require one)Are these correct assumptions. I am getting errors in my code and trying to debug. Thanks  ",
        "answers": [
            "That sounds all correct to me. "
        ]
    },
    "670": {
        "question": "Dear teaching team, I was revising the homework before submitting and one detail made me curious. In Part3, we read the following: My intuition says that the second alternative, using P(v) as a proxy, would be better since assuming a uniform distribution across all words could introduce bias. Is there any reason why the first alternative is the one recommended? Thank you!",
        "answers": [
            "The first version empirically works better on this data. I suspect its because the context is a very strong indicator for what the next word should be, so using the unigram probability will assign too much probability mass to some words. "
        ]
    },
    "671": {
        "question": "Hi! I am getting way too big numbers (about 10/10^2 off) from what has been mentioned on Ed. I'm not sure what is wrong with my code as each step seems to work/make sense on its own. Any tips as to what might be causing the issue? Thanks!",
        "answers": [
            "One thing Im noticing is that your smoothed_trigram_probability function isnt quite right. For a trigram (u, v, w) you are using u as the unigram and (u, v) as the bigram, rather than w for the unigram and (v, w) as the trigram. Additionaly, i dont think you need to worry about the case (START, w) and (START, START, w) in the smoothed probability if you are handling it correctly in the raw probability functions. "
        ]
    },
    "672": {
        "question": "Hi, I have a few questions needs to be clarified: In method count_ngrams, should we count the number of START tokens as 0? i. e. model. unigramcounts[('START', )] = 0 or regular counts number? Related to the question 1, if we DO NOT count the START tokens, then case ('START', x) would be a special case (division by zero). If so, should we treat ('START', x) = (x)? If we COUNT the START tokens, then case ('START', x) would be a regular case. Related to the question 2, should we assign 0 possibility to START in raw_unigram_probability? If we do so, then ('START', x) would be a division by zero case again. In method raw_trigram_probability, if count(u, v) = 0, then we use 1 / |V|. For |V|, should we minus the START token from the size of the lexicon? i. e. should we use 1 / (len(self. lexicon) - 1) or 1 / len(self. lexicon)? In method raw_trigram_probability, should we treat ('START', 'START', x) = ('START', x)? Or should we treat it as a regular case, and use number of sentences in corpus for ('START', 'START')? For essay score, I got 83. 6. Is this score acceptable or does it still indicate that there are some minor errors in my code? ",
        "answers": [
            "1. I think the START counts should be all 0. At least the probability for the start unigram should be 0. 2. You will have to treat (START, x) as a special case -- i recommend storing the number of sentences in a new instance variable, and then using this for the denominator. The same applies to (START, START, x). 3. Yes P(START) should be 0. 4. START should not be included in the lexicon. So either remove it from the lexicon or use 1 / (len(self. lexicon) - 1). 5. These two approaches should be equivalent since count(START, START, x) = count(START, x). 6. That seems just a tad low. Most implementations I have seen get 84-85%. "
        ]
    },
    "673": {
        "question": "Hello, Just making sure, but, in part 6, m and M refer to sentences and word tokens within the test data, correct? Does this also mean that before part 6, any mention of word count or sentence count refers only to the train data? ",
        "answers": [
            "Hm, I'm not sure I understand. When computing the probabilities that make up the mode, you can only rely on the training data. For the perplexity calculation, m and M refer to the sentence and token count in the test data."
        ]
    },
    "674": {
        "question": "I've seen a couple questions on here asking about their specific perplexity scores, but is there a concrete baseline for where our number should be? Thanks!",
        "answers": [
            "I'd say for training data around 34 and for test data around 200."
        ]
    },
    "675": {
        "question": "Write the method generate_sentence, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\", \"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each. Draw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated. Wanted to get the hint on the above as mentioned in courseworks.",
        "answers": [
            "In Python, I believe you can use random. choices while supplying the optional weights variable. ",
            "There are several approaches here. The simplest one for our use case:1. Draw a random number r between 0 and 1. 2. Iterate through the different event, keep a sum of event probabilities. 3. Once the sum exceeds r, return the current event. Another easy approach is to use the random. choice function: &gt;&gt;&gt; numpy. random. choice(['A', 'B', 'C'], p = [0. 7, 0. 2, 0. 1]) You should also skip UNK tokens. An easy way is to simply sample the next token again if you end up with UNK. "
        ]
    },
    "676": {
        "question": "When we use raw_bigram_probability and raw_trigram_probability, we utilise different denominators depending on the situation (e. g. the prior is ('START', ) in bigram, and ('START', 'START') in trigram, and bigramcounts[prior] is 0). Therefore, I think the sum of these two functions cannot be zero. Is this correct?",
        "answers": [
            "I'm not sure I understand your question, could you try and elaborate? Thanks!"
        ]
    },
    "677": {
        "question": "Just wanted to ask the exact expressions to compute the Raw Probabilities of (START, START) in case of Bigram, (START, X) in case of Bigram, (START, START, X) for trigram, and (START, X, X) for Trigram, as we would not be computing the unigram probabilities for START.",
        "answers": [
            "Hello! One thing that helped me was to distinguish between when we use probability versus count and which formulas use which. Some formulas will want the count of how many times a given ngram (such as START, START) occur in our corpus, and some will want the probability of that bigram, aka, what should the chances be of our model generating that bigram be. "
        ]
    },
    "678": {
        "question": "Per previous discussions, I understand that it does not make statistical sense to include the (\"START\", ) unigram when counting the total number of tokens in the corpus and each unigram's counts. Since the get_ngrams is only called when 1) calculating the counts, and 2) calculating sentence log-prob (with n=3), why do we even return (\"START\", )  as part of the unigrams list? Isn't this the equivalent of returning (\"START\", \"START\") when n=2 and (\"START\", \"START\", \"START\") when n=3 both which we do not do. ",
        "answers": [
            " In the comments of this https://edstem. org/us/courses/46417/discussion/3467312 post, I saw Prof. Daniel reply:\"It makes sense to not generate it at all. Feel free to implement accordingly (even if this contradicts the example). \" to the same question. Keeping the post up for more visibility if anyone else is curious. "
        ]
    },
    "679": {
        "question": "Just wanted to ask the exact expressions to compute the Raw Probabilities of (START, START) in case of Bigram, (START, X) in case of Bigram, (START, START, X) for trigram, and (START, X, X) for Trigram, as we would not be computing the unigram probabilities for START.",
        "answers": [
            "Assuming you're referring to part 3, I'd suggest looking at self. unigramcounts[], self. bigramcounts[] and self. trigramcounts[] when solving this question."
        ]
    },
    "680": {
        "question": "Hello, Is 287 an acceptable perplexity score?",
        "answers": [
            "That sounds reasonable. "
        ]
    },
    "681": {
        "question": "Hi, I am curious if we get additional marks for implementing Interlude - Generating text. Kind regards, Junbang",
        "answers": [
            "No, this part is entirely optional. "
        ]
    },
    "682": {
        "question": "So the instruction says that for step 6, the perplexity should be less than 400, is 308 acceptable? cuz i saw in other posts saying 304 is too high",
        "answers": [
            "Yes, I think most are in the range of 150 or so. There is some variation here in how you handle some corner cases. So 300 may be okay. "
        ]
    },
    "683": {
        "question": "Hi, I am not sure if this is a naive question but I was a bit confused about how to set up model to test count_ngrams in Part 2. The instructions say \"Where model is an instance of TrigramModel that has been trained on a corpus. \" but I am not totally sure what this means. Thank you in advance for any help!",
        "answers": [
            "Oh, we just mean that the variable \"model\" refers to a TrigramModel after it has been trained, for examplemodel = TrigramModel(\"brown_train. txt\")"
        ]
    },
    "684": {
        "question": "Hi I hope you are doing great when I am running the test for part 7, i commented out the two lines of code in the mainwhen i run it on the command line i am runningpython trigram_model. pyI am getting an EOL error so I think I am missing argument.  Is there something else we should be including on the command line for testing part 7thank you so much!",
        "answers": [
            "this is resolved, thank you so much!"
        ]
    },
    "685": {
        "question": "Hello, For the sequence [\"natural\", \"language\", \"processing\"] we know that the first trigram is ('START', 'START', 'natural')I wanted to confirm that for the sequence [\"natural\", \"language\", \"processing\", \"class\"] the ngrams when n=4 would be('START', 'START', 'START', 'natural'), ('START', 'START', 'natural', 'language'), ('START', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'class'), ('language', 'processing', 'class', 'END')",
        "answers": [
            "Yes, that's correct. "
        ]
    },
    "686": {
        "question": "Hi I hope you are doing great.  I have been wrestling with part 7 for a while and I think I am having trouble with the intuition of what its happening in the code of this part. there is one loop going through all the test files of group 1 and for each file is determining a perplexitythis is follwed bya second loop going through all the test files of group 2 and for each file is determining a perplexitybut group1 - is all test_highand group 2 - is all test_lowso how do we actually compare the perplexities to determine if something is correct? thank you so much!",
        "answers": [
            "For each document in group 1, run both models. If the \"high\" model has lower perplexity than the \"low\" model, you are classifying the document correctly, so you count 1 \"correct\" point. Otherwise, you are classifying it incorrectly. Then you repeat the same for each document in group 2, except that now the \"low\" model should have lower perplexity than the \"high\" model. "
        ]
    },
    "687": {
        "question": "I'm on part 6 of the hw and I'm getting really low perplexity values--like ~5 and ~3 When I find the summation of tokens in the corpus of brown_train. txt I get 98203 tokens, does this seem right? I figure I'm doing the smoothing or above incorrectly or I'm counting tokens incorrectly. Thanks in advance!",
        "answers": [
            "Make sure you use the number of tokens in the test data for M when you compute perplexity."
        ]
    },
    "688": {
        "question": "M in the equation for calculating perplexity means the total number of word tokens = number of unigram tokens, so does that mean M = (sum(self. unigramcounts. values()) - 1) The -1 here to exclude the START? Or does that mean M = sum(len(sentence) for sentence in corpus)? Same question for when counting the raw_unigram_probability, do we use(sum(self. unigramcounts. values()) - 1) or sum(len(sentence) for sentence in corpus) for N? ",
        "answers": [
            "M is the number of tokens in the test data, so your second expression. Similarly, N is the number of sentences in the test data. "
        ]
    },
    "689": {
        "question": "Hi, I just want to confirm some of the edge cases that result in different behaviors for the unigrams/bigrams/trigram probabilities. For unigrams, we must explicitly deal with the edge case of \"START\". For bigrams, we must explicitly deal with bigrams of the form (\"START\", x). For trigrams, we must explicitly deal with trigrams of the form (\"START\", \"START\", x) and the denominator being 0. Are there any others that I am missing?",
        "answers": [
            "Sounds correct to me! Although not necessary (see #52), you can also handle the edge case of bigrams having denominator of 0."
        ]
    },
    "690": {
        "question": "I tested the model. raw_trigram_probability(('START', 'START', 'the'))butself. bigramcounts[('START', 'START')]  does not exist. (KeyError: ('START', 'START'))Should we ignore this case? Thank you.",
        "answers": [
            "No, this is a special case you need to account for. My suggestion would be to keep track of the total number of sentences in the training data in a new instance variable, then use that value as the denominator when you compute P(the | START, START). "
        ]
    },
    "691": {
        "question": "Hi, I wanted to make clarify:The raw uni/bi/trigram probability methods returns the probability for a specific ngram and not for the whole set, correct? Best, Omer",
        "answers": [
            "Right this method returns a single P(w|u, v) probability. "
        ]
    },
    "692": {
        "question": "HI! I hope you have a great day! There are a couple of questions in HW1:I know someone already asked this, but I'm unsure if I fully grasped the concept. Is the reason why we don't encounter cases where unigram(prior) = 0 in bigrams (given (prior, word)) because we treat those values as UNK? We don't count the START token, but why do we include START in the lexicon? In raw_unigram_probability, one of the questions that was already asked was if we should return 0 when encountering the 'START' or 'STOP' token, and the answer indicated 'yes'. I understand for START since it's not included, but shouldn't the count for STOP be non-zero since it's included? In raw_bigram_probability and raw_trigram_probability, we didn't include (START) in unigrams and (START, START) in bigrams. Should we set the value to the number of sentences in the corpus when we need to count these cases to get the denominator? Similarly, for the bigram (START, START), is this different from the case where count(u, v) = 0? For computing perplexity, is M the number of words before being processed by get_ngrams? Or is it the count after adding the STOP token? Thank you so much!",
        "answers": [
            "1. correct -- all unseen tokens have been replaced with UNK by the corpus reader. There should not be any cases where you encounter a unigram in the test data that has not been seen during training. 2. It's not really necessary to include START in the lexicon. In fact, when you use the size of the lexicon make sure START is not included. 3. It's debatable if you should consider STOP as part of the unigram distribution, but I think it makes sense to include it (thus not set it to 0). Imagine generating a document by randomly sampling from this distribution repeatedly until you encounter STOP. 4. Yes, you can use the number of sentences in the training corpus here. 5. Yes, that's a different scenario. You DO know the distribution of words at the beginning of the sentence. 6. When computing perplexity, M is the number of tokens in the test data, including STOP (i. e. , the number of predictions the language model makes on the data. "
        ]
    },
    "693": {
        "question": "In the solution \"There are 6 words, plus the END symbol, so |V|=7\". I understand that there are 6 words, but why do we not count \"START + END\" and only count \"END\"? ",
        "answers": [
            "When you think of the model as generating sentences by drawing the next token from the conditional distribution, given the context, it will at some point select the STOP token. But it will never sample the START token because there is no probability for P(START | x). "
        ]
    },
    "694": {
        "question": "Hello, what is an acceptable perplexity score? Is 304 acceptable?",
        "answers": [
            "That would be a little bit too high, but we will let you guys know what would be the baseline for this part"
        ]
    },
    "695": {
        "question": "Hi, I was wondering if the case that we encounter when \"count(u, w) is also 0\" in the trigram probability model will also apply to the bigram probability model. Will we ever see a time where we haven't seen the first bigram word in a sequence and we should do the same uniform distribution as with the trigram case?",
        "answers": [
            "You mean the case where you havent seen the context u for a bigram (u, v)? That cannot happen because there are no unseen unigrams. The corpus reader automatically replaces them with UNK. "
        ]
    },
    "696": {
        "question": "Hi, What's an acceptable accuracy score for the essay_scoring_experiment() function. Is an 84% accuracy acceptable or should I try and increase it?",
        "answers": [
            "That seems okay to me. "
        ]
    },
    "697": {
        "question": "Hello, I apologize if this is a very simple fix but I am very, very new to programming in Python and keep getting the following error message: My understanding is that msilib is a Windows specific package (? ) but despite trying to manually install it I keep getting this message. I appreciate any help! Thanks!",
        "answers": [
            "Hm, according to the traceback this is an import in the trigram_model. py file? This is definitely not in the template :) see what happens if you delete that line. "
        ]
    },
    "698": {
        "question": "I had a few questions:While computing the raw probability for bigrams, if we come across a word that is not seen before (i. e, unigram count is zero), should we treat it as UNK, and instead use the count for ('UNK', )? While finding the number of words for the denominator in calculating the raw unigram probabilities, if there are repeated words, they should be counted multiple times right? That is, we are not finding the number of unique words but instead finding the total number of words? Similarly, while calculating perplexity, does the denominator (number of word tokens) count repeated words multiple times? Also, while counting the number of word tokens, should we count 'STOP' multiple times, or only once? For the generate_sentence() function, to generate the next word, my understanding is that we should we randomly choose from all words that appears in the given context (last two words). Is this correct?",
        "answers": [
            "1.  No the corpses reader does this automatically. If you look at the __init__ method, it takes a lexicon as parameter, which allows the corpus reader to identify unseen tokens. 2. Right. The denominator in the raw unigram probability is the number of tokens (word instances including duplicates) in the training data. 3. Yes. M in the perplexity formula is the number of tokens in the training data. 4. Yes, you would choose the word from the conditional distribution for the context. "
        ]
    },
    "699": {
        "question": "Hi, Is the OH on Saturday still happening or is there a different link that is being used?",
        "answers": [
            "Hi, May had to move her office hours to Thursday for this week only. Sorry for the inconvenience. "
        ]
    },
    "700": {
        "question": "Hi, I am wondering if we need to keep the directory in the original code when submitting. In order to test, I have changed it to my own directory and is it necessary to change it back to the original? Best, Xiaoyu",
        "answers": [
            "Please change any pathnames in the code back to defaults. I actually dont think we use this for testing because we call the methods individually, but better to be safe. "
        ]
    },
    "701": {
        "question": "For the denominator of the raw unigram probabilities, should we include \"STOP\" as part of the total word count, but not \"START\"? ",
        "answers": [
            "Thats  correct. Include STOP but not START. "
        ]
    },
    "702": {
        "question": "Hi, I have following questions for hw1, 1. Since we count ('STOP'), (? , 'STOP'), (? , ? , 'STOP') in unigramcounts, bigramcounts, trigramcounts respectively. How do we deal with ('START') in unigramcounts, (start, ? ) in bigramcounts, (start, start, ? ) and (start, ? , ? ) in trigramcounts. 2. If we do not store ('START') in unigram dictionary, (start, ? ) in bigram dictionary, (start, start, ? ) and (start, ? , ? ) in trigram dictionary. I am not sure how can we deal with P( (u, v) | 'START'), P(u, v, w | ('START', v) ) and P(u, v, w | ('START', 'START') ) as a special case. 3. Can we use basic numpy functions such as np. sum in our scripts? 4.  Shall I assume that the input sentence has already been tokenized in sentence_logprob(sentence)? 5. Shall I return zero as probability for raw_unigram_probability() when I get tokens 'START' or 'STOP'? 6. Is |V| in raw_bigram_probability() or raw_trigram_probability() the total number of distinct unigrams excluding 'START'? Sorry for the long questions. Kind regards, Junbang",
        "answers": [
            "1 and 2: you would count bigrams (START, x) and trigrams (START, x, x). I recommend not counting the bigram (START, START) or the unigram (START, ). Instead, keep track of the number of sentences in a separate instance variable.  3: I would prefer you didnt use anything but in the standard library because it slows down grading. The regular sum function or math. fsum should do the job here. 4: yes, the corpus reader tokenizes the sentences. 5: yes returning 0 makes sense. 6: yes. But you should only need |V|for the raw trigram probability when dealing with unseen contexts. I dont think its used for the bigram probabilities. "
        ]
    },
    "703": {
        "question": "Hi, In the last question in assignment 1, I found that it is common to encounter unseen words from training when calculating perplexity of testing dataset. This results in a probability of zero returned by the smoothed_trigram_probability() and calculating the log probability with zero results in math error. Is this expected and how can we resolve this problem. Kind regards, Junbang",
        "answers": [
            "Hmm something is wrong with your setup. There should be no unseen unigrams. The idea is that the model collects the lexicon during training and replaces all words appearing only once with UNK. During testing, the lexicon is passed to the corpus iterator, so that unseen words in the test data are replaced with UNK as well. "
        ]
    },
    "704": {
        "question": "Hi for the office hours today i am getting the message that the meeting id is not valid.  is there something else i can try?  thank you!",
        "answers": [
            "Hi can you try it again? Not sure if its my problem or zoom's problem"
        ]
    },
    "705": {
        "question": "Hi, Does \"total number of words tokens in the corpus\", which is referring to M, include bigrams and trigrams? Thank you",
        "answers": [
            "No, just the unigram tokens, including STOP (but not START). "
        ]
    },
    "706": {
        "question": "Hi I hope you are doing great. I just ran the test for my perplexity with argv[1]  being brown_train and argv[2] being brown_testThe value returned is:-1. 1662575414840142Does this make any sense? Thank you so much!",
        "answers": [
            "Something is off, that shouldnt happen. ",
            "So several things could happen. Perplexity is computed using the equation 2^l, where l represents the average of the log probabilities of sentences. The value of l should be negative, ensuring that 2^l is positive. If l is used directly as perplexity instead of 2^l, the result would be incorrect. Given your provided numbers, assuming that is you value of  l, calculating 2^l yields an implausibly small perplexity. A potential issue might be with the calculation of M. It's important to note that M should not be the total number of words in the train corpus (self. total_words) but rather the total number of words in the testing corpus. When iterating over the log probabilities of sentences, ensure that you count the number of words in the test set. Additionally, remember to count the \"STOP\" token for each sentence."
        ]
    },
    "707": {
        "question": "Hello, Could we possibly get the raw probabilities for some test ngrams just so I can check my understanding of Q3? Using the 1/|V| rule for where the conditioned on ngram = 0 if possible. .. For example:&gt;&gt; model. raw_trigram_probability[('START', 'START', 'the')]\n\n&gt;&gt; model. raw_trigram_probability[('fake, 'example', 'here')]\n\n&gt;&gt;&gt; model. raw_bigram_probability[('START', 'the')]\n\n&gt;&gt;&gt; model. raw_bigram_probability[('fake', 'example')]\n\n&gt;&gt;&gt; model. raw_unigram_probability[('the', )]\nThank you so much!",
        "answers": [
            "You could approximate this by doing some calculations on the corpus, especially for the start token:If you iterate through the corpus and check the first word of every sentence and build a distribution table of that, then you'll find approximately (barring differences stemming from tokenization) how common each word is as a starter word. That should roughly match the result of:&gt;&gt; model. raw_trigram_probability[('START', 'START', 'the')] At least. Because all the bigrams's it's checked against with are START START word and the denominator is just the number of words in the sentence. You should get a decently high value for picking a very common word like The, since lots of sentences start with it."
        ]
    },
    "708": {
        "question": "Hello, I am having some issues calculating perplexity, and I wanted to check that my understanding was correct. Right now, I am getting a perplexity of about 1 in the test set which I think is too small. Just to clarify, is the variable M in the perplexity formula essentially every word we encounter in the corpus not including start? I have attached my code for reference. Thanks",
        "answers": [
            "Hi Alexander, That's very close! The idea is that for each sentence in the corpus, you want to calculate the log probability of each word in it. For each word, increase M by 1 (except from START). Also, don't forget to account for the STOP symbol at the end. Hope this helps!"
        ]
    },
    "709": {
        "question": "Hi, Part 3 wants us to compute the raw probability, then are we still supposed to make P(v | u, w) = 1 / |V| (where |V| is the size of the lexicon), if count(u, w) is 0? Thank you",
        "answers": [
            "Yes, you will need a workaround for the situation where count(u, w) is 0 -- otherwise you will end up with a division by 0 error. My suggestion is to set P(v | u, w) = 1 / |V|. "
        ]
    },
    "710": {
        "question": "Hi, Just for clarification, is the size of lexicon basically the same as number of unigrams? Thank you",
        "answers": [
            "It's the size of the set of all unigrams. So if a word appears twice, it's only counted once."
        ]
    },
    "711": {
        "question": "Hello! Could I declare a total_tokens class variable, and set it to be the return value of count_ngrams? This way I wouldn't have to traverse the text again. I'm asking since  count_ngrams doesn't originally have any return value. Thanks for your help!",
        "answers": [
            "Thats fine. But why not just populate the total_token count inside of count_ngrams? "
        ]
    },
    "712": {
        "question": "Hi I hope you are doing great. I would like to double check my understanding of self  in python if that is okayi know that self refers to  an instance of a class, so if I have an instance variable insti can use it by tying self. inst I think that is right butI am less sure  using methods that have self as argumentsfor example in the definition of this function:def raw_trigram_probability(self, trigram):self is in an argument, so does that meanthat if I later use this method in a different method I can use it like: raw_trigram_probability(self, some_trigram)or do i have to say: self. raw_trigram_probability(self, some_trigram)thank you so much!",
        "answers": [
            "class Dog:\n    def __init__(self, name):\n        self. name = name\n    def bark(self):\n        print(\"Woof my name is\", self. name)\n\nrover = Dog(\"Rover\")\nrover. bark()\nThe \". \" passes the variable on the left as the first argument to the function. So to have Rover bark, you can either do Dog. bark(rover) or rover. bark()."
        ]
    },
    "713": {
        "question": "Hi all, I'm having some trouble understanding the instructions for the interlude part and would appreciate some clarification. The part I'm not clear of is the following regarding the selection of a new token to add to the existing sentence:Draw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. After we get all the words that appeared in the current context, we would have a list of candidates and their probabilities p(v | u, w). Does the above instruction mean that we would need to randomly select a word from all the words that appeared in the current context based on their raw trigram probabilities? For example, if \"the\" has a trigram probability of 0. 2 and \"of\" has a probability of \"0. 3\", both in the context of (\"START\", \"START\"), we should randomly select from these 2 to add to the sentence and \"of\" should have a higher probability of being selected than \"the\"? Any clarifications would be highly appreciated! Thank you.",
        "answers": []
    },
    "714": {
        "question": "Hi, Do we have to comment out all test code before submit? Thank you!",
        "answers": [
            "Yes please. "
        ]
    },
    "715": {
        "question": "Hi, Do we need to include the count of the \"UNK\" token when calculating the perplexity and do we need to skip the \"UNK\" word if the next generated word is \"UNK\" and generate a new word instead? Thanks a lot!",
        "answers": [
            "You should include it. From the perspective of the language model UNK is just another token (like any other). "
        ]
    },
    "716": {
        "question": "Hi, a couple clarification questions:What's the newest version of Python we can use for this assignment? May we import anything in the standard library to work on this? May we define custom helper functions? May we add type hints to all the function signatures? May we save the total token count in TrigramModel. init? If not, where? In TrigramModel. count_ngrams, are we obligated to use get_ngrams, or may we use any function that yields the correct answer? Should unigramcounts include \"START\"? Should unigramcounts include \"STOP\"? Should the \"if name == \"main\" clause do anything in particular when we finish? Can we set it to whatever we want? In TrigramModel. generate_sentence, should the model be capable of producing the \"UKN\" token? Do raw_{uni, bi, tri}gram_count_total accept tuples of one, two, and three strings, respectively? Thank you, Daniel Indictor",
        "answers": [
            "That's quite a few questions. Thanks for being efficient. 1. I would be careful using anything past 3. 92. Yes, the standard library is fair game. Third party libraries (including spacy, nltk, and even numpy) should not be used, unless otherwise specified. Generally, the assignments are designed to be solvable with only the information provided. That is, if you need something from the standard library (such as collections. Counter or collections. defaultdict) that will be mentioned in the assignment. 3. Sure4. Yes, because type annotations won't affect functionality. 5. Yes, that's a good idea. 6. I would recommend you use get_ngrams. We will test the function by functionality, so if it works correctly without calling get_ngrams that should be fine. But that suggests you are repeating code. 7. No. 8. Yes. The way to think about counting START and STOP is this: STOP is predicted when the model \"generates\" a sentence, but \"START\" is not. So the unigram event STOP should have some probability assigned to it, but START should not. Of course, then you won't be able to use count(START) when computing P(word | START. I suggest keeping track of the number of sentences instead and treating P(word | START) as a special case. 9. We won't test the \"main\" part, so it can be anything you want. 10. This part is optional, so you implement this anyway you like. But I would suggest avoiding the UNK token in the generated output. 11. Yes "
        ]
    },
    "717": {
        "question": "Hi I hope you are doing great! I am working on raw unigram probability method.  Taking the hint in the code:#hint: recomputing the denominator every time the method is called # can be slow! You might want to compute the total number of words once, # store in the TrigramModel instance, and then re-use it. My idea is to store the length of the corpus_file as an instance variable.  To calculate this it seems I would have to add to the  corpus_reader method that was given to us.  I don't think we are supposed to do thats  though. Is this what the hint meant, or am I misunderstanding something? Thank you so much!",
        "answers": [
            "I would count the total number of tokens (and maybe the total number of sentences) in the count_ngrams method. The problem with modifying the corpus_reader is that this function is separate from the TrigramModel class, so you wouldn't be able to directly store the token count as an instance variable. "
        ]
    },
    "718": {
        "question": "For the total number of word tokens here, should we count any padding we've added? Or just the actual words?",
        "answers": [
            "You should count the END marker, but not START. The idea is that the number or predictions the model makes does include END tokens, but START is never predicted. "
        ]
    },
    "719": {
        "question": "I'm confused about how Katz' Backoff for Trigrams works. I wonder if there is a more specific example about it, thank you so much!",
        "answers": [
            "The essential idea is: if you count for trigram is not 0, use trigram probability; if it's 0, then backoff to bigram. The idea is that the occurrence of trigram is self contained in bigram, but not the other way around. Then you continue the process from bigram to unigram and etc. The alpha term is a normalizing factor representing residual probability for backoff to ensure a normalized final probability distribution (summing up to 1). "
        ]
    },
    "720": {
        "question": "Hi I hope you are doing great! I have a question about these instructions:Counting n-gramsNow it's your turn again. In this step, you will implement the method count_ngramsthat should count the occurrence frequencies for ngrams in the corpus. The method already creates three instance variables of TrigramModel, which store the unigram, bigram, and trigram counts in the corpus. Each variable is a dictionary (a hash map) that maps the n-gram to its count in the corpus. For example, after populating these dictionaries, we want to be able to query&gt;&gt;&gt; model. trigramcounts[('START', 'START', 'the')]\n5478\n&gt;&gt;&gt; model. bigramcounts[('START', 'the')]\n5478\n&gt;&gt;&gt; model. unigramcounts[('the', )]\n61428\nWhere model is an instance of TrigramModel that has been trained on a corpus. Note that the unigrams are represented as one-element tuples (indicated by the , in the end). Note that the actual numbers might be slightly different depending on how you set things up. I am at this step and have been trying to test my count_ngrams, but I dont think I am doing it correctly. should i be able to just run this line:model. trigramcounts[('START', 'START', 'the')]in the main, or is there some other steps I should be taking? I tried putting the brown data in txt in the same folder, but i think I have gotten myself mixed up. Thank you so much!",
        "answers": [
            "you can run the program using command python trigram_model. py hw1_data/brown_train. txt. sys. argv[1] is the parameter name for the input fileif you run the program with brown_train. txt instead of brown_test. txt, you should get the same number as the example. that's what I have at least. .. and of course, you need to print the counts out using print statements under main"
        ]
    },
    "721": {
        "question": "Hi I hope you are having a great day! Im working through part 2, but i am unsure of how to test this part.  Do you have any advice on how to most effectively test the code part by part.  Thank you so much! ",
        "answers": [
            "In general, I would try to run the various parts on a small toy training corpus so that you can check counts and probabilities by hand. ",
            "i set up a unittest to do it; it helps a lot for automating testcases for edge cases. It also helps for not flooding stdout as only failed test cases are printed."
        ]
    },
    "722": {
        "question": "Hi I hope you are doing great! In Part 2s code it has a comment:        self. unigramcounts = {} # might want to use defaultdict or Counter instead\n        self. bigramcounts = {}\n        self. trigramcounts = {}so if i want use a default dict or Counter, is it okay if I just change the definitions above? Also there is an empty return at the end of the method.  If I'm not returning anything should I leave the return there or can i remove it? Thank you so much!",
        "answers": [
            "Yes you can simply replace the initialization.  You can remove the return statement at the end. "
        ]
    },
    "723": {
        "question": "Hi I hope you are doing great.  Even though we are not changing this part of the given code, I'm a little unclear on this part : \"The idea is that words that appear only once are so rare that they are a good stand-in for words that have not been seen at all in unseen text. You do not have to modify this function. \"  What does this actually mean? How are they a stand- in? Thank you so much!",
        "answers": [
            "This is because the words that appears only once are so rare so that you can treat them as any other OOV words that you might encounter in the future. The main intuition behind this approach is to (1) it allows you to cuts off the long tail of the distribution (2) create smooth probability for new OOV words."
        ]
    },
    "724": {
        "question": "Where can we find the answers to the ungraded exercises, if there is any?",
        "answers": [
            "I will post these usually with a delay. So if it isnt up yet, check back the next day. "
        ]
    },
    "725": {
        "question": "As covered during today's lectures I'm a bit confused between the difference of unseen tokens, unseen contexts and unknown tokens. Is unseen tokens merely the process of replacing words with unknown tokens and unseen contexts the process of assigning values to the the unknown tokens to mellow out the spikes? Thanks!",
        "answers": [
            "Hello! This is how I understand it, TA's please correct me if I'm wrong:The idea of something being \"seen\" has to do with whether or not the model has encountered it (the token, the ngram) after going through all the training data. For individual unseen tokens, the question is: how do we have our model handle tokens in the test data that we never encountered in the training data? The approach we discussed in lecture was: we treat all tokens we haven't seen as the same token, which we label as unknown (UNK). Now the question is, how do we prepare our model to handle the token UNK? The strategy we discussed was to pick a number k, and say that for any token in our training data, if the token shows up fewer than k times, we replace it with UNK. Then we treat all instances of UNK  in our training data the same as any other token, calculating unigram and ngram probabilities or whatever approach we're choosing. Now that we've made sure that our model thinks it's seen every individual token it encounters in the test data, the question for unseen contexts was: how do we calculate the probability of some ngram, e. g. the trigram (w | u, v), when it's possible that  count(u, v) is 0, meaning, we've never seen v in the context of u. And that part of lecture discussed different strategies for figuring out how to calculate probabilities like (w | u, v) given that possibility. I hope that helps!"
        ]
    },
    "726": {
        "question": "Hi Prof. Bauer and TAs, In part 1 example, The first return element has two 'START's, whereas the previous n=1 and n=2 examples have only 1 'START' in the first return element. I am confused on the padding rules here. ..",
        "answers": [
            "I have the same question. Is it an arbitrary pattern? ",
            "From my understanding, we pad n-grams with \"START\" tokens to provide the appropriate context (i. e. , indicate the start of a sentence). In the case of bigrams, we pad the first bigram with one \"START\" token to generate the bigram that only includes the first word. In the case of trigrams, to generate the trigram that only includes the first word, we need to pad it with two \"START\" tokens, and to generate the trigram that only includes the first two words, we need to pad it with one \"START\" token. And so on. .. for quadgrams, we would need three \"START\" tokens in the first quadgram, etc. For instance, in the case of trigrams, if we only had one \"START\" token, the first trigram generated would be (\"START\", \"natural\", \"language\") (i. e. , a sentence that begins with \"natural language\"). But this precludes something else we might be interested in: the sentence that starts with just \"natural\"! So we would also include the trigram (\"START\", \"START\", \"natural\"). In some implementations, we would actually directly add the (n-1) \"START\" tokens to the beginning of the sentence and generate the n-grams that way, but the gist is that the number of \"START\" tokens depends on the number of empty/incomplete \"spots\" at the beginning of the n-gram that we need to fill in. ",
            "In my opinion, after today's class, I understand that we need two w_0 word vector to create Markov chain to predict the future probability of the next word, so there should be two 'START' to create a beginning. Hope this can help you!"
        ]
    },
    "727": {
        "question": "Hi I hope you are soing great, In class when we were going over naive bayes we reviewed to formulas, one for the prior probabilty and one for the conditional probability.  In the the prior probability are counts were of documents, and in our conditional probability our counts were of words.  Why dont we do both calculations with word count? Thank you!",
        "answers": [
            "Hi, I think for $y$ we are looking at the classes of these documents instead of classes of words, which is why we do the calculation with document count instead of word count. "
        ]
    },
    "728": {
        "question": "Hi I hope you are doing great! what does this symbol mean:from this equation:thank you!",
        "answers": [
            "Hi, is a normalizer. It's stated on page 23 of lecture 3's slides :)"
        ]
    },
    "729": {
        "question": "Hi there! I'd like to know if the teaching team has any recommendations as to what sources we should use to get extra experience with topics covered in ungraded assignments such as Naive Bayes. It would be great to train and check solutions to verify our understanding, even if the assignments themselves don't affect our grades. Thank you!",
        "answers": [
            "Thats what the ungraded exercises are supposed to be for. You could check the Jurafsky and Martin textbook to see if there are any additional exercises. "
        ]
    },
    "730": {
        "question": "Dear TAs,    I have a question about part2 of the assignment. In this part, there is a request:which means, when searching for 3-grams word group\"START START the\", the function in the program should print 5478, but I noticed that there was no possible to find a group of \"START START the\" in the processed file(added START and STOP at each sentence in the \"brown_train. txt\"), so I tried to search the 3-gram group\"STOP START the\"and the function printed successfully with the number\"5477\":So I'm wondering whether there is a problem in this part or not? Thank you! Sincerely, YUNING",
        "answers": [
            "I hope I understand your question correctly, but the count_ngrams counts the n-grams calculated using the corpus. You might want to incorporate what you implemented in part 1 for this problem. Hope this helps!"
        ]
    },
    "731": {
        "question": "Hi I hope you are doing great,  Sorry for all the questions. On a slide from today:I get the first line that B and C are becoming independent of each other. But I dont get the equivalently part.  Like for each part below is it only true given A, or shouldn't we also meet the criteria of  =  P(B| C), and =P(C| B)  (for each part respectively wouldn't that also have to be true)Thank you!",
        "answers": [
            "I believe that if we were to add the criteria of = P(B| C), and =P(C| B) then we would no longer have events B, C solely dependent on A but rather dependent on each other and on A. This would mean that they would no longer be conditionally independent. ",
            "So B and C are conditionally independent given A. Here is the full chain of equations that might be helpful. Derived using the chain rule of probability (line 1) and rule of conditional independence (line 2), ",
            "Both the equations make sense. The equation on the top makes sense because both B and C are independent to each other, such that A has already occurred. The second equation simply suggests that the knowledge of C occurring does. Ot provide any knowledge of B occurring and vice-versa. The left equation suggests that even if both A and C were to occur together jointly, only the occurrence of A alone would suffice to suggest about the occurrence of B. The equation on the right suggests that even if A and B were to occur together, only the occurrence of A alone would suffice to suggest about the occurrence. Both together imply that the occurrence of B in no way affects occurrence of C and vice-versa, even if these variables were to occur in the presence of A (jointly), whose occurrence does provide information of the occurrence of both B and C, as these two are dependent on A. "
        ]
    },
    "732": {
        "question": "Hi I hope you are doing great! In class when we were doing examples  and using bayes rule, we were able to just ignore the denominator.  Is this just because we can drop the denominator anytime we are calculating the probability of a particular class,  or is there some other rule that decides when we can do the calculation without the denominator. Thnak you so much!",
        "answers": [
            "The denominator can take on the value of some constant alpha (as defined in class). This is because for each calculation the denominator value remains the same. When looking for a label y that can maximize the probability we can choose based on a reduced simpler version that has the alpha.",
            "In other words, we are always comparing two Probabilities, P(y=1 | x) vs P(y=0 | x). When you write them out using Bayes rules, you will notice that on both sides of the equation, the same denominator p(x) is present. Algebraically we can cancel this from our equation, so when we are formalizing what the Naive Bayes Classifier score is, we can just ignore the denominator completely. "
        ]
    },
    "733": {
        "question": "Hi I hope you  are doing great. What does this symbol stand for:Thank you!",
        "answers": [
            "The capital pi is the product operator, and you can think of it as the multiplicative analog of the summation operator (capital sigma). For instance:\\sum_{n=1}^4n=1+2+3+4=10\\prod_{n=1}^4n=1\\cdot2\\cdot3\\cdot4=24"
        ]
    },
    "734": {
        "question": "Greetings to all the teaching staffs, Does anyone have an idea when the TAs' OHs and the DUE dates for all assignments will be up for this course? Thank you for your time and attention!",
        "answers": [
            "Dear all, We've posted our TA Office Hours on the Canvas syllabus. Although due dates vary from Mon-Fri, we'll try our best to ensure there's an Office Hour on each due date. Check Canvas for details."
        ]
    }
}