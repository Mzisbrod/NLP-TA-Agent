[
    {
        "context": "Final Exam Solution",
        "statements": [
            {
                "source": "Will the final exam solution be provided so that we can check our deduction? Thanks",
                "target": [
                    "Most rubric items will contain an explanation for the correct/expected answers. I wont be able to provide a complete sample solution. "
                ]
            }
        ]
    },
    {
        "context": "Will our participation grade also be added to Canvas?",
        "statements": [
            {
                "source": "I was wondering whether our participation grade would also be added to Canvas, or if the grade we currently see with the exam taken into account already includes that",
                "target": [
                    "The participation grade is not included on Courseworks currently. I can post these tomorrow (essentially, unless there is no record of you ever participating in the course, you will get 100% of the participation score. ) Note that you need to weigh the different scores according to the formula on the rubric to compute your final score. "
                ]
            }
        ]
    },
    {
        "context": "Final Exam Grade",
        "statements": [
            {
                "source": "When can we expect to get our final exam grades back?",
                "target": [
                    "Grading is currently a little more than halfway complete. We're hoping to finish this week."
                ]
            }
        ]
    },
    {
        "context": "HW5 Part 3, count punctuation in input/output pairs?",
        "statements": [
            {
                "source": "In the example input/output sequence resulting from the sequence:['&lt;START&gt;', 'a', 'black', 'dog', '. ', '&lt;END&gt;'] It seems the period token ('. ') is skipped as the last input/output pair shown consists of:input = [START, a, black, dog]output = ENDIs this a typo? Should this input/output pair be:input = [START, a, black, dog]output = . and then an additional input/output pair would be the following? input = [START, a, black, dog, . ]output = ENDThanks!",
                "target": [
                    "Yes the missing . is a typo. "
                ]
            }
        ]
    },
    {
        "context": "HW5 Part1: Creating Encodings",
        "statements": [
            {
                "source": "In HW5 Part 1, it says \"We will need to create encodings for all images and store them in one big matrix\". Does this mean we should be encoding an image with img_encoder. predict() , and then resizing it to 1x299x299x3, since it returns a 2048 size vector? Or does \"create encodings\" mean just resizing the image to be 299x299x3, and then returning that \"encoding\" of the image in required shape? ",
                "target": [
                    "I think the first one (resizing it to 1x299x299x3), as the Model. predict_generator function waas asking for a 4-dimensional array. "
                ]
            }
        ]
    },
    {
        "context": "Model Accuracy",
        "statements": [
            {
                "source": "Should we decrease/increase the number of epochs so that the model should finish training around 40%?",
                "target": [
                    "The accuracy should be at least 40% for reasonable results. If that takes more epochs, its okay to increase the number of epochs. "
                ]
            }
        ]
    },
    {
        "context": "HW5 Submission",
        "statements": [
            {
                "source": "My machine does not process the pdf downloading command on anaconda jupyter, so I printed the html to get pdf instead. And HW5 asks for pdf submission, but coursework only allows compressed files. Shall we submit a link containing the pdf, or will a compressed pdf file be fine? Thank you!",
                "target": [
                    "You should submit a zip file that contains both the notebook and the PDF."
                ]
            }
        ]
    },
    {
        "context": "Ungraded Exercise:IBM model",
        "statements": [
            {
                "source": "regarding the second part of the question, why are all q=1, is it always the case? q(3 | 1, 3, 3) = 1q(2 | 2, 3, 3) = 1q(1 | 3, 3, 3) = 1q(5 | 1, 5, 5) = 1q(4 | 2, 5, 5) = 1q(3 | 3, 5, 5) = 1q(2 | 4, 5, 5) = 1q(1 | 5, 5, 5) = 1",
                "target": [
                    "I think it's because there's no other way to map the alignments one you have some already known"
                ]
            }
        ]
    },
    {
        "context": "Secure Exam Proctor",
        "statements": [
            {
                "source": "Do people who take exam via Zoom today also have to set this up?  I cant find an announcement regarding this.",
                "target": [
                    "No, students taking the exam on Zoom will have real-time proctoring (i. e. I or one of the TAs will be watching you take the exam). "
                ]
            }
        ]
    },
    {
        "context": "Semantic Parsing",
        "statements": [
            {
                "source": "I just want to make sure, I see that there's a (*) next to the Semantic Parsing topic on the exam topic overview. Would all of the concepts under it not be tested in the exam?",
                "target": [
                    "that's correct, none of these topics will be tested on the exam. Note that this only applies to full-sentence semantic parsing, not semantic role labeling. "
                ]
            }
        ]
    },
    {
        "context": "Constraints in RNN based SRL",
        "statements": [
            {
                "source": "How are these constraint penalty scores c(w, y_1:t) computed? They need to be differentiable",
                "target": [
                    "Good question. I tried to find this in the He et al paper, but they simply say \"Each constraint function c applies a non-negative penalty given the input w and a length-t prefixy1:t\"https://aclanthology. org/P17-1044/"
                ]
            }
        ]
    },
    {
        "context": "How is a Parse Tree Path be encoded into a log-linear model as a feature?",
        "statements": [
            {
                "source": "One way is to one-hot encode over all possible paths upto some reasonable length, say 10. Is there another way such paths are encoded to form a feature in practice?",
                "target": [
                    "You would one-hot encode the possible paths. Note that you would only instantiate the features that are actually seen during training, so you wouldn't have to create a dimension for each possible path -- only for the ones that have been observed. "
                ]
            }
        ]
    },
    {
        "context": "Cheat sheet",
        "statements": [
            {
                "source": "Hi! I was wondering whether the cheat sheet needs to be handwritten? Or can it be printed? Thanks",
                "target": [
                    "It can be handwritten or typed, but it must be a physical copy."
                ]
            }
        ]
    },
    {
        "context": "Feature functions",
        "statements": [
            {
                "source": "Hi , In this solution there is a Start symbol and so can we assume that there is an end symbol as well?",
                "target": [
                    "Sure. "
                ]
            }
        ]
    },
    {
        "context": "Bonus Ungraded Exercise: Viterbi Algorithm",
        "statements": [
            {
                "source": "Hi, For this exercise I was wondering if I am interpreting the solution correctly. So is the reason that we are returning the sum over states (sum_s) rather than the max (max_s) so that we can consider multiple paths rather than the most likely path? In what use case would quantifying the possible tag sequences be relevant?",
                "target": [
                    "Your understanding is correct. I can't really think of a specific use case -- maybe you want to get an estimate of how ambiguous a sentence is. The number of possible POS sequences might be a good proxy for that. "
                ]
            }
        ]
    },
    {
        "context": "Dummy Exam",
        "statements": [
            {
                "source": "Hello -- has the gradescope dummy exam been posted yet?",
                "target": [
                    "I still get an error when I click on gradescope that I have no account, so I think not? Unless something is wrong on my endI also can't make an account without a course ID",
                    "It's available now. Please do NOT try to add the course using a course ID, but instead use the link on Courseworks to access Gradescope. "
                ]
            }
        ]
    },
    {
        "context": "Gradescope Dummy Exam",
        "statements": [
            {
                "source": "HI Professor Bauer/TAs, I was wondering when the dummy exam will be posted? When I clicked on Gradescope via coursework, this course was not added yet. THanks! ",
                "target": [
                    "Same situation here; I assume it is not activated yet.",
                    "#283 "
                ]
            }
        ]
    },
    {
        "context": "Ungraded exercise - CKY Does order of nonterminals matter?",
        "statements": [
            {
                "source": "Hi, If we have grammar:S -&gt; NP VVP -&gt; V NPWanted to confirm: does the order matter? If we have a \"NP V\" can it only be S, or will it be S and VP? Thanks, Vidur",
                "target": [
                    "#81"
                ]
            }
        ]
    },
    {
        "context": "Ungraded exercise - bigram",
        "statements": [
            {
                "source": "Why does END symbol gets added to the 6 words but not the START symbol even though they are both given in the corpus of sentences? ",
                "target": [
                    "#14"
                ]
            }
        ]
    },
    {
        "context": "Bigram Lang Model",
        "statements": [
            {
                "source": "Would it be possible to just explain in words why this base case works? I am just trying to get the big picture. Also what does the p and r represent? ",
                "target": [
                    "Does this help? , P() is the probability, q and r are words for arbitrary indices in a sentence. Here, assumed q occurs before r."
                ]
            }
        ]
    },
    {
        "context": "CVN asynchronous format",
        "statements": [
            {
                "source": "For CVN students taking the exam asynchronously, will we be filling out our answers on Courseworks, or do we need to print out and scan answers? Additionally, do we need to print out the \"cheat sheet\" or can we keep it alongside Proctorio when taking the exam? ",
                "target": [
                    "You need to print out the pdf, or write your answers on a different set of sheets if necessary. Then scan the solution and upload to Courseworks. The cheat sheet should be a physical paper, not a pdf. "
                ]
            }
        ]
    },
    {
        "context": "N-Gram exercise",
        "statements": [
            {
                "source": "In the solutions, P(eats | dog)= (1+1)/(3+7) = 1/5 is it (1+1) for 'eats' because 'dog' is only followed by 'eats' 1 time? I was under the impression that it was the number of occurrences of the word + 1 for smoothing. For n-grams, are we looking for the number of occurrences of the n-gram phrase? Such as \"Dog eats\" only appears once hence the (1+1)? dog bites humandog bites duckdog eats duckduck bites humanhuman eats duckhuman eats salada) dog eats saladP(dog | START)= (3+1)/(6+7) = 4/13\nP(eats | dog)= (1+1)/(3+7) = 1/5\nP(salad | eats)= (1+1)/(3+7) = 1/5\nP(END | salad)= (1+1)/(1+7) = 1/4\nP(dog eats salad) =\nP(dog | START) P(eats | dog) P(salad | eats) P(END | salad) =\n4/13 x 1/5 x 1/5 x 1/4 = 1/325 = 0. 0031",
                "target": [
                    "Thats correct. The context dog (which appears three times in total) is followed only once by eats. Therefore the raw probability would be 1/3 and with add-one smoothing (1+1)/(3+7). "
                ]
            }
        ]
    },
    {
        "context": "Parameters of a trigram model (ungraded Viterbi exercise)",
        "statements": [
            {
                "source": "Hi! For the ungraded exercise on the Viterbi &amp; forward algorithms, I was confused by  c): \". .. write out the parameters of a trigram model that assigns the same probability distribution to a set of surface sequences. .. \". Are the parameters in question the probabilities for any one transition (i. e. N-&gt;N, N-&gt;V, etc. ). If so, would a trigram model require more nodes (since the transition probability depends on the bigram context, so N-&gt;V would need to be a single node, and so on)?  Hope this makes sense! ",
                "target": [
                    "So the trigram model should be a set of trigram probabilities defined over words, not POS tags. Yet, it should assign the same probability to any surface word sequence that the HMM does. "
                ]
            }
        ]
    },
    {
        "context": "will topics from the last lecture be on the final exam?",
        "statements": [
            {
                "source": "^^ since I do not see them on the review sheet. .. Thanks!",
                "target": [
                    "Pretty sure the professor said they will not be on the exam"
                ]
            }
        ]
    },
    {
        "context": "Question regarding soltuion of ungraded exercise: PCFGs and trigram models",
        "statements": [
            {
                "source": "Hi, I don't quite understand the calculations for:P(Alex | Alex saw) = 3/4P(Kim | Alex saw) = 1/4P(Alex | Kim saw) = 3/4P(Kim | Kim saw) = 1/4Could you please show how it was calculated? Thanks.",
                "target": [
                    "I believe there are four possible sentences based on the given PCFG:s1: \"START START Alex saw Kim STOP\"s2: \"START START Kim saw Alex STOP\"s3: \"START START Alex saw Alex STOP\"s4: \"START START Kim saw Kim STOP\"After \"Alex saw\" or \"Kim saw\" for each sentence, the word will be either \"Alex\" (probability = 3/4) or \"Kim (probability = 1/4)."
                ]
            }
        ]
    },
    {
        "context": "TA led review",
        "statements": [
            {
                "source": "Hi, Was just wondering if the TA led review was posted on Courseworks yet. Thanks! !",
                "target": [
                    "There was no TA led review this semester. Sorry. "
                ]
            }
        ]
    },
    {
        "context": "Questions from old exams which are not used",
        "statements": [
            {
                "source": "Hi, I think in a previous lecture, professor had mentioned he may post some questions from old exams which are not going to be used anymore. I was wondering if there was a way to get them, as was not able to find it. Thanks! Vidur",
                "target": [
                    "These have been posted in the assignments section, labeled bonus questions. "
                ]
            }
        ]
    },
    {
        "context": "Final Exam CVN Synchronous Zoom Link",
        "statements": [
            {
                "source": "Hello, Maybe I missed this but could anyone please inform which link are CVN students using for the synchronous final exam? Thank you in advance!",
                "target": [
                    "The regular class link, but I will send out a reminder tomorrow. "
                ]
            }
        ]
    },
    {
        "context": "ungraded exercise- IBM model 2",
        "statements": [
            {
                "source": "for the IBM model 2 question part 3, why is it q(5 | 1, 5, 5) * t(skwk | fly) not q(1 | 5, 5, 5) * t(skwk | fly)?",
                "target": [
                    "From my understanding, the parameters are: q(j|i, l, m) which correspond to alignment value i takes value j given source sentence length l and foreign m. q(5 | 1, 5, 5) = q(fly|skwk, 5, 5)I think there might be a typo in the last one:q(5 | 1, 5, 5) * t(et | the) should actually be: q(1| 5, 5, 5)* t(et | the)"
                ]
            }
        ]
    },
    {
        "context": "Knowledge of Databases like WordNet, VerbNet, etc",
        "statements": [
            {
                "source": "For the exam, will we need to have memorized some of the internal structures of these large databases mentioned in class? Or just generally know what they are used for and what information they hold?",
                "target": [
                    "You should know the structure, for example the various lexical relations recorded in wordnet, as well as what a synset is etc. "
                ]
            }
        ]
    },
    {
        "context": "IBM Model 2 Parameter",
        "statements": [
            {
                "source": "In part 3 of the last ungraded exercise, why is the last q(a_i | i) term q(5|1, 5, 5) and not q(1|5, 5, 5)?  For reference the solution has:q(5 | 1, 5, 5) * t(skwk | fly) *q(4 | 2, 5, 5) * t(e| a) *q(3 | 3, 5, 5) * t(qta | ate) *q(2 | 4, 5, 5) * t(skwk | bird) *q(5 | 1, 5, 5) * t(et | the) = 2/3",
                "target": [
                    "Because the question asks specifically about example 3, which is \"the bird ate a fly\". So, for q(5 | 1, 5, 5), 5 indicates et(the). "
                ]
            }
        ]
    },
    {
        "context": "How to show arc standard sequences are not unique?",
        "statements": [
            {
                "source": "Could you please provide some hints on how to answer part b? Thank you",
                "target": [
                    "I think to prove b you would just need get the dependency tree in a different way from a, which you would do by using at least 1+ different transition to arrive at the same result. I think you would focus on trying get to some variation using arc-right vs arc-left as a strategy, which can be done by changing how you use shift."
                ]
            }
        ]
    },
    {
        "context": "Spurious ambiguity?",
        "statements": [
            {
                "source": "I just wasn't quite certain what this term meant as I'm going through the lecture notes. Is it referrig to a case where a sentence seems to be ambiguous but it is not? Or am I off base here",
                "target": [
                    "Which lecture is this from? Thank you!",
                    "Correct me if I'm wrong, but I think it's when a parsing algorithm has multiple paths to get to the same state.",
                    "I agree with Chris. I think it can happen when we are confused about meaning of sentence and this is probably a good queue to use some sort of semantic analysis and contextual clues."
                ]
            }
        ]
    },
    {
        "context": "Ungraded exercise b) Transition-based dependency parsing",
        "statements": [
            {
                "source": "For the part b) of transition-based parsing ungraded exercise, does this require that we show another sequence for the same sentence, or use another sentence? Thank you!",
                "target": [
                    "I believe part b) is asking us to show another sequence for the same sentence. Please see #263."
                ]
            }
        ]
    },
    {
        "context": "condition on l, m in the estimate for q",
        "statements": [
            {
                "source": "(1) Is l, length of the source sentence? and m, of the target sentence? We condition on l, m when estimating \"how often does position i align with position j\" . I see why if one sentence is much longer than the other the indices that are aligned are different. But in general, for sentences of close-enough length which seems a reasonable assumption fot translations, is there still a need to condition on the sentence lengths being exactly the same?  (2)The denominator in this expression: Isn't this simply \"count of occurrences of position i such that source language sentence length is l and target language sentence length is m\"Which (for any i &lt;=m) would simply be the number of sentences in the corpus such that source language sentence length is l and target language sentence length is m?",
                "target": [
                    "1) m is the length of the source sentence (the F sentence) and l is the length of the target sentence (the E sentence). Even if two sentence pairs are close in length, they would be handled differently. This is obviously a very limiting assumption in IBM Model 2. 2) Yes, correct. The denominator is simply the number of all sentences with length less than m. "
                ]
            }
        ]
    },
    {
        "context": "LSTM loss: Softmax or Binary Classification with Negative Sampling?",
        "statements": [
            {
                "source": "In practice, is Negative Sampling with Binary Classification loss (with Sigmoid) commonly used instead of softmax for training LSTMs efficiently ?",
                "target": [
                    "As far as I'm aware, negative sampling is not commonly used when training RNNs. "
                ]
            }
        ]
    },
    {
        "context": "Are we allowed to get some blank sheets for calculations/rough work",
        "statements": [
            {
                "source": "Hi, Are we allowed to bring some blank sheets or will they be provided at the exam for rough work/calculations/drawing Viterbi calculation graph /transition graphs etc. It would be very helpful during the exam. Thank you",
                "target": [
                    "You will be provided blank sheets , however bring your calculators incase you need to do some calculations !"
                ]
            }
        ]
    },
    {
        "context": "Ungraded Exercise",
        "statements": [
            {
                "source": "Hi Professor Bauer, I was wondering if you are going to post more ungraded exercises (e. g. AMR) on the later half of the semester? Thanks! ",
                "target": [
                    "Sorry for that, I'm going to upload some more material tomorrow, but don't expect a ton of it. "
                ]
            }
        ]
    },
    {
        "context": "Ungraded Exercise: HMM - Viterbi & Forward Algorithm Confusion",
        "statements": [
            {
                "source": "For the forward algorithm, this is one of the equations in the solution:[2, V] = [1, N]  P(V|N)  P(flies |V) + [1, V]  P(N|V)  P(flies|N)) = (1/4 2/4  2/6) + (1/12  0  2/6)) = 1/24 = 0. 04166I am confused as to where the numbers for the second part of this equation ([1, V]  P(N|V)  P(flies|N)) are coming from. I thought the equation would be [2, V] = [1, N]  P(V|N)  P(flies |V) + [1, V]  P(V|V)  P(flies|V)), but perhaps I am understanding the forward algorithm wrong? Thanks in advance",
                "target": [
                    "I don't think you can go from V to V and that's why P(V|V) is not possible. I think the forward algorithm is the same as Viterbi but instead of taking the max we sum all the probabilities. "
                ]
            }
        ]
    },
    {
        "context": "Smoothing and Back-off",
        "statements": [
            {
                "source": "Regarding the n-gram an data sparsity issues, do we have to master at applying the smoothing and back-off approaches (like discounting, Katz' Backoff) to refine the probability for the final exam? Thank you~",
                "target": [
                    "Yes, you should understand the various methods in sufficient detail to be able to apply them to a (artificially small) data set. "
                ]
            }
        ]
    },
    {
        "context": "Ungraded exercise - viterbi algorithm",
        "statements": [
            {
                "source": "Hi, Q1) What do the red arrows signify in this solution? Q2) Can we draw the diagram as above for exam instead of drawing the alternative table representation? Can you also provide the solution for part c please. Thank you",
                "target": [
                    "For Q1, I believe that the red arrows indicate possible transitions from one state back to a previous state."
                ]
            }
        ]
    },
    {
        "context": "CVN Student - Async Exam",
        "statements": [
            {
                "source": "In the past, I have gotten emails with instructions for taking the exam with Proctorio with notes from the instructor (timing, cheatsheet, etc. .). Will this be sent out?",
                "target": [
                    "Yes. This will be sent out early next week. "
                ]
            }
        ]
    },
    {
        "context": "What is this topic?",
        "statements": [
            {
                "source": "In the Exam topics post on courseworks, there is an incomplete topic heading \"Struc\" and I was wondering what this overarching topic might be? Thanks so much!",
                "target": [
                    "That's a typo. You can safely ignore it."
                ]
            }
        ]
    },
    {
        "context": "Ungraded Exercise : Linear Models and Feature Functions",
        "statements": [
            {
                "source": "Hi, I was going over the solution for the feature functions exercise, I am not sure I understand how to solve the problem. Could you please help me with the same since I am really confused? 1) How is the order in which you select the word wi decided? Since there are 3 sentences and the solution doesn't necessarily select words in any of the sentence order. 2) Why is there no ti-1 for below feature function. 3) Why do we have one wi with time, when time appears thrice in the three sentences with different parts of speech (N and V)Part b) The weight vector has all 1s because we want all the features to be true for correct POS tagging? Part c) Can you please release the solutionThank you.",
                "target": [
                    "I'm also struggling to understand this concept, so any further explanation would be very helpful. Thanks!",
                    "1) I don't believe it necessarily matters how the word w_i is being selected, as feature functions can be any type of function that takes into account some or all of the possible arguments (w_{i-1}, wi, t_{i-1}, and t_i in this case) and outputs 0 or 1. 2) This feature function is just an example. I believe that w_i = \"fruit\" and t_i = \"N\" is sufficient in this case because all examples of \"fruit\" in the training data are of the form \"fruit/N. \" In the case of \"flies, \" for example, where both \"flies/V\" and \"flies/N\" appear in the training data, the corresponding feature functions include additional information (e. g. , w_{i-1}) that describe whether \"flies\" is being used as a noun or verb. 3) Like in the case of \"flies/N, \" in which there are two feature functions in the given solution (6, 7), we could have more than one w_i with time feature function if we chose to create one or more additional feature functions.",
                    "Seems like basically we need to get all highest amount of params possible that will hold true for all the given sentences. Hence, some have more parameters if they are all holding true and some are very limited (like unigram type)"
                ]
            }
        ]
    },
    {
        "context": "Final Exam - Calculator",
        "statements": [
            {
                "source": "I believe this was covered in class already but just want to confirm whether or not we need a calculator for the exam? Is it required or just encouraged (to make our lives easier)?",
                "target": [
                    "It's not a hard requirement, but a basic 4-function calculator may make some computations easier."
                ]
            }
        ]
    },
    {
        "context": "Will we be able to see HW4 results before next week?",
        "statements": [
            {
                "source": "Hi! Would it be possible to know HW4 results a little before the middle of next week, so that we know if we should try to submit HW5? ",
                "target": [
                    "We're hoping to finish before HW5's due date so that everyone can make an informed decision about HW5. We'll do our best to finish grading as fast as possible. :)"
                ]
            }
        ]
    },
    {
        "context": "Part 4 left and right context",
        "statements": [
            {
                "source": "Do we need to consider left and right context for part 4 like we do in part 5, or is this something we can do for part 6? ",
                "target": [
                    "Part 4 does not consider the left and right context at all, only the target word itself. So this is something you could incorporate in part 6. "
                ]
            }
        ]
    },
    {
        "context": "best_words in part 5",
        "statements": [
            {
                "source": "Are we allowed to change the number of best words (from 10) returned from the bert predictor? And if the resulting list doesnt contain any candidate words, should we be returning nothing?",
                "target": [
                    "Yes and yes.  "
                ]
            }
        ]
    },
    {
        "context": "NLTK Library",
        "statements": [
            {
                "source": "Are we allowed to use word_tokenize from the NLTK library?",
                "target": [
                    "Yes."
                ]
            }
        ]
    },
    {
        "context": "Illegal division by 0 in score. pl",
        "statements": [
            {
                "source": "There are some final tweaks I'd like to make to improve my precision and recall, but there are times where I will make an edit, re-write the output to the predict file, and run perl, but then will get the error I have illegal division. Even when I revert the code back to its original form, it will from thereon return an error. Is this happening for others and are there any ways to resolve this? ",
                "target": [
                    "The reason is that the . predict file is formatted incorrectly. If you are running Python in windows, it writes out windows style line endings (CRLF). But the perl evaluation script expects Unix style line endings (CR). Opening the file in a text editor and saving it again usually fixes the issue. "
                ]
            }
        ]
    },
    {
        "context": "HW5 Data - Page Doesn't Exist",
        "statements": [
            {
                "source": "Hi everyone! I'm trying to download the data for HW5, but get the following error. I've tried downloading both through the requests package (I'm on Windows) and by visiting the site directly. Any help would be greatly appreciated!",
                "target": [
                    "Please try again, there was a permission issue that has now been fixed. ",
                    "I think I was having the same issue but after using incognito tab, it worked. Might be a caching issue."
                ]
            }
        ]
    },
    {
        "context": "Bert vs Word2Vec Scores",
        "statements": [
            {
                "source": "Hi, I am getting interesting results where my Word2Vec score is higher than Bert. Wondering if this could be possible or anyone has thoughts on what could be the issue:Word2Vec: Bert:Thanks!",
                "target": [
                    "I am having the same issue with mine :/",
                    "Could you look at cases where the predictions disagrees and BERT gets it wrong. Then try to see out why ?  Sometimes it is because of incorrect masking index. Check if word2vec candidates appear in the BERT lexicon? If it does, how far is the rank of candidates in the list of most probable substitutes ? "
                ]
            }
        ]
    },
    {
        "context": "Part 5 Error No module named 'keras. saving. hdf5_format'",
        "statements": [
            {
                "source": "If anyone comes across this error, you can comment in modeling_tf_utils. py in the transformers package in Lib/site-packages/transformers to fix it",
                "target": [
                    "Great, thanks for sharing this with the cohort!"
                ]
            }
        ]
    },
    {
        "context": "Part2",
        "statements": [
            {
                "source": "Hi, Part 2 says : Note that you have to sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets. So if a lemma appears in 2 different synsets, do I have to count this lemma twice or do I have to consider them as 2 distinct lemma? Also, What can cause a variability in the accuracy in part 2? I ran the same code twice and I got 2 different results: ",
                "target": [
                    "You would have to sum together the . count value for the two lexemes to get a single score for the lemma. The result should always be the same for part 2-- this is a bit odd. "
                ]
            }
        ]
    },
    {
        "context": "Part 6 Computational Time",
        "statements": [
            {
                "source": "Are there any computation restrictions for this assignment? My solution for Part 6 takes about ~3. 5 hours of computational time to complete. I have not been using GPU. Given that I also submit the actual part6. predict file, is this acceptable?",
                "target": [
                    "Yes, as long as you document your approach in a comment in this case, since we wont actually be able to run it. "
                ]
            }
        ]
    },
    {
        "context": "Difference and connection between lemma, lexeme, and word form",
        "statements": [
            {
                "source": "Hi, I just wanted to make sure that I understand the following terms correctly. Lemma: is the base form of a word. (e. g. run)Word form: is any modified or inflected version of the lemma ( e. g. runner, running, runs, etc. )Lexeme: represents the core meaning and all related word forms of a word. This means it covers the lemma and its inflected forms (e. g. run ( base form) + runner, running, runs (inflected forms)) . Is that true ? ",
                "target": [
                    "Yes, a lexeme is a word with a particular sense. There may be multiple lexemes for the same lemma.  "
                ]
            }
        ]
    },
    {
        "context": "part4 synonym not in word2vec model",
        "statements": [
            {
                "source": "Hi, Would like to ask what to do the synonym when the word2vec model doesn' t have its embeddings. There are some synonym use multiple words and '-' to make a new word and those the gensim model could not find embeddings for them. Thanks",
                "target": [
                    "I asked a similar question privately in Ed  and this is the response the professor provided:  Its okay to ignore candidates that do not have a word2vec entry. One thing you could try is to replace the white space in multi word candidates with an underscore _. Another option would be to obtain word vectors for moving and picture and then average them. (The example I provided was a multi word moving picture)"
                ]
            }
        ]
    },
    {
        "context": "hw4 pt 3 - repeating words in context/definition",
        "statements": [
            {
                "source": "Just wondering having repeated overlapping words in context/def would mean for the overlap count. So for example:Context: bank bankDefinition: bank notWould the overlap here be one or two? Context: bank notDefinition: bank bankHow about here?",
                "target": [
                    "It's my understanding that you can treat the context and definition as a set - only one instance of the word is counted. For all the examples the overlap would be 1."
                ]
            }
        ]
    },
    {
        "context": "Part 1 check",
        "statements": [
            {
                "source": "In part 1, do the order of the lemmas need to match the output in the assignment description? ",
                "target": [
                    "I think as long as it has the same contents then it should be fine."
                ]
            }
        ]
    },
    {
        "context": "Why is dependency parsing considered discriminative model?",
        "statements": [
            {
                "source": "Hi, I remember professor mentioning that dependency parsing models are considered discriminative models and not generative. Could you please elaborate more on why this is the case? Thankyou.",
                "target": [
                    "In transition based dependency parsing, the model predicts P(transition | state) directly, rather than using Bayesian inference to infer the best state. "
                ]
            }
        ]
    },
    {
        "context": "part 5 errors",
        "statements": [
            {
                "source": "Hi! For part 5, I keep getting errors like this, even when I try the verbose trick that was mentioned. Should I worry about it / will I get points off if I leave it like this? My part5. predict file looks like this:Thank you",
                "target": [
                    "When you call predict, you can add the parameter verbose=False, which should suppress the output. Alternatively, you can modify the code so that it writes your output directly into a text file, rather than printing to stdout and then piping it into a file. "
                ]
            }
        ]
    },
    {
        "context": "Question about CFGs being context free",
        "statements": [
            {
                "source": "From the lecture slides - \"CFGs are context free: applicability of a rule depends only on the nonterminal symbol, not on its context. Therefore, the order in which multiple non-terminals in a partially derived string are replaced does not matter. We can represent identical derivations in a derivation tree. \"However for HW3 and according to post #81, not considering the correct order for terminals would result in incorrect results. So does this mean, NP  D N cannot be written as NP -&gt; N Dbut NP  NP PP - (1)  does it mean just that it is okay to either substitute NP or PP first . Or does it mean (1) can be written as NP -&gt; PP NP ? (which doesn't seem to be correct)Please let me know if I am understanding this correctly. Thank you in advance!",
                "target": [
                    "When you have a rule NP  NP PP, it doesn't matter if you substitute NP or PP first in a derivation. The productions you can use to substitute NP depend only on the nonterminal itself, not on the context it appears in. "
                ]
            }
        ]
    },
    {
        "context": "Has anyone been able to get a VM w/ GPU in GCP recently?",
        "statements": [
            {
                "source": "If so, could you please share the zone that worked? I've tried several zones with no luck",
                "target": [
                    "Please refer to the video recently posted on Courseworks (recorded by TA - Shreya) in Files -&gt; homework5 -&gt; hw5_gcp_tutorial. mp4"
                ]
            }
        ]
    },
    {
        "context": "part 3, part 6",
        "statements": [
            {
                "source": "Hello teaching staff, for part 6 I just want to confirm: in the previous post, it said that any interesting or creative solution will get full credit. Does this mean even without being the best predictor(in terms of the highest precision and recall) is fine? And for part 3, I tried two ways to write the code, and seems like the result scores are different. For both ways, the simple lesk algorithm did not outperform the WordNet frequency baseline. I'm wondering if there is a standard answer to predictions in predict file for this part. Thank you!",
                "target": [
                    "Thats right, for part 6 you do not necessity have to get the highest score. Not sure I understand your question about part 3. There are some valid modifications in part 3 that would all be in line with the specification leading to different results. "
                ]
            }
        ]
    },
    {
        "context": "Identical results for Word2Vec and Bert predictors",
        "statements": [
            {
                "source": "Both my Word2Vec and Bert predictors produce the exact same precision and recall results, but I'm not sure if this means that there's something wrong with my Bert predictor since the assignment handout states that it performed slightly better than the Word2Vec predictor in experiments?  I can't seem to find any bugs in the Bert predictor and I'm following the encoding method directly from the handout, but I'm not sure if some kind of optimization needs to be added for the Bert predictor to perform better to achieve full credit?  I've included the results output for both parts below for reference:Total = 298, attempted = 298\nprecision = 0. 115, recall = 0. 115\nTotal with mode 206 attempted 206\nprecision = 0. 170, recall = 0. 170\n",
                "target": [
                    "I have the same concern, but for what it's worth, I have the same output that you do for both. ",
                    "My Bert predictor did outperform Word2Vec slightly and even if the scores are identical the predicted results should be different. Word2Vec will always give you the same synonym for the same lemma and pos regardless of context whereas Bert will give different answers. For instance, Word2Vec gives \"shining\" as the replacement for \"bright\" in the first two sentences whereas Bert gives \"promising\" and \"shiny\" respectively in my code. "
                ]
            }
        ]
    },
    {
        "context": "get_candidates()  input word includes underscore?",
        "statements": [
            {
                "source": "Just wondering what should happen if an input to this function is a word that contains an underscore. So something like hot_dog for instance. What I'm doing right now includes \"hot dog\" as a candidate, but I realize that may not be totally correct. Should we account for this?",
                "target": [
                    "Yes. Replacing underscores with white space is actually the correct way to handle this."
                ]
            }
        ]
    },
    {
        "context": "HW4: Part 6 - Refinements",
        "statements": [
            {
                "source": "\"In this part, you should implement your own refinements to the approaches proposed above, or even a completely different approach. \"Does this mean that we can implement a refinement for any (i. e. , at least one) of the four approaches from parts 2-5? Or we must implement a refinement for each of the approaches? Thanks!",
                "target": [
                    "Just one new refinement , not refinement for each !"
                ]
            }
        ]
    },
    {
        "context": "Any test cases for Part 2?",
        "statements": [
            {
                "source": "Can we get some test cases for part 2 wn_frequency_predictor ?",
                "target": [
                    "I dont think I have any specific test cases. Can you report your performance measures?"
                ]
            }
        ]
    },
    {
        "context": "GPU setup in google cloud",
        "statements": [
            {
                "source": "I was trying to follow the video tutorial to set up my GPU in google cloud. However, there is \"tensorflow has resource level errors\" when setting up the deep learning VM. Anyone having any idea on how to deal with this?",
                "target": [
                    "Yeah, it means that particular zone doesn't have enough resources. Try it with us-west4-a, worked for me"
                ]
            }
        ]
    },
    {
        "context": "Gradescope Course ID for Final",
        "statements": [
            {
                "source": "Since the Final is being administered on gradescope for Zoom participants, can we get the gradescope course ID to get setup there in preparation? Thanks",
                "target": [
                    "You wont need the course Id. Instead you will be able to access the exam through the Gradescope link on courseworks. I will post a dummy exam prior to the official exam so you can make sure everything works properly. "
                ]
            }
        ]
    },
    {
        "context": "Homework Solutions",
        "statements": [
            {
                "source": "Hi all, Are the solutions for any of the homeworks after Homework 1 going to be posted?",
                "target": [
                    "Solutions for HW2 and HW3 have been uploaded now. Please check the files section on the Courseworks page."
                ]
            }
        ]
    },
    {
        "context": "Part 5 predict file",
        "statements": [
            {
                "source": "I ran part 5 in CPU, and it turns out results like :Does anyone know how to get rid of the two-bar line? Thank you!",
                "target": [
                    "I was just told to try adding \"verbose=0\" when you call predict, i. e:self. model. predict(input_mat, verbose=0)\nAnd it seems to work!"
                ]
            }
        ]
    },
    {
        "context": "Lemma",
        "statements": [
            {
                "source": "Do we need to consider the lower or upper case of the synonyms/lemma/word? Thank you!",
                "target": [
                    "Please use lowercase."
                ]
            }
        ]
    },
    {
        "context": "HW4 Part 3 - Tie Breaking Logic Clarification",
        "statements": [
            {
                "source": "You would break the tie, so return one of the tied lexemes. There is actually a clever way to deal with this:First assign three values to each candidate lexeme &lt;synset s, lemma w&gt;a) the overlap score for the s. b) the . count() (frequency) for &lt;s, t&gt; where t is the target lemma. c) the . count() (frequency) for &lt;s, w&gt; Then, compute an aggregate score like this 1000*a + 100*b + c Sort all candidates by this score and select the highest (excluding lemmas that are identical to the target). This will automatically break ties. 1. Is the target lemma t = context. lemma? 2. Is lemma w in &lt;synset s, lemma w&gt; any of the possible lemmas in s? Thank you in advance!",
                "target": [
                    "Yes and yes. "
                ]
            }
        ]
    },
    {
        "context": "GCP VM",
        "statements": [
            {
                "source": "Hi all, Has anyone had any issues deploying a VM instance on GCP? I get this resource error:\"ResourceErrorMessage\":\"The zone 'projects/nlp-hw-394518/zones/us-west1-b' does not have enough resources available to fulfill the request. Try a different zone, or try again later. \"\nI have tried a few different regions and also waited a day but still no luck. Any advice would be appreciated. Many Thanks!",
                "target": [
                    "Having the same problem. .. I did part of my homework over the weekend, trying to finish it, no resources available for the past 2 days, tried moving to different zone, change machine type etc none worked. My data is practically stuck with GCP with no ways to retrieve it. I am just going to use lambda labs now: https://cloud. lambdalabs. com/instances at least it's a guarantee that you can make an instance, downside is 1) no persistent disk most of time (even if file system is available do save your data locally too otherwise when you need it the instance can attach disk might not be available) 2) you are paying out of pocket :(",
                    "FWIW, I had this problem in a different class and using us-central1-a worked for me.",
                    "Same here, I have tried a few different zones but still getting the error. "
                ]
            }
        ]
    },
    {
        "context": "Content for Final Reference/Cheat Sheet",
        "statements": [
            {
                "source": "Hello, I know we are allowed to bring a double-sided reference sheet in for the final. Is there some restriction in terms of what can be on it? For example: only formulas, etc. Thanks",
                "target": [
                    "There are no content restrictions for the cheat sheets. As long as you limit yours to one double-sided letter-sized sheet, you can add whatever you want to it."
                ]
            }
        ]
    },
    {
        "context": "Usage of tokenize(s) in part 3",
        "statements": [
            {
                "source": "Hi, Not sure if anyone else had this, but when I used the tokenize(s) function in part 3, I got an error as the compiler didn't recognize the \"string\" in \"string. punctuation\". When I added \"import string\" at the top of the file it seems to solve it. Hope a TA can confirm it's valid to add. Thanks!",
                "target": [
                    "Yes, please import string. "
                ]
            }
        ]
    },
    {
        "context": "Part 1 return type",
        "statements": [
            {
                "source": "Hi, for part 1 the description in Courseworks indicates we need to return a set of strings but the code indicates a list of strings; what is the expected return type? Thank you ",
                "target": [
                    "A set is the safer choice to make sure there are no duplicates. "
                ]
            }
        ]
    },
    {
        "context": "HW4 part6  Extra pre-trained models or packages",
        "statements": [
            {
                "source": "Dear Prof. and TAs, For the part 6, if we want to try some ensemble learning methods or other improvement which would involve other packages or pre-trained models. Is this allowed? Thank you~",
                "target": [
                    "Yes, this is allowed. Just make sure to clearly comment on any required dependencies to run your code!"
                ]
            }
        ]
    },
    {
        "context": "Difference between semantic and lexical ambiguity",
        "statements": [
            {
                "source": "Hi, I was a little confused about the difference between semantic and lexical ambiguity. I understand from the lectures that ambiguity can be broadly classified into structural and lexical ambiguity. Is semantic ambiguity a part of lexical ambiguity (and not structural), since there is confusion about the meaning interpretation? Thank you!",
                "target": [
                    "I don't believe the lecture notes explicitly describe the relationship between lexical ambiguity and semantic ambiguity. I looked on the internet and it states that lexical ambiguity is a subtype of semantic ambiguity, which I think makes sense since the lecture defines lexical ambiguity as a single word (e. g. , \"mouse\") that has two or more possible meanings and the latter as a group of words (e. g. , \"Stolen painting found by tree\") that have two or more possible meanings. Hope this helps!"
                ]
            }
        ]
    },
    {
        "context": "HW4 part2 performance same as smurf",
        "statements": [
            {
                "source": "Dear Prof. and TAs, Here is my screenshot of prediction for part 2However, when I try to get the scores, it turns out the same as what for smuf. Would you mind helping me to check whether my prediction is problematic, or the command to call for the scoring is wrong? Thank you!",
                "target": [
                    "This could be an issue with the file format, rather than your prediction. The perl script expects unix style file endings (LF only), while the file may have windows style line endings (CL-RF)."
                ]
            }
        ]
    },
    {
        "context": "Multiple Parallel Corpora Question",
        "statements": [
            {
                "source": "Hi folks, In terms of multiple parallel corpora, would using something like a multilingual Reuters dataset be sufficiently rigorous for the purposes of creating a corpus? I recognize that creative license may mean that the translations aren't necessarily perfect across different language versions of the same article, but would the faithfulness and fluency criteria likely be met? Thank you!",
                "target": [
                    "Sure that sounds like it would be a proper dataset for training an MT model. "
                ]
            }
        ]
    },
    {
        "context": "HW 4 Part 5 sanity check",
        "statements": [
            {
                "source": "If my BERT model performs somewhere between the WN frequency model and W2V, is that a definitive sign that my model has an issue? I am using BERT with the [MASK] token in the input.",
                "target": [
                    "I would expect the BERT model to work at least as well as the word2vec approach. "
                ]
            }
        ]
    },
    {
        "context": "Question regarding DistilBERT tokenizer implementation",
        "statements": [
            {
                "source": "Question regarding DistilBERT tokenizer implementation - \"The encode method tokenizes a string, then adds special symbols [CLS] and [SEP], and then encodes the input into a sequence of integers. \"Can we assume that the input string has only alphabets? If they already consist of numbers, how should that be handled? Thank you!",
                "target": [
                    "You can assume the tokenizer handles numeric tokens correctly. Specifically it uses a kind of subword tokenization  we will discuss in more detail on Tuesday. "
                ]
            }
        ]
    },
    {
        "context": "Part 3 clarification",
        "statements": [
            {
                "source": "If this is the case (or if there is a tie), you should select the most frequent synset (i. e. the Synset with which the target word forms the most frequent lexeme, according to WordNet). So if there is a tie, do we pick the higher frequency one between the tie, or it's still the highest frequency from the entire set? Also, it is possible for the most frequent synset or the synset with most overlap to contain only one lemma that is the target word itself. In this situation, what are we suppose to do? Find the next possible synset or just return nothing?",
                "target": [
                    "You would break the tie, so return one of the tied lexemes. There is actually a clever way to deal with this:First assign three values to each candidate lexeme &lt;synset s, lemma w&gt;a) the overlap score for the s. b) the . count() (frequency) for &lt;s, t&gt; where t is the target lemma. c) the . count() (frequency) for &lt;s, w&gt; Then, compute an aggregate score like this 1000*a + 100*b + c Sort all candidates by this score and select the highest (excluding lemmas that are identical to the target).  This will automatically break ties. "
                ]
            }
        ]
    },
    {
        "context": "Part 6: Custom Predictor",
        "statements": [
            {
                "source": "I'm wondering if the expectation for Part 6 for Homework 4 is for us to create a new function in the existing Predictor class or to create a new custom predictor class itself. And I am curious what type of innovative refinements are expected? Thanks in advance!",
                "target": [
                    "A new function is sufficient. The custom predictor does not have to perform significantly better than the other approaches in the assignment. Any interesting or creative solution will get full credit."
                ]
            }
        ]
    },
    {
        "context": "HW 4 submissions",
        "statements": [
            {
                "source": "Hi, will submissions for HW 4 be opened soon? Thanks",
                "target": [
                    "It should be open now. Please try again. "
                ]
            }
        ]
    },
    {
        "context": "HW 4: Sharing solutions to installation issues I encountered",
        "statements": [
            {
                "source": "My dev setup is ancient, so I ran into a number of issues when going through the installation steps for HW 4. Posting the solutions that worked for me here in case anyone else runs into the same issues. When trying to install gensim: error: Microsoft Visual C++ 14. 0 is required. Get it with \"Microsoft Visual C++ Build Tools\"Solution if you don't want to install all of VSCode: First answer in https://stackoverflow. com/questions/40018405/cannot-open-include-file-io-h-no-such-file-or-directory, though I chose the most recent compatible version of the packages instead. When trying to import gensim: ModuleNotFoundError: No module named 'dataclasses'Solution: pip install dataclassesWhen trying to install tokenizers: ERROR: Could not build wheels for tokenizers, which is required to install pyproject. toml-based projectsSolution: Upgraded to Python 3. 11. Maybe this would have solved some of the earlier problems too. The scoring script is written in Perl. You can install Perl at https://strawberryperl. com/ and restart your terminal for it take effect.",
                "target": [
                    "Adding on to this for anyone with M1 macs. .. I found these instructions helpful for resolving python &amp; tensorflow dependencies:  https://github. com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jan-2023. ipynb  "
                ]
            }
        ]
    },
    {
        "context": "Testing gensim installation",
        "statements": [
            {
                "source": "Hello, Just quickly mentioning that the homework describes that we should use the following command to view the vector representation for an individual word:&gt;&gt;&gt; v1 = model. wv['computer']\nI found that this command was throwing an error, but according to this stackoverflow post: https://stackoverflow. com/a/71550086, the . wv access field should not be necessary (also mentioned in the assignment)This is the command that worked for me:&gt;&gt;&gt; v1 = model['computer']\nIs my understanding correct or is it preferred to use v1 = model. get_vector('computer')? Thanks in advance",
                "target": [
                    "I think these are all identical  you could check a few examples and see if the different functions return identical vectors. ",
                    "I ran into this issue as well -- do we need a particular version of gensim to properly do the homework?  Just thinking of compatibility issues when someone else tries to run my homework code. "
                ]
            }
        ]
    },
    {
        "context": "Converting a Constituency Parse to Dependency Pares",
        "statements": [
            {
                "source": "From HW 3: \"There are also the following data files, corresponding to a standard split of the WSJ part of the Penn Treebank. The original Penn Treebank contains constituency parses, but these were converted automatically to dependencies. \"This made me curious- what is the standard way to convert constituency parses to dependency parses? Can a completely rules based approach suffice?",
                "target": [
                    "Yes, a rule-based approach is typically used for this. The tricky thing is to know which head to project up from the children to the parent. A head-percolation table (inspired by the annotation schema for the constituency representation) is used for this, sometimes combined with some postprocessing. "
                ]
            }
        ]
    },
    {
        "context": "Part 2 Wordnet Frequency Baseline Count",
        "statements": [
            {
                "source": "Hi, Just to clarify on how we should count the occurrence of the synonyms. So we can iterate all the synsets of a given context's lemma and pos, then for each lemma in the each synset, we accumulate the counts for that specific lemma's name? In the end we should just return the lemma name with the highest count and that lemma name can't be the input lemma name, is that the idea?",
                "target": [
                    "Yeap that sounds right to me. "
                ]
            }
        ]
    },
    {
        "context": "Arc Standard Transition based Dependency Parsing: All Arc-Lefts before Arc-Rights",
        "statements": [
            {
                "source": "Is it correct to say it is always possible to reach a final dependency tree A_d by following a sequence of transitions wherein all Left-Arcs are done before Right-Arcs ? Is it necessary?",
                "target": [
                    "I believe this answer is no. The sequence of transitions are based on the annotated dependency tree, which determines the relationships between words. In the arc-eager example in lecture 8, for example, the sequence of words in the buffer do not lend themselves to a sequence of transitions in which all left-arcs are done before right-arcs (unless you force incorrect relationships between words)."
                ]
            }
        ]
    },
    {
        "context": "Part 6 imports",
        "statements": [
            {
                "source": "Are we allowed to import anything we want to solve part 6, or should we stick to the libraries given?",
                "target": [
                    "We usually handle other imports on a case-by-case basis. What are you considering?"
                ]
            }
        ]
    },
    {
        "context": "Error in smurf. predict on line 299",
        "statements": [
            {
                "source": "When I run perl score. pl smurf. predict gold. trial I get this error. Is the perl script correct or I am doing something wrong? And this is my smurf. predict",
                "target": [
                    "Interestingly, I have the same problem, but for the BertPredictor in Part 5. I get this same print out on every line when I run the gold. trial check. I reviewed the perl script and it is related to some conditional that checks if the line is \"not empty\". I am not very familiar with Perl so I have not looked into it more than that. It seems strange since I checked the text file and nothing looks blank? My BertPredictor is getting ~17% precision, which seems to be around the expected result as well.",
                    "It's possible that perl doesn't like the encoding of the file. Try changing the encoding to UTF-8."
                ]
            }
        ]
    },
    {
        "context": "Homework 5 & exam",
        "statements": [
            {
                "source": "Apologies if this was already addressed in class: just to clarify, will Homework 5 and the Exam be due/administered in the same week? ",
                "target": [
                    "Yes, hw4 is already released due for Aug 4 ! you should ideally expect hw5 released by Aug 3rd night or so. Note : 1 Lowest score will be dropped if you submit all homework assignments."
                ]
            }
        ]
    },
    {
        "context": "Final submission",
        "statements": [
            {
                "source": "Do the . vocab and . h5 files need to be in a separate \"data\" folder in our final submission, similar to how hw3 has the . py files and then a \"data\" folder with all . vocab and . h5 within them? ",
                "target": [
                    "Yes, keep the model. h5 and pos. vocab and word. vocab files in data folder itself. data\n    words. vocab\n    pos. vocab\n    model. h5\n all . py files"
                ]
            }
        ]
    },
    {
        "context": "How does re-training the model affect final submission files?",
        "statements": [
            {
                "source": "Since we need to submit 9 files in all, will semi-constantly re-training the models affect any of our submission files? ",
                "target": [
                    "Retraining would change the weights (. h5 file)Only after you are done retraining and get your final output, should you submit all the files. Alternatively, you could also resubmit till the deadline"
                ]
            }
        ]
    },
    {
        "context": "HW3 Evaluation Results",
        "statements": [
            {
                "source": "My results seem to be coming out a little lower than expected. Is this indicative of a training error, or is it just necessary to tune parameters in the network topology? I've tried to re-run everything and retrain the model but the LAS scores are consistent.",
                "target": [
                    "Hard to say without looking at your code. If your previous parts are correct, I would try parameter tuning to increase the scores."
                ]
            }
        ]
    },
    {
        "context": "Training taking around 5 mins per epoch on a windows gpu setup?",
        "statements": [
            {
                "source": "I'm training on an nvidia 3070 and it's taking about 5 mins per epoch. I'm using conda and wsl on windows. I'm just wondering if anybody else gets a similar result? It seems suspiciously longer than a 2016 macbook but maybe it's just the way I set up tensorflow?",
                "target": [
                    "I am not sure, I also have 3070 but tensorflow does not recognize my GPU so I just trained with CPU and it took about 3mins or so per epoch, my tf version is 2. 13"
                ]
            }
        ]
    },
    {
        "context": "How should we use keras. utils. to_categorical in Part 2?",
        "statements": [
            {
                "source": "For get_output_representation, I think we can manually code the one-hot key vector relatively easily using self. output_labels without using keras. utils. to_categorical? Is there a reason that we should use keras. utils. to_categorical in this case? ",
                "target": [
                    "Yes I would actually recommend using the output_labels dictionary directly and constructing the output vector manually. "
                ]
            }
        ]
    },
    {
        "context": "Colab",
        "statements": [
            {
                "source": "Are we able to upload files to colab? or do we have to copy our python code out? How does this work with the data files?",
                "target": [
                    "Yes, and it's easy to do. Create a new directory in your Google Drive with the homework files. Then create a new Colab notebook, mount your drive, and then use the cd command until you're in the directory with the homework files. You should be able to run your code as you would locally."
                ]
            }
        ]
    },
    {
        "context": "Estimated time for testing?",
        "statements": [
            {
                "source": "How long should it take to return LAS after running evaluate. py data/model. h5 data/dev. conll? ",
                "target": [
                    "For me personally, it's taking about 1. 5 minutes to complete. My results are extremely close to the GCP with T4 results in #134 however I am using a Windows PC. A note too, it really helped adding tf. compat. v1. disable_eager_execution() after line 20 in evaluate. py, it sped up a lot. "
                ]
            }
        ]
    },
    {
        "context": "WSL & Nvidia GPU Opinion",
        "statements": [
            {
                "source": "Is anyone else using windows subsystem for Linux to use their GPU for training?  I set it up, and it looks like it is sort of working. I get 0% utilization when i look at nvidia-smi while training is running, but it does show my v-ram on my 3080ti is being used. So it looks like it is sort of working, but my mac from 2019 is also running just as fast, if not faster lol. Wanted to see if anyone else out there attempted to use their own gpu and had similar results.",
                "target": [
                    "Hey jonathan, I guess I double-posted maybe. I have practically the exact same setup with a 3070. I'm seeing training taking about 5 mins per epoch (longer than the prof's 2016 macbook? ) also with low usage on nvidia-smi, but with about 10% more usage compared to base and about 90% vram usage on task manager. It's quite strange. How long did training take for you per epoch? From videos online, it looks like that the tensorflow setup we're using and the architecture for this assignment might be more favorable for macs (when they work properly) and colab in training, so that could be it? I might also be completely wrong.",
                    "Tried on a native Windows laptop (tensorflow version 2. 10) with RTX 2070 super max-q and a linux desktop with Quadro T400, a lot more RAM and a more powerful CPU. Took about 90s on the laptop per epoch and 70s on the desktop. It might be worth trying on native Windows because the WSL setup can be tricky."
                ]
            }
        ]
    },
    {
        "context": "HW3-part3",
        "statements": [
            {
                "source": "What is the expected loss after 5 epoch? Thank youHamidSeyedhamidreza Alaie",
                "target": [
                    "You should be getting loss values between 0. 4 and 0. 6, and they should start to converge near 0. 4 after 5 epochs."
                ]
            }
        ]
    },
    {
        "context": "Results of part 4",
        "statements": [
            {
                "source": "Hi, I saw previous posts about this but when I run my code on Mac M2 and an older Mac the results are very different again, not matter what I try:M2:Older Mac:Any ideas what I can do to fix it? Thanks!",
                "target": [
                    "I think the only solution for the time being is to just run training on the CPU, unfortunately. "
                ]
            }
        ]
    },
    {
        "context": "Hw3 get_output_representation",
        "statements": [
            {
                "source": "I know in the description it suggests we use keras. utils. to_categorical functionCan we use the output_labels attribute of the FeatureExtractor class as another approach?",
                "target": [
                    "Yes thats fine. "
                ]
            }
        ]
    },
    {
        "context": "Hw3 Part 2",
        "statements": [
            {
                "source": "This may be a silly question but I am a bit confused for get_input_representationIf the result is the next 3 words on the stack followed by the next three words on the buffer, then where do the parameters \"words\" and \"pos\" come into play? ",
                "target": [
                    "The elements of the stack and the buffer are indices. You can use these 6 indices to index into words and pos and get the actual words and POS tags respectively."
                ]
            }
        ]
    },
    {
        "context": "Usage of GPU on Mac M2",
        "statements": [
            {
                "source": "Hi, I tried many solutions but I wasn't able to run part 3 with low losses. Once I added the following line as the first one under the main function of train_model. py: tensorflow. config. experimental. set_visible_devices([], 'GPU')\nMy output was:Epoch 1/5\n18996/18996 [==============================] - 52s 3ms/step - loss: 0. 5516\nEpoch 2/5\n18996/18996 [==============================] - 70s 4ms/step - loss: 0. 4543\nEpoch 3/5\n18996/18996 [==============================] - 92s 5ms/step - loss: 0. 4323\nEpoch 4/5\n18996/18996 [==============================] - 98s 5ms/step - loss: 0. 4243\nEpoch 5/5\n18996/18996 [==============================] - 99s 5ms/step - loss: 0. 4133\nShould I leave it in my code when submitting? Thanks!",
                "target": [
                    "Yes you may keep this in your code when you submit."
                ]
            }
        ]
    },
    {
        "context": "keras. optimizers. Adam for M1/M2 Macs",
        "statements": [
            {
                "source": "Hi, I have an M2 Mac and when I run part 3 I get the following warning:WARNING:absl:At this time, the v2. 11+ optimizer tf. keras. optimizers. Adam \nruns slowly on M1/M2 Macs, please use the legacy Keras optimizer instead,\nlocated at tf. keras. optimizers. legacy. Adam.\nAm I allowed to modify the train_model. py function accordingly? i. e. add:model. compile(keras. optimizers. legacy. Adam(lr=0. 01), loss=\"categorical_crossentropy\")Thanks! ",
                "target": [
                    "Sure! Let me know if that works "
                ]
            }
        ]
    },
    {
        "context": "HW3 P3 Network Topology",
        "statements": [
            {
                "source": "Should the softmax activation layer for the output be a Dense layer with the number of outputs as the number of units? Or, should it be a keras. layers. Softmax layer? In this case I don't know how to specify the correct size. ",
                "target": [
                    "The Softmax layer doesn't have any parameters (so nothing to train) and its output has the same shape as the input. If you use Softmax as a separate layer, the previous layer needs to output the right shape. In this case I would just use a Dense layer with softmax activation"
                ]
            }
        ]
    },
    {
        "context": "Issue with Importing 'keras'",
        "statements": [
            {
                "source": "Hi Professor/TAs, I have been experiencing a persistent issue when executing the scripts 'train_model. py' and 'extract_training_data. py'. The error message I received is 'No module names 'keras'. I have attempted to resolve this issue by installing keras directly through pip (pip install keras) as well as through the Anaconda environment. .. but still getting the same error. Thanks! ! ",
                "target": [
                    "Keras is now part of tensorflow. Replace the import with import tensorflow. keras"
                ]
            }
        ]
    },
    {
        "context": "Part 3 Increased Loss?",
        "statements": [
            {
                "source": "I am getting an increased loss for each epoch. I assume this is not expected? Any ideas why this might be? These are the commands I am running:python get_vocab. py data/train. conll data/words. vocab data/pos. vocab\npython extract_training_data. py data/train. conll data/input_train. npy data/target_train. npy\npython train_model. py data/input_train. npy data/target_train. npy data/model. h5\nDone loading data.\nEpoch 1/5\n2023-07-25 20:25:33. 550726: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry. cc:114] Plugin optimizer for device_type GPU is enabled.\n18996/18996 [==============================] - 158s 8ms/step - loss: 2. 3834\nEpoch 2/5\n18996/18996 [==============================] - 153s 8ms/step - loss: 19. 8371\nEpoch 3/5\n18996/18996 [==============================] - 154s 8ms/step - loss: 100. 4172\nEpoch 4/5\n18996/18996 [==============================] - 158s 8ms/step - loss: 233. 7903\nEpoch 5/5\n18996/18996 [==============================] - 158s 8ms/step - loss: 443. 5440\n",
                "target": [
                    "It's likely that your input and output representations are inconsistent between training examples. You should review that part. You could try running training on a single example, or on a small batch. The loss should decrease very quickly. Make sure the logic for looking up the word/pos for each value on the stack and buffer is correct. "
                ]
            }
        ]
    },
    {
        "context": "Hw3P2:",
        "statements": [
            {
                "source": "Hello, Assuming a token: \"GORBACHEV\" does not appear in the words. vocab file, should we consider it as \"&lt;UNK&gt;\" and store \"the index of &lt;UNK&gt; \" from words. vocab file to a one-dimensional vector? Thank you in advance!",
                "target": [
                    "\"GORBACHEV\" is probably tagged as NNP, so the correct representation would be the index for the &lt;NNP&gt; symbol. If it wasn't tagged NNP and indeed is not in words. vocab, then you should use the index for &lt;UNK&gt;It probably makes sense to lower-case the input, which will increase the likelihood of finding words at the beginning of the sentence. It's unlikely that \"gorbachev\" is in the lexicon either, though. "
                ]
            }
        ]
    },
    {
        "context": "HW3 P2: get_input_representation",
        "statements": [
            {
                "source": "I am having some confusion regarding the implementation of the method get_input_representation. The inputs to this method include the words and the pos tags. However, I am confused on when to use the pos tags. Should the index always be pulled from the words. vocab file, or is there a case where the index should be from the pos. vocab file?",
                "target": [
                    "My understanding is that words. vocab contain 5 special symbols, 2 of which are the pos CD and NNP, as indices 0 and 1 respectively. These should be extracted from words. vocab only when the element in pos, at the index that you're checking (from stack/buffer), is either CD or NNP. Maybe one of the teaching staff could clarify this."
                ]
            }
        ]
    },
    {
        "context": "Testing part 2",
        "statements": [
            {
                "source": "Hi, How can I test my part 2 of HW3? I tried printing the numpy array from the get_input_representation function but it doesn't help much with understanding if the mapping is correct (except from NULLs and the ROOT)Thanks!",
                "target": [
                    "I'm not sure there is a better way to test this, other than comparing to the indices in your dictionaries by hand. Ultimately, the test is to see if your model is training correctly. "
                ]
            }
        ]
    },
    {
        "context": "HW3 part 2- get_output_representation",
        "statements": [
            {
                "source": "For the get_output_representation(self, output_pair) method should we return a vector containing 90 zeroes and a single one (representing the output_pair)? If this is the case, do the indexes in the vector need to follow a specific order? For example, having the first 45 indexes reserved for \"left_arc\" transition pairs, the following 45 indexes for \"right_arc\" transition pairs and  ('shift', None) be at index 91? Thanks in advance! ",
                "target": [
                    "That is my understanding as well. Looking at the extractor's output_labels object, there is an index defined for each possible 91 transitions, so you can use those indices in extractor. output_labels to assign the \"1\". Hope that makes sense!",
                    "The specific order in which the transitions (with relations) map to the indices doesn't really matter, as long as it's consistent in your code between training and decoding. "
                ]
            }
        ]
    },
    {
        "context": "Use of test. conll file",
        "statements": [
            {
                "source": "Good evening, I was trying to understand when/where to use the test. conll file? Thank you",
                "target": [
                    "So, the way you would normally do this is to train on the training data with some initial hyperparameter setting (such as features you feed to the model, optimizer, learning rate, batch size, etc. ). Then, you can use the development set to tune these hyperparameters. Finally, you want to run an end-to-end evaluation using the test data. It's important that the test data is not touched during training and optimization at all. In this assignment, because you do not have to optimize any of the hyperparameters, we can get away with one held-out set. So you run your evaluation on . dev without ever touching . test. If you wanted to experiment more, I would recommend you repeat the evaluation on . dev and then run a final test on . text -- however that is not required for the assignment. "
                ]
            }
        ]
    },
    {
        "context": "Ungraded Exercise: Linear Models and Feature Functions solutions",
        "statements": [
            {
                "source": "Hi everyone, If you have a chance, do you mind releasing the solutions to the Linear Models and Feature Functions ungraded exercise? Thank you in advance!",
                "target": [
                    "It should be available in Pages section now."
                ]
            }
        ]
    },
    {
        "context": "Setup",
        "statements": [
            {
                "source": "Hi All, I have a windows 11 laptop with an nvidia 1060 card. I followed the instructions on tensorflows website as well as the README and I can't get tensorflow to print my gpu statistics. import tensorflow as tf \r\nfrom tensorflow import keras \r\n\r\nprint(\"GPU: \", tf. config. list_physical_devices('GPU'))The code above runs fine, it just prints an empty list for the array. A quick google tells me I need to install CUDA and cuDNN in order for get tf to work with my nvidia card. Does this sound right? ",
                "target": [
                    "Yes, this sounds reasonable.",
                    "I had the same issue. When using Windows native you have to use TensorFlow version 2. 10 or earlier for GPU support.",
                    "I went through the lenghthy process of setting up wsl and conda to get it working with the latest tf version. .. can report that just using the earlier tf version is the better option for sure"
                ]
            }
        ]
    },
    {
        "context": "Mac & Linux difference",
        "statements": [
            {
                "source": "I did the entire homework on Mac and regardless what I try and debug, the result always turned out horrible. Then I saw  #132, so following this thread, using my GCP instance, here is the comparison:Training:GCP with T4:Mac with M1:Evaluation:T4M1It's the same code. Didn't change anything.   TLDR: Don't use Mac.",
                "target": [
                    "Hi, I'm using an M2 MacBook that produces results similar to your T4, so I'm not sure if it's an M1 problem."
                ]
            }
        ]
    },
    {
        "context": "HW3 part 4 results drastically different on two device",
        "statements": [
            {
                "source": "Dear Prof. and TAs, I have a question regarding the results of my evaluation. I tried to run the evaluate. py on both windows (tensorflow/keras 2. 10. 0) and MAC M1 (tensorflow 2. 13. 0/keras 2. 13. 1 ). Then I got the results for win is similar to that on the coursework hints (upper). Then I tried the same command (with the model trained on Win) in MAC, however, the results turned to be quite different (below). Would you mind telling me which one is more reasonable, so that I can try to identify the problems in another program? Also, will the version causes such a big difference or there might be other reasons, and do we need to set the random seed?",
                "target": [
                    "Thats super surprising. I wonder if there is an I compatibly in the model files between the two versions? The results in the first figure are close to what I would expect. "
                ]
            }
        ]
    },
    {
        "context": "Ungraded exercise solution: Transition-based dependency parsing",
        "statements": [
            {
                "source": "Hi Professor Bauer/TAs. For the Ungraded Dependency Parsing problem, I am confused on we do not perform all the left-arc transitions first? Based on the example given in class, all the left-arc transitions were performed before the right-arc transitions. I thought the standard arc transition performs all the left before the right to maintain projectivity. Also could you please provide the solution to part b of the problem? Thanks! ! ",
                "target": [
                    "To your question 1, it indeed performs all the left-arc transitions first, look closely, the word 'meme' is parsed as an arc first (left-arc on 'a' and 'funny') [STEPS 8 &amp; 9] post which the right-arc from 'sent' to 'today' occurs [STEP 12] Also note the only difference is the solution here uses a slight notational variant, compared to the slides, where, the stack is arranged so that the top-most token is on the left, instead of on the right in the slides. You can use any notation, but it should be consistent."
                ]
            }
        ]
    },
    {
        "context": "Tensorflow-gpu is now part of regular tensorflow installation",
        "statements": [
            {
                "source": "Just wanted to note that tensor-flow-gpu is now part of the regular tensorflow installation. The regular installation (pip install tensorflow) should have GPU support. ",
                "target": [
                    "Yes, that should be fine. Do you see any concerns while running your code ?",
                    "Thanks. Ill definitely update the instructions accordingly. "
                ]
            }
        ]
    },
    {
        "context": "Question about Exam format for CVN student",
        "statements": [
            {
                "source": "Hi, Wondering what will be the exam format for CVN students?  Do we have to attend the exam at the same time slot as the on-campus students? Thanks!",
                "target": [
                    "I believe this was covered at the start of the last lecture. We can either do synchronously via Zoom or async in the time slot.",
                    "Correct you can either take the exam synchronously on zoom (preferred because you can ask questions), or using CVNs remote proctoring system.  I will send out a signup form later this week. "
                ]
            }
        ]
    },
    {
        "context": "Typo in Lecture 7?",
        "statements": [
            {
                "source": "Is the first cell on the right wrong? (VP -&gt; V NP)? Shouldnt the cell to the right be PP (or at least match the parent's right value)?",
                "target": [
                    "Yes, you are right, Anonymous ! The previous slide before the one you posted has it correctly annotated, its a minor mistake on the follow up slide, but yes, you are right [ S -&gt; NP VP]"
                ]
            }
        ]
    },
    {
        "context": "Simulation Based Bias Avoidance Model?",
        "statements": [
            {
                "source": "Hi Professor, Towards the end of Thursday's lecture you discussed the presence of bias in some of the contemporary language models. I've seen simulations used to train models in robotics and was wondering if simulating a desirable lexicon (let's say neutral and inclusive) would offset some of the less desirable cultural associations? Thank you in advance! ",
                "target": [
                    "Definitely Professor Daniel Bauer would be able to help guide on this, based on the context of this question from class. However, meanwhile, as a side note, maybe you want to take a look at simulation-based bias mitigation models , to generate data that is free of bias. This data can then be used to train language models that are less biased. Simulating a lexicon manually is one of the techniques, some other techniques like \"debiasing\" (training the model on a text dataset that has been modified to remove bias) and \"fairness training\" ( training the model to be fair in its predictions, regardless of the input data) are also used to create language models that are less biased and more inclusive."
                ]
            }
        ]
    },
    {
        "context": "HW 3 part 4 sanity check",
        "statements": [
            {
                "source": "In part 4, I had to convert my features into a 2d array (via np. array([features])) before passing them to predict; otherwise, I get a dimension mismatch. Is this expected, or have I done something wrong? The documentation seems to imply a 1d array would be accepted for a singular prediction, so I'm not sure why I had to do this. https://www. tensorflow. org/api_docs/python/tf/keras/Model#predict",
                "target": [
                    "Hi, Yes you may have to reshape before passing it to the predict function. One way to do it would be using features. reshape(1, -1). If you do this, the predict function will output a 2D array, but you only need to work with the first (and only) row. You would then index into the 2D array to perform the remaining operations."
                ]
            }
        ]
    },
    {
        "context": "HW 3 part 3 sanity check",
        "statements": [
            {
                "source": "In part 3, is it expected not to use all method params?",
                "target": [
                    "Are you talking about the params passed while building the Keras layers?",
                    "Yes, you're correct. The pos_types parameter is not used."
                ]
            }
        ]
    },
    {
        "context": "Typo in lecture11 p. 41?",
        "statements": [
            {
                "source": "Hi isn't itvector(apple) - vector(tree) + vector(vine) = vector(grape)instead of vector(apple) - vector(tree) + vector(grape) = vector(vine)? ",
                "target": [
                    "Hi Zoe, Yes you are right, The equation in the slide is incorrect. I believe Prof might have shared this in class, the correct equation would be:vector(apple) - vector(tree) + vector(vine) = vector(grape)\nThe correct equation, as shown above, subtracts the vector representing a tree from the vector representing an apple, and then adds the vector representing a vine. This results in a vector representing a grape as also shown in the image of slide too."
                ]
            }
        ]
    },
    {
        "context": "HW3 part 2 model optimization metrics",
        "statements": [
            {
                "source": "Dear Prof. and TAs, I have a question about the part 3 model. compile. Apart from the parameters 1) optimizer and 2) loss function, we also have a \"metrics\". What specific metrics do you suggest for our task, like accuracy, auc, etc? Thank you!",
                "target": [
                    "The metric wont affect the training process at all. But it does provide interesting information about how quickly the model is improving on the training data. For this task, accuracy should be sufficiently informative. "
                ]
            }
        ]
    },
    {
        "context": "Dutch Context-Free Example from Lecture",
        "statements": [
            {
                "source": "To clarify, the English version of this sentence is not context-free and can be expressed using a CFG? ",
                "target": [
                    "Yes the English translation doesnt show the cross linear dependency phenomenon. "
                ]
            }
        ]
    },
    {
        "context": "HW2 Python Libraries",
        "statements": [
            {
                "source": "Are we allowed to use/import numpy?",
                "target": [
                    "Yes, numpy is allowed."
                ]
            }
        ]
    },
    {
        "context": "Probs table accuracy?",
        "statements": [
            {
                "source": "Are there any values or range of values we should be getting for values in our probability table? ",
                "target": [
                    "There aren't any specific ranges that I can think of off the top of my head, but I believe there were quite a few negative ones."
                ]
            }
        ]
    },
    {
        "context": "Part 3 lexical rules probabilities",
        "statements": [
            {
                "source": "Hello, Given: tokens = ['flights', 'from', 'miami', 'to', 'cleveland', '. '], I got the following lexical rules probabilities:Should parse_with_backpointers  method choose the production rule 'FLIGHTS' ==&gt; 'flights' over the rule 'NP' ==&gt; 'flights' , as it has higher probability ? If the answer is yes, the example in part 4 is using  'NP' ==&gt; 'flights' rule (which is  less probable than 'FLIGHTS' ==&gt; 'flights'  based on what I got above.  Is the given example correct ? Am I missing something?",
                "target": [
                    "You need to add up the log probabilities at each node and \"FLIGHTS\" with any other non-terminals shouldn't give you productions. The final tree gives the most probable path and the example is correct"
                ]
            }
        ]
    },
    {
        "context": "One Hot Vector Length of 91",
        "statements": [
            {
                "source": "I'm not quite sure I understand why there are only 91 combinations for the one-hot-vector. I thought for each valid POS (48 - 3 = 45) there would be 3 possible transitions (left, right, shift), resulting in 45*3. Then there is one extra transition for the &lt;ROOT&gt; word for a total of 45*3 + 1 = 136. What am I missing in my understanding? Why do we only need a vector of length 91? ",
                "target": [
                    "If the transition is \"shift\", the dependency label is None. This means that there are 45*2+1 = 91 possible outputs."
                ]
            }
        ]
    },
    {
        "context": "Encoding Case Sensitive Words in HW3",
        "statements": [
            {
                "source": "Should words be case sensitive when encoding our words in Part 2? For example, the first case in the train. CONLL is: BUSH AND GORBACHEV WILL HOLD two days of informal talks next month. The training data recognizes that \"AND\" is a conjunction (CC), so pos. vocab easily recognizes it as \"CC\". However, word. vocab only has \"and\" listed. Given that Python dictionary keys are case sensitive, should \"AND\" be encoded as the index for \"and\" or \"&lt;UNK&gt;\"?",
                "target": [
                    "Use the lowercased version of the string when checking membership in the vocabulary. So in your example, it would be encoded as the index for \"and\" instead of \"&lt;UNK&gt;\"."
                ]
            }
        ]
    },
    {
        "context": "Examples in HW2",
        "statements": [
            {
                "source": "Are all examples in HW2 examples with no particular meaning to the \"correctness\" of our implementation? For example #99  was an example on a difference sentence. ",
                "target": [
                    "That's right -- the examples illustrate the format of the data structures, but they are not necessarily test cases for any particular grammar / sentence combination. One way to test your code for correctness is to create a small grammar file, for example using one of the grammars from the lecture slides. Then parse a sentence and compare the result to the correct solution by hand. "
                ]
            }
        ]
    },
    {
        "context": "HW2 part1 length of rhs",
        "statements": [
            {
                "source": "Dear Prof. and TAs, I have a question regarding verify_grammar. Apart from checking the sum of probability and (non)terminal is in lhs, do we have to ensure the length of rhs within {1, 2}?",
                "target": [
                    "Yes! The production rules on the RHS can only have a single terminal or 2 nonterminals. Anything else cannot be a valid rule."
                ]
            }
        ]
    },
    {
        "context": "Different Start Symbols",
        "statements": [
            {
                "source": "For Part 2, how do we make sure that the function works for different start symbols? Given a grammar, how would a user know what the start symbol is without looking at the grammar?",
                "target": [
                    "Avoid hard coding the start symbol and use self. grammar. startsymbol instead."
                ]
            }
        ]
    },
    {
        "context": "submission requirements",
        "statements": [
            {
                "source": "Hi teaching team, I noticed that the assignment requirements sayPack these files together in a zip or tgz file as described on top of this page. Please follow the submission instructions precisely!  but it seems that I can't find any specific requirements on the top. So, is there any formatting or naming convention for our submissions?",
                "target": [
                    "Please include cky. py, evaluate_parser. py, and grammar. py in your zip file. There's no required naming convention for the zip file itself, but including \"hw2\" and your UNI in the zip file's name would be helpful. :)"
                ]
            }
        ]
    },
    {
        "context": "question on referencing codes",
        "statements": [
            {
                "source": "Hello, I wonder how to make a reference for the codes that I gained some insights?",
                "target": [
                    "If you want to cite a snippet that you referenced, including the link in a comment would suffice."
                ]
            }
        ]
    },
    {
        "context": "HW2 Part 2 runtime",
        "statements": [
            {
                "source": "What is considered too long of a runtime when testing is_in_language on atis3. pcfg? I made a small test grammar and it returns immediately but using atis3. pcfg takes well over a minute to return the result. Are points deducted for runtime? ",
                "target": [
                    "This shouldn't take very long. Chances are you are not using the rhs_to_rules dictionary correctly, which will slow things down. Are you maybe iterating through the keys to find one that matches a right hand side, rather than just indexing by key?",
                    "This is a related question to my classmate's. Do we really care about the running time in this homework? Namely, can I call the more complex Part-3 function inside Part-2 and use that returning value to do some membership checking?"
                ]
            }
        ]
    },
    {
        "context": "HW3 - Part 4 Return Type",
        "statements": [
            {
                "source": "Making a post in case this will help othersUpon investigating the evaluate_parser code, it seems that unparsed lines are only counted upon a KeyError, as seen here:try:\n res = get_tree(chart, 0, len(tokens), parser. grammar. startsymbol)\nexcept KeyError:\n unparsed += 1\n res = tuple()\nThis caused issues for me as originally in part 4, I was returning None in the cases when NT was not within the parse table. Therefore, unparsed was not being incremented and the final evaluation scores were drastically different than what is expected (based on the professor's outputs). tldr : try raising a KeyError instead of returning None",
                "target": [
                    "To add in the post, there is also this check in place, so if it's an invalid sentence, if you return None for parse table directly would also work."
                ]
            }
        ]
    },
    {
        "context": "Hw3 Part 3",
        "statements": [
            {
                "source": "For example the value of probs[(0, 3)]['NP'] might be -12. 1324. What this the value for ['flights', 'from', 'miami', 'to', 'cleveland', '. '] , or a different sentence?",
                "target": [
                    "It was for a different sentence (and may in fact be made up). "
                ]
            }
        ]
    },
    {
        "context": "HW2 Part 1",
        "statements": [
            {
                "source": "For part 1, do we also need to verify if a symbol only contains letters or an apostrophe? Or is it simply check the probability sum and format?",
                "target": [
                    "You can assume that all symbols that appear as a rule left hand side are non terminals. All other symbols are terminals. I dont recommend checking for capitalization as this causes issues with numbers and punctuation symbols. Its simply becoming the rule format and the probabilities. "
                ]
            }
        ]
    },
    {
        "context": "Thursday OH online?",
        "statements": [
            {
                "source": "Hi, would it be possible to offer Thursday's OH over zoom to allow CVN students to join? That would be very helpful because currently there are no OH opportunities between Tuesday and Friday. Thank you",
                "target": [
                    "Due to the small number of TAs in the summer session we are unable to offer daily office hours that work for everyone. Shivansh is holding office hours online on Thursday though (3:00-4:30pm). Feel free to email me if you think you may be unable to get homework 2 done before the weekend. "
                ]
            }
        ]
    },
    {
        "context": "hw2 part 2",
        "statements": [
            {
                "source": "Could you please clarify what S is in the below? I understood this from the ungraded exercise, however, for hw2 there is no 'S' in the lhs rules. .. ",
                "target": [
                    "The S is the start symbol. So that line is saying that the algorithm returns True if the start symbol is in: pi[0, len(string)]. For the hw you can use self. grammar. startsymbol if that makes sense",
                    "S is the start symbol. It can be accessed in cky. py via self. grammar. startsymbol."
                ]
            }
        ]
    },
    {
        "context": "Possible Mock Exam?",
        "statements": [
            {
                "source": "Hello, Just wondering when we prepare for the final exam at the end of the course, is there any mock exam that can be used as a template for practice/preparation?",
                "target": [
                    "Unfortunately, there has not been a practice exam in previous iterations of the course, and I don't think that'll change this semester. Instead, please review the ungraded exercises and homework assignments to prepare for the final. The questions on the final generally resemble the structure of the ungraded exercises."
                ]
            }
        ]
    },
    {
        "context": "Typo in Homework 2 Part 3?",
        "statements": [
            {
                "source": "\"For example: table[(0, 3)]['NP'] returns the backpointers to the table entries that were used to create the NP phrase over the span 0 and 3. For example, the value of table[(0, 3)]['NP'] could be ((\"NP\", 0, 2), (\"FLIGHTS\", 2, 3)). This means that the parser has recognized an NP covering the span 0 to 3, consisting of another NP from 0 to 2 and FLIGHTS from 2 to 3. The split recorded in the table at table[(0, 3)]['NP'] is the one that results in the most probable parse for the span [0, 3] that is rooted in NP. \"I don't see how \"FLIGHTS from 2 to 3\" is possible. \"FLIGHTS\" is only available in the span from 0 to 1?  This is how I view the terminal symbols. 0            1          2            3           4                    5           6|---------|---------|---------|---------|----------------|---------|  Flights   from    Miami      to       Cleveland       . With the arrangement above how is this possible (as per the directions)? |-------------------|----------|          NP            FLIGHTS",
                "target": [
                    "The example for the table format just illustrates what the data structure looks like. Its not necessarily a continuation of the flights from Miami to Cleveland example. It wouldve been nicer to stick with the same example throughout. I will keep this in mind for the next revision of the assignment. "
                ]
            }
        ]
    },
    {
        "context": "Part 5",
        "statements": [
            {
                "source": "Hi, For part 4 I added the following lines at the top to check for validity of nt:    # if (not any(nt in inner_dict for inner_dict in chart. values()) or len(chart[(i, j)][nt]) == 0):\n    #     return None\nEverything seems to run smoothly, however when I ran part 5 I got the following result:Coverage: 86. 21%, Average F-score (parsed sentences): 0. 7413490818718977, Average F-score (all sentences): 0. 6390940360964636\nHowever, when I deleted the 2 lines from get_tree, I get the following result for part 5:Coverage: 67. 24%, Average F-score (parsed sentences): 0. 9504475408614075, Average F-score (all sentences): 0. 6390940360964636\nDid I do something wrong for part 4? Would appreciate any help. Thanks!",
                "target": [
                    "You don't need those first 2 lines. Since your second set of metrics looks good, it seems that you already handled the base case correctly."
                ]
            }
        ]
    },
    {
        "context": "Part 4 terminal input",
        "statements": [
            {
                "source": "For get_tree(chart, i, j, nt), can we assume the nt is always valid?",
                "target": [
                    "If nt is not in the table or the table entry is empty, get_tree should return None or raise ah exception. You will have to check for that. "
                ]
            }
        ]
    },
    {
        "context": "Hw2 Pt. 1",
        "statements": [
            {
                "source": "I just wanted to check to make sure that I got all the conditions for a grammar in CNF - in the slides it just says that each rule needs to be in the form A  B C, where A  N, B  N, C  Nor A  b, where A  N, b  In the assignment description, it gives the further requirement that all probabilities for the same lhs symbol must sum to 1. 0. I did have a few other questions for some edge cases though, I would really appreciate it if these could be verified/any gaps filled in (if that's allowed):Must every valid CNF grammar contain a valid startsymbol? Should we ensure that rhs and lhs rules are the same? Should we make sure that some rules exist for a certain set of symbols?",
                "target": [
                    "You can assume that the grammar is a valid CFG. You dont need to check the additional conditions you describe. You only need to check that each rule is in CNF format and that the probabilities for all rules with the same LHS sum to 1. 0! "
                ]
            }
        ]
    },
    {
        "context": "Part 3 log probability table",
        "statements": [
            {
                "source": "The second object is similar, but records log probabilities instead of backpointers. For example the value of probs[(0, 3)]['NP'] might be -12. 1324. This value represents the log probability of the best parse tree (according to the grammar) for the span 0, 3 that results in an NP. Like, does the probability include 'NP' itself? Say NP -&gt; NP PP, does the  -12. 1324 come from the sum of log probability on NP and PP on the right, or is the -12. 1324 from the whole thing, i. e. log probability of NP and PP AND the probability of rule 'NP -&gt; NP PP' itself? ",
                "target": [
                    "-12. 1324 would be the inside log probability of the entire subtree rooted in NP (that is, the product of all the rule probabilities). You compute it by adding the log probability for NP over the left subspan, the log probability for PP over the right subspan, and the log rule probability. "
                ]
            }
        ]
    },
    {
        "context": "Format in Part 3",
        "statements": [
            {
                "source": "The assignment outline in courseworks says the the table should have the structure: ((A, i, k), (B, k, j))But the check table structure function in the python file cky. py suggests: ((i, k, A), (k, j, B))It seems like either structure pass the check function, I'm just wondering which structure is preferred?",
                "target": [
                    "Ah thats a typo in the instructions. Use ((i, k, A), (k, j, B)). Either option will work though.  ",
                    "So what is the consensus on this? Could we get a fixed check_table_format function?"
                ]
            }
        ]
    },
    {
        "context": "part 3 log summation",
        "statements": [
            {
                "source": "Because we are using summation of math. log2() when comparing probabilities, a lower probability sum is better?",
                "target": [
                    "I am unsure if I understand your question correctly, but we should look for a higher probability sum. Math. log2() is monotonically increasing."
                ]
            }
        ]
    },
    {
        "context": "part 3 clarification",
        "statements": [
            {
                "source": "I am a bit confused by part 3 and am hoping to get some clarification on the example provided: For example: table[(0, 3)]['NP'] returns the backpointers to the table entries that were used to create the NP phrase over the span 0 and 3. For example, the value of table[(0, 3)]['NP'] could be ((\"NP\", 0, 2), (\"FLIGHTS\", 2, 3)). This means that the parser has recognized an NP covering the span 0 to 3, consisting of another NP from 0 to 2 and FLIGHTS from 2 to 3. The split recorded in the table at table[(0, 3)]['NP'] is the one that results in the most probable parse for the span [0, 3] that is rooted in NP. Is the most probable parse for the span [0, 3] the split with the highest probability sum (using log2)? ",
                "target": [
                    "I think so. The most probable parse for the span [0, 3] is the parse that results in the highest probability log sum."
                ]
            }
        ]
    },
    {
        "context": "hw 2 part 1 -",
        "statements": [
            {
                "source": "Part 1 of HW2 states the following:Then change the main section of grammar. py to read in the grammar, print out a confirmation if the grammar is a valid PCFG in CNF or print an error message if it is not. You should now be able to run grammar. py on grammars and verify that they are well formed for the CKY parser. If I add logic in the actual verify_grammar() function (instead of main) to print the errors / success, is that sufficient? I know it states to modify it in main, but adding it in the function gave me a more tailored/detailed explanation of the error. I want to make sure this is okay for grading purposes. TYIA!",
                "target": [
                    "I put the print function in the verify_grammar function, bringing more explanations for the errors as you said. I didn't realize the question states to modify the main. ",
                    "Modifying the verify_grammar function is okay. "
                ]
            }
        ]
    },
    {
        "context": "Part 3 - Data Structure",
        "statements": [
            {
                "source": "The first object is parse table containing backpointers, represented as a dictionary (this is more convenient in Python than a 2D array). Can we use a default dict here instead of a regular dict?",
                "target": [
                    "I personally used a defaultdict instead of a regular dict, I don't think that's a problem. If I'm not mistaken the only difference between the two is that the former is initialized to prevent a KeyError. ",
                    "I think it will be more convenient to use a default dict, and it has already been imported for us."
                ]
            }
        ]
    },
    {
        "context": "Part 2 and Part 3",
        "statements": [
            {
                "source": "Part 3 and Part 2 are basically identical, instead of repeating the code, is it ok to just call self. is_in_language(tokens) in parse_with_backpointers(), if I am already creating and saving the table in part2?",
                "target": [
                    "Yes, thats fine. "
                ]
            }
        ]
    },
    {
        "context": "Part 2",
        "statements": [
            {
                "source": "Hi, If I did part 3 before part 2, can I just use the following code for part 2?    def is_in_language(self, tokens):\n        \"\"\"\n        Membership checking. Parse the input tokens and return True if \n        the sentence is in the language described by the grammar. Otherwise\n        return False\n        \"\"\"\n        # TODO, part 2\n        table, probs = self. parse_with_backpointers(tokens)\n\n        if (0, len(tokens)) not in table or self. grammar. startsymbol not in table[0, len(tokens)]:\n            return False\n        return True\nThanks in advance.",
                "target": [
                    "Yes, this works."
                ]
            }
        ]
    },
    {
        "context": "Is it in the range of expectation for my CKY algorithm to have a . 82 F1 score",
        "statements": [
            {
                "source": "I am trying to determine whether I have to debug anything in my code. So far, each of the parts that I have implemented have shown expected behavior based on the examples given in the assignment page. The issue is when I run the evaluate_parser script I get lower than expected scores:Are these reasonable values should I spend some time debugging?",
                "target": [
                    "You should go back and debug. While your coverage looks good, your average F-scores should be much closer to 0. 95 and 0. 64."
                ]
            }
        ]
    },
    {
        "context": "CKY Algorithm",
        "statements": [
            {
                "source": "Hi Professor/TAs, When we are doing the CKY table, does the order of two terminals matter? For example, for the ungraded assignment, if we are looking to see if there is a production rule for (0, 2) from (0, 1) and (1, 1), does the ordering of D (0, 1) and N (1, 1) matters? In the production rule, we have NP -&gt; D N. Is that equivalent to  NP -&gt; N D? Not sure if my question is clear. .. but If (0, 1) was N and (1, 1) was D, can we still put NP in (0, 2)? Thanks! ! ",
                "target": [
                    "Yes! The order definitely matters! Think about what would happen if that was not the case: You could just swap two constituents and the sentence would still be grammatical. "
                ]
            }
        ]
    },
    {
        "context": "Additional Office Hours",
        "statements": [
            {
                "source": "Hi, is it possible to get some office hours during the week via Zoom for CVN students?",
                "target": [
                    "I'm holding office hours Tuesdays 3:00-4:00pm (both in person and on Zoom). Link: https://columbiauniversity. zoom. us/j/99416164945? pwd=R2E3TG5YYUhCc25tVUFSN3hKTnpPUT09I'll check with the TAs to see if we can reschedule some. Maybe we can move Shreya's Monday time slot to Zoom. "
                ]
            }
        ]
    },
    {
        "context": "Part 1 usage of isclose",
        "statements": [
            {
                "source": "Hi, I just wanted to confirm if we can import isclose from the math library (since without it we can't use the method), as we're told that:You may want to take a look at the isclose method in Python's math package. Links to an external site. Thanks.",
                "target": [
                    "Yes, you can (and should) import it."
                ]
            }
        ]
    },
    {
        "context": "Part 3 - Initialization of table",
        "statements": [
            {
                "source": "Terminal symbols in the table could just be represented as strings. For example the table entry for table[(2, 3)][\"FLIGHTS\"] should be \"flights\". What about the other rules for the token \"flights\"? Specially, ('flights', ) also makes \"NP\" but the table format requires a tuple of length 2 so I am not sure what table[(0, 1)]['NP'] should be.",
                "target": [
                    "table[(0, 1)]['NP'] should also be \"flights\" in this case. "
                ]
            }
        ]
    },
    {
        "context": "Can we use Numpy in Part 2: is_in_language?",
        "statements": [
            {
                "source": "In the initialization section of is_in_language, would it be allowed to use numpy to define the 2-D array for dynamic programming?",
                "target": [
                    "Yes, that's fine."
                ]
            }
        ]
    },
    {
        "context": "Ungraded Assignment - Viterbi Algorithm Implementation",
        "statements": [
            {
                "source": "I was wondering if this implementation of the Viterbi algorithm is correct, and whether it can be optimized in any way. The output is correct \"Most likely sequence of states: ['N', 'V', 'Prep', 'N']\", but I'm wondering if anything can be done more efficiently:Thank you in advance! here is the pseudocode:and this is my (slightly different) implementation:# Define the hidden states\r\nhidden_states = ['N', 'V', 'Prep']\r\n\r\n# Define the observed states\r\nobs_states = ['time', 'flies', 'like', 'leaves']\r\n\r\n# Transition probabilities\r\ntransition_prob = {\r\n    'Start': {'N': 1/2, 'V': 1/2, 'Prep': 0},\r\n    'N': {'N': 1/4, 'V': 1/2, 'Prep': 1/4},\r\n    'V': {'N': 2/3, 'V': 0, 'Prep': 1/3},\r\n    'Prep': {'N': 1, 'V': 0, 'Prep': 0},\r\n}\r\n\r\n# Emission probabilities\r\nemission_prob = {\r\n    'N': {'time': 2/4, 'flies': 1/4, 'like': 0, 'leaves': 1/4},\r\n    'V': {'time': 1/6, 'flies': 2/6, 'like': 2/6, 'leaves': 1/6},\r\n    'Prep': {'time': 0, 'flies': 0, 'like': 1, 'leaves': 0}\r\n}\r\n\r\n# Initialize dictionaries\r\nviterbi_prob = {state: [0] * len(obs_states) for state in hidden_states}\r\nbackpointer = {state: [None] * len(obs_states) for state in hidden_states}\r\n\r\n# Initialization\r\nstart_state = 'Start'\r\nfor state in hidden_states:\r\n    viterbi_prob[state][0] = transition_prob[start_state][state] * emission_prob[state][obs_states[0]]\r\n    backpointer[state][0] = start_state\r\n\r\n# Recursive step of the Viterbi algorithm\r\nfor t in range(1, len(obs_states)):\r\n    for state in hidden_states:\r\n        max_prob = 0\r\n        max_state = None\r\n        for prev_state in hidden_states:\r\n            prob = viterbi_prob[prev_state][t-1] * transition_prob[prev_state][state] * emission_prob[state][obs_states[t]]\r\n            if prob &gt; max_prob:\r\n                max_prob = prob\r\n                max_state = prev_state\r\n        viterbi_prob[state][t] = max_prob\r\n        backpointer[state][t] = max_state\r\n\r\n# Termination step\r\nmax_final_prob = 0\r\nmax_final_state = None\r\nfor state in hidden_states:\r\n    if viterbi_prob[state][-1] &gt; max_final_prob:\r\n        max_final_prob = viterbi_prob[state][-1]\r\n        max_final_state = state\r\n\r\n# Backtrace to find the most likely sequence of states\r\npath = [max_final_state]\r\nfor t in range(len(obs_states)-1, 0, -1):\r\n    path. insert(0, backpointer[path[0]][t])\r\n\r\nprint('Most likely sequence of states:', path)\r\n\n\n",
                "target": [
                    "This looks like a correct implementation. I don't see a way to optimize this. Is there a particular part of the algorithm you are concerned about in terms of efficiency?"
                ]
            }
        ]
    },
    {
        "context": "CKY Algorithm Bounds Clarification",
        "statements": [
            {
                "source": "For sake of example, lets say our tokens are: ['These', 'are', 'the', 'tokens']Clearly, the length of the sentence would be 4 (n = 4)Within the CKY algo using the psuedo code above, are the bounds on the loops inclusive? In other words, for the initialization step, would i take on values of 0, 1, 2, 3 or 0, 1, 2? Furthermore, in the main loop, would length take on values of 2, 3, 4 or 2, 3? Same with for k = i+1. .. j-1. Does k take on values i+1, i+2, . .. , j-2, j-1 or i+1, i+2, . .. , j-2?",
                "target": [
                    "The bounds are inclusive for all of those loops."
                ]
            }
        ]
    },
    {
        "context": "Part 1 - CNF format",
        "statements": [
            {
                "source": "For this instruction in part 1:Specifically you need to verify that each rule corresponds to one of the formats permitted in CNF. I just want to confirm I am understanding this correctly. We are only verifying that the format of the rule is in CNF but not verifying if the rhs correctly corresponds with the correct nonterminal on the lhs? In general, is formatting the only thing we need to check for in verifying if a PCFG is in CNF? Isn't it possible that the PCFG contains an incorrect rule but is in the proper format? ",
                "target": [
                    "Yes, you only have to verify the format is in CNF and ensure all probabilities for the same lhs symbol sum to 1. 0 (approximately).",
                    "What do you mean by \"incorrect rule\"? "
                ]
            }
        ]
    },
    {
        "context": "Ungraded assignment for CKY",
        "statements": [
            {
                "source": "Hi Professor/TAs - Could you please upload the solution for the Ungraded Exercise: CKY algorithm? Thanks! Jess ",
                "target": [
                    "Theyre available now in the pages section. "
                ]
            }
        ]
    },
    {
        "context": "Performance metrics for Part 5 (Evaluating the Parser)",
        "statements": [
            {
                "source": "Hi Professor/TAs:In the assignment it is mentioned the performance metrics after implementation was-\"Coverage: 67%, Average F-score (parsed sentences): 0. 95, Average F-score (all sentences): 0. 64\"Is this the expected value for full credit? Might be a trivial question, but I got very slightly lower value for  Average F-score (all sentences) (~0. 639, rounding to same metric value 0. 64).  Just wanted to confirm if this was okay. Thank you in advance!",
                "target": [
                    "0. 639 is fine."
                ]
            }
        ]
    },
    {
        "context": "Part 3 - Parsing with backpointers: \"math. log2\"",
        "statements": [
            {
                "source": "Hi, The algorithm for PCFG parsing seems to be multiplying log probabilities for the parents rule with the probabilities of children at each step. Is it recommended to use math. log2 with summation for this assignment ( similar to assignment 1) when dealing with probabilities? Thank you",
                "target": [
                    "Yes."
                ]
            }
        ]
    },
    {
        "context": "Part 4 get_tree function base case",
        "statements": [
            {
                "source": "Hello, While implementing homework 2's part 4 (get_tree), i initially included a base case where if (i, j) is not in the \"chart\", I return None which I found to be problematic with evaluate_parser. py's method of counting parsed trees (it uses try except KeyError to count the parsed trees)Therefore, should we jut leave this base case (key error) unhandled in part 4? Thank you for all the help in advance!",
                "target": [
                    "Yes, the key error is handled by the evaluation script, so that sentence would be counted as not-parseable. "
                ]
            }
        ]
    },
    {
        "context": "Additional Coding Challenges",
        "statements": [
            {
                "source": "Hi everyone, Would it be possible to release additional coding/implementation problems for any algorithms we're not exploring in the homework assignments? Thank you in advance!",
                "target": [
                    "Hi, Any specific recommendations for what kind of coding/implementation problems - algorithms for ?"
                ]
            }
        ]
    },
    {
        "context": "Solution for ungraded exercise on CKY algorithm and backpointers.",
        "statements": [
            {
                "source": "Good afternoon, Is it possible to upload the solution for the CKY ungraded exercise? It would be useful to test if our understanding is right while working on the next homework. Thank you.",
                "target": [
                    "Assignment Question - CKY Solution Link - CKY"
                ]
            }
        ]
    },
    {
        "context": "Ungraded Exercise Solution: n-Gram Language Models - Part 2",
        "statements": [
            {
                "source": "I am trying to wrap my head around the proof by induction in the solution and am confused by the beginning of the inductive step:I thought the inductive hypothesis only tells us for sequences of length n-1 that:\\sum_{w_{1:n-1}}^{ }P\\left(w_1. .. w_{n-1}\\right)\\ =\\sum_{w_{1:n-1}}^{ }P\\left(w_1 \\vert START\\right)\\prod_{i=2}^{n-1}P\\left(w_i\\vert w_{i-1}\\right) = 1. Is there a formula being used to make the jump that the left-hand-side is equivalent to \\sum_q^{ }P\\left(w_{n-1}=q\\right)? Thanks for posting a solution in the first place! Thank you, Nick",
                "target": [
                    "$$P(w_{n-1}=q)$$ is the sum of all length n-1 sequences that end in q. Summing over all different q gives us the sum of _all_ length n-1 sequences. And by the inductive hypothesis the sum of all length n-1 sequences is 1. Does that make sense?"
                ]
            }
        ]
    },
    {
        "context": "Friday Office Hours",
        "statements": [
            {
                "source": "Hi! Will there still be office hours this Friday afternoon? Thanks!",
                "target": [
                    "Yes, I'm holding them right now. The Zoom link is on the Courseworks page."
                ]
            }
        ]
    },
    {
        "context": "Homework assignment",
        "statements": [
            {
                "source": "Going forward, would it be possible to have two weekends to complete the HW rather than the one we currently get (have the assignment due Monday instead of Friday? ), even if this means more overlap of assignments. I feel it's not uncommon to attend/watch lectures during the week and review/ingest the material on the weekends, especially for those of us working full time. Having only the last weekend makes it a bit difficult time-wise to fully grasp everything from the week that follows and makes incorporating this material into the assignments more challenging. If there is an administrative/logistical reason that makes this impracticable that is totally understandable. Thank you!",
                "target": [
                    "I think the class ends on August 10th, with Summer B ending on the 11th. So, it seems like this could be possible for all but the last HW."
                ]
            }
        ]
    },
    {
        "context": "Raw Trigram and Bigram Prob",
        "statements": [
            {
                "source": "It is my understanding that for trigrams like ('START', 'START', 'the'), we should use count ('START', 'START', 'the') / # of sentences. Should we do this with bigrams like ('START', 'the') as well? In other words, should the raw probability of ('START', 'START', 'the') be the same as ('START', 'the')?",
                "target": [
                    "Yes the probability for the trigram and bigram would be the same. However you should use count('START', 'START', 'the') / num_sentences instead of 1 / num_sentences "
                ]
            }
        ]
    },
    {
        "context": "Perplexity testing",
        "statements": [
            {
                "source": "In the perplexity texts, when we provide the files in order of \"brown_train. txt brown_test. txt\" versus \"brown_test. txt brown_train. txt, \" why do the values differ? How exactly do they relate to each other? ",
                "target": [
                    "The first argument is the training corpus, the second argument the test corpus. So if you swap them you are training on brown_test. txt (a much smaller dataset) and testing on brown_train. txt. "
                ]
            }
        ]
    },
    {
        "context": "Part 7 Function Signature",
        "statements": [
            {
                "source": "For part 7, the main function in the skeleton code calls the essay_scoring_experiment asacc = essay_scoring_experiment('train_high. txt', 'train_low. txt\", \"test_high\", \"test_low\")However, when I run this function as is, I am getting a file not found error, so I had to change the parameters to their relative paths, as shown below:acc = essay_scoring_experiment(\"hw1_data/ets_toefl_data/train_high. txt\", \"hw1_data/ets_toefl_data/train_low. txt\", \"hw1_data/ets_toefl_data/test_high\", \"hw1_data/ets_toefl_data/test_low\")Is this acceptable, or are we expected to handle the first case with just the txt file names or directory names inside the essay_scoring_experiment function?",
                "target": [
                    "Yes, feel free to edit the code in the main function as you see fit. We will not run the main to test your code and instead call the individual functions / methods. "
                ]
            }
        ]
    },
    {
        "context": "raw_bigram_probability and sentence_logprob questions",
        "statements": [
            {
                "source": "a) In the case where the bigram and trigram has not been seen in the train. txt, should raw_bigram_probability and raw_trigram_probability return 0 ? b) In sentence_logprob, when the smoothed trigram probability returned is 0, there is a math domain error as 0 is out of domain for log_2. What is the expected fix here? c) For trigrams like ('START', 'START', 'the'), is the idea that when we use smoothed_trigram_probability, the probability of this trigram should be count('the')/ Total Words in test set?",
                "target": [
                    "a) If a bigram has not been seen you should return 0. Similarly for trigram probabilities: if the trigram has not been seen, but the bigram context has been seen, return 0. Otherwise, return 1/|V|, where V is the number of types in the lexicon, including STOP (but not including START). b) This should never happen because there are no unseen unigrams (the corpus reader replaces these). Make sure the corpus reader is initialized with the correct lexicon. c) No. For ('START', 'START', 'the'), that is trigrams at the beginning of the sentence, you should use count(('START', 'START', 'the)) / number_of_sentences. "
                ]
            }
        ]
    },
    {
        "context": "Error: TypeError: 'collections. defaultdict' object is not callable",
        "statements": [
            {
                "source": "For Part 3, the following line seems to be producing several errors: bigram_count = self. bigramcounts(tuple(trigram[:2]))These errors include \"TypeError: 'collections. defaultdict' object is not callable\" or (when I remove the tuple from the above line of code) \"TypeError: unhashable type: 'list'\". Is there a reason that this is producing this particular error, even though I assigning it as a tuple in order to make a slicing call? ",
                "target": [
                    "For the latter, dictionary keys need to be hashable types in python, such as a tuple. For the first error, hard to tell without reading code by looks like you might be trying to get values using dictionary(key) instead of dictionary[key]",
                    "You want to use bigram_count = self. bigramcounts[tuple(trigram[:2])]careful though, you probably want to use the last two elements of the trigram for the lookup, instead of the first two. If the trigram is ('natural', 'language', 'processing'), the bigram should be ('language', 'processing'). "
                ]
            }
        ]
    },
    {
        "context": "sentence_logprob",
        "statements": [
            {
                "source": "To confirm, sentence_logprob is to be computed using smoothed_trigram_probability, right?",
                "target": [
                    "I used smoothed_trigram_probability in computing sentence_logprob. ",
                    "Correct. "
                ]
            }
        ]
    },
    {
        "context": "Lemmatization of amusing",
        "statements": [
            {
                "source": "Should the lemmatized sentence have 'amuse'  for the word amusing?",
                "target": [
                    "Yeap you are right. The lemmatized version of amusing should be amuse. "
                ]
            }
        ]
    },
    {
        "context": "What is the instructor-provided entry code for Gradescope?",
        "statements": [
            {
                "source": "Hello, I'm sorry if I missed this, but what is the code to add this course on Gradescope? Thanks",
                "target": [
                    "May I know why you need access to Gradescope yet ? As of now all assignment submissions are through Courseworks submission only. Prof will let you know later about Gradescope access and use for final exams."
                ]
            }
        ]
    },
    {
        "context": "Perplexity",
        "statements": [
            {
                "source": "When calculating the perplexity using the following equation:Should the total number of words tokens (M) also exclude the START token? I would guess that the START and STOP tokens should be excluded since the sum of the log probabilities does not include the probability of sentences with START or STOP tokens.",
                "target": [
                    "Also what are the acceptable range for perplexity values? For both texts I am getting very small values in the (7-15) range while the HW description says that they must be lower than 400. Training is less than test as expected but because the range is very small, their difference isn't \"a lot\" as the hw assignment says.",
                    "the M in the perplexity calculations is the number of predictions the model makes on the test set. So it includes all tokens, plus the STOP tokens (one per sentence), but no START tokens. "
                ]
            }
        ]
    },
    {
        "context": "Very High Perplexity Values",
        "statements": [
            {
                "source": "I am getting extremely high perplexity values and having trouble pinpointing the reason why. I'm trying to figure out if there is an error with how I am calculating the log probabilities of a sentence, or if there is an issue with how I am calculating the perplexity value itself.  Some information about the log probabilities of the Brown test data when using the training model is pasted below:Range of Sentence Log Probabilities: (-1020. 8787566532884 -12. 231044294648944)Mode of Sentence Log Probabilities: (-15. 291353424804461) Mean of Sentence Log Probabilities: (-128. 44629381836992)Are these reasonable numbers for sentence log probabilities? As for the perplexity, 2^{-l}where, l=\\frac{1}{M}\\sum_{i=1}^m\\log_2P\\left(s_i\\right)Does M = total number of word tokens (including duplicates) or unique word tokens (lexicon)?",
                "target": [
                    "See #48  "
                ]
            }
        ]
    },
    {
        "context": "Iterating through generator sometimes gets strings instead of lists?",
        "statements": [
            {
                "source": "During pt 2 I noticed that iterating through the generator corpus passed in sometimes yields a string instead of a list (the example I found was the single character \"d\"). I ended up using a type conversion to make sure the sentence being passed into get_ngrams() was a list. I'm just wondering if this is expected behavior + if what I'm doing is acceptable",
                "target": [
                    "Check if you are passing the arguments correctly, ideally it should be consistent datatype."
                ]
            }
        ]
    },
    {
        "context": "Bigram START, START",
        "statements": [
            {
                "source": "Hello, The professor mentioned that we could store START, START in the bigram dictionary to help calculate probability for trigrams like (START, START, the), but my understanding was that the only reason we were storing multiple STARTs was out of necessity in (3+n)-gram where we need the first word in the sentence, in this case \"the\", without any other tokens, so that we can reduce the likelihood that the sequence was not seen in the corpus. For example, should get_ngrams([\"natural\", \"language\", \"processing\"], 4) contain:[('START', 'START', 'START', 'START'), ('START', 'START', 'START', 'natural'), ('START', 'START', 'natural', 'language'), ('START', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'STOP')]Is this understanding correct, and storing the START, START bigram is out of necessity, or is this generally best practice? Thank you",
                "target": [
                    "get_ngrams([\"natural\", \"language\", \"processing\"], 4) should contain:[('START', 'START', 'START', 'natural'), ('START', 'START', 'natural', 'language'), ('START', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'STOP')]This is consistent with the ngram example in part 1. Instead of storing a bigram count for (START, START) I recommend keeping track of the number of sentences in the training data using a separate instance variable. Then, when computing trigram probabilities, you need to treat (START, START, x) as a special case. "
                ]
            }
        ]
    },
    {
        "context": "Lexicon size",
        "statements": [
            {
                "source": "generator = corpus_reader(corpusfile)\nself. lexicon = get_lexicon(generator)\nself. lexicon_size = float(len(self. lexicon)+1)\nself. lexicon. add(\"UNK\")\nself. lexicon. add(\"START\")\nself. lexicon. add(\"STOP\")\nShould we be counting UNK, START and STOP as part of the lexicon size or only STOP?",
                "target": [
                    "Depends on what you use the lexicon size for. If you are computing a uniform distribution about possible tokens, you would want it to include UNK and STOP, but not START. "
                ]
            }
        ]
    },
    {
        "context": "Part 3 - Raw probability and handling unknown tokens",
        "statements": [
            {
                "source": "For the unigram probability isn't the total number of words simply the total number of words in the model's lexicon? If so, I assume we can simply take the len of the lexicon as |V|? And in general, in the raw probabilities functions: when the count of specific trigram/bigram/unigram is 0, should we try first to estimate the count again by replacing some words in the tuple with unknown toten? Thanks in advance!",
                "target": [
                    "For your first question: you need to count the number of word tokens (including duplicates). I was initially thinking about the case in which the context of a trigram is unseen. In that case you should use 1/V, where V contains all types in the lexicon. you can use len(lexicon) but you need to add 1 for the STOP symbol. For the second question: the corpus reader already takes care of replacing unknown tokens with UNK. You can therefore assume that all tokens have been seen.  There should be no 0 unigram probabilities. "
                ]
            }
        ]
    },
    {
        "context": "Are we allowed to add new functions to make the code more readable",
        "statements": [
            {
                "source": "Good afternoon, I wanted to check if we are allowed to add new functions for the HW, without changing the function definitions/signatures of the existing ones? Thank you",
                "target": [
                    "Yes, that's fine."
                ]
            }
        ]
    },
    {
        "context": "Part 3 [start, start, ] in tri-probability",
        "statements": [
            {
                "source": "Hi, I am wondering if we need to deal with the situation when there is [START, START, . .. ] in the raw_trigram_probability function. As the double start bigramcounts =0 in this case, should I use 1/lexion or the same probability of [START, . .. ] in bi-probability?",
                "target": [
                    "Yes you will have to address trigrams like (START, START, the). You will need the count for (START, START) for the denominator. You could either store this in the bigram dictionary (but be careful, because you dont want a bigram probability for P(START|START) ), or you could add an instance variable to count the number of sentences in the training data. "
                ]
            }
        ]
    },
    {
        "context": "Professor Bauer office hours",
        "statements": [
            {
                "source": "Hi! How can we access Prof. Bauer's OH? Do we use the class Zoom link or do we need to make an appointment? ",
                "target": [
                    "I found class zoom link was recording but no one there~",
                    "Sorry there was a scheduling issue today . .. I will hold make-up office hours tomorrow. I will send an email.  "
                ]
            }
        ]
    },
    {
        "context": "TA Office Hours During Week",
        "statements": [
            {
                "source": "Is it possible to get some TA office hours during the week via Zoom? For the office hours to be helpful (as it currently sits) we would need to know our questions nearly a week before the homework will be due.",
                "target": [
                    "I'll. bring this up during our TA meeting tonight. We should be able to move some office hours earlier in the week. "
                ]
            }
        ]
    },
    {
        "context": "Effects of using log2 and log on perplexity",
        "statements": [
            {
                "source": "Hi everyone! I'm working on optimizing the perplexity, and had a question about using log vs log2. When using math. log within the sentence_logprob and perplexity functions, this is the output:However, when using math. log2, this is the output (which is a considerable increase):If I understood it correctly, using the logarithm allows us to check the trends, so are log and log2  interchangeable? Also, are these generally acceptable values?",
                "target": [
                    "The perplexity calculation requires that you use log2 (otherwise you have to change the base of the exponent in the calculation as well. The convention is to use log2. The perplexity result you get with log2 is rather high. Did you make sure to use the number of sentences for lowercase-m and the number of tokens (including STOP but excluding START) for uppercase-M? "
                ]
            }
        ]
    },
    {
        "context": "STOP or END?",
        "statements": [
            {
                "source": "Hi all, In the lecture, I remember discussing START and END. However in the HW1, it seems to be START and STOP. Should we use END or STOP in that case for this homework, and/or they the same thing? Thanks, Vidur",
                "target": [
                    "They are the same thing -- either is fine as long as you use it consistently. "
                ]
            }
        ]
    },
    {
        "context": "Parameter Clarification Part 5",
        "statements": [
            {
                "source": "def sentence_logprob(self, sentence):\nIs sentence a list of strings? Or is it a single string that we need to split? There is no specification about the format of this paramter",
                "target": [
                    "It's a list of strings. "
                ]
            }
        ]
    },
    {
        "context": "ngrams to probability notation",
        "statements": [
            {
                "source": "Given the following trigram: ('natural', 'language', 'processing')If we were calculating the raw probability for this trigram, how would this convert to probability notation? Would it be: P('processing' | 'natural', 'language') = count(('natural', 'language', 'processing')) / count(('natural', 'language'))or something different?",
                "target": [
                    "Your calculation is correct. "
                ]
            }
        ]
    },
    {
        "context": "Part 3 - Undefined cases",
        "statements": [
            {
                "source": "My recommendation is to make P(v | u, w) = 1 / |V| (where |V| is the size of the lexicon)Another option would be to use the unigram probability for v, so P(v | u, w) = P(v). These are the recommendations given in part 3 for calculating raw probabilities when either the numerator and denominator or the denominator are 0Is there a difference in accuracy / performance? Is one typically favored in real-world scenarios over the other? Or does it depend on a case-to-case basis?",
                "target": [
                    "That's not quite right. If the numerator is 0, the raw probability should be 0. If the denominator is 0 then you would have to use one of the approaches described above. In principle, using the unigram probability should give you a better estimate. But on this particular task, empirically, setting the probability to 1/|V| works better. "
                ]
            }
        ]
    },
    {
        "context": "Part 7",
        "statements": [
            {
                "source": "I am a bit confused about how to make predictions. We have two trigram models, and we can calculate the perplexities of each essay with these two models. Do we compare these two perplexities of each essay and choose the smaller one as the prediction? Also, do we need to include anything in the main code for the homework? I was able to use it for testing. Do we need to keep those testing codes?",
                "target": [
                    "Do we compare these two perplexities of each essay and choose the smaller one as the prediction? Yes, thats the correct approach. You do not have to put anything specific in the main portion of the code. We will call the individual methods for testing. "
                ]
            }
        ]
    },
    {
        "context": "Testing part 2",
        "statements": [
            {
                "source": "Hi, I'm confused as to how can I test my part 2. Would appreciate any hints. Thank you!",
                "target": [
                    "There are some sample inputs and outputs you can try with your model and see if the counts match."
                ]
            }
        ]
    },
    {
        "context": "Additive smoothing inaccurate because of Zipf's law",
        "statements": [
            {
                "source": "Hi Professor/TAs, I was rewatching the recording where professor mentions about additive smoothing not always being accurate because of Zipf's distribution. Professor mentions that it would overestimate the probability for unseen words (2hr 34min) , but doesn't the probability depend on count of words, so shouldn't the probability of seen words be higher. It would be helpful if you could correct me here and how this relates to Zipf's distribution. Thank you.",
                "target": [
                    "Here is my understanding. Not sure if it's correct. In the testing data, we might encounter tokens not observed in the training data. These 'unseen' tokens could lead to a 'divide by 0' error. Additive smoothing is applied to solve the issue by assigning a probability of 1/V to unseen words (the count of unseen words is zero). However, the actual probability of unseen words in real-life is likely much lower than 1/V because of Zipf's distribution.",
                    "The way I thought of it is like inflation. Suppose you print some money and give everyone $1. Then the people who had more money to start with still have more money, but the value of a dollar decreases, so the people with $0 to start with have had their purchasing power disproportionately increased.",
                    "So one question is what \"accurate\" means here. Additive smoothing gives you a probability distribution, in the sense that all event probabilities will sum to 1. 0. We don't know what the \"real\" probability for unseen events is -- if there even is such a thing. We assume these probabilities are not 0 and that, instead, we simply have not seen evidence for these events because the training data is too small. So we use smoothing to come up with an estimate. The main issue with additive smoothing is that it treats all unseen events the same and it redistributes too much probability mass from seen events, especially if you have a limited amount of training data. Assume you have seen the trigram \"like eating pizza\" once, but your test data contains the trigram \"like eating escargot\". Add-one smoothing would make \"escargot\" in this context half as likely as \"pizza\", when you would really expect it to be much more rare. "
                ]
            }
        ]
    },
    {
        "context": "Part 3 raw probability",
        "statements": [
            {
                "source": "Does \"START\" and \"STOP\" count towards the total number of words?",
                "target": [
                    "The \"STOP\" / END token is counted in total number of words [ V ] as we need to calculate the probability of END being the next token given previous in order to know when a sentence has ended. The \"START\" token is never predicted hence should be excluded. Does that help answer your question ?"
                ]
            }
        ]
    },
    {
        "context": "Part 3",
        "statements": [
            {
                "source": "Does this imply that in situations where count(u, w, v) is 0 but count(u, w) is greater than 0, or count(u, w) is 0 but count(u) is greater than 0, we still assume a uniform distribution? Otherwise, in these two cases, the raw probability is 0. And we just leave it as zero?",
                "target": [
                    "Hi, When it's an unseen trigram (count(u, w, v)=0) but a seen bigram (count(u, w) is greater than 0), then you would return the raw probability as 0. For this case also - count(u, w) is 0 but count(u) is greater than 0, raw probability is 0"
                ]
            }
        ]
    },
    {
        "context": "Part 5 Handle Unknown Tokens in Sentence?",
        "statements": [
            {
                "source": "Hello, I have a question about part 5 of homework 1. Specifically, we created a lexicon with UNK tokens within the trigram model class. Then when we run sentence_logprob, do we need to process the passed sentence to replace unseen tokens as \"UNK\"? It seems if we do not do this, we would encounter log(0) math domain issue? Thank you for all the help in advance!",
                "target": [
                    "The corpus reader will already replace the unseen tokens for you (thats why you pass the lexicon parameter). So you wont have to worry about this issue. Effectively, all tokens you encounter will have been seen. "
                ]
            }
        ]
    },
    {
        "context": "Optional part",
        "statements": [
            {
                "source": "For the optional part, will there be extra credits if we do it? And is it okay to leave it blank if we choose not to do it? Thank you!",
                "target": [
                    "You can leave it blank and still get full credit on homework, but its interesting to see what the output looks like. There is no extra credit. "
                ]
            }
        ]
    },
    {
        "context": "Part6",
        "statements": [
            {
                "source": "Hello, I'm wondering if we need to consider and count \"START\" and \"STOP\" as tokens, or only the tokens existing in the corpus itself. Thank you!",
                "target": [
                    "Include the number of STOP tokens. However, exclude the number of START tokens because the model will never make a prediction for START."
                ]
            }
        ]
    },
    {
        "context": "Part 3 - Raw n-gram probabilities",
        "statements": [
            {
                "source": "I am a bit confused as to what the inputs are representing for these three functions we have to implement. What are trigram, bigram, and unigram respectively? Are they a specific instance of a trigram/bigram/unigram? (i. e. something like ('START', 'I', 'want') Or are they lists of trigrams/bigrams/unigrams? (i. e. something [('START', 'I'), ('I, 'want'), . .. ])",
                "target": [
                    "The input is a specific trigram/bigram/unigram."
                ]
            }
        ]
    },
    {
        "context": "End of N-Gram Generation",
        "statements": [
            {
                "source": "In the example given for a trigram: &gt;&gt;&gt; get_ngrams([\"natural\", \"language\", \"processing\"], 3)\n[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]Why does the list of tuples returned end with ('language', 'processing', 'STOP') instead of ('processing', 'STOP', 'STOP')? Would that be redundant? i. e. once an n-gram is generated with STOP, there would be no need to include anything else afterwards?",
                "target": [
                    "Yeah, it would be redundant since we know the sentence has ended. See 2:07:00 in the 7/7 lecture."
                ]
            }
        ]
    },
    {
        "context": "can we add instance variables in init",
        "statements": [
            {
                "source": "Hello, part three says that we may use an instance variable to track the total words. Can we compute this in any earlier method or should we compute that variable in that step? thanks",
                "target": [
                    "To track the total words, you may define the instance variable in TrigramModel class initialization __init__  function and then reference it as needed in different parts. Does that help answer your question?"
                ]
            }
        ]
    },
    {
        "context": "Use of libraries",
        "statements": [
            {
                "source": "Hello, Question for homework in general: are we allowed to import libraries beyond what is given to solve the questions? Thanks!",
                "target": [
                    "You shouldnt import any libraries not specifically mentioned without permission. "
                ]
            }
        ]
    },
    {
        "context": "Zoom Link for Today",
        "statements": [
            {
                "source": "I'm unable to find the Zoom link for today's lecture in the ZOom Class Sessions Tab. The next one shown is for tomorrow. Instead, I'm watching live in the Video Recordings",
                "target": [
                    "That's fine -- I think it's always the same zoom link. https://columbiauniversity. zoom. us/j/99539825258? pwd=a2pETFdJVjhocGpXVEZwVmljc2lBZz09"
                ]
            }
        ]
    }
]