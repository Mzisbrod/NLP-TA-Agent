[
  {
    "context": "Homework 4 question 1: Candidate Synonyms from WordNet. In this assignment you will work on a lexical substitution task, using, WordNet, pre-trained Word2Vec embeddings, and BERT. This task was first proposed as a shared task at SemEval 2007 Task 10\n\nIn this task, the goal is to find lexical substitutes for individual target words in context. For example, given the following sentence:\n\n\"Anyway , my pants are getting tighter every day .\" \n\nthe goal is to propose an alternative word for tight, such that the meaning of the sentence is preserved. Such a substitute could be constricting, small or uncomfortable.\n\nIn the sentence\n\n\"If your money is tight don't cut corners .\" \n\nthe substitute small would not fit, and instead possible substitutes include scarce, sparse, limitited, constricted. You will implement a number of basic approaches to this problem and compare their performance.",
    "statements": [
      {
        "source": "Write the function get_candidates(lemma, pos) that takes a lemma and part of speech ('a','n','v','r' for adjective, noun, verb, or adverb) as parameters and returns a set of possible substitutes. To do this, look up the lemma and part of speech in WordNet and retrieve all synsets that the lemma appears in. Then obtain all lemmas that appear in any of these synsets. Make sure that the output does not contain the input lemma itself. The output can contain multiword expressions such as \"turn around\". WordNet will represent such lemmas as \"turn_around\", so you need to remove the _.",
        "target": "def get_candidates(lemma, pos) -> List[str]:\n    # Part 1\n    lexemes = wn.lemmas(lemma,pos)\n    set_candidates = set()\n  \n    for syn in lexemes:\n      s1 = syn.synset()\n      for lem in s1.lemmas():\n        candidate = lem.name()\n        if candidate != lemma:\n          if '_' in candidate:\n            candidate = candidate.replace('_',' ')\n          set_candidates.add(candidate) \n    \n    return set_candidates "
      }
    ]
  },
  {
    "context": "Homework 4 question 2: WordNet Frequency Baseline. Write the function wn_frequency_predictor(context) that takes a context object as input and predicts the possible synonym with the highest total occurence frequency (according to WordNet). Note that you have to sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets. You can use the get_candidates method or just duplicate the code for finding candidate synonyms (this is possibly more convenient). Using this simple baseline should give you about 10% precision and recall. Take a look at the output to see what kinds of mistakes the system makes",
    "statements": [
      {
        "source": "Write the function wn_frequency_predictor(context) that takes a context object as input and predicts the possible synonym with the highest total occurence frequency (according to WordNet).",
        "target": "def wn_frequency_predictor(context : Context) -> str:\n    # Part 2\n    dic = defaultdict(int)\n\n    lexemes = wn.lemmas(context.lemma,context.pos) # Retrieve all lexemes\n    for syn in lexemes:\n      s1 = syn.synset() # get the synset for the first lexeme >Synset('interruption.n.02')\n      for lem in s1.lemmas(): # Get all lexemes in that synset [Lemma('interruption.n.02.interruption'), Lemma('interruption.n.02.break')]\n        candidate = lem.name()\n        if candidate != context.lemma:\n          if '_' in candidate:\n            candidate = candidate.replace('_',' ')\n          dic[candidate] += lem.count()\n    \n    return max(dic, key=dic.get)"
      }
    ]
  },
  {
    "context": "Homework 4 question 3: Simple Lesk Algorithm. To perform WSD, implement the simple Lesk algorithm. Look at all possible synsets that the target word apperas in. Compute the overlap between the definition of the synset and the context of the target word. You may want to remove stopwords (function words that don't tell you anything about a word's semantics)",
    "statements": [
      {
        "source": "Implement the function wn_simple_lesk_predictor(context). This function uses Word Sense Disambiguation (WSD) to select a synset for the target word. It should then return the most frequent synonym from that synset as a substitute",
        "target": ""
      }
    ]
  },
  {
    "context": "Homework 4 question 4: Most Similar Synonym. You will now implement approaches based on Word2Vec embeddings. These will be implemented as methods in the class Word2VecSubst. The reason these are methods is that the Word2VecSubst instance can store the word2vec model as an instance variable. The constructor for the class Word2VecSubst already includes code to load the model. You may need to change the value of W2VMODEL_FILENAME to point to the correct file",
    "statements": [
      {
        "source": "Write the method predict_nearest(context) that should first obtain a set of possible synonyms from WordNet (either using the method from part 1 or you can rewrite this code as you see fit), and then return the synonym that is most similar to the target word, according to the Word2Vec embeddings",
        "target": "    def predict_nearest(self,context : Context) -> str:\n        candidates = get_candidates(context.lemma, context.pos)\n        synonym = None # Stores the most similar synynom to target word\n        max_sim = -1 # Stores maximum similarity value, -1 is the minimum output of cosine function\n\n        # For each candidate word, check if its similarity value is larger that max_sim (if possible)\n        # and if so, assign it to max_sim and store the candidate in synonym.\n        # If not possible, move to the next candidate\n        for cand in candidates:\n            try:\n                if self.model.similarity(cand, context.lemma) >= max_sim:\n                    max_sim = self.model.similarity(cand, context.lemma)\n                    synonym = cand\n            except:\n                pass\n        return synonym"
      }
    ]
  },
  {
    "context": "Homework 4 question 5: Using BERT's masked language model. BERT is trained on a masked language model objective, that is, given some input sentence with some words replaced with a [MASK] symbol, the model tries to predict the best words to fit the masked positions. In this way, BERT can make use of both the left and right context to learn word representations (unlike ELMo and GPT).\n\nWe are going to use a pre-trained BERT model. Typically, BERT is used to compute contextualized word embeddings, or a dense sentence representation. In these applications, the masked LM layer, which uses the contextualized embeddings to predict masked words, is removed and only the embeddings are used.  However, we are going to use the masked language model as-is.\n\nThe basic idea is that we can feed the context sentence into BERT and it will tell us \"good\" replacements for the target word. The output will be a vector of length |V| for each word, containing a score for each possible output word. There are two possible approaches: We can either leave the target word in the input, or replace it with the [MASK] symbol. lexub_xml.Context works as follows: Obtain a set of candidate synonyms (for example, by calling get_candidates), Convert the information in context into a suitable masked input representation for the DistilBERT model. Make sure you store the index of the masked target word (the position of the [MASK] token), Run the DistilBERT model on the input representation, Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target position (i.e. the position of the masked word). Return this word.",
    "statements": [
      {
        "source": "Write the method predict(context) in the class BertPredictor, where context is an instance of lexsub_xml",
        "target": "    def predict(self, context : Context) -> str:\n        # Set of candidate synonyms\n        candidates = get_candidates(context.lemma, context.pos)\n\n        # Converts information of context into suitable masked input representation for DistilBERT model\n        masked_text = ' '.join(context.left_context + ['[MASK]'] + context.right_context)\n        input_toks = self.tokenizer.encode(masked_text)\n        indices = self.tokenizer.convert_ids_to_tokens(input_toks)\n        masked_index = indices.index('[MASK]')\n        input_mat = np.array(input_toks).reshape((1, -1))\n\n        # Runs the DistilBERT model on the input representation\n        outputs = self.model.predict(input_mat, verbose=0)\n        predictions = outputs[0]\n        best_words = np.argsort(predictions[0][masked_index])[::-1] # Sorts in increasing order\n        best_words = self.tokenizer.convert_ids_to_tokens(best_words)\n\n        # Selects from the set of WordNet derived candidate synonyms the highest-scoring word in\n        # the target position\n        for word in best_words:\n            if word in candidates:\n                return word\n            \n        # If not found, return blank\n        return ''"
      }
    ]
  },
  {
    "context": "Homework 4 question 6: Other ideas?. By now you should have realized that the lexical substitution task is far from trivial",
    "statements": [
      {
        "source": "implement your own refinements to the approaches proposed above, or even a completely different approach. Any small improvement (or conceptually interesting approach) will do to get credit for part 6",
        "target": "    def better_predict(self, context : Context) -> str:\n        # Set of candidate synonyms\n        candidates = get_candidates(context.lemma, context.pos)\n\n        # Converts information of context into suitable masked input representation for DistilBERT model\n        masked_text = ' '.join(context.left_context + ['[MASK]'] + context.right_context)\n        input_toks = self.tokenizer.encode(masked_text)\n        indices = self.tokenizer.convert_ids_to_tokens(input_toks)\n        masked_index = indices.index('[MASK]')\n        input_mat = np.array(input_toks).reshape((1, -1))\n\n        # Runs the DistilBERT model on best 100 input representation and stores result in best_words\n        outputs = self.model.predict(input_mat, verbose=0)\n        predictions = outputs[0]\n        best_words = np.argsort(predictions[0][masked_index])[-100:][::-1] # Sorts in decreasing order\n        best_words = self.tokenizer.convert_ids_to_tokens(best_words)\n\n        # Calculates score of each word based on appearance in the context and in the BERT's top 100 words\n        scores = []\n        for word in candidates:\n            score = 0\n            # If word in candidates is also in best_words, add more to the score based of how high\n            # it is located within the top 100\n            if word in best_words:\n                score += (100 - best_words.index(word)) * 1.5\n            # Subtracts absolute difference between length of word and target word from score\n            score -= abs(len(word) - len(context.lemma)) * 10\n\n            # Checks left context of target word to update score. Uses reversed list as going\n            # from closest to the target to the furthest\n            for count, value in enumerate(context.left_context[::-1]):\n                if value == word:\n                    score += 1/(count+1)\n            # Checks right context of target word to update score\n            for count, value in enumerate(context.right_context):\n                if value == word:\n                    score += 1/(count+1)\n            scores.append((score, word))\n\n        # Returns the candidate, in lowercase, with the highest score by sorting in reverse\n        scores.sort(reverse=True)\n        if scores and len(scores) > 0 and len(scores[0]) > 1:\n            return scores[0][1].lower()\n        \n        # If not found return blank\n        return ''"
      }
    ]
  }
]