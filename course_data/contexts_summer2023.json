[
  "Homework 1. Question 1: Extracting n-grams from a sentence. Complete the function get_ngrams, which takes a list of strings and an integer n as input, and returns padded n-grams over the list of strings. The result should be a list of Python tuples. For example:\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],1)\n[('START',), ('natural',), ('language',), ('processing',), ('STOP',)]\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],2)\n('START', 'natural'), ('natural', 'language'), ('language', 'processing'), ('processing', 'STOP')]\n>>> get_ngrams([\"natural\",\"language\",\"processing\"],3)\n[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]. Question 2: Counting n-grams in a corpus. We will work with two different data sets. The first data set is the Brown corpus, which is a sample of American written English collected in the 1950s. The format of the data is a plain text file brown_train.txt, containing one sentence per line. Each sentence has already been tokenized. For this assignment, no further preprocessing is necessary.\nDon't touch brown_test.txt yet. We will use this data to compute the perplexity of our language model.\nReading the Corpus and Dealing with Unseen Words\nThis part has been implemented for you and are explained in this section. Take a look at the function corpus_readerin trigram_model.py. This function takes the name of a text file as a parameter and returns a Python generator object. Generators allow you to iterate over a collection, one item at a time without ever having to represent the entire data set in a data structure (such as a list). This is a form of lazy evaluation. You could use this function as follows:\n>>> generator = corpus_reader(\"\")\n>>> for sentence in generator:\n             print(sentence)\n\n['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', 'atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n['the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n['the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan', 'allen', 'jr', '&', '.']\n...\nNote that iterating over this generator object works only once. After you are done, you need to create a new generator to do it again.\nAs discussed in class, there are two sources of data sparseness when working with language models: Completely unseen words and unseen contexts. One way to deal with unseen words is to use a pre-defined lexicon before we extract ngrams. The function corpus_reader has an optional parameter lexicon, which should be a Python set containing a list of tokens in the lexicon. All tokens that are not in the lexicon will be replaced with a special \"UNK\" token.\nInstead of pre-defining a lexicon, we collect one from the training corpus. This is the purpose of the function get_lexicon(corpus). This function takes a corpus iterarator (as returned by corpus_reader) as a parameter and returns a set of all words that appear in the corpus more than once. The idea is that words that appear only once are so rare that they are a good stand-in for words that have not been seen at all in unseen text. You do not have to modify this function.\nNow take a look at the __init__ method of TrigramModel (the constructor):\n    def __init__(self, corpusfile):\n\n        # Iterate through the corpus once to build a lexicon\n        generator = corpus_reader(corpusfile)\n        self.lexicon = get_lexicon(generator)\n        self.lexicon.add('UNK')\n        self.lexicon.add('START')\n        self.lexicon.add('STOP')\n        self.num_sentences = 0  # keep track of the number of sentences\n\n        # Now iterate through the corpus again and count ngrams\n        generator = corpus_reader(corpusfile, self.lexicon)\n        self.count_ngrams(generator)\n        self.total_num_words = sum(count for gram, count in self.unigramcounts.items()) - self.unigramcounts[('START',)] \nWhen a new TrigramModel is created, we pass in the filename of a corpus file. We then iterate through the corpus twice: once to collect the lexicon, and once to count n-grams. You will implement the method to count n-grams in the next step.\nNow it's your turn again. In this step, you will implement the method count_ngrams that should count the occurrence frequencies for ngrams in the corpus. The method already creates three instance variables of TrigramModel, which store the unigram, bigram, and trigram counts in the corpus. Each variable is a dictionary (a hash map) that maps the n-gram to its count in the corpus.\nFor example, after populating these dictionaries, we want to be able to query\n>>> model.trigramcounts[('START','START','the')]\n5478\n>>> model.bigramcounts[('START','the')]\n5478\n>>> model.unigramcounts[('the',)]\n61428\nWhere model is an instance of TrigramModel that has been trained on a corpus. Note that the unigrams are represented as one-element tuples (indicated by the , in the end). Note that the actual numbers might be slightly different depending on how you set things up. Question 3: Raw n-gram probabilities. Write the methods raw_trigram_probability(trigram),  raw_bigram_probability(bigram), and raw_unigram_probability(unigram).\nEach of these methods should return an unsmoothed probability computed from the trigram, bigram, and unigram counts. This part is easy, except that you also need to keep track of the total number of words in order to compute the unigram probabilities.\nIt's expected that many of the n-gram probabilities are 0, that is the ngram sequence has not been observed during training. One issue you will encounter is the case if which you have a trigram u,w,v   where  count(u,w,v) = 0 but count(u,w) is also 0. In that case, it is not immediately clear what P(v | u,w) should be. My recommendation is to make P(v | u,w) = 1 / |V|  (where |V| is the size of the lexicon), if count(u,w) is 0. That is, if the context for a trigram is unseen, the distribution over all possible words in that context is uniform.  Another option would be to use the unigram probability for v, so P(v | u,w) = P(v).\nInterlude - Generating text (OPTIONAL)\nThis part is a little trickier. Write the method generate_sentence, which should return a list of strings, randomly generated from the raw trigram model. You need to keep track of the previous two tokens in the sequence, starting with (\"START\",\"START\"). Then, to create the next word, look at all words that appeared in this context and get the raw trigram probability for each.\nDraw a random word from this distribution (think about how to do this -- I will give hints about how to draw a random value from a multinomial distribution on EdStem) and then add it to the sequence. You should stop generating words once the \"STOP\" token is generated. Here are some examples for how this method should behave:\nmodel.generate_sentence()\n['the', 'last', 'tread', ',', 'mama', 'did', 'mention', 'to', 'the', 'opposing', 'sector', 'of', 'our', 'natural', 'resources', '.', 'STOP']\n>>> model.generate_sentence()\n['the', 'specific', 'group', 'which', 'caused', 'this', 'to', 'fundamentals', 'and', 'each', 'berated', 'the', 'other', 'resident', '.', 'STOP']\nThe optional t parameter of the method specifies the maximum sequence length so that no more tokens are generated if the \"STOP\" token is not reached before t words. Question 4: Smoothed probabilities. Write the method smoothed_trigram_probability(self, trigram) which uses linear interpolation between the raw trigram, unigram, and bigram probabilities (see lecture for how to compute this). Set the interpolation parameters to lambda1 = lambda2 = lambda3 = 1/3. Use the raw probability methods defined before. Question 5: Computing sentence probability. Write the method sentence_logprob(sentence), which returns the log probability of an entire sequence (see lecture how to compute this). Use the get_ngrams function to compute trigrams and the smoothed_trigram_probability method to obtain probabilities. Convert each probability into logspace using math.log2. For example:\n>>> math.log2(0.8)\n-0.3219280948873623\nThen, instead of multiplying probabilities, add the log probabilities. Regular probabilities would quickly become too small, leading to numeric issues, so we typically work with log probabilities instead. Question 6: Perplexity. Write the method perplexity(corpus), which should compute the perplexity of the model on an entire corpus.\nCorpus is a corpus iterator (as returned by the corpus_reader method).\nRecall that the perplexity is defined as 2-l, where l is defined as:\nl=\\frac{1}{M}\\sum\\limits_{i=1}^m \\log P(s_i)\nHere M is the total number of word tokens, while m is the number of sentences in the test corpus. So to compute the perplexity, sum the log probability for each sentence, and then divide by the total number of words tokens in the corpus. For consistency, use the base 2 logarithm.\nRun the perplexity function on the test set for the Brown corpus brown_test.txt (see main section at the bottom of the Python file for how to do this). The perplexity should be less than 400. Also try computing the perplexity on the training data (which should be a lot lower, unsurprisingly).\nThis is a form of intrinsic evaluation. Question 7: Using the model for text classification. In this final part of the problem we will apply the trigram model to a text classification task. We will use a data set of essays written by non-native speakers of English for the ETS TOEFL test. These essays are scored according to skill level low, medium, or high. We will only consider essays that have been scored as \"high\" or \"low\". We will train a different language model on a training set of each category and then use these models to automatically score unseen essays. We compute the perplexity of each language model on each essay. The model with the lower perplexity determines the class of the essay.\nThe files ets_toefl_data/train_high.txt and ets_toefl_data/train_low.txt in the data zip file contain the training data for high and low skill essays, respectively. The directories ets_toefl_data/test_high and ets_toefl_data/test_low contain test essays (one per file) of each category.\nComplete the method essay_scoring_experiment. The method should be called by passing two training text files, and two testing directories (containing text files of individual essays). It returns the accuracy of the prediction.\nThe method already creates two trigram models, reads in the test essays from each directory, and computes the perplexity for each essay. All you have to do is compare the perplexities and the returns the accuracy (correct predictions / total predictions).\nOn the essay data set, you should easily get an accuracy of > 80%.\nData use policy: Note that the ETS data set is proprietary and licensed to Columbia University for research and educational use only (as part of the Linguistic Data Consortium. This data set is extracted from https://catalog.ldc.upenn.edu/LDC2014T06. You may not use or share this data set for any other purpose than for this class.",
  "Homework 2. Question 1: Reading the grammar and getting started. The class Pcfg represents a PCFG grammar in chomsky normal form. To instantiate a Pcfg object, you need to pass a file object to the constructor, which contains the data. You can then access the instance variables of the Pcfg instance to get information about the rules. The dictionary lhs_to_rules maps left-hand-side (lhs) symbols to lists of rules. Each rule in the list is represented as (lhs, rhs, probability) triple. The rhs_to_rules dictionary contains the same rules as values, but indexed by right-hand-side. Write the method verify_grammar, that checks that the grammar is a valid PCFG in Chomsky Normal form. Specifically you need to verify that each rule corresponds to one of the formats permitted in CNF. To do this, you can assume that the lhs symbols of the rules make up the inventory of nonterminals. Any other symbol should be interpreted as a terminal. \nYou also need to ensure all probabilities for the same lhs symbol sum to 1.0 (approximately). Then change the main section of grammar.py to read in the grammar, print out a confirmation if the grammar is a valid PCFG in CNF or print an error message if it is not. You should now be able to run grammar.py on grammars and verify that they are well formed for the CKY parser. Question 2: Membership checking with CKY. The file cky.py already contains a class CkyParser. When a CkyParser instance is created a grammar instance is passed to the constructor. The instance variable grammar can then be used to access this Pcfg object. Write the method is_in_language(self, tokens) by implementing the CKY algorithm. Your method should read in a list of tokens and return True if the grammar can parse this sentence and False otherwise. While parsing, you will need to access the dictionary self.grammar.rhs_to_rules. You can use any data structure you want to represent the parse table (or read ahead to question 3 of this assignment, where a specific data structure is prescribed). Question 3: Parsing with backpointers. The parsing method in part 2 can identify if a string is in the language of the grammar, but it does not produce a parse tree. It also does not take probabilities into account. You will now extend the parser so that it retrieves the most probable parse for the input sentence, given the PCFG probabilities in the grammar. The first object is parse table containing backpointers, represented as a dictionary (this is more convenient in Python than a 2D array). The keys of the dictionary are spans, for example table[(0,3)]  retrieves the entry for span 0 to 3 from the chart. The values of the dictionary should be dictionaries that map nonterminal symbols to backpointers. For example: table[(0,3)]['NP'] returns the backpointers to the table entries that were used to create the NP phrase over the span 0 and 3. For example, the value of table[(0,3)]['NP'] could be ((\"NP\",0,2),(\"FLIGHTS\",2,3)). This means that the parser has recognized an NP covering the span 0 to 3, consisting of another NP from 0 to 2 and FLIGHTS from 2 to 3. The split recorded in the table at table[(0,3)]['NP'] is the one that results in the most probable parse for the span [0,3] that is rooted in NP. \nTerminal symbols in the table could just be represented as strings. For example the table entry for table[(2,3)][\"FLIGHTS\"] should be \"flights\".\n\nThe second object is similar, but records log probabilities instead of backpointers. For example the value of probs[(0,3)]['NP'] might be -12.1324. This value represents the log probability of the best parse tree (according to the grammar) for the span 0,3 that results in an NP. During parsing, when you fill an entry on the backpointer parse table and iterate throught the possible splits for a (span/nonterminal) combination, that entry on the table will contain the back-pointers for the the current-best split you have found so far. For each new possible split, you need to check if that split would produce a higher log probability. If so, you update the entry in the backpointer table, as well as the entry in the probability table. \n\nAfter parsing has finished, the table entry table[0,len(toks)][grammar.startsymbol] will contain the best backpointers for the left and right subtree under the root node. probs[0,len(toks)][grammar.startsymbol] will contain the total log-probability for the best parse. \n\ncky.py contains two test functions check_table_format(table) and check_prob_format(probs) that you can use to make sure the two table data structures are formatted correctly. Both functions should return True. Note that passing this test does not guarantee that the content of the tables is correct, just that the data structures are probably formatted correctly. Write the method parse_with_backpointers(self, tokens). You should modify your CKY implementation from part 2, but use (and return) specific data structures. The method should take a list of tokens as input and returns a) the parse table b) a probability table. Both objects should be constructed during parsing. They replace whatever table data structure you used in part 2. Question 4: Retrieving a parse tree. You now have a working parser, but in order to evaluate its performance we still need to reconstruct a parse tree from the backpointer table returned by parse_with_backpointers. Note that the intended format is the same as the data in the treebank. Each tree is represented as tuple where the first element is the parent node and the remaining elements are children. Each child is either a tree or a terminal string. \n\nHint: Recursively traverse the parse chart to assemble this tree. Write the function get_tree(chart, i,j, nt) which should return the parse-tree rooted in non-terminal nt and covering span i,j. Question 5: Evaluating the parser. The program evaluate_parser.py evaluates your parser by comparing the output trees to the trees in the test data set. The program imports your CKY parser class. It then reads in the test file line-by-line. For each test tree, it extracts the yield of the tree (i.e. the list of leaf nodes). It then feeds this list as input to your parser, obtains a parse chart, and then retrieves the predicted tree by calling your get_tree method. It then compares the predicted tree against the target tree in the following way (this is a standard approach to evaluating constiuency parsers, called PARSEVAL): \nFirst it obtains a set of the spans in each tree (including the nonterminal label). It then computes precision, recall, and F-score (which is the harmonic mean between precision and recall) between these two sets. The script finally reports the coverage (percentage of test sentences that had any parse), the average F-score for parsed sentences, and the average F-score for all sentences (including 0 f-scores for unparsed sentences). \n\n What should the results on the atis3 test corpus be?",
  "Homework 3. Question 1: Obtaining the Vocabulary. In this assignment you will train a feed-forward neural network to predict the transitions of an arc-standard dependency parser. The input to this network will be a representation of the current state (including words on the stack and buffer). The output will be a transition (shift, left_arc, right_arc), together with a dependency relation label.\nMuch of the parser code  is provided, but you will need to implement the input representation for the neural net, decode the output of the network, and also specify the network architecture and train the model. The first 5 entries are special symbols. <CD> stands for any number (anything tagged with the POS tag CD), <NNP> stands for any proper name (anything tagged with the POS tag NNP). <UNK> stands for unknown words (in the training data, any word that appears only once). <ROOT> is a special root symbol (the word associated with the word 0, which is initially placed on the stack of the dependency parser). <NULL> is used to pad context windows. Run the following to generate an index of words and POS indices: $python get_vocab.py data/train.conll data/words.vocab data/pos.vocab\n This contains all words that appear more than once in the training data. What will the words look like? Question 2: Extracting Input/Output matrices for training. To train the neural network we first need to obtain a set of input/output training pairs. More specifically, each training example should be a pair (x,y), where x is a parser state and y is the transition the parser should make in that state.\n\nTake a look at the file extract_training_data.py \nStates: The input will be an instance of the class State, which represents a parser state. The attributes of this class consist of a stack, buffer, and partially built dependency structure deps. stack and buffer are lists of word ids (integers).\nThe top of the stack is the last word in the list stack[-1]. The next word on the buffer is also the last word in the list, buffer[-1].\nDeps is a list of (parent, child, relation) triples, where parent and child are integer ids and relation is a string (the dependency label). \n\nTransitions: The output is a pair (transition, label), where the transition can be one of \"shift\", \"left_arc\", or \"right_arc\" and the label is a dependency label. If the transition is \"shift\", the dependency label is None. Since there are 45 dependency relations (see list deps_relations), there are 45*2+1 possible outputs. \n\nObtaining oracle transitions and a sequence of input/output examples. \nAs discussed in class, we cannot observe the transitions directly from the treebank. We only see the resulting dependency structures. We therefore need to convert the trees into a sequence of (state, transition) pairs that we use for training. That part has already been implemented in the function get_training_instances(dep_structure). Given a DependencyStructure instance, this method returns a list of (State, Transition) pairs in the format described above. Your task will be to convert the input/output pairs into a representation suitable for the neural network. You will complete the method get_input_representation(self, words, pos, state) in the class FeatureExtractor. The constructor of the class FeatureExtractor takes the two vocabulary files as inputs (file objects). It then stores a word-to-index dictionary in the attribute word_vocab and POS-to-index dictionary in the attribute pos_vocab. Implement get_input_representation(self, words, pos, state), tt takes as parameters a list of words in the input sentence, a list of POS tags in the input sentence and an instance of class State. It should return an encoding of the input to the neural network, i.e. a single vector. Write also get_output_representation(self, output_pair), which takes a (transition, label) pair as parameter and return a one-hot representation of these actions. Question 3: Designing and Training the network. Network topology Now that we have training data, we can build the actual neural net. In the file train_model.py, write the function build_model(word_types, pos_types, outputs). word_types is the number of possible words, pos_types is the number of possible POS, and outputs is the size of the output vector. Start by building a network as follows:\n\nOne Embedding layer, the input_dimension should be the number possible words, the input_length is the number of words using this same embedding layer. This should be 6, because we use the 3 top-word on the stack and the 3 next words on the buffer. \nThe output_dim of the embedding layer should be 32.\nA Dense hidden layer of 100 units using relu activation. (note that you want to Flatten the output of the embedding layer first).  \nA Dense hidden layer of 10 units using relu activation. \nAn output layer using softmax activation.\nFinally, the method should prepare the model for training, in this case using categorical crossentropy  as the loss and the Adam optimizer with a learning rate of 0.01. Complete the build_model function that takes in word_types, pos_types, outputs and returns the model. Question 4: Greedy Parsing Algorithm - Building and Evaluating the Parser. We will now use the trained model to construct a parser. In the file decoder.py, take a look at the class Parser. The class constructor takes the name of a keras model file, loads the model and stores it in the attribute model. It also uses the feature extractor from question 2. The algorithm is the standard transition-based algorithm. As long as the buffer is not empty, we use the feature extractor to obtain a representation of the current state. We then call model.predict(features) and retrieve a softmax actived vector of possible actions. Write the method parse_sentence(self, words, pos), which takes as parameters a list of words and POS tags in the input sentence. The method will return an instance of DependencyStructure.",
  "Homework 4. Question 1: Candidate Synonyms from WordNet. In this assignment you will work on a lexical substitution task, using, WordNet, pre-trained Word2Vec embeddings, and BERT. This task was first proposed as a shared task at SemEval 2007 Task 10\n\nIn this task, the goal is to find lexical substitutes for individual target words in context. For example, given the following sentence:\n\n\"Anyway , my pants are getting tighter every day .\" \n\nthe goal is to propose an alternative word for tight, such that the meaning of the sentence is preserved. Such a substitute could be constricting, small or uncomfortable.\n\nIn the sentence\n\n\"If your money is tight don't cut corners .\" \n\nthe substitute small would not fit, and instead possible substitutes include scarce, sparse, limitited, constricted. You will implement a number of basic approaches to this problem and compare their performance. Write the function get_candidates(lemma, pos) that takes a lemma and part of speech ('a','n','v','r' for adjective, noun, verb, or adverb) as parameters and returns a set of possible substitutes. To do this, look up the lemma and part of speech in WordNet and retrieve all synsets that the lemma appears in. Then obtain all lemmas that appear in any of these synsets. Make sure that the output does not contain the input lemma itself. The output can contain multiword expressions such as \"turn around\". WordNet will represent such lemmas as \"turn_around\", so you need to remove the _. Question 2: WordNet Frequency Baseline. Write the function wn_frequency_predictor(context) that takes a context object as input and predicts the possible synonym with the highest total occurence frequency (according to WordNet). Note that you have to sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets. You can use the get_candidates method or just duplicate the code for finding candidate synonyms (this is possibly more convenient). Using this simple baseline should give you about 10% precision and recall. Take a look at the output to see what kinds of mistakes the system makes. Write the function wn_frequency_predictor(context) that takes a context object as input and predicts the possible synonym with the highest total occurence frequency (according to WordNet). Quesiton 3: Simple Lesk Algorithm. To perform WSD, implement the simple Lesk algorithm. Look at all possible synsets that the target word apperas in. Compute the overlap between the definition of the synset and the context of the target word. You may want to remove stopwords (function words that don't tell you anything about a word's semantics). Implement the function wn_simple_lesk_predictor(context). This function uses Word Sense Disambiguation (WSD) to select a synset for the target word. It should then return the most frequent synonym from that synset as a substitute. Question 4: Most Similar Synonym. You will now implement approaches based on Word2Vec embeddings. These will be implemented as methods in the class Word2VecSubst. The reason these are methods is that the Word2VecSubst instance can store the word2vec model as an instance variable. The constructor for the class Word2VecSubst already includes code to load the model. You may need to change the value of W2VMODEL_FILENAME to point to the correct file. Write the method predict_nearest(context) that should first obtain a set of possible synonyms from WordNet (either using the method from part 1 or you can rewrite this code as you see fit), and then return the synonym that is most similar to the target word, according to the Word2Vec embeddings. Question 5: Using BERT's masked language model. BERT is trained on a masked language model objective, that is, given some input sentence with some words replaced with a [MASK] symbol, the model tries to predict the best words to fit the masked positions. In this way, BERT can make use of both the left and right context to learn word representations (unlike ELMo and GPT).\n\nWe are going to use a pre-trained BERT model. Typically, BERT is used to compute contextualized word embeddings, or a dense sentence representation. In these applications, the masked LM layer, which uses the contextualized embeddings to predict masked words, is removed and only the embeddings are used.  However, we are going to use the masked language model as-is.\n\nThe basic idea is that we can feed the context sentence into BERT and it will tell us \"good\" replacements for the target word. The output will be a vector of length |V| for each word, containing a score for each possible output word. There are two possible approaches: We can either leave the target word in the input, or replace it with the [MASK] symbol. lexub_xml.Context works as follows: Obtain a set of candidate synonyms (for example, by calling get_candidates), Convert the information in context into a suitable masked input representation for the DistilBERT model. Make sure you store the index of the masked target word (the position of the [MASK] token), Run the DistilBERT model on the input representation, Select, from the set of wordnet derived candidate synonyms, the highest-scoring word in the target position (i.e. the position of the masked word). Return this word. Write the method predict(context) in the class BertPredictor, where context is an instance of lexsub_xml. Question 6: Other ideas?. By now you should have realized that the lexical substitution task is far from trivial. Implement your own refinements to the approaches proposed above, or even a completely different approach. Any small improvement (or conceptually interesting approach) will do to get credit for part 6.",
  "Homework 5. Part 1: Image Encodings (14 pts)\nThe files Flickr_8k.trainImages.txt Flickr_8k.devImages.txt Flickr_8k.testImages.txt, contain a list of training, development, and test images, respectively. Let's load these lists.\n\ndef load_image_list(filename):\n    with open(filename,'r') as image_list_f: \n        return [line.strip() for line in image_list_f]    \ntrain_list = load_image_list(os.path.join(FLICKR_PATH, 'Flickr_8k.trainImages.txt'))\ndev_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.devImages.txt'))\ntest_list = load_image_list(os.path.join(FLICKR_PATH,'Flickr_8k.testImages.txt'))\nLet's see how many images there are\n\nlen(train_list), len(dev_list), len(test_list)\nEach entry is an image filename.\n\ndev_list[20]\nThe images are located in a subdirectory.\n\nIMG_PATH = os.path.join(FLICKR_PATH, \"Flickr8k_Dataset\")\nWe can use PIL to open the image and matplotlib to display it.\n\nimage = PIL.Image.open(os.path.join(IMG_PATH, dev_list[20]))\nimage\nif you can't see the image, try\n\nplt.imshow(image)\nWe are going to use an off-the-shelf pre-trained image encoder, the Inception V3 network. The model is a version of a convolution neural network for object detection. The model requires that input images are presented as 299x299 pixels, with 3 color channels (RGB). The individual RGB values need to range between 0 and 1.0. The flickr images don't fit.\n\nnp.asarray(image).shape\nThe values range from 0 to 255.\n\nnp.asarray(image)\nWe can use PIL to resize the image and then divide every value by 255.\n\nnew_image = np.asarray(image.resize((299,299))) / 255.0\nplt.imshow(new_image)\nnew_image.shape\nLet's put this all in a function for convenience.\n\ndef get_image(image_name):\n    image = PIL.Image.open(os.path.join(IMG_PATH, image_name))\n    return np.asarray(image.resize((299,299))) / 255.0                     \nplt.imshow(get_image(dev_list[25]))\nNext, we load the pre-trained Inception model.\n\nimg_model = InceptionV3(weights='imagenet') # This will download the weight files for you and might take a while.\nimg_model.summary() # this is quite a complex model. \nThis is a prediction model,so the output is typically a softmax-activated vector representing 1000 possible object types. Because we are interested in an encoded representation of the image we are just going to use the second-to-last layer as a source of image encodings. Each image will be encoded as a vector of size 2048.\n\nWe will use the following hack: hook up the input into a new Keras model and use the penultimate layer of the existing model as output.\n\nnew_input = img_model.input\nnew_output = img_model.layers[-2].output\nimg_encoder = Model(new_input, new_output) # This is the final Keras image encoder model we will use.\nLet's try the encoder. At this point, you may want to add a GPU to the VM you are using (if not using already).\n\nencoded_image = img_encoder.predict(np.array([new_image]))\nencoded_image\nTODO: We will need to create encodings for all images and store them in one big matrix (one for each dataset, train, dev, test). We can then save the matrices so that we never have to touch the bulky image data again.\n\nTo save memory (but slow the process down a little bit) we will read in the images lazily using a generator. We will encounter generators again later when we train the LSTM. Write the following generator function, which should return one image at a time. img_list is a list of image file names (i.e. the train, dev, or test set). The return value should be a numpy array of shape (1,299,299,3).\n\ndef img_generator(img_list):\n    #...\n    yield #...\nNow we can encode all images (this takes a few minutes).\n\nenc_train = img_encoder.predict_generator(img_generator(train_list), steps=len(train_list), verbose=1)\nenc_train[11]\nenc_dev = img_encoder.predict_generator(img_generator(dev_list), steps=len(dev_list), verbose=1)\nenc_test = img_encoder.predict_generator(img_generator(test_list), steps=len(test_list), verbose=1)\nIt's a good idea to save the resulting matrices, so we do not have to run the encoder again.\n\n# Choose a suitable location here, please do NOT attempt to write your output files to the shared data directory. \nOUTPUT_PATH = \"hw5output\" \nif not os.path.exists(OUTPUT_PATH):\n    os.mkdir(OUTPUT_PATH)\nnp.save(os.path.join(OUTPUT_PATH,\"encoded_images_train.npy\"), enc_train)\nnp.save(os.path.join(OUTPUT_PATH,\"encoded_images_dev.npy\"), enc_dev)\nnp.save(os.path.join(OUTPUT_PATH,\"encoded_images_test.npy\"), enc_test). Part 2: Text (Caption) Data Preparation (14 pts)\nNext, we need to load the image captions and generate training data for the generator model.\n\nReading image descriptions\nTODO: Write the following function that reads the image descriptions from the file filename and returns a dictionary in the following format. Take a look at the file Flickr8k.token.txt for the format of the input file. The keys of the dictionary should be image filenames. Each value should be a list of 5 captions. Each caption should be a list of tokens.\n\nThe captions in the file are already tokenized, so you can just split them at white spaces. You should convert each token to lower case. You should then pad each caption with a START token on the left and an END token on the right.\n\ndef read_image_descriptions(filename):    \n    image_descriptions = defaultdict(list)    \n    # ...\n    return image_descriptions\ndescriptions = read_image_descriptions(f\"{FLICKR_PATH}/Flickr8k.token.txt\")\nprint(descriptions[dev_list[0]])\nRunning the previous cell should print:\n[['<START>', 'the', 'boy', 'laying', 'face', 'down', 'on', 'a', 'skateboard', 'is', 'being', 'pushed', 'along', 'the', 'ground', 'by', 'another', 'boy', '.', '<END>'], ['<START>', 'two', 'girls', 'play', 'on', 'a', 'skateboard', 'in', 'a', 'courtyard', '.', '<END>'], ['<START>', 'two', 'people', 'play', 'on', 'a', 'long', 'skateboard', '.', '<END>'], ['<START>', 'two', 'small', 'children', 'in', 'red', 'shirts', 'playing', 'on', 'a', 'skateboard', '.', '<END>'], ['<START>', 'two', 'young', 'children', 'on', 'a', 'skateboard', 'going', 'across', 'a', 'sidewalk', '<END>']] \n\nCreating Word Indices\nNext, we need to create a lookup table from the training data mapping words to integer indices, so we can encode input and output sequences using numeric representations. TODO create the dictionaries id_to_word and word_to_id, which should map tokens to numeric ids and numeric ids to tokens.\nHint: Create a set of tokens in the training data first, then convert the set into a list and sort it. This way if you run the code multiple times, you will always get the same dictionaries.\n\nid_to_word = #...\nword_to_id = #...\nword_to_id['dog'] # should print an integer\nid_to_word[1985] # should print a token\nNote that we do not need an UNK word token because we are generating. The generated text will only contain tokens seen at training time. Part 3: Basic Decoder Model (24 pts)\nFor now, we will just train a model for text generation without conditioning the generator on the image input.\n\nThere are different ways to do this and our approach will be slightly different from the generator discussed in class.\n\nThe core idea here is that the Keras recurrent layers (including LSTM) create an \"unrolled\" RNN. Each time-step is represented as a different unit, but the weights for these units are shared. We are going to use the constant MAX_LEN to refer to the maximum length of a sequence, which turns out to be 40 words in this data set (including START and END).\n\nmax(len(description) for image_id in train_list for description in descriptions[image_id])\nIn class, we discussed LSTM generators as transducers that map each word in the input sequence to the next word. Instead, we will use the model to predict one word at a time, given a partial sequence. For example, given the sequence [\"START\",\"a\"], the model might predict \"dog\" as the most likely word. We are basically using the LSTM to encode the input sequence up to this point. To train the model, we will convert each description into a set of input output pairs as follows. Note that we are using a Bidirectional LSTM, which encodes the sequence from both directions and then predicts the output. Also note the return_sequence=False parameter, which causes the LSTM to return a single output instead of one output per state.\n\nNote also that we use an embedding layer for the input words. The weights are shared between all units of the unrolled LSTM. We will train these embeddings with the model.\n\nMAX_LEN = 40\nEMBEDDING_DIM=300\nvocab_size = len(word_to_id)\n\n# Text input\ntext_input = Input(shape=(MAX_LEN,))\nembedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)\nx = Bidirectional(LSTM(512, return_sequences=False))(embedding)\npred = Dense(vocab_size, activation='softmax')(x)\nmodel = Model(inputs=[text_input],outputs=pred)\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n\nmodel.summary()\nThe model input is a numpy ndarray (a tensor) of size (batch_size, MAX_LEN). Each row is a vector of size MAX_LEN in which each entry is an integer representing a word (according to the word_to_id dictionary). If the input sequence is shorter than MAX_LEN, the remaining entries should be padded with 0.\n\nFor each input example, the model returns a softmax activated vector (a probability distribution) over possible output words. The model output is a numpy ndarray of size (batch_size, vocab_size). vocab_size is the number of vocabulary words.\n\nCreating a Generator for the Training Data\nTODO:\n\nWe could simply create one large numpy ndarray for all the training data. Because we have a lot of training instances (each training sentence will produce up to MAX_LEN input/output pairs, one for each word), it is better to produce the training examples lazily, i.e. in batches using a generator (recall the image generator in part I).\n\nWrite the function text_training_generator below, that takes as a paramater the batch_size and returns an (input, output) pair. input is a (batch_size, MAX_LEN) ndarray of partial input sequences, output contains the next words predicted for each partial input sequence, encoded as a (batch_size, vocab_size) ndarray.\n\nEach time the next() function is called on the generator instance, it should return a new batch of the training data. You can use train_list as a list of training images. A batch may contain input/output examples extracted from different descriptions or even from different images.\n\nYou can just refer back to the variables you have defined above, including descriptions, train_list, vocab_size, etc.\n\nHint: To prevent issues with having to reset the generator for each epoch and to make sure the generator can always return exactly batch_size input/output pairs in each step, wrap your code into a while True: loop. This way, when you reach the end of the training data, you will just continue adding training data from the beginning into the batch.\n\ndef text_training_generator(batch_size=128):\n    pass\n    # ... \nTraining the Model\nWe will use the fit_generator method of the model to train the model. fit_generator needs to know how many iterator steps there are per epoch.\n\nBecause there are len(train_list) training samples with up to MAX_LEN words, an upper bound for the number of total training instances is len(train_list)*MAX_LEN. Because the generator returns these in batches, the number of steps is len(train_list) * MAX_LEN // batch_size\n\nbatch_size = 128\ngenerator = text_training_generator(batch_size)\nsteps = len(train_list) * MAX_LEN // batch_size \nmodel.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=10)\nContinue to train the model until you reach an accuracy of at least 40%.\n\nGreedy Decoder\nTODO Next, you will write a decoder. The decoder should start with the sequence [\"<START>\"], use the model to predict the most likely word, append the word to the sequence and then continue until \"<END>\" is predicted or the sequence reaches MAX_LEN words.\n\ndef decoder():\n    # ...\n    return []\nprint(decoder())\nThis simple decoder will of course always predict the same sequence (and it's not necessarily a good one).\n\nModify the decoder as follows. Instead of choosing the most likely word in each step, sample the next word from the distribution (i.e. the softmax activated output) returned by the model. Take a look at the np.random.multinomial function to do this.\n\ndef sample_decoder():\n    #...\n    return []\nYou should now be able to see some interesting output that looks a lot like flickr8k image captions -- only that the captions are generated randomly without any image input.\n\nfor i in range(10): \n    print(sample_decoder()). Part 4: Conditioning on the Image (24 pts)\nWe will now extend the model to condition the next word not only on the partial sequence, but also on the encoded image.\n\nWe will project the 2048-dimensional image encoding to a 300-dimensional hidden layer. We then concatenate this vector with each embedded input word, before applying the LSTM.\n\nHere is what the Keras model looks like:\n\nMAX_LEN = 40\nEMBEDDING_DIM=300\nIMAGE_ENC_DIM=300\n\n# Image input\nimg_input = Input(shape=(2048,))\nimg_enc = Dense(300, activation=\"relu\") (img_input)\nimages = RepeatVector(MAX_LEN)(img_enc)\n\n# Text input\ntext_input = Input(shape=(MAX_LEN,))\nembedding = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_LEN)(text_input)\nx = Concatenate()([images,embedding])\ny = Bidirectional(LSTM(256, return_sequences=False))(x) \npred = Dense(vocab_size, activation='softmax')(y)\nmodel = Model(inputs=[img_input,text_input],outputs=pred)\nmodel.compile(loss='categorical_crossentropy', optimizer=\"RMSProp\", metrics=['accuracy'])\n\nmodel.summary()\nThe model now takes two inputs:\n\na (batch_size, 2048) ndarray of image encodings.\na (batch_size, MAX_LEN) ndarray of partial input sequences.\nAnd one output as before: a (batch_size, vocab_size) ndarray of predicted word distributions.\n\nTODO: Modify the training data generator to include the image with each input/output pair. Your generator needs to return an object of the following format: ([image_inputs, text_inputs], next_words). Where each element is an ndarray of the type described above.\n\nYou need to find the image encoding that belongs to each image. You can use the fact that the index of the image in train_list is the same as the index in enc_train and enc_dev.\n\nIf you have previously saved the image encodings, you can load them from disk:\n\nenc_train = np.load(f\"{OUTPUT_PATH}/encoded_images_train.npy\")\nenc_dev = np.load(f\"{OUTPUT_PATH}/encoded_images_dev.npy\")\ndef training_generator(batch_size=128):\n    #...\n    yield #...\nYou should now be able to train the model as before:\n\nbatch_size = 128\ngenerator = training_generator(batch_size)\nsteps = len(train_list) * MAX_LEN // batch_size \nmodel.fit_generator(generator, steps_per_epoch=steps, verbose=True, epochs=20)\nAgain, continue to train the model until you hit an accuracy of about 40%. This may take a while. I strongly encourage you to experiment with cloud GPUs using the GCP voucher for the class.\n\nYou can save your model weights to disk and continue at a later time.\n\nmodel.save_weights(f\"{OUTPUT_PATH}/model.h5\")\nto load the model:\n\nmodel.load_weights(f\"{OUTPUT_PATH}/model.h5\")\nTODO: Now we are ready to actually generate image captions using the trained model. Modify the simple greedy decoder you wrote for the text-only generator, so that it takes an encoded image (a vector of length 2048) as input, and returns a sequence.\n\ndef image_decoder(enc_image): \n    # ...\n    \nAs a sanity check, you should now be able to reproduce (approximately) captions for the training images.\n\nplt.imshow(get_image(train_list[0]))\nimg_decoder(enc_train[0])\nYou should also be able to apply the model to dev images and get reasonable captions:\n\nplt.imshow(get_image(dev_list[1]))\nimg_decoder(enc_dev[0])\nFor this assignment we will not perform a formal evaluation.\n\nFeel free to experiment with the parameters of the model or continue training the model. At some point, the model will overfit and will no longer produce good descriptions for the dev images. Part 5: Beam Search Decoder (24 pts)\nTODO Modify the simple greedy decoder for the caption generator to use beam search. Instead of always selecting the most probable word, use a beam, which contains the n highest-scoring sequences so far and their total probability (i.e. the product of all word probabilities). I recommend that you use a list of (probability, sequence) tuples. After each time-step, prune the list to include only the n most probable sequences.\n\nThen, for each sequence, compute the n most likely successor words. Append the word to produce n new sequences and compute their score. This way, you create a new list of n*n candidates.\n\nPrune this list to the best n as before and continue until MAX_LEN words have been generated.\n\nNote that you cannot use the occurence of the \"<END>\" tag to terminate generation, because the tag may occur in different positions for different entries in the beam.\n\nOnce MAX_LEN has been reached, return the most likely sequence out of the current n.\n\ndef img_beam_decoder(n, image_enc):\n   \n       #...\nbeam_decoder(3, dev_list[1])\nTODO Finally, before you submit this assignment, please show 5 development images, each with 1) their greedy output, 2) beam search at n=3 3) beam search at n=5.",
  "Ungraded Exercise: Naive Bayes. Consider the following training corpus of emails with the class labels ham and spam. The content of each email has already been processed and is provided as a bag of words.\n\nEmail1 (spam): buy car Nigeria profit\nEmail2 (spam): money profit home bank\nEmail3 (spam): Nigeria bank check wire\nEmail4 (ham): money bank home car\nEmail5 (ham): home Nigeria fly\n\na) Based on this data, estimate the prior probability for a random email to be spam or ham if we don't know anything about its content, i.e. P(Class)?\n\nb) Based on this data, estimate the conditional probability distributions for each word given the class, i.e. P(Word | Class). You can write down these distributions in a table. \n\nc) Using the Naive Bayes' approach and your probability estimates, what is the predicted class label for each of the following emails?\n\nNigeria\nNigeria home\nhome bank money",
  "Ungraded Exercise: n-gram language models. Part 1\n\nAssume a bigram language model is trained on the following corpus of sentences.\n\nSTART dog bites human END\nSTART dog bites duck END\nSTART dog eats duck END\nSTART duck bites human END\nSTART human eats duck END\nSTART human eats salad END\n\nAssume maximum likelihood estimates (MLE) with add-one smoothing (also called Laplace smoothing) were used to compute bigram probabilities. What is the probability assigned to the following sentences by the bigram model?\n\na) dog eats salad\n\nb) human bites dog\n\nPart 2  (optional and more difficult)\n\nShow that, if you sum up the probabilities of all sentence of length n under a bigram language model, this sum is exactly 1 (i.e. the model defines a proper probability distribution). Assume a vocabulary size of V. Hint: Use induction over the the sentence length. \n\nComment: This property actually holds for any m-gram model, but you only have to show it for bigrams.\n\n",
  "Ungraded Exercise: HMM - Viterbi & Forward Algorithm. The following state transition diagram and the emission probabilities below specify a Hidden Markov Model (HMM): starts-> V: 1/2, start->N: 1/2, N->N: 1/4, N->V: 2/4, N->Prep: 1/4, V->N: 2/3, V->Prep: 1/3, Prep->N: 1, P(time|N) = 2/4\nP(flies|N) = 1/4\nP (leaves|N ) = 1/4\n\n\nP(time|V ) = 1/6 \nP(flies|V ) = 2/6\nP (leaves|V ) = 1/6\nP (like|V ) = 2/6\n\nP(like|Prep) = 1\n\n\na) Use the Viterbi algorithm to compute the most likely sequence of hidden states (POS tags) for the observed sequence \"time flies like leaves\" (surface words).\n\nb) Use the Forward algorithm to compute the probability of the surface sequence \"time flies like leaves\".\n\nc) Can you write out the parameters of a trigram model that assigns the same probability distribution to a set of surface sequences as the HMM above? Is this always possible?",
  "Ungraded Exercise: CKY algorithm. Consider the following grammar in Chomsky Normal form:\n\nS -> NP VP\nS -> NP V\nNP -> D N\nVP -> V Adv\nVP -> V NP\n\nNP -> rain\nN -> rain\nN -> rains\nV -> rain\nV -> rains\nD -> the\nAdv -> down\n\na) Show how the CKY algorithm for membership checking would parse the sentence \"the rain rains down\" by filling a 5x5 table with rows and columns from 0-4. b) Now consider a version of CKY that also records backpointers. Write down all parse trees for the sentence according to the grammar.",
  "Ungraded Exercise: Transition-based dependency parsing. a)  Assuming an arc-standard transition system, write down the sequence of transitions that the oracle algorithm discussed in class would produce for the following gold dependency tree: root (pred)->sent, sent (nsubj)->he, sent(jobj)->her, sent(advmod)->today, sent(dobj)->meme, meme (det)->a, meme (amod)->funny. Start at the following initial state:\n\n([root ]  , [he, sent, her, a, funny, meme, today ]  , {} A )\n\nAlso write down the state resulting from each transition. \n\nb) Show that arc-standard sequences are not unique, that is there are multiple transitions that result in the same dependency tree.",
  "Ungraded Exercise: Linear Models and Feature Functions. In this problem we will use a linear model for Part-of-Speach (POS) tagging. The model predicts the POS tag t_i for each word w_i given the word itself, the previous word w_i-1, and the POS of the previous word t_i-1. To predict the POS for the word at position i, the model evaluated t_i* = argmax_t_i((w_i-1, w_i, t_i-1, t_i)v=argmax_t_i(sum_j(_j(w_i-1, w_i, t_i-1, t_i))v_j, where  is a feature function defined over the input words, previous tag, and a candidate output POS tag t_i, and v is a weight vector. Consider the following training data:\n\nSentence 1: time/N flies/V like/P flies/N\nSentence 2: fruit/N flies/N like/V fruit/N\nSentence 3: flies/N time/V fruit/N\nTODO: \n\na) Define  by specifying the conditions under which each dimension of the feature vector is 1. \n\nb)  Define a weight vector v so that the model assigns the correct POS to all tokens in the training corpus.  \n\nc) More advanced: define the parameters of a log-linear model to make the correct POS prediction for each token in the training data.",
  "Ungraded Exercise: IBM Model 2. Consider the following parallel training corpus.\n\nExample #\tEnglish sentence E\tForeign sentence F\n1\ta frog ate\tqta brk e\n2\tthe bird ate\tqta rwk et\n3\tthe bird ate a fly\tskwk e qta skwk et\n4\ta frog ate a fly\tskwk e qta brk e\n5\ta bird ate the frog\tbrk et qta skwk e\n \n\n1) Looking at the examples, can you figure out which words correspond to each other and create an E<->F dictionary.  Note: there is a case of lexical divergence in this dataset.\n\n2) Specify t(f |e) and q(j|i, l, m) parameter values for IBM model 2 that maximize P(F,A|E) for all training pairs shown in the table above. Do not specify parameters with value 0. Assume that there is only one \"correct\" alignment -- that is the one you determined in part 1).\n\n3) Using your parameters, compute the probability P(F|E) for training example 3.",
  "Ungraded Exercise: PCFGs and Trigram Models. Recall that a PCFG assigns a probability P(t) to each parse tree t. Let T(s) be the set of possible parse trees for a sentence s, according to some PCFG. Then the probability of the sentence is the sum of all tree probabilities: P(s)=sum_t in T(s)(P(t)). Consider the following PCFG\n\nS -> NP VP; 1\nVP -> V NP; 1\nNP -> Alex; 3/4\nNP -> Kim; 1/4\nV -> saw; 1\n\na) Specify the parameters of a trigram language model that assigns the same sentence probabilities as the PCFG to all sentences generated by the PCFG. (Note that each sentence in the trigram model must be terminated by a STOP marker, which does not appear in the sentences generated by the PCFG).\n\nb) Is it always possible to create a trigram language model that computes the same probability distribution over sentences as a given PCFG? Explain why or why not.",
  "Ungraded Exercise: Viterbi Algorithm. Recall the Viterbi algorithm that solves the decoding problem for HMMs. This algorithm computes the maximum joint probability of the token sequence w and any POS tag sequence (hidden state sequence). It uses a 2-dimensional table pi.\n\nInput: An input sequence w, consisting of tokens w[1] ... w[n]\n//initialization\nInitialize pi[0,start] = 1.0\nFor each tag t in T, initialize pi[0,t] = 0.0\n\n//main loop\nfor k = 1 ... n:\n    for each tag t in T:\n      pi[k,t] = max_s pi[k-1, s] * P(t|s) * P(w[k] | t)\nreturn max_s pi[n, s]\nThere are at most |T|^n sequences of tags. However, not all tags t are possible for each time step k, either because the emission probability P(w[k]|t) = 0 or because some transition probabilities P (t|s) = 0.\nModify the Viterbi algorithm so that it computes the number of possible tag sequences for the observed token sequence that result in a non-0 joint probability.",
  "Ungraded Exercise: Graph-Based Dependency Parsing. In class (and in homework 3), we discussed transition-based dependency parsing. An alternative approach is graph-based dependency parsing.\n\n\nIn graph-based dependency parsing, we are given an input sentence [root, w1, w2, . . . , wm]. For each wi, our goal is to identify the correct parent word in the dependency tree by computing a score score(i,j) for each possible parent wj , as well as a score score(i, root). We then select the parent with the maximal score (we are ignoring dependency labels in this task).\n\n\nAssume we first use BERT to convert the input sentence into a sequence of d-dimensional word representations [hroot, h1, h2, . . . , hm].\nWe then compute score(i, j)=(h_i^TW)h_j where W is a dxd weight matrix. a) Is this approach guaranteed to always produce a single tree? Can the model generate trees that the\ntransition-based model cannot generate? If so, which?\n\nb) How could you use the scores to compute the probability P(head = j|dependent =i)\n\nc) Assume we are given some training corpus of N sentences, each annotated with a dependency tree. Define a training objective for the scoring model described above.",
  "Syllabus. Course Description\nThis course provides an introduction to the field of Natural Language Processing (NLP). We will discuss properties of human language at different levels of representation (morphology, syntax , semantics, pragmatics), and will learn how to create systems that can analyze, understand, and generate natural language. We  will introduce core NLP techniques for language modeling, tagging, parsing, and word-sense disambiguation. We will  also discuss applications such as machine translation and image caption generation. We will study machine learning methods used in NLP, such as various forms of Neural Networks. We will discuss ethical aspects of NLP research and applications. Homework assignments  will consist of programming projects in Python.\n\nPrerequisites\nData Structures (COMS 3134 or COMS 3137), and Discrete Math (COMS 3203). Some experience programming Python and background in probability/statistics and Linear Algebra is helpful. Some previous or concurrent exposure to AI and machine learning is beneficial, but not required. \n\nMain Course topics\n\nLinguistics background: Levels of linguistic representation. Ambiguity. Diversity in human languages.\nMachine Learning techniques for NLP (supervised machine learning, bayesian models, hidden markov models, neural networks, RNNs, transformers).\nLanguage modeling. \nTagging models (part-of-speech tagging, named-entity recognition).\nLinguistic representations, language complexity, and grammar formalisms. \nSyntactic analysis (dependency and constituency parsing).\nLexical semantics, distributed word representations (embeddings), word-sense-disambiguation.\nComputational semantics. Semantic parsing.\nMachine translation. \nEthics, Bias, and Fairness in NLP.\nSupplemental Textbooks\nYou do not have to buy any textbooks for this class. The following books are available online. From time to time I will post additional research papers and reading material on Courseworks.\nDaniel Jurafsky, James H. Martin, Speech and language processing, 2nd edition. Prentice Hall. 2009.\nA draft of the third edition is available here: https://web.stanford.edu/~jurafsky/slp3/ed3book.pdfLinks to an external site.\n\nYoav Goldberg, Neural Network Methods for Natural Language Processing. Morgan & Claypool (Synthesis Lectures on Human Language Technologies). 2017\nAvailable as an e-book through the Columbia library (https://clio.columbia.edu/catalog/13676351)\n\nRequirements and Grading\nYour course grade will be based on 5 programming assignments, lowest score dropped (60%, 15% each), a final exam (30%), and participation (online participation on Zoom and Ed counts) 10%. \n\nFinal Exam\nThursday 8/10, 4:10pm. 180min. Closed notes / closed book. You may bring one double-sided letter-sized \"cheat sheet\". Calculators recommended.\n\nHomework Policy\nYou will have approximately 1 week to complete each homework assignment. There is no late deadline\nExtensions will be granted for emergencies only. There will not be any opportunities for extra credit in this course. \nRegrade requests will only be accepted up to 72 hours after homework grades are released on Courseworks.\n\nTentative schedule of topics:\n\nDate\tTopics\nThu 7/6\tIntroduction and course overview. Levels of linguistic representation. Ambiguity.\n\n\nFri 7/7\nmakeup date for 7/4! Zoom only\nProbability review and machine learning basics.\nText classification, Naive Bayes.\nn-gram language models, smoothing. \n\nTue 7/11\t\n\nSequence labeling. Hidden Markov models (HMMs). Part-of-speech tagging. Named-entity recognition. \n\nSyntax and grammar. Context-free grammars (CFGs).\n\n \n\nThu 7/13\t\nParsing with CFG and PCFGs (probabilistic CFGs): CKY.\n\nDependency structures. Transition-based Dependency Parsing. \n\nTue 7/18\t\nMachine Learning for NLP. Feature functions. Log-linear models.\n\nIntroduction to Neural Networks.\n\nThu 7/20\t\nVector-based lexical semantics: Distributed representations and word embeddings (word2vec). Pretraining. Neural Language Models.\n\nTue 7/25\t\nWordNet and Words Sense Disambiguation.\nRecurrent Neural Networks and LSTMs. Contextualized embeddings. ELMo. \n\nThu 7/27\t\nMachine Translation, Statistical MT and Neural MT.\n\nAttention. \n\nTue 8/1\t\nTransformer based models, BERT, GPT and applications.\n\nThu 8/3\t\nSentence-level Semantics: Semantic Role Labeling (FrameNet, PropBank).  Compositionality and logical forms. Abstract Meaning Representation.\n\nTA-led review session for the final exam TBA\n\nTue 8/8\t\nMultimodal NLP / Language and Vision\nSummary and Review. \n\nThu 8/10\t\nFinal Exam\n\n \n\nAttendance Policy, Classroom Interaction\nAttendance, in person or online with instructor approval, is expected for all sessions and is reflected in the participation grade. The instructor understands that there may be extenuating circumstances that prevent you from attending all sessions. If you have to miss a session (or part of it), you are responsible for catching up on the material. \n\nDuring class, you are expected to behave professionally, respectful and courteous to all course participants. You will use your time in class time most effectively if you fully participating in the class by asking questions and engaging in classroom activities.\n\nIf you are participating on zoom, keep your camera turned on and keep yourself muted. Ask questions in the chat. From time-to time, I may unmute individual participants. I will  also occasionally use polls and ask that you participate using the yes/no/hands up functionality.\n\nDisability-Related Accommodations\nTo receive disability-related academic accommodations for this course, students must first be registered with their school Disability Services (DS) office. Detailed information is available online for both the Columbia and Barnard registration processes. Refer to the appropriate website for information regarding deadlines, disability documentation requirements, and drop-in hours (Columbia)/intake session (Barnard).\n\nFor this course, students registered with the Columbia DS office can refer to the Courses that do not require professor signature section of the DS Testing Accommodations page for more information about accessing their accommodations.\n\nAcademic Honesty Policy\nIt is important that you read and understand this section. Any form of academic misconduct will result in a homework or exam grade of zero and can potentially be reported to the appropriate office.\n\nInteraction With Other Students: All homework assignments must be solved individually. You are encouraged to discuss problems with others, but when you sit down to code up your solution you must work on your own, without any further interaction. You are not allowed to share your solutions (literal code and theory solutions) with other students or other groups. \n\nOnline Material: Treat coding problems like essay assignments: You are not permitted to copy any part of other peoples work without attribution. This applies to code produced by other students and to material found on the internet. The problems in this course are designed to be solved with the course materials only. However, sometimes online sources (for instance Stackoverflow) can be useful as a reference. If you have to use code snippets found online you must attribute your source in a comment (complete link). You are not allowed to copy non-trivial code fragments from these sources.\n\nCode is non-trivial if either of the following applies: \n\nAny code that directly solves the the assignment.\nAny code you do not fully understand.\nCode longer than three lines.\nIf we find that you copied a source with attribution, but you used it to solve part of the problem, we will deduct points, but we will not treat this as an example of academic misconduct. \n\nNote that you may use any code provided in the course materials and in the textbook without attribution. \n\nIn addition to this policy, the CS departments academic honesty policy, as well as the individual policies of your school applies to this course.\n\nAI-Tools/LLMs: The use of language model based AI to complete programming assignments is not permitted in this course, unless specified as part of an assignment.\n\nCampus Resources\nThe instructor is committed to promoting students' well being and advancing an inclusive and welcoming campus culture. He is aware that students may experience personal, social, or financial challenges, whether related or unrelated  to their coursework, that may affect their health and academic performance. In addition, the high levels of stress experienced by many Columbia students may affect their mental and physical health. These effects may be exacerbated during the Covid-19 pandemic.\nIf you are in need of support, you are encouraged to reach out to your school's adviser. If you feel comfortable notifying the instructor, he will make every effort to provide support and connect you to available Columbia resources.\n\nIf you or someone you know feels overwhelmed or suffers from depression or anxiety, please contact\n\nCounseling and Psychological Services (CPS, Columbia) - 212-853-2878\nFurman Counseling Center (Barnard) - 212-854-2092\nFor additional campus resources, see https://universitylife.columbia.edu/student-resources-directory."
]