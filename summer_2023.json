{
    "0": {
        "title": "Final Exam Solution",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Will the final exam solution be provided so that we can check our deduction?</paragraph><paragraph/><paragraph>Thanks</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Most rubric items will contain an explanation for the correct/expected answers. I won\u2019t be able to provide a complete sample solution.\u00a0</paragraph></document>"
        ]
    },
    "1": {
        "title": "Will our participation grade also be added to Canvas?",
        "category": "General",
        "question": "<document version=\"1.0\"><paragraph>I was wondering whether our participation grade would also be added to Canvas, or if the grade we currently see with the exam taken into account already includes that</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The participation grade is not included on Courseworks currently. I can post these tomorrow (essentially, unless there is no record of you ever participating in the course, you will get 100% of the participation score.)</paragraph><paragraph>\u00a0Note that you need to weigh the different scores according to the formula on the rubric to compute your final score.\u00a0</paragraph></document>"
        ]
    },
    "2": {
        "title": "Final Exam Grade",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>When can we expect to get our final exam grades back?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Grading is currently a little more than halfway complete. We're hoping to finish this week.</paragraph></document>"
        ]
    },
    "3": {
        "title": "HW5 Part 3, count punctuation in input/output pairs?",
        "category": "Homework 5",
        "question": "<document version=\"2.0\"><paragraph>In the example input/output sequence resulting from the sequence:</paragraph><paragraph>['&lt;START&gt;', 'a', 'black', 'dog', '.', '&lt;END&gt;'] </paragraph><paragraph>It seems the period token ('.') is skipped as the last input/output pair shown consists of:<break/>input = [<code>START</code>,<code>a</code>, <code>black</code>, <code>dog</code>]</paragraph><paragraph>output = END</paragraph><paragraph>Is this a typo? Should this input/output pair be:</paragraph><paragraph>input = [<code>START</code>,<code>a</code>, <code>black</code>, <code>dog</code>]</paragraph><paragraph>output = .</paragraph><paragraph>and then an additional input/output pair would be the following?</paragraph><paragraph>input = [<code>START</code>,<code>a</code>, <code>black</code>, <code>dog</code>, .]</paragraph><paragraph>output = END</paragraph><paragraph/><paragraph>Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes the missing . is a typo.\u00a0</paragraph></document>"
        ]
    },
    "4": {
        "title": "HW5 Part1: Creating Encodings",
        "category": "Homework 5",
        "question": "<document version=\"2.0\"><paragraph>In HW5 Part 1, it says \"We will need to create encodings for all images and store them in one big matrix\". </paragraph><paragraph>Does this mean we should be encoding an image with <code>img_encoder.predict()</code> , and then resizing it to 1x299x299x3, since it returns a 2048 size vector? Or does \"create encodings\" mean just resizing the image to be 299x299x3, and then returning that \"encoding\" of the image in required shape? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I think the first one (resizing it to 1x299x299x3), as the Model.predict_generator function waas asking for a 4-dimensional array. </paragraph><paragraph/></document>"
        ]
    },
    "5": {
        "title": "Model Accuracy",
        "category": "Homework 5",
        "question": "<document version=\"2.0\"><paragraph>Should we decrease/increase the number of epochs so that the model should finish training around 40%?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The accuracy should be at least 40% for reasonable results. If that takes more epochs, it\u2019s okay to increase the number of epochs.\u00a0</paragraph></document>"
        ]
    },
    "6": {
        "title": "HW5 Submission",
        "category": "Homework 5",
        "question": "<document version=\"2.0\"><paragraph>My machine does not process the pdf downloading command on anaconda jupyter, so I printed the html to get pdf instead. </paragraph><paragraph>And HW5 asks for pdf submission, but coursework only allows compressed files. Shall we submit a link containing the pdf, or will a compressed pdf file be fine?</paragraph><paragraph>Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You should submit a zip file that contains both the notebook and the PDF.</paragraph></document>"
        ]
    },
    "7": {
        "title": "Ungraded Exercise:IBM model",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>regarding the second part of the question, why are all q=1, is it always the case?</paragraph><paragraph>q(3 | 1, 3,3) = 1<break/>q(2 | 2, 3,3) = 1<break/>q(1 | 3, 3,3) = 1<break/>q(5 | 1, 5, 5) = 1<break/>q(4 | 2, 5, 5) = 1<break/>q(3 | 3, 5, 5) = 1<break/>q(2 | 4, 5, 5) = 1<break/>q(1 | 5, 5, 5) = 1</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I think it's because there's no other way to map the alignments one you have some already known</paragraph></document>"
        ]
    },
    "8": {
        "title": "Secure Exam Proctor",
        "category": "Exam",
        "question": "<document version=\"1.0\"><paragraph>Do people who take exam via Zoom today also have to set this up? \u00a0I can\u2019t find an announcement regarding this.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>No, students taking the exam on Zoom will have real-time proctoring (i.e. I or one of the TAs will be watching you take the exam). </paragraph></document>"
        ]
    },
    "9": {
        "title": "Semantic Parsing",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>I just want to make sure, I see that there's a <code>(*)</code> next to the Semantic Parsing topic on the exam topic overview. Would all of the concepts under it not be tested in the exam?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>that's correct, none of these topics will be tested on the exam. Note that this only applies to full-sentence semantic parsing, not semantic role labeling. </paragraph></document>"
        ]
    },
    "10": {
        "title": "Constraints in RNN based SRL",
        "category": "General",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/2lRHU9hFhUeyquQQdnXF0Ros\" width=\"658\" height=\"466.32534678436315\"/></figure><paragraph><break/>How are these constraint penalty scores c(w,y_1:t) computed?<break/>They need to be differentiable</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Good question. I tried to find this in the He et al paper, but they simply say <break/>\"Each constraint function c applies a non-negative penalty given the input w and a length-t prefix<break/>y1:t\"</paragraph><paragraph>https://aclanthology.org/P17-1044/</paragraph></document>"
        ]
    },
    "11": {
        "title": "How is a Parse Tree Path be encoded into a log-linear model as a feature?",
        "category": "General",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/C8LHe3dSviaAooWnkDwI0LzW\" width=\"658\" height=\"497.69642857142856\"/></figure><paragraph>One way is to one-hot encode over all possible paths upto some reasonable length, say 10. <break/><break/><break/>Is there another way such paths are encoded to form a feature in practice?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>You would one-hot encode the possible paths. Note that you would only instantiate the features that are actually seen during training, so you wouldn't have to create a dimension for each possible path -- only for the ones that have been observed. </paragraph></document>"
        ]
    },
    "12": {
        "title": "Cheat sheet",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>I was wondering whether the cheat sheet needs to be handwritten? Or can it be printed?</paragraph><paragraph>Thanks</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>It can be handwritten or typed, but it must be a physical copy.</paragraph></document>"
        ]
    },
    "13": {
        "title": "Feature functions",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hi , In this solution there is a Start symbol and so can we assume that there is an end symbol as well?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XO2SCW2WMOhkJIYbpEXgvwJM\" width=\"658\" height=\"451.05426356589146\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Sure. </paragraph></document>"
        ]
    },
    "14": {
        "title": "Bonus Ungraded Exercise: Viterbi Algorithm",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For this exercise I was wondering if I am interpreting the solution correctly. So is the reason that we are returning the sum over states (<bold>sum_s</bold>) rather than the max (<bold>max_s</bold>) so that we can consider multiple paths rather than the most likely path? In what use case would quantifying the possible tag sequences be relevant?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Your understanding is correct. </paragraph><paragraph>I can't really think of a specific use case -- maybe you want to get an estimate of how ambiguous a sentence is. The number of possible POS sequences might be a good proxy for that. </paragraph></document>"
        ]
    },
    "15": {
        "title": "Dummy Exam",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hello -- has the gradescope dummy exam been posted yet?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I still get an error when I click on gradescope that I have no account, so I think not? Unless something is wrong on my end</paragraph><paragraph/><figure><image src=\"https://static.us.edusercontent.com/files/pMtBLI8rLmpofrlF7bA7enZh\" width=\"698\" height=\"129.1971153846154\"/></figure><paragraph/><paragraph>I also can't make an account without a course ID</paragraph></document>",
            "<document version=\"2.0\"><paragraph>It's available now. Please do NOT try to add the course using a course ID, but instead use the link on Courseworks to access Gradescope. </paragraph></document>"
        ]
    },
    "16": {
        "title": "Gradescope Dummy Exam",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>HI Professor Bauer/TAs, </paragraph><paragraph>I was wondering when the dummy exam will be posted? When I clicked on Gradescope via coursework, this course was not added yet. </paragraph><paragraph>THanks! </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Same situation here; I assume it is not activated yet.</paragraph></document>",
            "<document version=\"2.0\"><paragraph>#283 </paragraph><paragraph/></document>"
        ]
    },
    "17": {
        "title": "Ungraded exercise - CKY Does order of nonterminals matter?",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>If we have grammar:</paragraph><paragraph>S -&gt; NP V<break/>VP -&gt; V NP</paragraph><paragraph/><paragraph>Wanted to confirm: does the order matter? If we have a \"NP V\" can it only be S, or will it be S and VP?</paragraph><paragraph>Thanks,</paragraph><paragraph>Vidur</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>#81</paragraph></document>"
        ]
    },
    "18": {
        "title": "Ungraded exercise - bigram",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Why does END symbol gets added to the 6 words but not the START symbol even though they are both given in the corpus of sentences? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/GyhuhOKd8threv3SJ84eWN9N\" width=\"658\" height=\"139.42118863049095\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>#14</paragraph></document>"
        ]
    },
    "19": {
        "title": "Bigram Lang Model",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Would it be possible to just explain in words why this base case works? I am just trying to get the big picture. Also what does the p and r represent? </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/dfGs4VTJfHdNv4ZhdpyXgCc0\" width=\"658\" height=\"440.86659979939816\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Does <link href=\"https://edstem.org/us/courses/40443/discussion/3259587\">this</link> help?, P() is the probability, q and r are words for arbitrary indices in a sentence. Here, assumed q occurs before r.</paragraph><paragraph/></document>"
        ]
    },
    "20": {
        "title": "CVN asynchronous format",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>For CVN students taking the exam asynchronously, will we be filling out our answers on Courseworks, or do we need to print out and scan answers?<break/><break/>Additionally, do we need to print out the \"cheat sheet\" or can we keep it alongside Proctorio when taking the exam? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You need to print out the pdf, or write your answers on a different set of sheets if necessary. Then scan the solution and upload to Courseworks.\u00a0</paragraph><paragraph>The cheat sheet should be a physical paper, not a pdf.\u00a0</paragraph></document>"
        ]
    },
    "21": {
        "title": "N-Gram exercise",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>In the solutions, <bold><underline>P(eats | dog)= (1+1)/(3+7) = 1/5</underline></bold> </paragraph><paragraph>is it (1+1) for 'eats' because 'dog' is only followed by 'eats' 1 time? I was under the impression that it was the number of occurrences of the word + 1 for smoothing. For n-grams, are we looking for the number of occurrences of the n-gram phrase? Such as \"Dog eats\" only appears once hence the (1+1)?</paragraph><paragraph><italic>dog bites human</italic></paragraph><paragraph><italic>dog bites duck</italic></paragraph><paragraph><italic>dog eats duck</italic></paragraph><paragraph><italic>duck bites human</italic></paragraph><paragraph><italic>human eats duck</italic></paragraph><paragraph><italic>human eats salad</italic></paragraph><paragraph>a) <italic>dog eats salad</italic></paragraph><pre>P(dog | START)= (3+1)/(6+7) = 4/13\n<bold><underline>P(eats | dog)= (1+1)/(3+7) = 1/5</underline></bold>\nP(salad | eats)= (1+1)/(3+7) = 1/5\nP(END | salad)= (1+1)/(1+7) = 1/4\nP(dog eats salad) =\nP(dog | START) P(eats | dog) P(salad | eats) P(END | salad) =\n4/13 x 1/5 x 1/5 x 1/4 = 1/325 = 0.0031</pre></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>That\u2019s correct. The context \u201cdog\u201d (which appears three times in total) is followed only once by \u201ceats\u201d. Therefore the raw probability would be 1/3 and with add-one smoothing (1+1)/(3+7).\u00a0</paragraph></document>"
        ]
    },
    "22": {
        "title": "Parameters of a trigram model (ungraded Viterbi exercise)",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi!</paragraph><paragraph>For the ungraded exercise on the Viterbi &amp; forward algorithms, I was confused by  c): \"...write out the parameters of a trigram model that assigns the same probability distribution to a set of surface sequences...\". Are the parameters in question the probabilities for any one transition (i.e. N-&gt;N, N-&gt;V, etc.). If so, would a trigram model require more nodes (since the transition probability depends on the bigram context, so N-&gt;V would need to be a single node, and so on)?  Hope this makes sense! </paragraph><paragraph><break/></paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>So the trigram model should be a set of trigram probabilities defined over words, not POS tags.\u00a0<break/>Yet, it should assign the same probability to any surface word sequence that the HMM does.\u00a0</paragraph></document>"
        ]
    },
    "23": {
        "title": "will topics from the last lecture be on the final exam?",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>^^ since I do not see them on the review sheet... </paragraph><paragraph>Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Pretty sure the professor said they will not be on the exam</paragraph></document>"
        ]
    },
    "24": {
        "title": "Question regarding soltuion of ungraded exercise: PCFGs and trigram models",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I don't quite understand the calculations for:</paragraph><blockquote>P(Alex | Alex saw) = 3/4<break/>P(Kim | Alex saw) = 1/4<break/>P(Alex | Kim saw) = 3/4<break/>P(Kim | Kim saw) = 1/4</blockquote><paragraph>Could you please show how it was calculated? Thanks.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I believe there are four possible sentences based on the given PCFG:</paragraph><paragraph>s1: \"START START Alex saw Kim STOP\"</paragraph><paragraph>s2: \"START START Kim saw Alex STOP\"</paragraph><paragraph>s3: \"START START Alex saw Alex STOP\"</paragraph><paragraph>s4: \"START START Kim saw Kim STOP\"</paragraph><paragraph>After \"Alex saw\" or \"Kim saw\" for each sentence, the word will be either \"Alex\" (probability = 3/4) or \"Kim (probability = 1/4).</paragraph></document>"
        ]
    },
    "25": {
        "title": "Question regarding different languages",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph/><paragraph>I had quick question regarding different languages and using it in MT. In unsupervised learning, is there a way, that the MT can detect a brand new language (which it never encountered before) and learn it based on the structure and how repetitive some words may be? And, in that case, is there some NLP problem which we can devise which creates a \"new language\" based on what the model thinks is relevant. </paragraph><paragraph>Essentially, I am trying to understand the relevancy of a new language vs. new types of sentences/words. Is the syntax of a language something that can be picked up?</paragraph><paragraph>Thanks,</paragraph><paragraph>Vidur</paragraph></document>",
        "comments": [],
        "answers": []
    },
    "26": {
        "title": "TA led review",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Was just wondering if the TA led review was posted on Courseworks yet. Thanks!!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>There was no TA led review this semester. Sorry.\u00a0</paragraph></document>"
        ]
    },
    "27": {
        "title": "Questions from old exams which are not used",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I think in a previous lecture, professor had mentioned he may post some questions from old exams which are not going to be used anymore. I was wondering if there was a way to get them, as was not able to find it.</paragraph><paragraph>Thanks!</paragraph><paragraph>Vidur</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>These have been posted in the assignments section, labeled \u201cbonus\u201d questions.\u00a0</paragraph></document>"
        ]
    },
    "28": {
        "title": "Final Exam CVN Synchronous Zoom Link",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Maybe I missed this but could anyone please inform which link are CVN students using for the synchronous final exam?</paragraph><paragraph>Thank you in advance!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The regular class link, but I will send out a reminder tomorrow.\u00a0</paragraph></document>"
        ]
    },
    "29": {
        "title": "ungraded exercise- IBM model 2",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>for the IBM model 2 question part 3, </paragraph><paragraph>why is it q(5 | 1, 5, 5) * t(skwk | fly) not q(1 | 5, 5, 5) * t(skwk | fly)?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>From my understanding, the parameters are: q(j|i,l,m) which correspond to alignment value <italic>i</italic> takes value <italic>j</italic> given source sentence length <italic>l</italic> and foreign <italic>m</italic>.</paragraph><paragraph>q(5 | 1, 5, 5) = q(fly|skwk, 5, 5)</paragraph><paragraph>I think there might be a typo in the last one:</paragraph><paragraph>q(5 | 1, 5, 5) * t(et | the) should actually be: q(1| 5, 5, 5)* t(et | the)</paragraph></document>"
        ]
    },
    "30": {
        "title": "Knowledge of Databases like WordNet, VerbNet, etc",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>For the exam, will we need to have memorized some of the internal structures of these large databases mentioned in class? Or just generally know what they are used for and what information they hold?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You should know the structure, for example the various lexical relations recorded in wordnet, as well as what a synset is etc.\u00a0<break/></paragraph></document>"
        ]
    },
    "31": {
        "title": "log-linear model exercise",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Dear Prof. and TAs, still confused about the solutions in Ungraded Exercise Solution: Linear Models and Feature Functions.</paragraph><paragraph>For part a), it seems that the feature function consider the unique combinations of <bold>Current Word and its Candidate POS Tag, Previous word and its POS Tag .</bold></paragraph><paragraph>According to the training data, Unique words (V): time, flies, like, fruit (Total: 4)</paragraph><paragraph>Unique POS tags (T): N, V, P (Total: 3). Therefore, The total number of unique combinations for the feature function \u03a6 based on the provided training data is much more than 8 </paragraph><paragraph><underline>My question is: How we decide which example to give?</underline></paragraph><paragraph>For part b), is the assigned vectors are arbitary or by calculatio; If calculate, would you mind showing the process?</paragraph><paragraph>For the weight vector, my understanding is it ensures the dot product between v and the feature vector for each word and its correct tag is maximized, leading to the correct POS assignment for all tokens in the training corpus. </paragraph><paragraph><underline>My question is: do we just assign arbitrarily a high weight like 1, then the dimension is depending on the number of functions in 1)?</underline></paragraph><paragraph>For part c), For a log linear model, </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/nQKj6XqlUuk6mBXHNjs4qTuW\" width=\"520\" height=\"63\"/></figure><paragraph>Then how are we expected to show the parameters, i.e., the vector v? Can you please give a sample~</paragraph><paragraph>Thank you!</paragraph></document>",
        "comments": [],
        "answers": []
    },
    "32": {
        "title": "Interesting application of GPT2 for Personalized Recommendation",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I personally work on recommendations and ranking use GPT-4 not only to generate high quality relevance labels at scale but also generate feature scores to be fed into a more light-weight transformer at production, generate queries and more. LLMs/GPT models have many new applications in the field of recommendations. This paper is an interesting application of using GPT for query generation given item sequences a user has interacted with.<break/><break/><link href=\"https://arxiv.org/abs/2304.03879\">[2304.03879] GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation (arxiv.org)</link> </paragraph><paragraph/></document>",
        "comments": [
            "<document version=\"2.0\"><paragraph>Wow! So cool!! </paragraph><paragraph>Just quick question -- is there a way here, or on what you work with, for spam recommendations to get filtered out?</paragraph><paragraph/><paragraph> I.e. I am assuming that new responses are used to \"re-train\" further recommendations like in pre-training. I guess there may be malicious people who want to wrongly influence the models, such as maybe a company recommending its products and \"inserting itself\" into the model through user input. Is there a way we have devised to prevent that or at least detect that in some way? </paragraph></document>"
        ],
        "answers": []
    },
    "33": {
        "title": "IBM Model 2 Parameter",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>In part 3 of the last ungraded exercise, why is the last q(a_i | i) term q(5|1,5,5) and not q(1|5,5,5)?  </paragraph><paragraph>For reference the solution has:</paragraph><paragraph>q(5 | 1, 5, 5) * t(skwk | fly) *<break/>q(4 | 2, 5, 5) * t(e| a) *<break/>q(3 | 3, 5, 5) * t(qta | ate) *<break/>q(2 | 4, 5, 5) * t(skwk | bird) *<break/>q(5 | 1, 5, 5) * t(et | the) = 2/3</paragraph><paragraph/><paragraph/><paragraph/><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Because the question asks specifically about example 3, which is \"the bird ate a fly\". </paragraph><paragraph><break/>So, for q(5 | 1,5,5), 5 indicates et(the). </paragraph><paragraph/></document>"
        ]
    },
    "34": {
        "title": "How to show arc standard sequences are not unique?",
        "category": "Exam",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/bWnGtX5z5Zowhcy74mdTogkz\" width=\"548\" height=\"320\"/></figure><paragraph>Could you please provide some hints on how to answer part b?<break/><break/>Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I think to prove b you would just need get the dependency tree in a different way from a, which you would do by using at least 1+ different transition to arrive at the same result. I think you would focus on trying get to some variation using arc-right vs arc-left as a strategy, which can be done by changing how you use shift.</paragraph></document>"
        ]
    },
    "35": {
        "title": "Spurious ambiguity?",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I just wasn't quite certain what this term meant as I'm going through the lecture notes. Is it referrig to a case where a sentence seems to be ambiguous but it is not? Or am I off base here</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Which lecture is this from? Thank you!</paragraph></document>",
            "<document version=\"2.0\"><paragraph>Correct me if I'm wrong, but I think it's when a parsing algorithm has multiple paths to get to the same state.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/ike2floAZOUOXQwmoZnRIYOZ\" width=\"494\" height=\"124\"/></figure></document>",
            "<document version=\"2.0\"><paragraph>I agree with Chris. I think it can happen when we are confused about meaning of sentence and this is probably a good queue to use some sort of semantic analysis and contextual clues.</paragraph></document>"
        ]
    },
    "36": {
        "title": "Training RNNs with (sampled,hierarchical, adaptive) Softmax",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I was curious about this and asked Bing Chat: <break/><break/>One of the challenges of training RNNs is that the output vocabulary can be very large, especially for natural language tasks. This means that the softmax layer, which computes the probability distribution over the output vocabulary, can be very expensive to compute. The softmax layer requires computing the dot product between the hidden state and each output word vector, and then applying the exponential function and normalization. This can be slow and memory-intensive, especially for long sequences.</paragraph><paragraph>There are several methods that have been proposed to make the training of RNNs with softmax more efficient. Some of these methods are:</paragraph><list style=\"unordered\"><list-item><paragraph><bold>Sampled softmax</bold>: This method reduces the computation by sampling a subset of the output vocabulary and computing the softmax only over this subset. The subset usually includes the true output word and some negative samples drawn from a predefined distribution. This way, the softmax layer only needs to compute a small number of dot products and exponentials, instead of the entire vocabulary. <code><link href=\"https://arxiv.org/abs/2203.16868\">Sampled softmax can greatly reduce memory consumption and still maintain accuracy</link></code> <link href=\"https://arxiv.org/abs/2203.16868\">1</link> <link href=\"https://arxiv.org/abs/2103.13076\">2</link>.</paragraph></list-item><list-item><paragraph><bold>Hierarchical softmax</bold>: This method organizes the output vocabulary into a tree structure, where each node represents a group of words or a single word. The softmax layer then computes the probability of each node along the path from the root to the true output word, instead of computing the probability of each word in the vocabulary. This way, the softmax layer only needs to compute a logarithmic number of dot products and exponentials, instead of a linear number. <link href=\"https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85\">Hierarchical softmax can speed up training and also handle large vocabularies3</link><link href=\"https://arxiv.org/pdf/1706.09528.pdf\">4</link>.</paragraph></list-item><list-item><paragraph><bold>Adaptive softmax</bold>: This method partitions the output vocabulary into several clusters based on their frequency, such that more frequent words are in smaller clusters and less frequent words are in larger clusters. The softmax layer then computes the probability of each cluster and each word within the cluster separately, using different parameters for each cluster. <link href=\"https://doi.org/10.48550/arXiv.2203.16868\">This way, the softmax layer can adapt to the distribution of the output vocabulary and reduce computation for more frequent words5</link>.</paragraph></list-item></list><paragraph/></document>",
        "comments": [],
        "answers": []
    },
    "37": {
        "title": "Ungraded exercise b) Transition-based dependency parsing",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>For the part b) of transition-based parsing ungraded exercise, does this require that we show another sequence for the same sentence, or use another sentence? Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I believe part b) is asking us to show another sequence for the same sentence. Please see #263.</paragraph></document>"
        ]
    },
    "38": {
        "title": "condition on l,m in the estimate for q",
        "category": "General",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/orxYhG6tBtBtzf4Msz4bECjr\" width=\"658\" height=\"498.29871645274216\"/></figure><paragraph>(1) Is l, length of the source sentence? and m, of the target sentence? <break/><break/>We condition on l,m when estimating \"how often does position i align with position j\" . I see why if one sentence is much longer than the other the indices that are aligned are different. But in general, for sentences of close-enough length which seems a reasonable assumption fot translations, is there still a need to condition on the sentence lengths being exactly the same?  </paragraph><paragraph><break/>(2)<break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/zL4Hjk3ApGQuzvxErM4mcPlV\" width=\"643\" height=\"62.27421236872812\"/></figure><paragraph>The denominator in this expression: Isn't this simply \"count of occurrences of position i such that source language sentence length is l and target language sentence length is m\"<break/><break/>Which (for any i &lt;=m) would simply be the number of sentences in the corpus such that source language sentence length is l and target language sentence length is m<break/>?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>1) m is the length of the source sentence (the F sentence) and l is the length of the target sentence (the E sentence). Even if two sentence pairs are close in length, they would be handled differently. This is obviously a very limiting assumption in IBM Model 2. </paragraph><paragraph>2) Yes, correct. The denominator is simply the number of all sentences with length less than m. </paragraph><paragraph/></document>"
        ]
    },
    "39": {
        "title": "LSTM loss: Softmax or Binary Classification with Negative Sampling?",
        "category": "General",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/dFYlBCbdABlcJQ4pu8RNonv6\" width=\"658\" height=\"483.7470355731225\"/></figure><paragraph>In practice, is Negative Sampling with Binary Classification loss (with Sigmoid) commonly used instead of softmax for training LSTMs efficiently ?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>As far as I'm aware, negative sampling is not commonly used when training RNNs. </paragraph></document>"
        ]
    },
    "40": {
        "title": "Are we allowed to get some blank sheets for calculations/rough work",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hi,<break/><break/>Are we allowed to bring some blank sheets or will they be provided at the exam for rough work/calculations/drawing Viterbi calculation graph /transition graphs etc. It would be very helpful during the exam.<break/><break/>Thank you</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>You will be provided blank sheets , however bring your calculators incase you need to do some calculations !</paragraph></document>"
        ]
    },
    "41": {
        "title": "Ungraded Exercise",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor Bauer,</paragraph><paragraph>I was wondering if you are going to post more ungraded exercises (e.g. AMR) on the later half of the semester? </paragraph><paragraph>Thanks! </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Sorry for that, I'm going to upload some more material tomorrow, but don't expect a ton of it. </paragraph></document>"
        ]
    },
    "42": {
        "title": "Ungraded Exercise: HMM - Viterbi & Forward Algorithm Confusion",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>For the forward algorithm, this is one of the equations in the solution:<break/>\u03c0[2,V] = \u03c0[1,N] \u00b7 P(V|N) \u00b7 P(flies |V) + \u03c0[1,V] \u00b7 P(N|V) \u00b7 P(flies|N)) = (1/4 \u00b72/4 \u00b7 2/6) + (1/12 \u00b7 <bold>0</bold> \u00b7 2/6)) = 1/24 = 0.04166</paragraph><paragraph>I am confused as to where the numbers for the second part of this equation (\u03c0[1,V] \u00b7 P(N|V) \u00b7 P(flies|N)) are coming from. I thought the equation would be \u03c0[2,V] = \u03c0[1,N] \u00b7 P(V|N) \u00b7 P(flies |V) + \u03c0[1,V] \u00b7 P(V|V) \u00b7 P(flies|V)), but perhaps I am understanding the forward algorithm wrong?</paragraph><paragraph>Thanks in advance</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I don't think you can go from V to V and that's why P(V|V) is not possible. I think the forward algorithm is the same as Viterbi but instead of taking the max we sum all the probabilities. </paragraph><paragraph/></document>"
        ]
    },
    "43": {
        "title": "Smoothing and Back-off",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Regarding the n-gram an data sparsity issues, do we have to master at applying the smoothing and back-off approaches (like discounting, Katz' Backoff) to refine the probability for the final exam?</paragraph><paragraph>Thank you~</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, you should understand the various methods in sufficient detail to be able to apply them to a (artificially small) data set. </paragraph></document>"
        ]
    },
    "44": {
        "title": "Ungraded exercise - viterbi algorithm",
        "category": "Exam",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/7tvWJEDATIWOVdUsppZZ3Jlg\" width=\"476\" height=\"236\"/></figure><paragraph>Hi,<break/><break/>Q1) What do the red arrows signify in this solution?<break/>Q2) Can we draw the diagram as above for exam instead of drawing the alternative table representation?<break/><break/><break/>Can you also provide the solution for part c please.<break/><break/>Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>For Q1, I believe that the red arrows indicate possible transitions from one state back to a previous state.</paragraph></document>"
        ]
    },
    "45": {
        "title": "CVN Student - Async Exam",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>In the past, I have gotten emails with instructions for taking the exam with Proctorio with notes from the instructor (timing, cheatsheet, etc..). Will this be sent out?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes. This will be sent out early next week.\u00a0</paragraph></document>"
        ]
    },
    "46": {
        "title": "What is this topic?",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>In the Exam topics post on courseworks, there is an incomplete topic heading \"Struc\" and I was wondering what this overarching topic might be? Thanks so much!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/72FwGzwtGBp213jx89HBAV1K\" width=\"658\" height=\"400.96875\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>That's a typo. You can safely ignore it.</paragraph></document>"
        ]
    },
    "47": {
        "title": "Ungraded Exercise : Linear Models and Feature Functions",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hi,<break/>I was going over the solution for the feature functions exercise, I am not sure I understand how to solve the problem. Could you please help me with the same since I am really confused?</paragraph><paragraph>1) How is the order in which you select the word wi decided? Since there are 3 sentences and the solution doesn't necessarily select words in any of the sentence order.<break/></paragraph><paragraph>2) Why is there no ti-1 for below feature function. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/IpydYUV96TJaeVexqnZ3Opbu\" width=\"658\" height=\"81.64963503649635\"/></figure><paragraph>3) Why do we have one wi with time, when time appears thrice in the three sentences with different parts of speech (N and V)</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/HhPkjycCNHZMfDj7S0IpHCgW\" width=\"658\" height=\"73.01677419354839\"/></figure><paragraph>Part b) The weight vector has all 1s because we want all the features to be true for correct POS tagging?<break/><break/>Part c) Can you please release the solution</paragraph><paragraph>Thank you.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I'm also struggling to understand this concept, so any further explanation would be very helpful. Thanks!</paragraph></document>",
            "<document version=\"2.0\"><paragraph>1) I don't believe it necessarily matters how the word w_i is being selected, as feature functions can be any type of function that takes into account some or all of the possible arguments (w_{i-1}, wi, t_{i-1}, and t_i in this case) and outputs 0 or 1.</paragraph><paragraph>2) This feature function is just an example. I believe that w_i = \"fruit\" and t_i = \"N\" is sufficient in this case because all examples of \"fruit\" in the training data are of the form \"fruit/N.\" In the case of \"flies,\" for example, where both \"flies/V\" and \"flies/N\" appear in the training data, the corresponding feature functions include additional information (e.g., w_{i-1}) that describe whether \"flies\" is being used as a noun or verb.</paragraph><paragraph>3) Like in the case of \"flies/N,\" in which there are two feature functions in the given solution (6, 7), we could have more than one w_i with time feature function if we chose to create one or more additional feature functions.</paragraph></document>",
            "<document version=\"2.0\"><paragraph>Seems like basically we need to get all highest amount of params possible that will hold true for all the given sentences. Hence, some have more parameters if they are all holding true and some are very limited (like unigram type)</paragraph></document>"
        ]
    },
    "48": {
        "title": "Final Exam - Calculator",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I believe this was covered in class already but just want to confirm whether or not we need a calculator for the exam? Is it required or just encouraged (to make our lives easier)?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>It's not a hard requirement, but a basic 4-function calculator may make some computations easier.</paragraph></document>"
        ]
    },
    "49": {
        "title": "Will we be able to see HW4 results before next week?",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi! Would it be possible to know HW4 results a little before the middle of next week, so that we know if we should try to submit HW5? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>We're hoping to finish before HW5's due date so that everyone can make an informed decision about HW5. We'll do our best to finish grading as fast as possible. :)</paragraph></document>"
        ]
    },
    "50": {
        "title": "Part 4 left and right context",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Do we need to consider left and right context for part 4 like we do in part 5, or is this something we can do for part 6? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Part 4 does not consider the left and right context at all, only the target word itself. So this is something you could incorporate in part 6. </paragraph></document>"
        ]
    },
    "51": {
        "title": "best_words in part 5",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Are we allowed to change the number of best words (from 10) returned from the bert predictor? And if the resulting list doesnt contain any candidate words, should we be returning nothing?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes and yes. \u00a0</paragraph></document>"
        ]
    },
    "52": {
        "title": "NLTK Library",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Are we allowed to use word_tokenize from the NLTK library?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes.</paragraph></document>"
        ]
    },
    "53": {
        "title": "Illegal division by 0 in score.pl",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>There are some final tweaks I'd like to make to improve my precision and recall, but there are times where I will make an edit, re-write the output to the predict file, and run perl, but then will get the error I have illegal division. </paragraph><paragraph>Even when I revert the code back to its original form, it will from thereon return an error. Is this happening for others and are there any ways to resolve this? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The reason is that the .predict file is formatted \u201cincorrectly\u201d. If you are running Python in windows, it writes out windows style line endings (CRLF). But the perl evaluation script expects Unix style line endings (CR).\u00a0<break/>Opening the file in a text editor and saving it again usually fixes the issue.\u00a0</paragraph></document>"
        ]
    },
    "54": {
        "title": "HW5 Data - Page Doesn't Exist",
        "category": "Homework 5",
        "question": "<document version=\"2.0\"><paragraph>Hi everyone!</paragraph><paragraph>I'm trying to download the data for HW5, but get the following error.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/giAYPCoGxIlwO8JiOivERY8v\" width=\"658\" height=\"146.069109947644\"/></figure><paragraph>I've tried downloading both through the requests package (I'm on Windows) and by visiting the site directly. Any help would be greatly appreciated!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Please try again, there was a permission issue that has now been fixed. </paragraph></document>",
            "<document version=\"2.0\"><paragraph>I think I was having the same issue but after using incognito tab, it worked. Might be a caching issue.</paragraph></document>"
        ]
    },
    "55": {
        "title": "precision part 3",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>What is the approximate accuracy we should expect from lesk algorithm? I am getting fairly low number and was unsure why.</paragraph><paragraph/><paragraph>Thanks</paragraph><paragraph>hamid</paragraph><paragraph>Seyedhamidreza Alaie</paragraph></document>",
        "comments": [],
        "answers": []
    },
    "56": {
        "title": "Bert vs Word2Vec Scores",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I am getting interesting results where my Word2Vec score is higher than Bert. Wondering if this could be possible or anyone has thoughts on what could be the issue:</paragraph><paragraph><bold>Word2Vec:</bold> </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/YiFyYIoNf1bXWhqu2qMmu9cQ\" width=\"658\" height=\"85.4825327510917\"/></figure><paragraph><bold>Bert:</bold></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/rnUJeKo4R2vEDAoK0miC5f1b\" width=\"658\" height=\"85.4825327510917\"/></figure><paragraph/><paragraph>Thanks!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>I am having the same issue with mine :/</paragraph></document>",
            "<document version=\"2.0\"><paragraph>Could you look at cases where the predictions disagrees and BERT gets it wrong. Then try to see out why ?  Sometimes it is because of incorrect masking index.</paragraph><paragraph>Check if word2vec candidates appear in the BERT lexicon? If it does, how far is the rank of candidates in the list of most probable substitutes ? </paragraph></document>"
        ]
    },
    "57": {
        "title": "Part 5 Error No module named 'keras.saving.hdf5_format'",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>If anyone comes across this error, you can comment in modeling_tf_utils.py in the transformers package in Lib/site-packages/transformers to fix it</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/lq8euOFsHuFSUxYCgwtSUlj6\" width=\"470\" height=\"65.94530321046373\"/></figure><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Great, thanks for sharing this with the cohort!</paragraph></document>"
        ]
    },
    "58": {
        "title": "Part2",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph/><paragraph>Part 2 says : Note that you have to sum up the occurence counts for all senses of the word if the word and the target appear together in multiple synsets. So if a lemma appears in 2 different synsets, do I have to count this lemma twice or do I have to consider them as 2 distinct lemma? </paragraph><paragraph>Also, What can cause a variability in the accuracy in part 2? I ran the same code twice and I got 2 different results: </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/lvBQ33BiXp7JT2hjiJVVdLgY\" width=\"658\" height=\"213.64691358024692\"/></figure><paragraph/><paragraph/><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>You would have to sum together the .count value for the two lexemes to get a single score for the lemma. </paragraph><paragraph>The result should always be the same for part 2-- this is a bit odd. </paragraph></document>"
        ]
    },
    "59": {
        "title": "Part 6 Computational Time",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Are there any computation restrictions for this assignment?</paragraph><paragraph>My solution for Part 6 takes about ~3.5 hours of computational time to complete. I have not been using GPU. Given that I also submit the actual part6.predict file, is this acceptable?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, as long as you document your approach in a comment in this case, since we won\u2019t actually be able to run it.\u00a0</paragraph></document>"
        ]
    },
    "60": {
        "title": "Difference and connection between lemma, lexeme, and word form",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>I just wanted to make sure that I understand the following terms correctly. </paragraph><paragraph>Lemma: is the base form of a word. (e.g. run)</paragraph><paragraph>Word form: is any modified or inflected version of the lemma ( e.g. runner, running, runs, etc.)</paragraph><paragraph>Lexeme: represents the core meaning and all related word forms of a word. This means it covers the lemma and its inflected forms (e.g. run ( base form) + runner, running, runs (inflected forms)) . </paragraph><paragraph>Is that true ? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, a lexeme is a word with a particular sense. There may be multiple lexemes for the same lemma. \u00a0</paragraph></document>"
        ]
    },
    "61": {
        "title": "part4 synonym not in word2vec model",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph><break/>Would like to ask what to do the synonym when the word2vec model doesn' t have its embeddings.</paragraph><paragraph>There are some synonym use multiple words and '-' to make a new word and those the gensim model could not find embeddings for them.<break/><break/>Thanks</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>I asked a similar question privately in Ed \u00a0and this is the response the professor provided: <break/>\u201c It\u2019s okay to ignore candidates that do not have a word2vec entry.\u00a0</paragraph><paragraph>One thing you could try is to replace the white space in multi word candidates with an underscore _.\u00a0</paragraph><paragraph>Another option would be to obtain word vectors for moving and picture and then average them.\u201d</paragraph><paragraph>\u00a0(The example I provided was a multi word \u201cmoving picture\u201d)</paragraph></document>"
        ]
    },
    "62": {
        "title": "hw4 pt 3 - repeating words in context/definition",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Just wondering having repeated overlapping words in context/def would mean for the overlap count. So for example:</paragraph><paragraph>Context: <italic>bank bank</italic></paragraph><paragraph>Definition: <italic>bank not</italic></paragraph><paragraph>Would the overlap here be one or two?</paragraph><paragraph>Context: <italic>bank not</italic></paragraph><paragraph>Definition: <italic>bank bank</italic></paragraph><paragraph>How about here?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>It's my understanding that you can treat the context and definition as a set - only one instance of the word is counted. For all the examples the overlap would be 1.</paragraph><paragraph/></document>"
        ]
    },
    "63": {
        "title": "Part 1 check",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>In part 1, do the order of the lemmas need to match the output in the assignment description? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I think as long as it has the same contents then it should be fine.</paragraph></document>"
        ]
    },
    "64": {
        "title": "Final Exam Content Inquiry",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hello Professor Bauer,</paragraph><paragraph>Would you be able to clarify the weightage of the final exam points, especially for the last two weeks' material? It's complex and some of us may find reviewing it challenging due to time constraints.</paragraph><paragraph>Thanks for your guidance.</paragraph></document>",
        "comments": [],
        "answers": []
    },
    "65": {
        "title": "Downloading GoogleNews faster",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>For anyone having trouble / waiting a long time to upload the GoogleNews file to colab or GCP (like I was):<break/><break/>pip install gdown<break/>gdown <link href=\"https://drive.google.com/u/1/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&amp;export=download\"><underline>https://drive.google.com/u/1/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&amp;export=download</underline></link><break/><break/><break/>source: <link href=\"https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive?page=1&amp;tab=scoredesc#tab-top\">https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive?page=1&amp;tab=scoredesc#tab-top</link></paragraph><paragraph> </paragraph><paragraph/></document>",
        "comments": [],
        "answers": []
    },
    "66": {
        "title": "Why is dependency parsing considered discriminative model?",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I remember professor mentioning that dependency parsing models are considered discriminative models and not generative. Could you please elaborate more on why this is the case? </paragraph><paragraph>Thankyou.</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>In transition based dependency parsing, the model predicts P(transition | state) directly, rather than using Bayesian inference to infer the best state.\u00a0</paragraph></document>"
        ]
    },
    "67": {
        "title": "part 5 errors",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi! For part 5, I keep getting errors like this, even when I try the verbose trick that was mentioned.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/FiyHmPvRDXLcyFxn6NjwVO9s\" width=\"259\" height=\"83\"/></figure><paragraph>Should I worry about it / will I get points off if I leave it like this? My part5.predict file looks like this:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/2KwXUIMgvhK4r4dZbeglPSzh\" width=\"386\" height=\"93\"/></figure><paragraph>Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>When you call predict, you can add the parameter verbose=False, which should suppress the output. </paragraph><paragraph>Alternatively, you can modify the code so that it writes your output directly into a text file, rather than printing to stdout and then piping it into a file. </paragraph></document>"
        ]
    },
    "68": {
        "title": "Missing solution for ungraded exercise: HMM - Viterbi & Forward Algorithm",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I noticed the solution for the ungraded exercise of HMM - Viterbi &amp; Forward algorithm contains only parts a and b. Could you maybe post the answer to part c as well?</paragraph><paragraph>Thank you!</paragraph></document>",
        "comments": [],
        "answers": []
    },
    "69": {
        "title": "Question about CFGs being context free",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>From the lecture slides - \"CFGs are context free: applicability of a rule depends only on the nonterminal symbol, not on its context.</paragraph><paragraph>\u2022 Therefore, the order in which multiple non-terminals in a partially derived string are replaced does not matter. We can represent identical derivations in a derivation tree.\"<break/><break/>However for HW3 and according to post #81, not considering the correct order for terminals would result in incorrect results.<break/><break/>So does this mean, <break/>NP \u2192 D N cannot be written as NP -&gt; N D<break/>but <break/>NP \u2192 NP PP - (1)  does it mean just that it is okay to either substitute NP or PP first . <break/>Or does it mean (1) can be written as NP -&gt; PP NP ? (which doesn't seem to be correct)<break/><break/>Please let me know if I am understanding this correctly.<break/><break/>Thank you in advance!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>When you have a rule NP \u2192 NP PP, it doesn't matter if you substitute NP or PP first in a derivation. The productions you can use to substitute NP depend only on the nonterminal itself, not on the context it appears in. </paragraph></document>"
        ]
    },
    "70": {
        "title": "Has anyone been able to get a VM w/ GPU in GCP recently?",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>If so, could you please share the zone that worked? I've tried several zones with no luck</paragraph></document>",
        "comments": [
            "<document version=\"2.0\"><paragraph>was able to get an L4 in us-central-a</paragraph></document>"
        ],
        "answers": [
            "<document version=\"2.0\"><paragraph>Please refer to the video recently posted on Courseworks (recorded by TA - Shreya) in Files -&gt; homework5 -&gt; hw5_gcp_tutorial.mp4</paragraph></document>"
        ]
    },
    "71": {
        "title": "part 3, part 6",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hello teaching staff, for part 6 I just want to confirm: in the previous post, it said that any interesting or creative solution will get full credit. Does this mean even without being the best predictor(in terms of the highest precision and recall) is fine?</paragraph><paragraph>And for part 3, I tried two ways to write the code, and seems like the result scores are different. For both ways, the simple lesk algorithm did not outperform the WordNet frequency baseline. I'm wondering if there is a standard answer to predictions in predict file for this part.</paragraph><paragraph>Thank you!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>That\u2019s right, for part 6 you do not necessity have to get the highest score.\u00a0</paragraph><paragraph>Not sure I understand your question about part 3. There are some valid modifications in part 3 that would all be in line with the specification leading to different results.\u00a0</paragraph></document>"
        ]
    },
    "72": {
        "title": "Identical results for Word2Vec and Bert predictors",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Both my Word2Vec and Bert predictors produce the exact same precision and recall results, but I'm not sure if this means that there's something wrong with my Bert predictor since the assignment handout states that it performed slightly better than the Word2Vec predictor in experiments?  I can't seem to find any bugs in the Bert predictor and I'm following the encoding method directly from the handout, but I'm not sure if some kind of optimization needs to be added for the Bert predictor to perform better to achieve full credit?  I've included the results output for both parts below for reference:</paragraph><pre>Total = 298, attempted = 298\nprecision = 0.115, recall = 0.115\nTotal with mode 206 attempted 206\nprecision = 0.170, recall = 0.170\n</pre></document>",
        "comments": [
            "<document version=\"2.0\"><paragraph>I get the exact same performance for both the models as well. According to post #182  I am guessing that should be okay.</paragraph><paragraph/></document>"
        ],
        "answers": [
            "<document version=\"2.0\"><paragraph>I have the same concern, but for what it's worth, I have the same output that you do for both. </paragraph><paragraph/><paragraph/></document>",
            "<document version=\"2.0\"><paragraph>My Bert predictor did outperform Word2Vec slightly and even if the scores are identical the predicted results should be different. Word2Vec will always give you the same synonym for the same lemma and pos regardless of context whereas Bert will give different answers. For instance, Word2Vec gives \"shining\" as the replacement for \"bright\" in the first two sentences whereas Bert gives \"promising\" and \"shiny\" respectively in my code. </paragraph><paragraph/></document>"
        ]
    },
    "73": {
        "title": "get_candidates()  input word includes underscore?",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Just wondering what should happen if an input to this function is a word that contains an underscore. So something like hot_dog for instance. What I'm doing right now includes \"hot dog\" as a candidate, but I realize that may not be totally correct. Should we account for this?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes. Replacing underscores with white space is actually the correct way to handle this.</paragraph></document>"
        ]
    },
    "74": {
        "title": "HW4: Part 6 - Refinements",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>\"In this part, you should implement your own refinements to the approaches proposed above, or even a completely different approach. \"</paragraph><paragraph>Does this mean that we can implement a refinement for any (i.e., at least one) of the four approaches from parts 2-5? Or we must implement a refinement for each of the approaches? </paragraph><paragraph>Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Just one new refinement , not refinement for each !</paragraph></document>"
        ]
    },
    "75": {
        "title": "Any test cases for Part 2?",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Can we get some test cases for part 2 <code>wn_frequency_predictor</code> ?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>I don\u2019t think I have any specific test cases. Can you report your performance measures?</paragraph></document>"
        ]
    },
    "76": {
        "title": "GPU setup in google cloud",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>I was trying to follow the video tutorial to set up my GPU in google cloud. However, there is \"tensorflow has resource level errors\" when setting up the deep learning VM. Anyone having any idea on how to deal with this?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yeah, it means that particular zone doesn't have enough resources. Try it with us-west4-a, worked for me</paragraph></document>"
        ]
    },
    "77": {
        "title": "Part 6",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>After going to OH I'm confused as to how can I possibly compute the similarity without using the gensim model from part 4 (when I tried that it took a very long time to load).<break/>Here's what I have so far:</paragraph><pre>def better_predict(self, context : Context) -&gt; str:\n        # Converts information of context into suitable masked input representation for DistilBERT model\n        masked_text = ' '.join(context.left_context + ['[MASK]'] + context.right_context)\n        input_toks = self.tokenizer.encode(masked_text)\n        indices = self.tokenizer.convert_ids_to_tokens(input_toks)\n        masked_index = indices.index('[MASK]')\n        input_mat = np.array(input_toks).reshape((1, -1))\n\n        # Runs the DistilBERT model on the input representation but only takes first 100 words\n        outputs = self.model.predict(input_mat, verbose=0)\n        predictions = outputs[0]\n        best_words = np.argsort(predictions[0][masked_index])[::-1] # Sorts in increasing order\n        best_words = self.tokenizer.convert_ids_to_tokens(best_words[:100])\n        \n        max_sim = -1\n        synonym = None\n        #for word in best_words:\n            # compute similarity between word and context.lemma, check if larger than max and replace\n            # max_sim and synonym accordingly\n        #return synonym\n        # Overall idea: have bert predict 100 best candidates and out of those select the one most similar to the target word\n        return \"\"\n</pre><paragraph>As mentioned in the comments, the idea is to iterate over best_words, and calculate each word's similarity with context.lemma, then return the maximum synonym found.</paragraph><paragraph>Any ideas will be appreciated. Thanks in advance!</paragraph></document>",
        "comments": [],
        "answers": []
    },
    "78": {
        "title": "Gradescope Course ID for Final",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Since the Final is being administered on gradescope for Zoom participants, can we get the gradescope course ID to get setup there in preparation? Thanks</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You won\u2019t need the course Id. Instead you will be able to access the exam through the Gradescope link on courseworks. I will post a dummy exam prior to the official exam so you can make sure everything works properly.\u00a0</paragraph></document>"
        ]
    },
    "79": {
        "title": "Homework Solutions",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>Are the solutions for any of the homeworks after Homework 1 going to be posted?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Solutions for HW2 and HW3 have been uploaded now. Please check the files section on the Courseworks page.</paragraph></document>"
        ]
    },
    "80": {
        "title": "Part 5 predict file",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>I ran part 5 in CPU, and it turns out results like :</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/3eOgnbiuP2hBGUMXOSvjMXjq\" width=\"426\" height=\"65\"/></figure><paragraph/><paragraph>Does anyone know how to get rid of the two-bar line? Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I was just told to try adding \"verbose=0\" when you call predict, i.e:</paragraph><pre>self.model.predict(input_mat, verbose=0)\n</pre><paragraph>And it seems to work!</paragraph></document>"
        ]
    },
    "81": {
        "title": "Lemma",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Do we need to consider the lower or upper case of the synonyms/lemma/word? Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Please use lowercase.</paragraph></document>"
        ]
    },
    "82": {
        "title": "HW4 Part 3 - Tie Breaking Logic Clarification",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph><italic>You would break the tie, so return one of the tied lexemes.</italic> </paragraph><paragraph><italic>There is actually a clever way to deal with this:<break/>First assign three values to each candidate lexeme &lt;synset s, lemma w&gt;</italic></paragraph><paragraph><italic>a) the overlap score for the s. <break/>b) the .count() (frequency) for &lt;s,t&gt; where t is the target lemma. <break/>c) the .count() (frequency) for &lt;s,w&gt;</italic> </paragraph><paragraph><italic>Then, compute an aggregate score like this 1000*a + 100*b + c</italic> </paragraph><paragraph><italic>Sort all candidates by this score and select the highest (excluding lemmas that are identical to the target). This will automatically break ties.</italic> </paragraph><paragraph><bold>1. Is the target lemma t = context.lemma?</bold></paragraph><paragraph><bold>2. Is lemma w in &lt;synset s, lemma w&gt; any of the possible lemmas in s?</bold></paragraph><paragraph>Thank you in advance!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes and yes.\u00a0</paragraph></document>"
        ]
    },
    "83": {
        "title": "SSL error/safely accessing Wordnet",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>When I run a Python interpreter and try to include the Wordnet package, I get the following SSL error: \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate.\"  </paragraph><paragraph>Some StackOverflow posts said to disable SSL checks, but can I confirm what is the best way to do this without totally disabling this? </paragraph></document>",
        "comments": [
            "<document version=\"2.0\"><paragraph>This sounds like you might be running into certificate errors when downloading the Wordnet package using Python. I don't know if you've already resolved this, but you could try installing a number of default Root Certificates for Python (assuming that you're on a Mac here).<break/><break/>Try running:</paragraph><pre>/Path-to-Python-Installation/Python\\ 3.9/Install\\ Certificates.command</pre><paragraph>If you have something other than 3.9 installed, swap that out. Usually the path is:</paragraph><pre>/Applications/Python</pre><paragraph>Hopefully this resolves it for you. Disabling SSL checks would be risky if you forget to turn them back on.</paragraph></document>"
        ],
        "answers": []
    },
    "84": {
        "title": "GCP VM",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph/><paragraph>Has anyone had any issues deploying a VM instance on GCP?</paragraph><paragraph>I get this resource error:</paragraph><pre>\"ResourceErrorMessage\":\"The zone 'projects/nlp-hw-394518/zones/us-west1-b' does not have enough resources available to fulfill the request. Try a different zone, or try again later.\"\n</pre><paragraph>I have tried a few different regions and also waited a day but still no luck.</paragraph><paragraph>Any advice would be appreciated.</paragraph><paragraph/><paragraph>Many Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Having the same problem...I did part of my homework over the weekend, trying to finish it, no resources available for the past 2 days, tried moving to different zone, change machine type etc none worked. My data is practically stuck with GCP with no ways to retrieve it.</paragraph><paragraph>I am just going to use lambda labs now: <link href=\"https://cloud.lambdalabs.com/instances\">https://cloud.lambdalabs.com/instances</link> at least it's a guarantee that you can make an instance, downside is 1) no persistent disk most of time (even if file system is available do save your data locally too otherwise when you need it the instance can attach disk might not be available) 2) you are paying out of pocket :(<break/></paragraph></document>",
            "<document version=\"2.0\"><paragraph>FWIW, I had this problem in a different class and using <code>us-central1-a</code> worked for me.</paragraph></document>",
            "<document version=\"2.0\"><paragraph>Same here, I have tried a few different zones but still getting the error. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/LkCpcwyAbcN6gDMhNRO7GBFk\" width=\"759\" height=\"133.62035928143712\"/></figure></document>"
        ]
    },
    "85": {
        "title": "Content for Final Reference/Cheat Sheet",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>I know we are allowed to bring a double-sided reference sheet in for the final. Is there some restriction in terms of what can be on it? For example: only formulas, etc.</paragraph><paragraph/><paragraph>Thanks</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>There are no content restrictions for the cheat sheets. As long as you limit yours to one double-sided letter-sized sheet, you can add whatever you want to it.</paragraph></document>"
        ]
    },
    "86": {
        "title": "Usage of tokenize(s) in part 3",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Not sure if anyone else had this, but when I used the tokenize(s) function in part 3, I got an error as the compiler didn't recognize the \"string\" in \"string.punctuation\". When I added \"import string\" at the top of the file it seems to solve it.</paragraph><paragraph>Hope a TA can confirm it's valid to add. Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, please import string. </paragraph></document>"
        ]
    },
    "87": {
        "title": "Part 1 return type",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi, for part 1 the description in Courseworks indicates we need to return a set of strings but the code indicates a list of strings; what is the expected return type?</paragraph><paragraph>Thank you </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>A set is the safer choice to make sure there are no duplicates.\u00a0</paragraph></document>"
        ]
    },
    "88": {
        "title": "HW4 part6  Extra pre-trained models or packages",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Dear Prof. and TAs, </paragraph><paragraph>For the part 6, if we want to try some ensemble learning methods or other improvement which would involve other packages or pre-trained models. Is this allowed? Thank you~</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, this is allowed. Just make sure to clearly comment on any required dependencies to run your code!</paragraph></document>"
        ]
    },
    "89": {
        "title": "Difference between semantic and lexical ambiguity",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi,<break/><break/>I was a little confused about the difference between semantic and lexical ambiguity.<break/><break/>I understand from the lectures that ambiguity can be broadly classified into structural and lexical ambiguity. Is semantic ambiguity a part of lexical ambiguity (and not structural), since there is confusion about the meaning interpretation?</paragraph><paragraph>Thank you!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I don't believe the lecture notes explicitly describe the relationship between lexical ambiguity and semantic ambiguity. I looked on the internet and it states that lexical ambiguity is a subtype of semantic ambiguity, which I think makes sense since the lecture defines lexical ambiguity as a single word (e.g., \"mouse\") that has two or more possible meanings and the latter as a group of words (e.g., \"Stolen painting found by tree\") that have two or more possible meanings. Hope this helps!</paragraph><paragraph/></document>"
        ]
    },
    "90": {
        "title": "HW4 part2 performance same as smurf",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Dear Prof. and TAs,</paragraph><paragraph>Here is my screenshot of prediction for part 2</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/AAOpc9LIL7L7dU82avTCM1Ju\" width=\"452\" height=\"1119\"/></figure><paragraph>However, when I try to get the scores, it turns out the same as what for smuf. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/NugRdcT48LEJ5I2cWVy9d93b\" width=\"643\" height=\"68.01425356339085\"/></figure><paragraph/><paragraph>Would you mind helping me to check whether my prediction is problematic, or the command to call for the scoring is wrong?</paragraph><paragraph>Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>This could be an issue with the file format, rather than your prediction. The perl script expects unix style file endings (LF only), while the file may have windows style line endings (CL-RF).</paragraph></document>"
        ]
    },
    "91": {
        "title": "Multiple Parallel Corpora Question",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi folks,</paragraph><paragraph>In terms of multiple parallel corpora, would using something like a multilingual Reuters dataset be sufficiently rigorous for the purposes of creating a corpus? </paragraph><paragraph>I recognize that creative license may mean that the translations aren't necessarily perfect across different language versions of the same article, but would the faithfulness and fluency criteria likely be met?</paragraph><paragraph>Thank you!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Sure that sounds like it would be a proper dataset for training an MT model.\u00a0</paragraph></document>"
        ]
    },
    "92": {
        "title": "HW 4 Part 5 sanity check",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>If my BERT model performs somewhere between the WN frequency model and W2V, is that a definitive sign that my model has an issue? I am using BERT with the [MASK] token in the input.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>I would expect the BERT model to work at least as well as the word2vec approach.\u00a0</paragraph></document>"
        ]
    },
    "93": {
        "title": "Question regarding DistilBERT tokenizer implementation",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Question regarding DistilBERT tokenizer implementation - \"<bold>The <code>encode</code> method tokenizes a string, then adds special symbols [CLS] and [SEP], and then encodes the input into a sequence of integers.</bold>\"</paragraph><paragraph>Can we assume that the input string has only alphabets? If they already consist of numbers, how should that be handled?<break/><break/>Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You can assume the tokenizer handles numeric tokens correctly. Specifically it uses a kind of subword tokenization \u2014 we will discuss in more detail on Tuesday.\u00a0</paragraph></document>"
        ]
    },
    "94": {
        "title": "Part 3 clarification",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph><bold>If this is the case (or if there is a tie), you should select the most frequent synset (i.e. the Synset with which the target word forms the most frequent lexeme, according to WordNet).</bold></paragraph><paragraph>So if there is a tie, do we pick the higher frequency one between the tie, or it's still the highest frequency from the entire set?</paragraph><paragraph>Also, it is possible for the most frequent synset or the synset with most overlap to contain only one lemma that is the target word itself. In this situation, what are we suppose to do? Find the next possible synset or just return nothing?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You would break the tie, so return one of the tied lexemes.\u00a0</paragraph><paragraph>There is actually a clever way to deal with this:<break/>First assign three values to each candidate lexeme &lt;synset s, lemma w&gt;</paragraph><paragraph>a) the overlap score for the s.\u00a0<break/>b) the .count() (frequency) for &lt;s,t&gt; where t is the target lemma.\u00a0<break/>c) the .count() (frequency) for &lt;s,w&gt;\u00a0</paragraph><paragraph>Then, compute an aggregate score like this 1000*a + 100*b + c\u00a0</paragraph><paragraph>Sort all candidates by this score and select the highest (excluding lemmas that are identical to the target). \u00a0This will automatically break ties.\u00a0<break/></paragraph></document>"
        ]
    },
    "95": {
        "title": "Part 6: Custom Predictor",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>I'm wondering if the expectation for Part 6 for Homework 4 is for us to create a new function in the existing Predictor class or to create a new custom predictor class itself. And I am curious what type of innovative refinements are expected?</paragraph><paragraph>Thanks in advance!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>A new function is sufficient. The custom predictor does not have to perform significantly better than the other approaches in the assignment. Any interesting or creative solution will get full credit.</paragraph></document>"
        ]
    },
    "96": {
        "title": "HW 4 submissions",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi, will submissions for HW 4 be opened soon? Thanks</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>It should be open now. Please try again.\u00a0</paragraph></document>"
        ]
    },
    "97": {
        "title": "HW 4: Sharing solutions to installation issues I encountered",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>My dev setup is ancient, so I ran into a number of issues when going through the installation steps for HW 4. Posting the solutions that worked for me here in case anyone else runs into the same issues.</paragraph><list style=\"number\"><list-item><paragraph>When trying to install gensim: <code>error: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\"</code></paragraph><list style=\"number\"><list-item><paragraph>Solution if you don't want to install all of VSCode: First answer in <link href=\"https://stackoverflow.com/questions/40018405/cannot-open-include-file-io-h-no-such-file-or-directory\">https://stackoverflow.com/questions/40018405/cannot-open-include-file-io-h-no-such-file-or-directory</link>, though I chose the most recent compatible version of the packages instead.</paragraph></list-item></list></list-item><list-item><paragraph>When trying to <code>import gensim</code>: <code>ModuleNotFoundError: No module named 'dataclasses'</code></paragraph><list style=\"number\"><list-item><paragraph>Solution: <code>pip install dataclasses</code></paragraph></list-item></list></list-item><list-item><paragraph>When trying to install tokenizers: <code>ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects</code></paragraph><list style=\"number\"><list-item><paragraph>Solution: Upgraded to Python 3.11. Maybe this would have solved some of the earlier problems too.</paragraph></list-item></list></list-item><list-item><paragraph>The scoring script is written in Perl. You can install Perl at <link href=\"https://strawberryperl.com/\">https://strawberryperl.com/</link> and restart your terminal for it take effect.</paragraph></list-item></list><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Adding on to this for anyone with M1 macs... I found these instructions helpful for resolving python &amp; tensorflow dependencies:  </paragraph><paragraph><link href=\"https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jan-2023.ipynb\">https://github.com/jeffheaton/t81_558_deep_learning/blob/master/install/tensorflow-install-mac-metal-jan-2023.ipynb</link> </paragraph><paragraph/><paragraph> </paragraph></document>"
        ]
    },
    "98": {
        "title": "Testing gensim installation",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Just quickly mentioning that the homework describes that we should use the following command to view the vector representation for an individual word:</paragraph><pre>&gt;&gt;&gt; v1 = model.wv['computer']\n</pre><paragraph>I found that this command was throwing an error, but according to this stackoverflow post: <link href=\"https://stackoverflow.com/a/71550086\">https://stackoverflow.com/a/71550086</link>, the .wv access field should not be necessary (also mentioned in the assignment)</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Vfiw8Jmeiidb5r4WEXFzJZey\" width=\"643\" height=\"92.28941176470589\"/></figure><paragraph>This is the command that worked for me:</paragraph><pre>&gt;&gt;&gt; v1 = model['computer']\n</pre><figure><image src=\"https://static.us.edusercontent.com/files/qqi4PZraCxF84kJw1y3GJyHw\" width=\"658\" height=\"167.11111111111111\"/></figure><paragraph>Is my understanding correct or is it preferred to use <code>v1 = model.get_vector('computer')</code>? Thanks in advance</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>I think these are all identical \u2014 you could check a few examples and see if the different functions return identical vectors.\u00a0</paragraph></document>",
            "<document version=\"2.0\"><paragraph>I ran into this issue as well -- do we need a particular version of gensim to properly do the homework?  Just thinking of compatibility issues when someone else tries to run my homework code. </paragraph></document>"
        ]
    },
    "99": {
        "title": "Converting a Constituency Parse to Dependency Pares",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>From HW 3: \"There are also the following data files, corresponding to a standard split of the WSJ part of the Penn Treebank. The original Penn Treebank contains constituency parses, but these were converted automatically to dependencies. \"<break/><break/>This made me curious- what is the standard way to convert constituency parses to dependency parses? Can a completely rules based approach suffice?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, a rule-based approach is typically used for this. The tricky thing is to know which head to project up from the children to the parent. A head-percolation table (inspired by the annotation schema for the constituency representation) is used for this, sometimes combined with some postprocessing.\u00a0</paragraph></document>"
        ]
    },
    "100": {
        "title": "Part 2 Wordnet Frequency Baseline Count",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Just to clarify on how we should count the occurrence of the synonyms.</paragraph><paragraph>So we can iterate all the synsets of a given context's lemma and pos, then for each lemma in the each synset, we accumulate the counts for that specific lemma's name?</paragraph><paragraph>In the end we should just return the lemma name with the highest count and that lemma name can't be the input lemma name, is that the idea?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yeap that sounds right to me.\u00a0</paragraph></document>"
        ]
    },
    "101": {
        "title": "Arc Standard Transition based Dependency Parsing: All Arc-Lefts before Arc-Rights",
        "category": "General",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/99ZW7komu763KlIndYNpQjuV\" width=\"658\" height=\"652.9384615384615\"/></figure><paragraph><break/>Is it correct to say it is always possible to reach a final dependency tree A_d by following a sequence of transitions wherein all Left-Arcs are done before Right-Arcs ? <break/><break/>Is it necessary?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I believe this answer is no. The sequence of transitions are based on the annotated dependency tree, which determines the relationships between words. In the arc-eager example in lecture 8, for example, the sequence of words in the buffer do not lend themselves to a sequence of transitions in which all left-arcs are done before right-arcs (unless you force incorrect relationships between words).</paragraph><paragraph/></document>"
        ]
    },
    "102": {
        "title": "Part 6 imports",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>Are we allowed to import anything we want to solve part 6, or should we stick to the libraries given?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>We usually handle other imports on a case-by-case basis. What are you considering?</paragraph></document>"
        ]
    },
    "103": {
        "title": "Error in smurf.predict on line 299",
        "category": "Homework 4",
        "question": "<document version=\"2.0\"><paragraph>When I run <bold>perl score.pl smurf.predict gold.trial</bold> I get this error. Is the perl script correct or I am doing something wrong?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1HPxf4aiiQhU2dirrAWbbfmH\" width=\"428\" height=\"227\"/></figure><paragraph>And this is my smurf.predict</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/XHRq6OUieDRhBCcn8MsMlbyJ\" width=\"382\" height=\"590\"/></figure><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Interestingly, I have the same problem, but for the BertPredictor in Part 5. I get this same print out on every line when I run the gold.trial check.</paragraph><paragraph>I reviewed the perl script and it is related to some conditional that checks if the line is \"not empty\". I am not very familiar with Perl so I have not looked into it more than that. It seems strange since I checked the text file and nothing looks blank? My BertPredictor is getting ~17% precision, which seems to be around the expected result as well.</paragraph></document>",
            "<document version=\"1.0\"><paragraph>It's possible that perl doesn't like the encoding of the file. Try changing the encoding to UTF-8.</paragraph></document>"
        ]
    },
    "104": {
        "title": "Homework 5 & exam",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Apologies if this was already addressed in class: just to clarify, will Homework 5 and the Exam be due/administered in the same week? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, hw4 is already released due for Aug 4 ! you should ideally expect hw5 released by Aug 3rd night or so.<break/>Note : 1 Lowest score will be dropped if you submit all homework assignments.</paragraph></document>"
        ]
    },
    "105": {
        "title": "Final submission",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Do the .vocab and .h5 files need to be in a separate \"data\" folder in our final submission, similar to how hw3 has the .py files and then a \"data\" folder with all .vocab and .h5 within them? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, keep the model.h5 and pos.vocab and word.vocab files in data folder itself.</paragraph><pre>\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 words.vocab\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pos.vocab\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 model.h5\n\u251c\u2500\u2500 all .py files</pre></document>"
        ]
    },
    "106": {
        "title": "CPU vs GPU Train",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>CPU training for me was significantly faster(2x) in comparison to GPU training on an Nvidia GPU. Is this expected?</paragraph><paragraph>CPU: AMD Ryzen 9 6900HX </paragraph><paragraph>GPU: Laptop 3070Ti </paragraph><paragraph>CPU Results:</paragraph><pre>Epoch 1/5\n18996/18996 [==============================] - 59s 3ms/step - loss: 0.5998\nEpoch 2/5\n18996/18996 [==============================] - 57s 3ms/step - loss: 0.3834\nEpoch 3/5\n18996/18996 [==============================] - 57s 3ms/step - loss: 0.3430\nEpoch 4/5\n18996/18996 [==============================] - 58s 3ms/step - loss: 0.3199\nEpoch 5/5\n18996/18996 [==============================] - 56s 3ms/step - loss: 0.3045\n</pre><paragraph>GPU Results:</paragraph><pre>Epoch 1/5\r\n2023-07-28 20:36:57.712676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n2023-07-28 20:36:57.730501: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f92f7acd9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2023-07-28 20:36:57.730585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti Laptop GPU, Compute Capability 8.6\r\n2023-07-28 20:36:57.755000: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2023-07-28 20:36:58.281511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800\r\n2023-07-28 20:36:58.502551: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n18996/18996 [==============================] - 147s 7ms/step - loss: 0.5637\r\nEpoch 2/5\r\n18996/18996 [==============================] - 127s 7ms/step - loss: 0.3674\r\nEpoch 3/5\r\n18996/18996 [==============================] - 129s 7ms/step - loss: 0.3301\r\nEpoch 4/5\r\n18996/18996 [==============================] - 128s 7ms/step - loss: 0.3099\r\nEpoch 5/5\r\n18996/18996 [==============================] - 131s 7ms/step - loss: 0.2967\r\n/home/kguda/mambaforge/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.</pre><paragraph/></document>",
        "comments": [
            "<document version=\"2.0\"><paragraph>Same thing happened here, not sure why lol.</paragraph></document>",
            "<document version=\"2.0\"><paragraph/><paragraph>You batch size is  small. As long as each   batch fits in GPU memory, processing each  batch takes the same amount of time ( regardless of its size). Therefore if you are using GPU you should increase the batch size (like 10000 instead of 10) to actually gain from the GPU. </paragraph></document>"
        ],
        "answers": []
    },
    "107": {
        "title": "How does re-training the model affect final submission files?",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Since we need to submit 9 files in all, will semi-constantly re-training the models affect any of our submission files? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Retraining would change the weights (.h5 file)</paragraph><paragraph>Only after you are done retraining and get your final output, should you submit all the files. </paragraph><paragraph>Alternatively, you could also resubmit till the deadline</paragraph></document>"
        ]
    },
    "108": {
        "title": "HW3 Evaluation Results",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>My results seem to be coming out a little lower than expected. Is this indicative of a training error, or is it just necessary to tune parameters in the network topology?</paragraph><paragraph>I've tried to re-run everything and retrain the model but the LAS scores are consistent.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/r3zusS2YzCQDmMH1JQghRu20\" width=\"652\" height=\"187\"/></figure><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Hard to say without looking at your code. If your previous parts are correct, I would try parameter tuning to increase the scores.</paragraph></document>"
        ]
    },
    "109": {
        "title": "Training taking around 5 mins per epoch on a windows gpu setup?",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>I'm training on an nvidia 3070 and it's taking about 5 mins per epoch. I'm using conda and wsl on windows. I'm just wondering if anybody else gets a similar result? It seems suspiciously longer than a 2016 macbook but maybe it's just the way I set up tensorflow?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I am not sure, I also have 3070 but tensorflow does not recognize my GPU so I just trained with CPU and it took about 3mins or so per epoch, my tf version is 2.13</paragraph></document>"
        ]
    },
    "110": {
        "title": "How should we use keras.utils.to_categorical in Part 2?",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>For <code>get_output_representation</code>, I think we can manually code the one-hot key vector relatively easily using <code>self.output_labels</code> without using <code>keras.utils.to_categorical</code>? </paragraph><paragraph>Is there a reason that we should use <code>keras.utils.to_categorical</code> in this case? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes I would actually recommend using the output_labels dictionary directly and constructing the output vector manually.\u00a0</paragraph></document>"
        ]
    },
    "111": {
        "title": "Colab",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Are we able to upload files to colab? or do we have to copy our python code out? How does this work with the data files?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, and it's easy to do. Create a new directory in your Google Drive with the homework files. Then create a new Colab notebook, mount your drive, and then use the cd command until you're in the directory with the homework files. You should be able to run your code as you would locally.</paragraph></document>"
        ]
    },
    "112": {
        "title": "Estimated time for testing?",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>How long should it take to return LAS after running evaluate.py data/model.h5 data/dev.conll? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>For me personally, it's taking about 1.5 minutes to complete. My results are extremely close to the GCP with T4 results in #134 however I am using a Windows PC.</paragraph><paragraph>A note too, it really helped adding tf.compat.v1.disable_eager_execution() after line 20 in evaluate.py, it sped up a lot. </paragraph></document>"
        ]
    },
    "113": {
        "title": "WSL & Nvidia GPU Opinion",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Is anyone else using windows subsystem for Linux to use their GPU for training?  I set it up, and it looks like it is sort of working. I get 0% utilization when i look at nvidia-smi while training is running, but it does show my v-ram on my 3080ti is being used. So it looks like it is sort of working, but my mac from 2019 is also running just as fast, if not faster lol. Wanted to see if anyone else out there attempted to use their own gpu and had similar results.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Hey jonathan, I guess I double-posted maybe.</paragraph><paragraph>I have practically the exact same setup with a 3070. I'm seeing training taking about 5 mins per epoch (longer than the prof's 2016 macbook?) also with low usage on nvidia-smi, but with about 10% more usage compared to base and about 90% vram usage on task manager. It's quite strange.</paragraph><paragraph>How long did training take for you per epoch?</paragraph><paragraph>From videos online, it looks like that the tensorflow setup we're using and the architecture for this assignment might be more favorable for macs (when they work properly) and colab in training, so that could be it? I might also be completely wrong.</paragraph></document>",
            "<document version=\"2.0\"><paragraph>Tried on a native Windows laptop (tensorflow version 2.10) with RTX 2070 super max-q and a linux desktop with Quadro T400, a lot more RAM and a more powerful CPU. Took about 90s on the laptop per epoch and 70s on the desktop. </paragraph><paragraph>It might be worth trying on native Windows because the WSL setup can be tricky.</paragraph><paragraph/></document>"
        ]
    },
    "114": {
        "title": "HW3-part3",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>What is the expected loss after 5 epoch?</paragraph><paragraph/><paragraph>Thank you</paragraph><paragraph>Hamid</paragraph><paragraph>Seyedhamidreza Alaie</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You should be getting loss values between 0.4 and 0.6, and they should start to converge near 0.4 after 5 epochs.</paragraph></document>"
        ]
    },
    "115": {
        "title": "Results of part 4",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I saw previous posts about this but when I run my code on Mac M2 and an older Mac the results are very different again, not matter what I try:</paragraph><paragraph>M2:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/cxSmFONRz0MoFa0f8c8aBu3U\" width=\"659\" height=\"229.55519828510182\"/></figure><paragraph>Older Mac:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/0MRUPqjNFGlS3kQgb7xnlLL4\" width=\"659\" height=\"191.2006880733945\"/></figure><paragraph>Any ideas what I can do to fix it?</paragraph><paragraph>Thanks!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>I think the only solution for the time being is to just run training on the CPU, unfortunately.\u00a0</paragraph></document>"
        ]
    },
    "116": {
        "title": "Hw3 get_output_representation",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I know in the description it suggests we use keras.utils.to_categorical function</paragraph><paragraph>Can we use the output_labels attribute of the FeatureExtractor class as another approach?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes that\u2019s fine.\u00a0</paragraph></document>"
        ]
    },
    "117": {
        "title": "Hw3 Part 2",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>This may be a silly question but I am a bit confused for get_input_representation</paragraph><paragraph>If the result is the next 3 words on the stack followed by the next three words on the buffer, then where do the parameters \"words\" and \"pos\" come into play? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The elements of the stack and the buffer are indices. You can use these 6 indices to index into words and pos and get the actual words and POS tags respectively.</paragraph></document>"
        ]
    },
    "118": {
        "title": "Usage of GPU on Mac M2",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I tried many solutions but I wasn't able to run part 3 with low losses. Once I added the following line as the first one under the main function of train_model.py:</paragraph><pre> tensorflow.config.experimental.set_visible_devices([], 'GPU')\n</pre><paragraph>My output was:</paragraph><pre>Epoch 1/5\n18996/18996 [==============================] - 52s 3ms/step - loss: 0.5516\nEpoch 2/5\n18996/18996 [==============================] - 70s 4ms/step - loss: 0.4543\nEpoch 3/5\n18996/18996 [==============================] - 92s 5ms/step - loss: 0.4323\nEpoch 4/5\n18996/18996 [==============================] - 98s 5ms/step - loss: 0.4243\nEpoch 5/5\n18996/18996 [==============================] - 99s 5ms/step - loss: 0.4133\n</pre><paragraph>Should I leave it in my code when submitting?</paragraph><paragraph>Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes you may keep this in your code when you submit.</paragraph></document>"
        ]
    },
    "119": {
        "title": "Solution for M1/M2 Mac - uninstall tensorflow-metal",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Seems like everyone with M1/M2 Mac is having issue, following last thread #134, did a quick googling and on apple dev forum there is a similar post: <link href=\"https://developer.apple.com/forums/thread/696474\">https://developer.apple.com/forums/thread/696474</link></paragraph><paragraph><bold>Apple has confirmed that there is bug with GPU training with M1/M2, and the issue has not been resolved.</bold></paragraph><paragraph>The solution mentioned is to uninstall tensorflow-metal, use CPU only training. I tried was getting the results normally now.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/iENTJYiIZvT5UKIfkOBZqBFw\" width=\"655\" height=\"168.55048859934854\"/></figure></document>",
        "comments": [],
        "answers": []
    },
    "120": {
        "title": "keras.optimizers.Adam for M1/M2 Macs",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I have an M2 Mac and when I run part 3 I get the following warning:</paragraph><pre>WARNING:absl:At this time, the v2.11+ optimizer <code>tf.keras.optimizers.Adam</code> \nruns slowly on M1/M2 Macs, please use the legacy Keras optimizer instead,\nlocated at <code>tf.keras.optimizers.legacy.Adam</code>.\n</pre><paragraph>Am I allowed to modify the train_model.py function accordingly? i.e. add:</paragraph><paragraph>model.compile(keras.optimizers.<underline><bold>legacy</bold></underline>.Adam(lr=0.01), loss=\"categorical_crossentropy\")</paragraph><paragraph>Thanks! </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Sure! Let me know if that works\u00a0</paragraph></document>"
        ]
    },
    "121": {
        "title": "HW3 P3 Network Topology",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Should the softmax activation layer for the output be a Dense layer with the number of outputs as the number of units? Or, should it be a keras.layers.Softmax layer? In this case I don't know how to specify the correct size. </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>The Softmax layer doesn't have any parameters (so nothing to train) and its output has the same shape as the input. If you use Softmax as a separate layer, the previous layer needs to output the right shape. In this case I would just use a Dense layer with softmax activation</paragraph><paragraph/></document>"
        ]
    },
    "122": {
        "title": "Issue with Importing 'keras'",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor/TAs, </paragraph><paragraph>I have been experiencing a persistent issue when executing the scripts 'train_model.py' and 'extract_training_data.py'. The error message I received is 'No module names 'keras'. </paragraph><paragraph>I have attempted to resolve this issue by installing keras directly through pip (pip install keras) as well as through the Anaconda environment...but still getting the same error. </paragraph><paragraph>Thanks!! </paragraph><paragraph/><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Keras is now part of tensorflow. Replace the import with\u00a0</paragraph><paragraph>import tensorflow.keras</paragraph></document>"
        ]
    },
    "123": {
        "title": "Part 3 Increased Loss?",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>I am getting an increased loss for each epoch. I assume this is not expected?</paragraph><paragraph>Any ideas why this might be?</paragraph><paragraph>These are the commands I am running:</paragraph><pre>python get_vocab.py data/train.conll data/words.vocab data/pos.vocab\npython extract_training_data.py data/train.conll data/input_train.npy data/target_train.npy\npython train_model.py data/input_train.npy data/target_train.npy data/model.h5\n</pre><pre>Done loading data.\nEpoch 1/5\n2023-07-25 20:25:33.550726: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n18996/18996 [==============================] - 158s 8ms/step - loss: 2.3834\nEpoch 2/5\n18996/18996 [==============================] - 153s 8ms/step - loss: 19.8371\nEpoch 3/5\n18996/18996 [==============================] - 154s 8ms/step - loss: 100.4172\nEpoch 4/5\n18996/18996 [==============================] - 158s 8ms/step - loss: 233.7903\nEpoch 5/5\n18996/18996 [==============================] - 158s 8ms/step - loss: 443.5440\n</pre></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>It's likely that your input and output representations are inconsistent between training examples. You should review that part. </paragraph><paragraph>You could try running training on a single example, or on a small batch. The loss should decrease very quickly. </paragraph><paragraph>Make sure the logic for looking up the word/pos for each value on the stack and buffer is correct. </paragraph></document>"
        ]
    },
    "124": {
        "title": "Hw3P2:",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Assuming a token: \"GORBACHEV\" does not appear in the <italic>words.vocab</italic> file, should we consider it as \"&lt;UNK&gt;\" and store \"the index of &lt;UNK&gt; \" from words.vocab file to a one-dimensional vector\uff1f</paragraph><paragraph>Thank you in advance!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><list style=\"bullet\"><list-item><paragraph>\"GORBACHEV\" is probably tagged as NNP, so the correct representation would be the index for the &lt;NNP&gt; symbol. </paragraph></list-item><list-item><paragraph>If it wasn't tagged NNP and indeed is not in words.vocab, then you should use the index for &lt;UNK&gt;</paragraph></list-item><list-item><paragraph>It probably makes sense to lower-case the input, which will increase the likelihood of finding words at the beginning of the sentence. It's unlikely that \"gorbachev\" is in the lexicon either, though. </paragraph></list-item></list></document>"
        ]
    },
    "125": {
        "title": "HW3 P2: get_input_representation",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>I am having some confusion regarding the implementation of the method get_input_representation.</paragraph><paragraph>The inputs to this method include the words and the pos tags. However, I am confused on when to use the pos tags. Should the index always be pulled from the words.vocab file, or is there a case where the index should be from the pos.vocab file?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>My understanding is that words.vocab contain 5 special symbols, 2 of which are the pos CD and NNP, as indices 0 and 1 respectively. These should be extracted from words.vocab only when the element in pos, at the index that you're checking (from stack/buffer), is either CD or NNP. </paragraph><paragraph>Maybe one of the teaching staff could clarify this.</paragraph></document>"
        ]
    },
    "126": {
        "title": "Testing part 2",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>How can I test my part 2 of HW3? I tried printing the numpy array from the get_input_representation function but it doesn't help much with understanding if the mapping is correct (except from NULLs and the ROOT)</paragraph><paragraph>Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I'm not sure there is a better way to test this, other than comparing to the indices in your dictionaries by hand. </paragraph><paragraph>Ultimately, the test is to see if your model is training correctly. </paragraph></document>"
        ]
    },
    "127": {
        "title": "HW3 part 2- get_output_representation",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>For the get_output_representation(self, output_pair) method should we return a vector containing 90 zeroes and a single one (representing the output_pair)? If this is the case, do the indexes in the vector need to follow a specific order? For example, having the first 45 indexes reserved for \"left_arc\" transition pairs, the following 45 indexes for \"right_arc\" transition pairs and  ('shift', None) be at index 91?<break/><break/>Thanks in advance! </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>That is my understanding as well. Looking at the extractor's output_labels object, there is an index defined for each possible 91 transitions, so you can use those indices in extractor.output_labels to assign the \"1\". Hope that makes sense!</paragraph></document>",
            "<document version=\"2.0\"><paragraph>The specific order in which the transitions (with relations) map to the indices doesn't really matter, as long as it's consistent in your code between training and decoding. </paragraph></document>"
        ]
    },
    "128": {
        "title": "Use of test.conll file",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Good evening,</paragraph><paragraph>I was trying to understand when/where to use the test.conll file?<break/><break/>Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>So, the way you would normally do this is to train on the training data with some initial hyperparameter setting (such as features you feed to the model, optimizer, learning rate, batch size, etc.). Then, you can use the development set to tune these hyperparameters. </paragraph><paragraph>Finally, you want to run an end-to-end evaluation using the test data. It's important that the test data is not touched during training and optimization at all. </paragraph><paragraph>In this assignment, because you do not have to optimize any of the hyperparameters, we can get away with one held-out set. So you run your evaluation on .dev without ever touching .test. If you wanted to experiment more, I would recommend you repeat the evaluation on .dev and then run a final test on .text -- however that is not required for the assignment. </paragraph></document>"
        ]
    },
    "129": {
        "title": "Ungraded Exercise: Linear Models and Feature Functions solutions",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>If you have a chance, do you mind releasing the solutions to the Linear Models and Feature Functions ungraded exercise?</paragraph><paragraph>Thank you in advance!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>It should be available in Pages section now.</paragraph></document>"
        ]
    },
    "130": {
        "title": "Setup",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Hi All,</paragraph><paragraph>I have a windows 11 laptop with an nvidia 1060 card. I followed the instructions on tensorflows website as well as the README and I can't get tensorflow to print my gpu statistics. </paragraph><snippet language=\"py\" runnable=\"true\" line-numbers=\"true\"><snippet-file id=\"code\">import tensorflow as tf \r\nfrom tensorflow import keras \r\n\r\nprint(\"GPU: \", tf.config.list_physical_devices('GPU'))</snippet-file></snippet><paragraph>The code above runs fine, it just prints an empty list for the array. A quick google tells me I need to install CUDA and cuDNN in order for get tf to work with my nvidia card. Does this sound right? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, this sounds reasonable.</paragraph></document>",
            "<document version=\"1.0\"><paragraph>I had the same issue. When using Windows native you have to use TensorFlow version 2.10 or earlier for GPU support.</paragraph></document>",
            "<document version=\"2.0\"><paragraph>I went through the lenghthy process of setting up wsl and conda to get it working with the latest tf version... can report that just using the earlier tf version is the better option for sure</paragraph></document>"
        ]
    },
    "131": {
        "title": "Mac & Linux difference",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>I did the entire homework on Mac and regardless what I try and debug, the result always turned out horrible. Then I saw  #132, so following this thread, using my GCP instance, here is the comparison:</paragraph><paragraph><bold>Training:</bold></paragraph><paragraph>GCP with T4:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/hMEKAyahvKCllkGsJtZktQyv\" width=\"656\" height=\"176.94736842105263\"/></figure><paragraph>Mac with M1:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/k27SDI5N3EExzxHne3gOBsLk\" width=\"641\" height=\"91.40613718411552\"/></figure><paragraph><bold>Evaluation:</bold></paragraph><paragraph>T4</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/2uVbQ6CAq8fCSeQZlABcagq3\" width=\"641\" height=\"117.86129032258066\"/></figure><paragraph>M1</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/VHS8d6Ja17dIy3g6pIl7oP6c\" width=\"641\" height=\"115.23595505617976\"/></figure><paragraph>It's the same code. Didn't change anything.   </paragraph><paragraph/><paragraph>TLDR: Don't use Mac.</paragraph><paragraph/></document>",
        "comments": [
            "<document version=\"2.0\"><paragraph>I hope I saw this earlier. I did a lot to my code and finally realized it's about Mac. After switching to Linux, everything became fine.</paragraph><paragraph/></document>"
        ],
        "answers": [
            "<document version=\"2.0\"><paragraph>Hi, I'm using an M2 MacBook that produces results similar to your T4, so I'm not sure if it's an M1 problem.</paragraph></document>"
        ]
    },
    "132": {
        "title": "HW3 part 4 results drastically different on two device",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Dear Prof. and TAs,</paragraph><paragraph>I have a question regarding the results of my evaluation. I tried to run the evaluate.py on both windows (tensorflow/keras 2.10.0) and MAC M1 (tensorflow 2.13.0/keras 2.13.1 ). </paragraph><paragraph>Then I got the results for win is similar to that on the coursework hints (upper).</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Z6zV7SAlHbknxLYOlTcOKvoi\" width=\"566\" height=\"163\"/></figure><figure><image src=\"https://static.us.edusercontent.com/files/MTBNg7GVWd0ahGKmLag6mo1R\" width=\"571\" height=\"127\"/></figure><paragraph>Then I tried the same command (with the model trained on Win) in MAC, however, the results turned to be quite different (below).</paragraph><paragraph>Would you mind telling me which one is more reasonable, so that I can try to identify the problems in another program?</paragraph><paragraph>Also, will the version causes such a big difference or there might be other reasons, and do we need to set the random seed?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>That\u2019s super surprising. I wonder if there is an I compatibly in the model files between the two versions?\u00a0</paragraph><paragraph>The results in the first figure are close to what I would expect.\u00a0</paragraph></document>"
        ]
    },
    "133": {
        "title": "Ungraded exercise solution: Transition-based dependency parsing",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor Bauer/TAs. </paragraph><paragraph>For the Ungraded Dependency Parsing problem, I am confused on we do not perform all the left-arc transitions first? Based on the example given in class, all the left-arc transitions were performed before the right-arc transitions. I thought the standard arc transition performs all the left before the right to maintain projectivity.</paragraph><paragraph>Also could you please provide the solution to part b of the problem? </paragraph><paragraph>Thanks!! </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>To your question 1, it indeed performs all the left-arc transitions first, look closely, the word 'meme' is parsed as an arc first (left-arc on 'a' and 'funny') <bold>[STEPS 8 &amp; 9]</bold> post which the right-arc from 'sent' to 'today' occurs <bold>[STEP 12]</bold> </paragraph><paragraph>Also note the only difference is the <link href=\"https://courseworks2.columbia.edu/courses/176463/pages/ungraded-exercise-solution-transition-based-dependency-parsing\">solution here</link> uses a slight notational variant, compared to the slides, where, the stack is arranged so that the <bold>top-most token is on the left</bold>, instead of on the right in the slides. You can use any notation, but it should be consistent.</paragraph></document>"
        ]
    },
    "134": {
        "title": "Tensorflow-gpu is now part of regular tensorflow installation",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Just wanted to note that tensor-flow-gpu is now part of the regular tensorflow installation. The regular installation (pip install tensorflow) should have GPU support. </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/GPdKtIAi3qMIhQytbXQ8KyIz\" width=\"658\" height=\"146.47652173913042\"/></figure><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, that should be fine. Do you see any concerns while running your code ?</paragraph></document>",
            "<document version=\"1.0\"><paragraph>Thanks. I\u2019ll definitely update the instructions accordingly.\u00a0</paragraph></document>"
        ]
    },
    "135": {
        "title": "Question about Exam format for CVN student",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Wondering what will be the exam format for CVN students?  Do we have to attend the exam at the same time slot as the on-campus students?<break/><break/>Thanks!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>I believe this was covered at the start of the last lecture. We can either do synchronously via Zoom or async in the time slot.</paragraph></document>",
            "<document version=\"1.0\"><paragraph>Correct you can either take the exam synchronously on zoom (preferred because you can ask questions), or using CVN\u2019s remote proctoring system. \u00a0I will send out a signup form later this week.\u00a0</paragraph></document>"
        ]
    },
    "136": {
        "title": "Typo in Lecture 7?",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Is the first cell on the right wrong? (VP -&gt; V NP)? Shouldnt the cell to the right be PP (or at least match the parent's right value)?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/aD8uEJ5r4Hj1CZFk1VR9eXLb\" width=\"658\" height=\"486.4247311827957\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, you are right, Anonymous !<break/>The previous slide before the one you posted has it correctly annotated, its a minor mistake on the follow up slide, but yes, you are right [ S -&gt; NP VP]</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/opbbUQQG1bEzPygKcvcb4ZvI\" width=\"632\" height=\"477.2113821138212\"/></figure><paragraph/></document>"
        ]
    },
    "137": {
        "title": "Simulation Based Bias Avoidance Model?",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor,</paragraph><paragraph>Towards the end of Thursday's lecture you discussed the presence of bias in some of the contemporary language models. I've seen simulations used to train models in robotics and was wondering if simulating a desirable lexicon (let's say neutral and inclusive) would offset some of the less desirable cultural associations?</paragraph><paragraph>Thank you in advance! </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Definitely Professor <mention id=\"322362\">Daniel Bauer</mention> would be able to help guide on this, based on the context of this question from class.</paragraph><paragraph><break/>However, meanwhile, as a side note, maybe you want to take a look at <link href=\"https://arxiv.org/pdf/2005.00965.pdf\">simulation-based bias mitigation models</link> , to generate data that is free of bias. This data can then be used to train language models that are less biased.</paragraph><paragraph>Simulating a lexicon manually is one of the techniques, some other techniques like <bold>\"debiasing\"</bold> (training the model on a text dataset that has been modified to remove bias) and <bold>\"fairness training\"</bold> ( training the model to be fair in its predictions, regardless of the input data) are also used to create language models that are less biased and more inclusive.</paragraph></document>"
        ]
    },
    "138": {
        "title": "HW 3 part 4 sanity check",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>In part 4, I had to convert my features into a 2d array (via <code>np.array([features])</code>) before passing them to <code>predict</code>; otherwise, I get a dimension mismatch. Is this expected, or have I done something wrong?</paragraph><paragraph>The documentation seems to imply a 1d array would be accepted for a singular prediction, so I'm not sure why I had to do this. https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Yes you may have to reshape before passing it to the predict function. One way to do it would be using <italic>features.reshape(1,-1).</italic></paragraph><paragraph>If you do this, the predict function will output a 2D array, but you only need to work with the first (and only) row. You would then index into the 2D array to perform the remaining operations.</paragraph></document>"
        ]
    },
    "139": {
        "title": "HW 3 part 3 sanity check",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>In part 3, is it expected not to use all method params?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Are you talking about the params passed while building the Keras layers?</paragraph></document>",
            "<document version=\"1.0\"><paragraph>Yes, you're correct. The pos_types parameter is not used.</paragraph></document>"
        ]
    },
    "140": {
        "title": "Typo in lecture11 p.41?",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi isn't it</paragraph><paragraph>vector(apple) - vector(tree) + vector(vine) = vector(grape)</paragraph><paragraph>instead of </paragraph><paragraph>vector(apple) - vector(tree) + vector(grape) = vector(vine)? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Hi Zoe,</paragraph><paragraph>Yes you are right, The equation in the slide is incorrect. I believe Prof might have shared this in class, the correct equation would be:</paragraph><pre>vector(apple) - vector(tree) + vector(vine) = vector(grape)\n</pre><paragraph>The correct equation, as shown above, subtracts the vector representing a tree from the vector representing an apple, and then adds the vector representing a vine. This results in a vector representing a grape as also shown in the image of slide too.</paragraph></document>"
        ]
    },
    "141": {
        "title": "HW3 part 2 model optimization metrics",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Dear Prof. and TAs,</paragraph><paragraph>I have a question about the part 3 model.compile. Apart from the parameters 1) optimizer and 2) loss function, we also have a \"metrics\". What specific metrics do you suggest for our task, like accuracy, auc, etc?</paragraph><paragraph>Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The metric won\u2019t affect the training process at all. But it does provide interesting information about how quickly the model is improving on the training data. For this task, accuracy should be sufficiently informative.\u00a0</paragraph></document>"
        ]
    },
    "142": {
        "title": "Dutch Context-Free Example from Lecture",
        "category": "General",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/MbySkaDuehJQF74js195268Z\" width=\"658\" height=\"449.7576285963383\"/></figure><paragraph>To clarify, the English version of this sentence is not context-free and <bold>can</bold> be expressed using a CFG? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes the English translation doesn\u2019t show the cross linear dependency phenomenon.\u00a0</paragraph></document>"
        ]
    },
    "143": {
        "title": "HW2 Python Libraries",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Are we allowed to use/import numpy?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, numpy is allowed.</paragraph></document>"
        ]
    },
    "144": {
        "title": "Probs table accuracy?",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Are there any values or range of values we should be getting for values in our probability table? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>There aren't any specific ranges that I can think of off the top of my head, but I believe there were quite a few negative ones.</paragraph></document>"
        ]
    },
    "145": {
        "title": "Part 3 lexical rules probabilities",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hello, </paragraph><paragraph>Given: tokens = ['flights', 'from','miami', 'to', 'cleveland','.'], I got the following lexical rules probabilities:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/aJelkKCWVNGrNIVBR07QLB6o\" width=\"658\" height=\"104.51985559566788\"/></figure><paragraph>Should <italic>parse_with_backpointers</italic>  method choose the production rule 'FLIGHTS' ==&gt; 'flights' over the rule 'NP' ==&gt; 'flights' , as it has higher probability ? </paragraph><paragraph/><paragraph>If the answer is yes, the example in part 4 is using  'NP' ==&gt; 'flights' rule (which is  less probable than 'FLIGHTS' ==&gt; 'flights'  based on what I got above.  </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1P7Uf45ItZCWyBPcawTn5RTU\" width=\"658\" height=\"119.13028764805415\"/></figure><paragraph>Is the given example correct ? Am I missing something?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>You need to add up the log probabilities at each node and \"FLIGHTS\" with any other non-terminals shouldn't give you productions. The final tree gives the most probable path and the example is correct</paragraph><paragraph/></document>"
        ]
    },
    "146": {
        "title": "One Hot Vector Length of 91",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>I'm not quite sure I understand why there are only 91 combinations for the one-hot-vector. I thought for each valid POS (48 - 3 = 45) there would be 3 possible transitions (left, right, shift), resulting in 45*3. Then there is one extra transition for the &lt;ROOT&gt; word for a total of 45*3 + 1 = 136. </paragraph><paragraph>What am I missing in my understanding? Why do we only need a vector of length 91? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>If the transition is \"shift\", the dependency label is None. This means that there are 45*2+1 = 91 possible outputs.</paragraph></document>"
        ]
    },
    "147": {
        "title": "Encoding Case Sensitive Words in HW3",
        "category": "Homweork 3",
        "question": "<document version=\"2.0\"><paragraph>Should words be case sensitive when encoding our words in Part 2? </paragraph><paragraph>For example, the first case in the train.CONLL is: </paragraph><paragraph><italic>BUSH AND GORBACHEV WILL HOLD two days of informal talks next month.</italic> </paragraph><paragraph>The training data recognizes that \"AND\" is a conjunction (CC), so pos.vocab easily recognizes it as \"CC\". However, word.vocab only has \"and\" listed. </paragraph><paragraph>Given that Python dictionary keys are case sensitive, should \"AND\" be encoded as the index for \"and\" or \"&lt;UNK&gt;\"?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Use the lowercased version of the string when checking membership in the vocabulary. So in your example, it would be encoded as the index for \"and\" instead of \"&lt;UNK&gt;\".</paragraph></document>"
        ]
    },
    "148": {
        "title": "Examples in HW2",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Are all examples in HW2 examples with no particular meaning to the \"correctness\" of our implementation? </paragraph><paragraph>For example #99  was an example on a difference sentence. </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>That's right -- the examples illustrate the format of the data structures, but they are not necessarily test cases for any particular grammar / sentence combination. </paragraph><paragraph>One way to test your code for correctness is to create a small grammar file, for example using one of the grammars from the lecture slides. Then parse a sentence and compare the result to the correct solution by hand. </paragraph></document>"
        ]
    },
    "149": {
        "title": "HW2 part1 length of rhs",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Dear Prof. and TAs,</paragraph><paragraph>I have a question regarding verify_grammar. Apart from checking the sum of probability and (non)terminal is in lhs, do we have to ensure the length of rhs within {1,2}?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/vyUZigSNxk4uMreIl6NAOKJE\" width=\"658\" height=\"286.4503464203233\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes! The production rules on the RHS can only have a single terminal or 2 nonterminals. Anything else cannot be a valid rule.</paragraph></document>"
        ]
    },
    "150": {
        "title": "Different Start Symbols",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>For Part 2, how do we make sure that the function works for different start symbols? Given a grammar, how would a user know what the start symbol is without looking at the grammar?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Avoid hard coding the start symbol and use self.grammar.startsymbol instead.</paragraph></document>"
        ]
    },
    "151": {
        "title": "submission requirements",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hi teaching team,</paragraph><paragraph>I noticed that the assignment requirements say</paragraph><blockquote>Pack these files together in a zip or tgz file as described on top of this page. <bold>Please follow the submission instructions precisely!</bold> </blockquote><paragraph> but it seems that I can't find any specific requirements on the top. So, is there any formatting or naming convention for our submissions?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Please include cky.py, evaluate_parser.py, and grammar.py in your zip file. There's no required naming convention for the zip file itself, but including \"hw2\" and your UNI in the zip file's name would be helpful. :)</paragraph></document>"
        ]
    },
    "152": {
        "title": "question on referencing codes",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hello, <break/><break/>I wonder how to make a reference for the codes that I gained some insights?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>If you want to cite a snippet that you referenced, including the link in a comment would suffice.</paragraph></document>"
        ]
    },
    "153": {
        "title": "HW2 Part 2 runtime",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>What is considered too long of a runtime when testing is_in_language on atis3.pcfg? I made a small test grammar and it returns immediately but using atis3.pcfg takes well over a minute to return the result. Are points deducted for runtime? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>This shouldn't take very long. Chances are you are not using the rhs_to_rules dictionary correctly, which will slow things down. Are you maybe iterating through the keys to find one that matches a right hand side, rather than just indexing by key?</paragraph></document>",
            "<document version=\"2.0\"><paragraph>This is a related question to my classmate's.</paragraph><paragraph>Do we really care about the running time in this homework? Namely, can I call the more complex Part-3 function inside Part-2 and use that returning value to do some membership checking?</paragraph></document>"
        ]
    },
    "154": {
        "title": "HW3 - Part 4 Return Type",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Making a post in case this will help others</paragraph><paragraph>Upon investigating the evaluate_parser code, it seems that unparsed lines are only counted upon a KeyError, as seen here:</paragraph><pre>try:\n res = get_tree(chart,0,len(tokens),parser.grammar.startsymbol)\nexcept KeyError:\n unparsed += 1\n res = tuple()\n</pre><paragraph>This caused issues for me as originally in part 4, I was returning None in the cases when NT was not within the parse table. Therefore, unparsed was not being incremented and the final evaluation scores were drastically different than what is expected (based on the professor's outputs). </paragraph><paragraph>tldr : try raising a KeyError instead of returning None</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/qCNUGeo0mYBaemdTrr8AoxvU\" width=\"616\" height=\"126\"/></figure><paragraph>To add in the post, there is also this check in place, so if it's an invalid sentence, if you return None for parse table directly would also work.</paragraph><paragraph/></document>"
        ]
    },
    "155": {
        "title": "Hw3 Part 3",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph><bold><italic>For example the value of probs[(0,3)]['NP'] might be -12.1324.</italic></bold></paragraph><paragraph>What this the value for ['flights', 'from', 'miami', 'to', 'cleveland', '.']\u00a0, or a different sentence?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>It was for a different sentence (and may in fact be made up).\u00a0</paragraph></document>"
        ]
    },
    "156": {
        "title": "HW2 Part 1",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>For part 1, do we also need to verify if a symbol only contains letters or an apostrophe? Or is it simply check the probability sum and format?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You can assume that all symbols that appear as a rule left hand side are non terminals. All other symbols are terminals. I don\u2019t recommend checking for capitalization as this causes issues with numbers and punctuation symbols.\u00a0</paragraph><paragraph>It\u2019s simply becoming the rule format and the probabilities.\u00a0</paragraph></document>"
        ]
    },
    "157": {
        "title": "Thursday OH online?",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi, would it be possible to offer Thursday's OH over zoom to allow CVN students to join? That would be very helpful because currently there are no OH opportunities between Tuesday and Friday.</paragraph><paragraph>Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Due to the small number of TAs in the summer session we are unable to offer daily office hours that work for everyone. Shivansh is holding office hours online on Thursday though (3:00-4:30pm). Feel free to email me if you think you may be unable to get homework 2 done before the weekend.\u00a0</paragraph></document>"
        ]
    },
    "158": {
        "title": "hw2 part 2",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Could you please clarify what S is in the below? I understood this from the ungraded exercise, however, for hw2 there is no 'S' in the lhs rules... </paragraph><figure><image src=\"https://static.us.edusercontent.com/files/73TkJdUG70y1uAqb7cnmfLnd\" width=\"299\" height=\"251\"/></figure><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>The S is the start symbol. So that line is saying that the algorithm returns True if the start symbol is in: </paragraph><paragraph>pi[0, len(string)]. For the hw you can use self.grammar.startsymbol if that makes sense</paragraph></document>",
            "<document version=\"1.0\"><paragraph>S is the start symbol. It can be accessed in cky.py via self.grammar.startsymbol.</paragraph></document>"
        ]
    },
    "159": {
        "title": "Possible Mock Exam?",
        "category": "Exam",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Just wondering when we prepare for the final exam at the end of the course, is there any mock exam that can be used as a template for practice/preparation?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Unfortunately, there has not been a practice exam in previous iterations of the course, and I don't think that'll change this semester. Instead, please review the ungraded exercises and homework assignments to prepare for the final. The questions on the final generally resemble the structure of the ungraded exercises.</paragraph></document>"
        ]
    },
    "160": {
        "title": "Typo in Homework 2 Part 3?",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>\"For example: <italic>table[(0,3)]['NP']</italic> returns the backpointers to the table entries that were used to create the NP phrase over the span 0 and 3. For example, the value of <italic>table[(0,3)]['NP']</italic> could be <italic>((\"NP\",0,2),(\"FLIGHTS\",2,3))</italic>. <bold><underline>This means that the parser has recognized an NP covering the span 0 to 3, consisting of another NP from 0 to 2 and FLIGHTS from 2 to 3.</underline></bold> The split recorded in the table at table[(0,3)]['NP'] is the one that results in the <italic>most probable parse for the span [0,3] that is rooted in NP. \"</italic><break/><break/>I don't see how \"FLIGHTS from 2 to 3\" is possible. \"FLIGHTS\" is only available in the span from 0 to 1?  This is how I view the terminal symbols.<break/><break/>0            1          2            3           4                    5           6<break/>|---------|---------|---------|---------|----------------|---------|<break/>  Flights   from    Miami      to       Cleveland       .<break/><break/>With the arrangement above how is this possible (as per the directions)?<break/><break/><break/>|-------------------|----------|<break/>          NP            FLIGHTS</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The example for the table format just illustrates what the data structure looks like. It\u2019s not necessarily a continuation of the \u201cflights from Miami to Cleveland\u201d example. It would\u2019ve been nicer to stick with the same example throughout. I will keep this in mind for the next revision of the assignment.\u00a0</paragraph></document>"
        ]
    },
    "161": {
        "title": "Part 5",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>For part 4 I added the following lines at the top to check for validity of nt:</paragraph><pre>    # if (not any(nt in inner_dict for inner_dict in chart.values()) or len(chart[(i,j)][nt]) == 0):\n    #     return None\n</pre><paragraph>Everything seems to run smoothly, however when I ran part 5 I got the following result:</paragraph><pre>Coverage: 86.21%, Average F-score (parsed sentences): 0.7413490818718977, Average F-score (all sentences): 0.6390940360964636\n</pre><paragraph>However, when I deleted the 2 lines from get_tree, I get the following result for part 5:</paragraph><pre>Coverage: 67.24%, Average F-score (parsed sentences): 0.9504475408614075, Average F-score (all sentences): 0.6390940360964636\n</pre><paragraph>Did I do something wrong for part 4? Would appreciate any help.</paragraph><paragraph>Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You don't need those first 2 lines. Since your second set of metrics looks good, it seems that you already handled the base case correctly.</paragraph></document>"
        ]
    },
    "162": {
        "title": "Part 4 terminal input",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>For get_tree(chart, i,j,nt), can we assume the nt is always valid?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>If nt is not in the table or the table entry is empty, get_tree should return None or raise ah exception. You will have to check for that.\u00a0</paragraph></document>"
        ]
    },
    "163": {
        "title": "Hw2 Pt. 1",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>I just wanted to check to make sure that I got all the conditions for a grammar in CNF - in the slides it just says that each rule needs to be in the form </paragraph><paragraph>A \u2192 B C, where A \u2208 N, B \u2208 N, C \u2208 N</paragraph><paragraph>or </paragraph><paragraph>A \u2192 b, where A \u2208 N, b \u2208 \u03a3</paragraph><paragraph>In the assignment description, it gives the further requirement that all probabilities for the same lhs symbol must sum to 1.0.</paragraph><paragraph>I did have a few other questions for some edge cases though, I would really appreciate it if these could be verified/any gaps filled in (if that's allowed):</paragraph><list style=\"bullet\"><list-item><paragraph>Must every valid CNF grammar contain a valid startsymbol?</paragraph></list-item><list-item><paragraph>Should we ensure that rhs and lhs rules are the same?</paragraph></list-item><list-item><paragraph>Should we make sure that some rules exist for a certain set of symbols?</paragraph></list-item></list></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You can assume that the grammar is a valid CFG. You don\u2019t need to check the additional conditions you describe.\u00a0</paragraph><paragraph>You only need to check that each rule is in CNF format and that the probabilities for all rules with the same LHS sum to 1.0!\u00a0</paragraph></document>"
        ]
    },
    "164": {
        "title": "Part 3 log probability table",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>The <bold>second object is similar, but records log probabilities</bold> instead of backpointers. For example the value of <italic>probs[(0,3)]['NP']</italic> might be -12.1324. This value represents the log probability of the <italic>best parse tree (according to the grammar)</italic> for the span 0,3 that results in an NP.</paragraph><paragraph>Like, does the probability include 'NP' itself? Say NP -&gt; NP PP, does the  -12.1324 come from the sum of log probability on NP and PP on the right, or is the -12.1324 from the whole thing, i.e. log probability of NP and PP AND the probability of rule 'NP -&gt; NP PP' itself? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>-12.1324 would be the inside log probability of the entire subtree rooted in NP (that is, the product of all the rule probabilities).\u00a0</paragraph><paragraph>You compute it by adding the log probability for NP over the left subspan, the log probability for PP over the right subspan, and the log rule probability.\u00a0</paragraph></document>"
        ]
    },
    "165": {
        "title": "Format in Part 3",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>The assignment outline in courseworks says the the table should have the structure: <italic>((A,i,k),(B,k,j))</italic></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/NEYZ06Z5cq8aaRtCAaS7G7eB\" width=\"658\" height=\"208.91309385863266\"/></figure><paragraph>But the check table structure function in the python file cky.py suggests: <italic>((i,k,A),(k,j,B))</italic></paragraph><paragraph>It seems like either structure pass the check function, I'm just wondering which structure is preferred?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Ah that\u2019s a typo in the instructions. Use ((i,k,A),(k,j,B)).\u00a0<break/>Either option will work though. \u00a0</paragraph></document>",
            "<document version=\"2.0\"><paragraph>So what is the consensus on this? Could we get a fixed check_table_format function?</paragraph></document>"
        ]
    },
    "166": {
        "title": "part 3 log summation",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Because we are using summation of math.log2() when comparing probabilities, a lower probability sum is better?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I am unsure if I understand your question correctly, but we should look for a higher probability sum. Math.log2() is monotonically increasing.</paragraph></document>"
        ]
    },
    "167": {
        "title": "part 3 clarification",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>I am a bit confused by part 3 and am hoping to get some clarification on the example provided: </paragraph><paragraph>For example: <italic>table[(0,3)]['NP']</italic> returns the backpointers to the table entries that were used to create the NP phrase over the span 0 and 3. For example, the value of <italic>table[(0,3)]['NP']</italic> could be <italic>((\"NP\",0,2),(\"FLIGHTS\",2,3))</italic>. This means that the parser has recognized an NP covering the span 0 to 3, consisting of another NP from 0 to 2 and FLIGHTS from 2 to 3. The split recorded in the table at table[(0,3)]['NP'] is the one that results in the<italic><bold> most probable parse for the span [0,3] that is rooted in NP. </bold></italic></paragraph><paragraph>Is the most probable parse for the span [0,3] the split with the highest probability sum (using log2)? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I think so. The most probable parse for the span [0,3] is the parse that results in the highest probability log sum.</paragraph><paragraph/></document>"
        ]
    },
    "168": {
        "title": "hw 2 part 1 -",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Part 1 of HW2 states the following:<break/></paragraph><list style=\"unordered\"><list-item><paragraph>Then change the main section of grammar.py to read in the grammar, print out a confirmation if the grammar is a valid PCFG in CNF or print an error message if it is not. You should now be able to run grammar.py on grammars and verify that they are well formed for the CKY parser. </paragraph></list-item></list><paragraph/><paragraph>If I add logic in the actual verify_grammar() function (instead of main) to print the errors / success, is that sufficient? I know it states to modify it in main, but adding it in the function gave me a more tailored/detailed explanation of the error. I want to make sure this is okay for grading purposes.</paragraph><paragraph/><paragraph>TYIA!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I put the print function in the verify_grammar function, bringing more explanations for the errors as you said. I didn't realize the question states to modify the main. </paragraph><paragraph/></document>",
            "<document version=\"1.0\"><paragraph>Modifying the verify_grammar function is okay.\u00a0</paragraph></document>"
        ]
    },
    "169": {
        "title": "Part 3 - Data Structure",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph><italic>The first object is <bold>parse table containing backpointers</bold>, represented as a dictionary (this is more convenient in Python than a 2D array).</italic></paragraph><paragraph>Can we use a default dict here instead of a regular dict?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I personally used a defaultdict instead of a regular dict, I don't think that's a problem. If I'm not mistaken the only difference between the two is that the former is initialized to prevent a KeyError. </paragraph><paragraph/></document>",
            "<document version=\"2.0\"><paragraph>I think it will be more convenient to use a default dict, and it has already been imported for us.</paragraph></document>"
        ]
    },
    "170": {
        "title": "Part 2 and Part 3",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Part 3 and Part 2 are basically identical, instead of repeating the code, is it ok to just call self.is_in_language(tokens) in parse_with_backpointers(), if I am already creating and saving the table in part2?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, that\u2019s fine.\u00a0</paragraph></document>"
        ]
    },
    "171": {
        "title": "Part 2",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>If I did part 3 before part 2, can I just use the following code for part 2?</paragraph><pre>    def is_in_language(self,tokens):\n        \"\"\"\n        Membership checking. Parse the input tokens and return True if \n        the sentence is in the language described by the grammar. Otherwise\n        return False\n        \"\"\"\n        # TODO, part 2\n        table, probs = self.parse_with_backpointers(tokens)\n\n        if (0,len(tokens)) not in table or self.grammar.startsymbol not in table[0,len(tokens)]:\n            return False\n        return True\n</pre><paragraph/><paragraph>Thanks in advance.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, this works.</paragraph></document>"
        ]
    },
    "172": {
        "title": "Is it in the range of expectation for my CKY algorithm to have a .82 F1 score",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>I am trying to determine whether I have to debug anything in my code. So far, each of the parts that I have implemented have shown expected behavior based on the examples given in the assignment page. The issue is when I run the evaluate_parser script I get lower than expected scores:<break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/hqauEhDbUKwo2RUlbZZsk4pI\" width=\"658\" height=\"15.543307086614174\"/></figure><paragraph>Are these reasonable values should I spend some time debugging?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You should go back and debug. While your coverage looks good, your average F-scores should be much closer to 0.95 and 0.64.</paragraph></document>"
        ]
    },
    "173": {
        "title": "CKY Algorithm",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor/TAs, </paragraph><paragraph>When we are doing the CKY table, does the order of two terminals matter? </paragraph><paragraph>For example, for the ungraded assignment, if we are looking to see if there is a production rule for (0,2) from (0,1) and (1,1), does the ordering of D (0,1) and N (1,1) matters? In the production rule, we have NP -&gt; D N. Is that equivalent to  NP -&gt; N D? </paragraph><paragraph>Not sure if my question is clear...but If (0,1) was N and (1,1) was D, can we still put NP in (0,2)? </paragraph><paragraph>Thanks!! </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes! The order definitely matters!</paragraph><paragraph>Think about what would happen if that was not the case: You could just swap two constituents and the sentence would still be grammatical. </paragraph></document>"
        ]
    },
    "174": {
        "title": "Additional Office Hours",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi, is it possible to get some office hours during the week via Zoom for CVN students?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I'm holding office hours Tuesdays 3:00-4:00pm (both in person and on Zoom). Link: <link href=\"https://columbiauniversity.zoom.us/j/99416164945?pwd=R2E3TG5YYUhCc25tVUFSN3hKTnpPUT09\">https://columbiauniversity.zoom.us/j/99416164945?pwd=R2E3TG5YYUhCc25tVUFSN3hKTnpPUT09</link></paragraph><paragraph>I'll check with the TAs to see if we can reschedule some. Maybe we can move Shreya's Monday time slot to Zoom. </paragraph></document>"
        ]
    },
    "175": {
        "title": "Part 1 usage of isclose",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I just wanted to confirm if we can import isclose from the math library (since without it we can't use the method), as we're told that:</paragraph><blockquote>You may want to take a look at the <italic>isclose</italic> method in <link href=\"https://docs.python.org/3/library/math.html\">Python's <italic>math</italic> package.Links to an external site.</link></blockquote><paragraph>Thanks.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, you can (and should) import it.</paragraph></document>"
        ]
    },
    "176": {
        "title": "Part 3 - Initialization of table",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph><italic>Terminal symbols in the table could just be represented as strings. For example the table entry for table[(2,3)][\"FLIGHTS\"] should be \"flights\".</italic></paragraph><paragraph>What about the other rules for the token \"flights\"? Specially, ('flights', ) also makes \"NP\" but the table format requires a tuple of length 2 so I am not sure what <code>table[(0,1)]['NP']</code> should be.</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>table[(0,1)]['NP'] should also be \"flights\" in this case. </paragraph></document>"
        ]
    },
    "177": {
        "title": "Can we use Numpy in Part 2: is_in_language?",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>In the initialization section of is_in_language, would it be allowed to use numpy to define the 2-D array for dynamic programming?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, that's fine.</paragraph></document>"
        ]
    },
    "178": {
        "title": "Ungraded Assignment - Viterbi Algorithm Implementation",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I was wondering if this implementation of the Viterbi algorithm is correct, and whether it can be optimized in any way. The output is correct \"Most likely sequence of states: ['N', 'V', 'Prep', 'N']\", but I'm wondering if anything can be done more efficiently:<break/><break/>Thank you in advance!<break/><break/>here is the pseudocode:<break/></paragraph><figure><image src=\"https://static.us.edusercontent.com/files/Tvb9fN74FjIcgx9Qqa90gxWf\" width=\"644\" height=\"258.3276836158192\"/></figure><paragraph><break/>and this is my (slightly different) implementation:<break/></paragraph><pre># Define the hidden states\r\nhidden_states = ['N', 'V', 'Prep']\r\n\r\n# Define the observed states\r\nobs_states = ['time', 'flies', 'like', 'leaves']\r\n\r\n# Transition probabilities\r\ntransition_prob = {\r\n    'Start': {'N': 1/2, 'V': 1/2, 'Prep': 0},\r\n    'N': {'N': 1/4, 'V': 1/2, 'Prep': 1/4},\r\n    'V': {'N': 2/3, 'V': 0, 'Prep': 1/3},\r\n    'Prep': {'N': 1, 'V': 0, 'Prep': 0},\r\n}\r\n\r\n# Emission probabilities\r\nemission_prob = {\r\n    'N': {'time': 2/4, 'flies': 1/4, 'like': 0, 'leaves': 1/4},\r\n    'V': {'time': 1/6, 'flies': 2/6, 'like': 2/6, 'leaves': 1/6},\r\n    'Prep': {'time': 0, 'flies': 0, 'like': 1, 'leaves': 0}\r\n}\r\n\r\n# Initialize dictionaries\r\nviterbi_prob = {state: [0] * len(obs_states) for state in hidden_states}\r\nbackpointer = {state: [None] * len(obs_states) for state in hidden_states}\r\n\r\n# Initialization\r\nstart_state = 'Start'\r\nfor state in hidden_states:\r\n    viterbi_prob[state][0] = transition_prob[start_state][state] * emission_prob[state][obs_states[0]]\r\n    backpointer[state][0] = start_state\r\n\r\n# Recursive step of the Viterbi algorithm\r\nfor t in range(1, len(obs_states)):\r\n    for state in hidden_states:\r\n        max_prob = 0\r\n        max_state = None\r\n        for prev_state in hidden_states:\r\n            prob = viterbi_prob[prev_state][t-1] * transition_prob[prev_state][state] * emission_prob[state][obs_states[t]]\r\n            if prob &gt; max_prob:\r\n                max_prob = prob\r\n                max_state = prev_state\r\n        viterbi_prob[state][t] = max_prob\r\n        backpointer[state][t] = max_state\r\n\r\n# Termination step\r\nmax_final_prob = 0\r\nmax_final_state = None\r\nfor state in hidden_states:\r\n    if viterbi_prob[state][-1] &gt; max_final_prob:\r\n        max_final_prob = viterbi_prob[state][-1]\r\n        max_final_state = state\r\n\r\n# Backtrace to find the most likely sequence of states\r\npath = [max_final_state]\r\nfor t in range(len(obs_states)-1, 0, -1):\r\n    path.insert(0, backpointer[path[0]][t])\r\n\r\nprint('Most likely sequence of states:', path)\r\n\n\n</pre></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>This looks like a correct implementation. I don't see a way to optimize this. Is there a particular part of the algorithm you are concerned about in terms of efficiency?</paragraph></document>"
        ]
    },
    "179": {
        "title": "CKY Algorithm Bounds Clarification",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>For sake of example, lets say our tokens are: ['These', 'are', 'the', 'tokens']</paragraph><paragraph>Clearly, the length of the sentence would be 4 (n = 4)</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/zmwLhmI89GLKQVbfPzVLSYkm\" width=\"438\" height=\"301\"/></figure><paragraph>Within the CKY algo using the psuedo code above, are the bounds on the loops inclusive? In other words, for the initialization step, would <italic>i</italic> take on values of 0,1,2,3 or 0,1,2? Furthermore, in the main loop, would <italic>length</italic> take on values of 2,3,4 or 2,3?</paragraph><paragraph>Same with for k = i+1...j-1. Does k take on values i+1, i+2, ..., j-2, j-1 or i+1, i+2, ..., j-2?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The bounds are inclusive for all of those loops.</paragraph></document>"
        ]
    },
    "180": {
        "title": "Part 1 - CNF format",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>For this instruction in part 1:</paragraph><paragraph><bold>Specifically you need to verify that each rule corresponds to one of the formats permitted in CNF.</bold></paragraph><paragraph>I just want to confirm I am understanding this correctly. We are only verifying that the format of the rule is in CNF but not verifying if the rhs correctly corresponds with the correct nonterminal on the lhs? In general, is formatting the only thing we need to check for in verifying if a PCFG is in CNF? Isn't it possible that the PCFG contains an incorrect rule but is in the proper format? </paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, you only have to verify the format is in CNF and ensure all probabilities for the same lhs symbol sum to 1.0 (approximately).</paragraph></document>",
            "<document version=\"2.0\"><paragraph>What do you mean by \"incorrect rule\"? </paragraph></document>"
        ]
    },
    "181": {
        "title": "Ungraded assignment for CKY",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor/TAs - Could you please upload the solution for the Ungraded Exercise: CKY algorithm?</paragraph><paragraph>Thanks! <break/>Jess </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>They\u2019re available now in the \u201cpages\u201d section.\u00a0</paragraph></document>"
        ]
    },
    "182": {
        "title": "Performance metrics for Part 5 (Evaluating the Parser)",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor/TAs:</paragraph><paragraph>In the assignment it is mentioned the performance metrics after implementation was-</paragraph><paragraph>\"Coverage: 67%, Average F-score (parsed sentences): 0.95, Average F-score (all sentences): 0.64\"</paragraph><paragraph>Is this the expected value for full credit? <break/><break/>Might be a trivial question, but I got very slightly lower value for  Average F-score (all sentences) (~0.639, rounding to same metric value 0.64).  Just wanted to confirm if this was okay.</paragraph><paragraph>Thank you in advance!<break/></paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>0.639 is fine.</paragraph></document>"
        ]
    },
    "183": {
        "title": "Part 3 - Parsing with backpointers: \"math.log2\"",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hi, </paragraph><paragraph>The algorithm for PCFG parsing seems to be multiplying log probabilities for the parents rule with the probabilities of children at each step.</paragraph><paragraph>Is it recommended to use math.log2 with summation for this assignment ( similar to assignment 1) when dealing with probabilities?</paragraph><paragraph>Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes.</paragraph></document>"
        ]
    },
    "184": {
        "title": "Part 4 get_tree function base case",
        "category": "Homework 2",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>While implementing homework 2's part 4 (get_tree), i initially included a base case where </paragraph><paragraph>if (i, j) is not in the \"chart\", I return None which I found to be problematic with evaluate_parser.py's method of counting parsed trees (it uses try except KeyError to count the parsed trees)</paragraph><paragraph>Therefore, should we jut leave this base case (key error) unhandled in part 4?</paragraph><paragraph/><paragraph>Thank you for all the help in advance!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, the key error is handled by the evaluation script, so that sentence would be counted as not-parseable.\u00a0</paragraph></document>"
        ]
    },
    "185": {
        "title": "Additional Coding Challenges",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi everyone,</paragraph><paragraph>Would it be possible to release additional coding/implementation problems for any algorithms we're not exploring in the homework assignments?</paragraph><paragraph>Thank you in advance!</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>Any specific recommendations for what kind of coding/implementation problems - algorithms for ?</paragraph></document>"
        ]
    },
    "186": {
        "title": "Solution for ungraded exercise on CKY algorithm and backpointers.",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Good afternoon,</paragraph><paragraph>Is it possible to upload the solution for the CKY ungraded exercise? It would be useful to test if our understanding is right while working on the next homework.</paragraph><paragraph>Thank you.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph><link href=\"https://courseworks2.columbia.edu/courses/176463/assignments/1074299\">Assignment Question - CKY</link> <break/><link href=\"https://courseworks2.columbia.edu/courses/176463/pages/ungraded-exercise-solution-cky\">Solution Link - CKY</link></paragraph></document>"
        ]
    },
    "187": {
        "title": "Ungraded Exercise Solution: n-Gram Language Models - Part 2",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I am trying to wrap my head around the proof by induction in the solution and am confused by the beginning of the inductive step:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/qSjJrQ63pOIhnazplVFxNvEC\" width=\"659\" height=\"79.29382604776927\"/></figure><paragraph>I thought the inductive hypothesis only tells us for sequences of length n-1 that:</paragraph><math>\\sum_{w_{1:n-1}}^{ }P\\left(w_1...w_{n-1}\\right)\\ =\\sum_{w_{1:n-1}}^{ }P\\left(w_1 \\vert START\\right)\\prod_{i=2}^{n-1}P\\left(w_i\\vert w_{i-1}\\right) = 1.</math><paragraph>Is there a formula being used to make the jump that the left-hand-side is equivalent to </paragraph><math>\\sum_q^{ }P\\left(w_{n-1}=q\\right)?</math><paragraph>Thanks for posting a solution in the first place!</paragraph><paragraph>Thank you,</paragraph><paragraph>Nick</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>$$P(w_{n-1}=q)$$ is the sum of all length n-1 sequences that end in q. Summing over all different q gives us the sum of _all_ length n-1 sequences.\u00a0</paragraph><paragraph>And by the inductive hypothesis the sum of all length n-1 sequences is 1.\u00a0</paragraph><paragraph>Does that make sense?</paragraph></document>"
        ]
    },
    "188": {
        "title": "Friday Office Hours",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi! Will there still be office hours this Friday afternoon? Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, I'm holding them right now. The Zoom link is on the Courseworks page.</paragraph></document>"
        ]
    },
    "189": {
        "title": "Homework assignment",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Going forward, would it be possible to have two weekends to complete the HW rather than the one we currently get (have the assignment due Monday instead of Friday?), even if this means more overlap of assignments. </paragraph><paragraph>I feel it's not uncommon to attend/watch lectures during the week and review/ingest the material on the weekends, especially for those of us working full time. Having only the last weekend makes it a bit difficult time-wise to fully grasp everything from the week that follows and makes incorporating this material into the assignments more challenging. <break/><break/>If there is an administrative/logistical reason that makes this impracticable that is totally understandable. Thank you!</paragraph></document>",
        "comments": [
            "<document version=\"1.0\"><paragraph>I totally agree with the proposal. It would benefit all students, including those working in the summer.\u00a0</paragraph></document>"
        ],
        "answers": [
            "<document version=\"2.0\"><paragraph>I think the class ends on August 10th, with Summer B ending on the 11th. So, it seems like this could be possible for all but the last HW.</paragraph></document>"
        ]
    },
    "190": {
        "title": "Raw Trigram and Bigram Prob",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>It is my understanding that for trigrams like ('START','START','the'), we should use count ('START','START','the') / # of sentences. Should we do this with bigrams like ('START', 'the') as well?</paragraph><paragraph>In other words, should the raw probability of ('START','START','the') be the same as ('START','the')?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes the probability for the trigram and bigram would be the same. However you should use\u00a0</paragraph><paragraph>count('START','START','the') / num_sentences instead of 1 / num_sentences\u00a0</paragraph></document>"
        ]
    },
    "191": {
        "title": "Perplexity testing",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>In the perplexity texts, when we provide the files in order of \"brown_train.txt brown_test.txt\" versus \"brown_test.txt brown_train.txt,\" why do the values differ? How exactly do they relate to each other? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The first argument is the training corpus, the second argument the test corpus. So if you swap them you are training on brown_test.txt (a much smaller dataset) and testing on brown_train.txt.\u00a0</paragraph></document>"
        ]
    },
    "192": {
        "title": "Part 7 Function Signature",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>For part 7, the main function in the skeleton code calls the essay_scoring_experiment as</paragraph><snippet language=\"py\" runnable=\"true\" line-numbers=\"true\"><snippet-file id=\"code\">acc = essay_scoring_experiment('train_high.txt', 'train_low.txt\",\"test_high\", \"test_low\")</snippet-file></snippet><paragraph>However, when I run this function as is, I am getting a file not found error, so I had to change the parameters to their relative paths, as shown below:</paragraph><web-snippet><web-snippet-file language=\"html\">acc = essay_scoring_experiment(\"hw1_data/ets_toefl_data/train_high.txt\", \"hw1_data/ets_toefl_data/train_low.txt\", \"hw1_data/ets_toefl_data/test_high\", \"hw1_data/ets_toefl_data/test_low\")</web-snippet-file><web-snippet-file language=\"css\"/><web-snippet-file language=\"js\"/></web-snippet><paragraph>Is this acceptable, or are we expected to handle the first case with just the txt file names or directory names inside the essay_scoring_experiment function?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yes, feel free to edit the code in the main function as you see fit. We will not run the main to test your code and instead call the individual functions / methods. </paragraph></document>"
        ]
    },
    "193": {
        "title": "raw_bigram_probability and sentence_logprob questions",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>a) In the case where the bigram and trigram has not been seen in the train.txt, should <break/>raw_bigram_probability and raw_trigram_probability <bold>return 0 ?</bold></paragraph><paragraph/><paragraph>b) In sentence_logprob, when the smoothed trigram probability returned is 0, there is a math domain error as 0 is out of domain for log_2. What is the expected fix here?</paragraph><paragraph/><paragraph>c) For trigrams like ('START', 'START', 'the'), is the idea that when we use smoothed_trigram_probability, the probability of this trigram should be count('the')/ Total Words in test set<break/>?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>a) If a bigram has not been seen you should return 0. Similarly for trigram probabilities: if the trigram has not been seen, but the bigram context has been seen, return 0. Otherwise, return 1/|V|, where V is the number of types in the lexicon, including STOP (but not including START). </paragraph><paragraph>b) This should never happen because there are no unseen unigrams (the corpus reader replaces these). Make sure the corpus reader is initialized with the correct lexicon. </paragraph><paragraph>c) No. For ('START','START','the'), that is trigrams at the beginning of the sentence, you should use count(('START','START','the)) / number_of_sentences. <break/></paragraph></document>"
        ]
    },
    "194": {
        "title": "Error: TypeError: 'collections.defaultdict' object is not callable",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>For Part 3, the following line seems to be producing several errors: bigram_count = self.bigramcounts(tuple(trigram[:2]))</paragraph><paragraph>These errors include \"TypeError: 'collections.defaultdict' object is not callable\" or (when I remove the tuple from the above line of code) \"TypeError: unhashable type: 'list'\". Is there a reason that this is producing this particular error, even though I assigning it as a tuple in order to make a slicing call? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>For the latter, dictionary keys need to be hashable types in python, such as a tuple. <break/><break/>For the first error, hard to tell without reading code by looks like you might be trying to get values using dictionary(key) instead of dictionary[key]</paragraph></document>",
            "<document version=\"2.0\"><paragraph>You want to use </paragraph><paragraph>bigram_count = self.bigramcounts[tuple(trigram[:2])]</paragraph><paragraph>careful though, you probably want to use the last two elements of the trigram for the lookup, instead of the first two. If the trigram is ('natural','language','processing'), the bigram should be ('language','processing'). </paragraph></document>"
        ]
    },
    "195": {
        "title": "sentence_logprob",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>To confirm, sentence_logprob is to be computed using smoothed_trigram_probability, right?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I used smoothed_trigram_probability in computing sentence_logprob. </paragraph></document>",
            "<document version=\"2.0\"><paragraph>Correct. </paragraph></document>"
        ]
    },
    "196": {
        "title": "Lemmatization of amusing",
        "category": "General",
        "question": "<document version=\"2.0\"><figure><image src=\"https://static.us.edusercontent.com/files/QJkyeZhC7HPhagrzV8wNYG25\" width=\"658\" height=\"476.9943630214205\"/></figure><paragraph>Should the lemmatized sentence have 'amuse'  for the word amusing?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yeap you are right. The lemmatized version of amusing should be amuse. </paragraph></document>"
        ]
    },
    "197": {
        "title": "What is the instructor-provided entry code for Gradescope?",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I'm sorry if I missed this, but what is the code to add this course on Gradescope?</paragraph><paragraph>Thanks</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/1vLZcvmv3LYqFWZlbEJApKSd\" width=\"658\" height=\"352.37781629116114\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>May I know why you need access to Gradescope yet ?</paragraph><paragraph>As of now all assignment submissions are through Courseworks submission only.</paragraph><paragraph>Prof will let you know later about Gradescope access and use for final exams.</paragraph></document>"
        ]
    },
    "198": {
        "title": "Perplexity",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>When calculating the perplexity using the following equation:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/sGgutv8NLCbnb0APhlE5OiZH\" width=\"334\" height=\"114\"/></figure><paragraph>Should the total number of words tokens (M) also exclude the START token? I would guess that the START and STOP tokens should be excluded since the sum of the log probabilities does not include the probability of sentences with START or STOP tokens.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Also what are the acceptable range for perplexity values? For both texts I am getting very small values in the (7-15) range while the HW description says that they must be lower than 400. Training is less than test as expected but because the range is very small, their difference isn't \"a lot\" as the hw assignment says.</paragraph></document>",
            "<document version=\"2.0\"><paragraph>the M in the perplexity calculations is the number of predictions the model makes on the test set. So it includes all tokens, plus the STOP tokens (one per sentence), but no START tokens. </paragraph><paragraph/></document>"
        ]
    },
    "199": {
        "title": "Very High Perplexity Values",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>I am getting extremely high perplexity values and having trouble pinpointing the reason why. I'm trying to figure out if there is an error with how I am calculating the log probabilities of a sentence, or if there is an issue with how I am calculating the perplexity value itself.  Some information about the log probabilities of the Brown test data when using the training model is pasted below:</paragraph><paragraph>Range of Sentence Log Probabilities: (-1020.8787566532884 -12.231044294648944)</paragraph><paragraph>Mode of Sentence Log Probabilities: (-15.291353424804461) </paragraph><paragraph>Mean of Sentence Log Probabilities: (-128.44629381836992)</paragraph><paragraph><bold>Are these reasonable numbers for sentence log probabilities?</bold></paragraph><paragraph/><paragraph>As for the perplexity, </paragraph><math>2^{-l}</math><paragraph>where,</paragraph><math>l=\\frac{1}{M}\\sum_{i=1}^m\\log_2P\\left(s_i\\right)</math><paragraph><bold>Does M = total number of word tokens (including duplicates) or unique word tokens (lexicon)?</bold></paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>See #48  </paragraph><paragraph/></document>"
        ]
    },
    "200": {
        "title": "Iterating through generator sometimes gets strings instead of lists?",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>During pt 2 I noticed that iterating through the generator corpus passed in sometimes yields a string instead of a list (the example I found was the single character \"d\"). I ended up using a type conversion to make sure the sentence being passed into get_ngrams() was a list. I'm just wondering if this is expected behavior + if what I'm doing is acceptable</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Check if you are passing the arguments correctly, ideally it should be consistent datatype.</paragraph></document>"
        ]
    },
    "201": {
        "title": "Bigram START, START",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>The professor mentioned that we could store START, START in the bigram dictionary to help calculate probability for trigrams like (\u2018START\u2019, \u2018START\u2019, \u2018the\u2019), but my understanding was that the only reason we were storing multiple STARTs was out of necessity in (3+n)-gram where we need the first word in the sentence, in this case \"the\", without any other tokens, so that we can reduce the likelihood that the sequence was not seen in the corpus.</paragraph><paragraph>For example, should get_ngrams([\"natural\", \"language\", \"processing\"], 4) contain:<break/>[<bold>('START', 'START', 'START', 'START')</bold>, ('START', 'START', 'START', 'natural'), ('START', 'START', 'natural', 'language'), ('START', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'STOP')]</paragraph><paragraph>Is this understanding correct, and storing the START,START bigram is out of necessity, or is this generally best practice? Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>get_ngrams([\"natural\", \"language\", \"processing\"], 4) should contain:<break/>[('START', 'START', 'START', 'natural'), ('START', 'START', 'natural', 'language'), ('START', 'natural', 'language', 'processing'), ('natural', 'language', 'processing', 'STOP')]<break/>This is consistent with the ngram example in part 1.\u00a0</paragraph><paragraph>Instead of storing a bigram count for (\u2018START\u2019, \u2018START\u2019) I recommend keeping track of the number of sentences in the training data using a separate instance variable.\u00a0<break/>Then, when computing trigram probabilities, you need to treat (\u2018START\u2019, \u2018START\u2019, x) as a special case.\u00a0</paragraph></document>"
        ]
    },
    "202": {
        "title": "Lexicon size",
        "category": "General",
        "question": "<document version=\"2.0\"><pre>generator = corpus_reader(corpusfile)\nself.lexicon = get_lexicon(generator)\nself.lexicon_size = float(len(self.lexicon)+1)\nself.lexicon.add(\"UNK\")\nself.lexicon.add(\"START\")\nself.lexicon.add(\"STOP\")\n</pre><paragraph>Should we be counting UNK, START and STOP as part of the lexicon size or only STOP?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Depends on what you use the lexicon size for.\u00a0</paragraph><paragraph>If you are computing a uniform distribution about possible tokens, you would want it to include UNK and STOP, but not START.\u00a0</paragraph></document>"
        ]
    },
    "203": {
        "title": "Part 3 - Raw probability and handling unknown tokens",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>For the unigram probability isn't the total number of words simply the total number of words in the model's lexicon? If so, I assume we can simply take the len of the lexicon as |V|?<break/><break/>And in general, in the raw probabilities functions: when the count of specific trigram/bigram/unigram is 0, should we try first to estimate the count again by replacing some words in the tuple with unknown toten? <break/><break/>Thanks in advance!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>For your first question: you need to count the number of word tokens (including duplicates). </paragraph><paragraph>I was initially thinking about the case in which the context of a trigram is unseen. In that case you should use 1/V, where V contains all types in the lexicon. you can use len(lexicon) but you need to add 1 for the STOP symbol.\u00a0</paragraph><paragraph>For the second question: the corpus reader already takes care of replacing unknown tokens with UNK. You can therefore assume that all tokens have been seen. \u00a0There should be no 0 unigram probabilities.\u00a0</paragraph></document>"
        ]
    },
    "204": {
        "title": "Are we allowed to add new functions to make the code more readable",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Good afternoon,</paragraph><paragraph>I wanted to check if we are allowed to add new functions for the HW, without changing the function definitions/signatures of the existing ones?</paragraph><paragraph>Thank you</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes, that's fine.</paragraph></document>"
        ]
    },
    "205": {
        "title": "Part 3 [start, start, ] in tri-probability",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I am wondering if we need to deal with the situation when there is [START, START, ...] in the raw_trigram_probability function. </paragraph><paragraph>As the double start bigramcounts =0 in this case, should I use 1/lexion or the same probability of [START, ...] in bi-probability?</paragraph><paragraph/><paragraph/><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Yes you will have to address trigrams like (\u2018START\u2019, \u2018START\u2019, \u2018the\u2019).\u00a0</paragraph><paragraph>You will need the count for (\u2018START\u2019,\u2019START\u2019) for the denominator. You could either store this in the bigram dictionary (but be careful, because you don\u2019t want a bigram probability for P(START|START) ), or you could add an instance variable to count the number of sentences in the training data.\u00a0</paragraph></document>"
        ]
    },
    "206": {
        "title": "Professor Bauer office hours",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Hi! How can we access Prof. Bauer's OH? Do we use the class Zoom link or do we need to make an appointment? </paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I found class zoom link was recording but no one there~</paragraph></document>",
            "<document version=\"2.0\"><paragraph>Sorry there was a scheduling issue today ... I will hold make-up office hours tomorrow. I will send an email.  </paragraph></document>"
        ]
    },
    "207": {
        "title": "TA Office Hours During Week",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>Is it possible to get some TA office hours during the week via Zoom?</paragraph><paragraph>For the office hours to be helpful (as it currently sits) we would need to know our questions nearly a week before the homework will be due.</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>I'll. bring this up during our TA meeting tonight. We should be able to move some office hours earlier in the week. </paragraph></document>"
        ]
    },
    "208": {
        "title": "Effects of using log2 and log on perplexity",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hi everyone!</paragraph><paragraph>I'm working on optimizing the perplexity, and had a question about using log vs log2.<break/><break/>When using math.log within the sentence_logprob and perplexity functions, this is the output:</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/wUxErPq5NjtaalUPNFhpWN17\" width=\"658\" height=\"82.25\"/></figure><paragraph>However, when using math.log2, this is the output (which is a considerable increase):</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/5jSe85mnBCEIe1giZSp0pO10\" width=\"658\" height=\"74.61855670103094\"/></figure><paragraph>If I understood it correctly, using the logarithm allows us to check the trends, so are log and log2  interchangeable?</paragraph><paragraph>Also, are these generally acceptable values?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>The perplexity calculation requires that you use log2 (otherwise you have to change the base of the exponent in the calculation as well. The convention is to use log2. </paragraph><paragraph>The perplexity result you get with log2 is rather high. Did you make sure to use the number of sentences for lowercase-m and the number of tokens (including STOP but excluding START) for uppercase-M? </paragraph></document>"
        ]
    },
    "209": {
        "title": "STOP or END?",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hi all,</paragraph><paragraph>In the lecture, I remember discussing START and END. However in the HW1, it seems to be START and STOP. Should we use END or STOP in that case for this homework, and/or they the same thing?</paragraph><paragraph/><paragraph>Thanks,</paragraph><paragraph>Vidur</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>They are the same thing -- either is fine as long as you use it consistently. </paragraph></document>"
        ]
    },
    "210": {
        "title": "Parameter Clarification Part 5",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><pre>def sentence_logprob(self, sentence):\n</pre><paragraph>Is sentence a list of strings? Or is it a single string that we need to split? There is no specification about the format of this paramter</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>It's a list of strings. </paragraph></document>"
        ]
    },
    "211": {
        "title": "ngrams to probability notation",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Given the following trigram: ('natural', 'language', 'processing')</paragraph><paragraph>If we were calculating the raw probability for this trigram, how would this convert to probability notation? Would it be: </paragraph><paragraph>P('processing' | 'natural', 'language') = </paragraph><paragraph>count(('natural', 'language', 'processing')) / count(('natural', 'language'))</paragraph><paragraph>or something different?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Your calculation is correct. </paragraph></document>"
        ]
    },
    "212": {
        "title": "Part 3 - Undefined cases",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph><italic>My recommendation is to make P(v | u,w) = 1 / |V| (where |V| is the size of the lexicon)</italic></paragraph><paragraph><italic>Another option would be to use the unigram probability for v, so P(v | u,w) = P(v).</italic> </paragraph><paragraph>These are the recommendations given in part 3 for calculating raw probabilities when either the numerator and denominator or the denominator are 0</paragraph><paragraph>Is there a difference in accuracy / performance? Is one typically favored in real-world scenarios over the other? Or does it depend on a case-to-case basis?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>That's not quite right. If the <underline>numerator</underline> is 0, the raw probability should be 0. If the <underline>denominator</underline> is 0 then you would have to use one of the approaches described above. </paragraph><paragraph>In principle, using the unigram probability should give you a better estimate. But on this particular task, empirically, setting the probability to 1/|V| works better. </paragraph></document>"
        ]
    },
    "213": {
        "title": "Part 7",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>I am a bit confused about how to make predictions. We have two trigram models, and we can calculate the perplexities of each essay with these two models. Do we compare these two perplexities of each essay and choose the smaller one as the prediction? </paragraph><paragraph>Also, do we need to include anything in the main code for the homework? I was able to use it for testing. Do we need to keep those testing codes?</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>\u201cDo we compare these two perplexities of each essay and choose the smaller one as the prediction?\u201d\u00a0<break/>Yes, that\u2019s the correct approach.\u00a0</paragraph><paragraph>You do not have to put anything specific in the main portion of the code. We will call the individual methods for testing.\u00a0</paragraph></document>"
        ]
    },
    "214": {
        "title": "Testing part 2",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>I'm confused as to how can I test my part 2. Would appreciate any hints.</paragraph><paragraph>Thank you!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>There are some sample inputs and outputs you can try with your model and see if the counts match.</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/okDSztbhkwsIIa68xO8y04j8\" width=\"344\" height=\"119\"/></figure><paragraph/></document>"
        ]
    },
    "215": {
        "title": "Additive smoothing inaccurate because of Zipf's law",
        "category": "Lectures",
        "question": "<document version=\"2.0\"><paragraph>Hi Professor/TAs,<break/><break/>I was rewatching the recording where professor mentions about additive smoothing not always being accurate because of Zipf's distribution.<break/><break/>Professor mentions that it would overestimate the probability for unseen words (2hr 34min) , but doesn't the probability depend on count of words, so shouldn't the probability of seen words be higher. <break/>It would be helpful if you could correct me here and how this relates to Zipf's distribution.<break/><break/>Thank you.</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Here is my understanding. Not sure if it's correct. In the testing data, we might encounter tokens not observed in the training data. These 'unseen' tokens could lead to a 'divide by 0' error. Additive smoothing is applied to solve the issue by assigning a probability of 1/V to unseen words (the count of unseen words is zero). However, the actual probability of unseen words in real-life is likely much lower than 1/V because of Zipf's distribution.</paragraph><paragraph/></document>",
            "<document version=\"2.0\"><paragraph>The way I thought of it is like inflation. Suppose you print some money and give everyone $1. Then the people who had more money to start with still have more money, but the value of a dollar decreases, so the people with $0 to start with have had their purchasing power disproportionately increased.</paragraph><paragraph/></document>",
            "<document version=\"2.0\"><paragraph>So one question is what \"accurate\" means here. Additive smoothing gives you a probability distribution, in the sense that all event probabilities will sum to 1.0. </paragraph><paragraph>We don't know what the \"real\" probability for unseen events is -- if there even is such a thing. We assume these probabilities are not 0 and that, instead, we simply have not seen evidence for these events because the training data is too small. So we use smoothing to come up with an estimate. </paragraph><paragraph>The main issue with additive smoothing is that it treats all unseen events the same and it redistributes too much probability mass from seen events, especially if you have a limited amount of training data. </paragraph><paragraph>Assume you have seen the trigram \"like eating pizza\" once, but your test data contains the trigram \"like eating escargot\". Add-one smoothing would make \"escargot\" in this context half as likely as \"pizza\", when you would really expect it to be much more rare. </paragraph><paragraph/></document>"
        ]
    },
    "216": {
        "title": "Part 3 raw probability",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Does \"START\" and \"STOP\" count towards the total number of words?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>The \"STOP\" / END token is counted in total number of words [ V ] as we need to calculate the probability of END being the next token given previous in order to know when a sentence has ended. The \"START\" token is never predicted hence should be excluded.</paragraph><paragraph>Does that help answer your question ?</paragraph></document>"
        ]
    },
    "217": {
        "title": "Groupme chat",
        "category": "Social",
        "question": "<document version=\"1.0\"><paragraph>Hi everyone :)</paragraph><paragraph>I created a Groupme chat for this class. Feel free to join!<break/>https://groupme.com/join_group/95010380/7yt7hjn1</paragraph></document>",
        "comments": [],
        "answers": []
    },
    "218": {
        "title": "Part 3",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Does this imply that in situations where count(u, w, v) is 0 but count(u, w) is greater than 0, or count(u, w) is 0 but count(u) is greater than 0, we still assume a uniform distribution? </paragraph><paragraph>Otherwise, in these two cases, the raw probability is 0. And we just leave it as zero?</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/D827XHzq8aLHFOs6JyFGgv4z\" width=\"658\" height=\"48.451306413301666\"/></figure><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Hi,</paragraph><paragraph>When it's an unseen trigram (count(u, w, v)=0) but a seen bigram (count(u, w) is greater than 0), then you would return the raw probability as 0. </paragraph><paragraph>For this case also - <italic>count(u, w) is 0 but count(u) is greater than 0</italic>, raw probability is 0</paragraph><paragraph/></document>"
        ]
    },
    "219": {
        "title": "Part 5 Handle Unknown Tokens in Sentence?",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>I have a question about part 5 of homework 1. Specifically, we created a lexicon with UNK tokens within the trigram model class. Then when we run sentence_logprob, do we need to process the passed sentence to replace unseen tokens as \"UNK\"? It seems if we do not do this, we would encounter log(0) math domain issue?</paragraph><paragraph/><paragraph>Thank you for all the help in advance!</paragraph><paragraph/><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The corpus reader will already replace the unseen tokens for you (that\u2019s why you pass the lexicon parameter). So you won\u2019t have to worry about this issue. Effectively, all tokens you encounter will have been seen.\u00a0</paragraph></document>"
        ]
    },
    "220": {
        "title": "Optional part",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>For the optional part, will there be extra credits if we do it? And is it okay to leave it blank if we choose not to do it? Thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/BlXnD8JMaEPBF6HItgdIeAqg\" width=\"658\" height=\"117.14682080924855\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You can leave it blank and still get full credit on homework, but it\u2019s interesting to see what the output looks like. There is no extra credit.\u00a0</paragraph></document>"
        ]
    },
    "221": {
        "title": "Part6",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hello, I'm wondering if we need to consider and count \"START\" and \"STOP\" as tokens, or only the tokens existing in the corpus itself. Thank you!</paragraph><figure><image src=\"https://static.us.edusercontent.com/files/2q61Vo9Btitfn5HDijXbz35t\" width=\"658\" height=\"206.9579831932773\"/></figure></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>Include the number of STOP tokens. However, exclude the number of START tokens because the model will never make a prediction for START.</paragraph></document>"
        ]
    },
    "222": {
        "title": "Part 3 - Raw n-gram probabilities",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I am a bit confused as to what the inputs are representing for these three functions we have to implement. </paragraph><paragraph>What are trigram, bigram, and unigram respectively? Are they a specific instance of a trigram/bigram/unigram? (i.e. something like ('START', 'I', 'want') Or are they lists of trigrams/bigrams/unigrams? (i.e. something [('START', 'I'), ('I, 'want'), ...])</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>The input is a specific trigram/bigram/unigram.</paragraph></document>"
        ]
    },
    "223": {
        "title": "End of N-Gram Generation",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>In the example given for a trigram: </paragraph><pre>&gt;&gt;&gt; get_ngrams([\"natural\",\"language\",\"processing\"],3)\n[('START', 'START', 'natural'), ('START', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'STOP')]</pre><paragraph>Why does the list of tuples returned end with ('language', 'processing', 'STOP') instead of ('processing', 'STOP', 'STOP')?</paragraph><paragraph>Would that be redundant? i.e. once an n-gram is generated with STOP, there would be no need to include anything else afterwards?</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>Yeah, it would be redundant since we know the sentence has ended. See 2:07:00 in the 7/7 lecture.</paragraph></document>"
        ]
    },
    "224": {
        "title": "can we add instance variables in init",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph/><paragraph>part three says that we may use an instance variable to track the total words. Can we compute this in any earlier method or should we compute that variable in that step?</paragraph><paragraph/><paragraph>thanks</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>To track the total words, you may define the instance variable in TrigramModel class initialization <code>__init__</code>  function and then reference it as needed in different parts.</paragraph><paragraph>Does that help answer your question?</paragraph></document>"
        ]
    },
    "225": {
        "title": "Use of libraries",
        "category": "Homework 1",
        "question": "<document version=\"2.0\"><paragraph>Hello,</paragraph><paragraph>Question for homework in general: are we allowed to import libraries beyond what is given to solve the questions? Thanks!</paragraph></document>",
        "comments": [],
        "answers": [
            "<document version=\"1.0\"><paragraph>You shouldn\u2019t import any libraries not specifically mentioned without permission.\u00a0</paragraph></document>"
        ]
    },
    "226": {
        "title": "Zoom Link for Today",
        "category": "General",
        "question": "<document version=\"2.0\"><paragraph>I'm unable to find the Zoom link for today's lecture in the ZOom Class Sessions Tab. The next one shown is for tomorrow. Instead, I'm watching live in the Video Recordings</paragraph><paragraph/></document>",
        "comments": [],
        "answers": [
            "<document version=\"2.0\"><paragraph>That's fine -- I think it's always the same zoom link. </paragraph><paragraph><link href=\"https://columbiauniversity.zoom.us/j/99539825258?pwd=a2pETFdJVjhocGpXVEZwVmljc2lBZz09\">https://columbiauniversity.zoom.us/j/99539825258?pwd=a2pETFdJVjhocGpXVEZwVmljc2lBZz09</link></paragraph><paragraph/></document>"
        ]
    }
}